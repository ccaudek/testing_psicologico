[
  {
    "objectID": "chapters/measurement/01_scores_scales.html",
    "href": "chapters/measurement/01_scores_scales.html",
    "title": "2  Punteggi e scale",
    "section": "",
    "text": "2.1 Introduzione\nQuesto capitolo si propone di introdurre l’utilizzo del software “R”, ponendo l’attenzione sulla differenza tra valutazioni normative e criteriali.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#tipologie-di-dati",
    "href": "chapters/measurement/01_scores_scales.html#tipologie-di-dati",
    "title": "2  Punteggi e scale",
    "section": "2.2 Tipologie di Dati",
    "text": "2.2 Tipologie di Dati\nSi possono identificare quattro principali categorie di dati: nominali, ordinali, di intervallo e di rapporto. È opportuno notare che, in funzione dell’utilizzo della variabile, i dati possono rientrare in più di una categoria. La tipologia del dato influisce significativamente sulle modalità di analisi applicabili. A titolo esemplificativo, l’analisi statistica parametrica (come la regressione lineare) presuppone che i dati siano di intervallo o di rapporto.\n\n2.2.1 Dati Nominali\nI dati nominali si configurano come categorie distinte, caratterizzate da natura categorica e prive di ordinamento. Tali dati non esprimono affermazioni di natura quantitativa, bensì rappresentano entità nominabili (ad esempio, “felino” e “canino”). Sebbene possano essere rappresentati numericamente, come nel caso dei codici postali o dei codici identificativi di genere, etnia o razza dei partecipanti, è fondamentale sottolineare che valori numerici più elevati non riflettono livelli superiori (o inferiori) del costrutto, in quanto i numeri rappresentano meramente categorie prive di ordine intrinseco.\n\n\n2.2.2 Dati Ordinali\nI dati ordinali si distinguono per essere categorie ordinate: possiedono una denominazione e un ordine. Non forniscono informazioni sulla distanza concettuale tra i ranghi, ma indicano esclusivamente che valori più elevati rappresentano livelli superiori (o inferiori) del costrutto. Un esempio paradigmatico è costituito dalle posizioni in classifica successive a una competizione: il concorrente classificato al primo posto ha concluso la gara prima del secondo classificato, il quale a sua volta ha preceduto il terzo (1 &gt; 2 &gt; 3 &gt; 4). È cruciale evidenziare che la distanza concettuale tra numeri adiacenti non è necessariamente equivalente.\n\n\n2.2.3 Dati di Intervallo\nI dati di intervallo sono caratterizzati da un ordine e da distanze significative (ovvero, intervalli equidistanti). Questi dati consentono operazioni di somma (ad esempio, 2 dista 2 unità da 4), ma non di moltiplicazione (\\(2 \\times 2 \\ne 4\\)). Esempi emblematici sono le temperature espresse in gradi Fahrenheit o Celsius: 100 gradi Fahrenheit non equivalgono al doppio di 50 gradi Fahrenheit. È importante sottolineare che, sebbene in psicologia molti dati presentino la medesima distanza matematica tra gli intervalli, è probabile che tali intervalli non rappresentino la medesima distanza concettuale.\n\n\n2.2.4 Dati di Rapporto\nI dati di rapporto si distinguono per essere ordinati, caratterizzati da distanze significative e da uno zero assoluto che rappresenta l’assenza del costrutto. In questa tipologia di dati, le relazioni moltiplicative risultano valide. Un esempio paradigmatico è la temperatura espressa in gradi Kelvin: 100 gradi Kelvin corrispondono effettivamente al doppio di 50 gradi Kelvin. Nel campo della psicologia, l’aspirazione a disporre di scale di rapporto persiste, nonostante la difficoltà di definire uno zero assoluto per i costrutti psicologici: come si potrebbe, infatti, concettualizzare l’assenza totale di depressione?",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#punteggi-grezzi-e-trasformati",
    "href": "chapters/measurement/01_scores_scales.html#punteggi-grezzi-e-trasformati",
    "title": "2  Punteggi e scale",
    "section": "2.3 Punteggi Grezzi e Trasformati",
    "text": "2.3 Punteggi Grezzi e Trasformati\nNell’ambito dei test psicometrici, il punteggio grezzo costituisce la valutazione più immediata e si basa sulla somma delle risposte categorizzate, come quelle corrette o errate, o vero o falso. Nonostante la sua immediatezza, il punteggio grezzo presenta limitazioni interpretative, poiché non considera fattori contestuali quali il numero totale di domande o il livello di difficoltà di queste.\nPer mitigare queste limitazioni, i punteggi grezzi vengono spesso convertiti in formati che permettono un’interpretazione più contestualizzata, quali i punteggi standardizzati o scalati. Queste trasformazioni facilitano l’interpretazione dei risultati ottenuti.\nL’interpretazione dei risultati dei test necessita di un riferimento comparativo. A seconda del contesto, può essere utile confrontare le prestazioni con una norma di riferimento o con criteri specifici.\nLe interpretazioni basate sulla norma confrontano la performance di un individuo con quella di un gruppo di riferimento o normativo, offrendo una valutazione relativa alla prestazione tipica o “normale”. Un esempio è rappresentato dai test di intelligenza. Al contrario, le interpretazioni basate sul criterio valutano le prestazioni rispetto a un livello di competenza specifico, indipendentemente dalla performance altrui.\nUn altro approccio interpretativo è offerto dalla Teoria della Risposta agli Item (IRT), che fornisce un’analisi avanzata delle prestazioni nei test, permettendo un’esplorazione dettagliata delle risposte individuali.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#interpretazioni-basate-sulla-norma-norm-referenced",
    "href": "chapters/measurement/01_scores_scales.html#interpretazioni-basate-sulla-norma-norm-referenced",
    "title": "2  Punteggi e scale",
    "section": "2.4 Interpretazioni Basate sulla Norma (Norm-Referenced)",
    "text": "2.4 Interpretazioni Basate sulla Norma (Norm-Referenced)\nPer valutare la performance in un test psicologico, può essere utile confrontarla con quella di un gruppo predefinito. I punteggi grezzi acquisiscono significato quando messi a confronto con le prestazioni di un gruppo normativo. In questo contesto, i punteggi grezzi vengono trasformati in punteggi derivati basati sulle performance di un gruppo normativo specifico.\nUn aspetto cruciale in queste interpretazioni è la pertinenza del gruppo di riferimento. È fondamentale che questo gruppo sia rappresentativo degli individui ai quali il test è destinato o con cui il partecipante viene confrontato.\nLa selezione del campione normativo, chiamato anche campione di standardizzazione, segue il principio del campionamento casuale stratificato proporzionale, assicurando che il campione rifletta proporzionalmente le caratteristiche demografiche nazionali. Tale rappresentatività è vitale per l’interpretazione basata sulla norma, rendendo necessaria l’accurata selezione e descrizione del campione da parte degli sviluppatori del test.\nQuando si utilizzano questi test, è cruciale valutare se il campione di standardizzazione è rappresentativo per l’uso previsto e se le caratteristiche demografiche del campione corrispondono a quelle dei soggetti testati. La pertinenza e l’attualità del campione, insieme alla sua dimensione, sono fattori chiave per garantire interpretazioni valide e affidabili.\nUna considerazione finale riguardante le interpretazioni basate sulla norma è l’importanza della standardizzazione nella somministrazione. È fondamentale che il campione di riferimento venga sottoposto al test nelle stesse condizioni e secondo le stesse procedure amministrative che saranno utilizzate nella pratica effettiva. Di conseguenza, quando il test viene somministrato in contesti clinici, è cruciale che l’utente del test segua attentamente le procedure amministrative prescritte. Ad esempio, nel caso di test standardizzati, è essenziale leggere le istruzioni testuali esattamente come sono fornite e rispettare rigorosamente i limiti di tempo. Sarebbe irragionevole confrontare la performance dell’esaminando in un test a tempo con quella di un campione di standardizzazione che ha avuto più o meno tempo per completare gli item. Questa necessità di seguire procedure standardizzate si applica a tutti i test standardizzati, sia quelli con interpretazioni basate sulla norma che quelli basati sul criterio.\n\n2.4.1 Punteggi Derivati\nIn ambito psicometrico, i punteggi derivati da test possono assumere diverse forme, ciascuna con implicazioni specifiche per l’interpretazione dei dati. Esploreremo le tipologie più comuni:\n\nPunteggi Standardizzati:\n\nQuesti punteggi trasformano i punteggi grezzi (ad esempio, il numero di risposte corrette) in misure standardizzate. Ciò permette di ottenere valori invarianti rispetto a variabili come l’età dell’individuo.\nSi calcolano stabilendo una media e una deviazione standard specifiche a priori.\nEsempi:\n\nz-scores: Misurano la distanza di un punteggio dalla media, espressa in deviazioni standard. Hanno una media di 0 e una deviazione standard di 1.\nT-scores: Trasformano i punteggi in valori positivi, con una media di 50 e una deviazione standard di 10.\nPunteggi di QI: Tipici delle scale di intelligenza, hanno una media di 100 e una deviazione standard di 15.\n\n\nPunteggi Standardizzati Normalizzati:\n\nQuando i punteggi originali non seguono una distribuzione normale, si utilizzano trasformazioni non lineari per normalizzarli.\nEsempi:\n\nStanine: Suddividono i punteggi in 9 categorie (da 1 a 9).\nPunteggi scalati di Wechsler: Utilizzati nei test di intelligenza di Wechsler.\nEquivalenti della Curva Normale (NCE): Esprimono la posizione di un punteggio rispetto alla distribuzione normale.\n\n\nRanghi Percentili:\n\nVanno da 1 a 99 e indicano la posizione relativa di un soggetto rispetto alla popolazione.\nAd esempio, un punteggio al 75° percentile significa che il soggetto ha ottenuto un risultato migliore del 75% della popolazione.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#interpretazioni-basate-su-criteri",
    "href": "chapters/measurement/01_scores_scales.html#interpretazioni-basate-su-criteri",
    "title": "2  Punteggi e scale",
    "section": "2.5 Interpretazioni Basate su Criteri",
    "text": "2.5 Interpretazioni Basate su Criteri\nL’approccio delle valutazioni basate su criteri specifici è diventato sempre più rilevante nel mondo dell’educazione e della psicometria a partire dagli anni Sessanta. Questo approccio, noto anche come valutazione basata su contenuti, dominio o obiettivi, si concentra sulla misurazione delle competenze individuali rispetto a standard definiti, piuttosto che sul confronto con le prestazioni di un gruppo di riferimento.\nEcco alcune metodologie e applicazioni comuni:\n\nPercentuale di Risposte Corrette:\n\nQuesto metodo fornisce un’indicazione diretta delle competenze di uno studente.\nAd esempio, se uno studente risponde correttamente all’85% delle domande di matematica, l’insegnante può valutare le sue abilità in modo specifico.\n\nTest di Padronanza:\n\nQuesti test determinano se uno studente ha acquisito una competenza specifica.\nAd esempio, gli esami per la patente di guida valutano se lo studente ha raggiunto il livello di padronanza richiesto.\n\nValutazioni Basate su Standard:\n\nQueste valutazioni classificano i risultati in categorie di prestazione (ad esempio, base, competente, avanzato).\nSpesso, i punteggi vengono correlati a voti letterali basati su una percentuale di correttezza.\n\n\nI punti di forza delle valutazioni basate su criteri includono:\n\nComparazione con Standard Predefiniti:\n\nValutano il raggiungimento di competenze o obiettivi specifici, indipendentemente dalle prestazioni altrui.\nQuesto approccio evita il bias derivante dal confronto con altri studenti.\n\nFocalizzazione su Competenze Specifiche:\n\nQuesti test richiedono una definizione precisa dell’area di conoscenza o abilità valutata.\nSono ideali per valutare aree di contenuto specifiche.\n\n\n\n2.5.0.1 Benefici\n\nValutazione Mirata delle Competenze: Fornisce una verifica concreta del conseguimento delle conoscenze e abilità delineate dal programma di studi.\nPersonalizzazione dell’Insegnamento: Identifica le aree di debolezza, consentendo un approccio didattico più focalizzato e personalizzato.\n\nIn conclusione, le valutazioni basate su criteri rappresentano un’alternativa preziosa ai metodi di valutazione tradizionali, specialmente in contesti in cui è fondamentale misurare le competenze individuali. Questo approccio è in crescente adozione in ambiti educativi e formativi, enfatizzando l’importanza dell’acquisizione di conoscenze e abilità mirate.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#analisi-comparativa-tra-valutazioni-normative-e-basate-su-criteri",
    "href": "chapters/measurement/01_scores_scales.html#analisi-comparativa-tra-valutazioni-normative-e-basate-su-criteri",
    "title": "2  Punteggi e scale",
    "section": "2.6 Analisi Comparativa tra Valutazioni Normative e Basate su Criteri",
    "text": "2.6 Analisi Comparativa tra Valutazioni Normative e Basate su Criteri\nLa distinzione tra valutazioni normative (norm-referenced) e basate su criteri (criterion-referenced) è fondamentale per interpretare le prestazioni individuali nei test. Sebbene un test possa teoricamente adottare entrambi gli approcci interpretativi, di solito si orienta verso uno dei due, a seconda dell’obiettivo specifico.\nEcco una panoramica delle differenze:\n\nValutazioni Normative:\n\nVersatilità: Si applicano a test che valutano una vasta gamma di dimensioni, come attitudini, risultati scolastici, interessi, atteggiamenti e comportamenti.\nAmpio Quadro: Ideali per esplorare costrutti generali come l’attitudine generale o l’intelligenza.\nSelezione delle Domande: Preferiscono domande di difficoltà intermedia, evitando quelle troppo semplici o complesse.\n\nValutazioni Basate su Criteri:\n\nSpecificità: Associate principalmente a test che mirano a valutare conoscenze o competenze specifiche.\nFocalizzazione: Concentrate su abilità e competenze ben definite.\nCalibrazione delle Domande: La difficoltà delle domande è tarata in base alle conoscenze o abilità specifiche da valutare.\n\n\nÈ importante notare che queste interpretazioni non sono mutuamente esclusive. Alcuni test offrono sia valutazioni normative che basate su criteri, fornendo una visione completa delle prestazioni relative rispetto a un gruppo di riferimento e del livello di competenza in un ambito specifico. Questa dualità interpretativa è preziosa in vari contesti.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#analisi-dei-punteggi-secondo-la-teoria-della-risposta-agli-item",
    "href": "chapters/measurement/01_scores_scales.html#analisi-dei-punteggi-secondo-la-teoria-della-risposta-agli-item",
    "title": "2  Punteggi e scale",
    "section": "2.7 Analisi dei Punteggi secondo la Teoria della Risposta agli Item",
    "text": "2.7 Analisi dei Punteggi secondo la Teoria della Risposta agli Item\nLa Teoria della Risposta agli Item (IRT) rappresenta un notevole avanzamento nel campo della psicometria, fornendo strumenti essenziali per valutare con precisione le capacità e i tratti latenti degli individui.\nFondamenti e Principi dell’IRT: L’IRT si basa sull’assunto che ogni persona possieda un livello di un tratto latente, come l’intelligenza, che è indipendente dalle specifiche domande del test o dal metodo di valutazione utilizzato. Attraverso l’applicazione di modelli matematici complessi, l’IRT consente di posizionare ogni individuo su un continuum di tratto latente, offrendo una misurazione delle capacità più precisa rispetto ai tradizionali punteggi grezzi.\nVantaggi dei Punteggi basati sull’IRT: I punteggi derivati dall’IRT presentano significativi vantaggi. Essi sono trattati come punteggi a intervalli costanti, consentendo comparazioni valide tra le performance di soggetti o gruppi diversi. Inoltre, questi punteggi mantengono una deviazione standard uniforme attraverso diverse fasce d’età, rendendoli particolarmente adatti per monitorare l’evoluzione o il progresso delle abilità nel tempo.\nApplicazioni Pratiche e Prospettive Future dell’IRT: Una delle applicazioni più innovative dell’IRT è lo sviluppo dei test adattivi computerizzati (CAT), in cui le domande vengono selezionate dinamicamente in base alle risposte precedenti del candidato. Questo metodo consente valutazioni precise ed efficienti delle abilità in tempo reale. Ad esempio, i punteggi IRT, come i W-scores nel Woodcock-Johnson IV, vengono utilizzati per analizzare variazioni nelle capacità cognitive legate ai processi di apprendimento o ai declini cognitivi.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#la-selezione-del-punteggio-appropriato-per-la-valutazione",
    "href": "chapters/measurement/01_scores_scales.html#la-selezione-del-punteggio-appropriato-per-la-valutazione",
    "title": "2  Punteggi e scale",
    "section": "2.8 La Selezione del Punteggio Appropriato per la Valutazione",
    "text": "2.8 La Selezione del Punteggio Appropriato per la Valutazione\nDeterminare il tipo di punteggio più adeguato per un test è essenziale per ottenere informazioni specifiche e pertinenti dalla valutazione. Le diverse categorie di punteggi forniscono risposte a domande distinte riguardo alle prestazioni degli esaminandi:\n\nPunteggi Grezzi:\n\nRappresentano la quantità totale di risposte corrette accumulate da un individuo.\nOffrono una visione immediata del livello di prestazione e permettono di stabilire un ordine tra i partecipanti.\nSono utili per identificare rapidamente il posizionamento relativo di un individuo all’interno di un gruppo.\n\nPunteggi Norm-Referenced Standard:\n\nForniscono un confronto diretto tra le prestazioni di un individuo e quelle di un gruppo normativo.\nConsentono di interpretare la prestazione su una scala relativa, facilitando la comprensione del rendimento in termini di posizione all’interno di una popolazione di riferimento.\n\nPunteggi Criterion-Referenced:\n\nIndicano se un individuo ha raggiunto un determinato standard di competenza.\nSono particolarmente indicati per valutare il conseguimento di obiettivi specifici o competenze chiave.\n\nPunteggi Basati sull’IRT (Inclusi i Punteggi Rasch):\n\nOffrono una misurazione su scala a intervalli costanti, riflettendo la posizione di un individuo su un continuum di un tratto latente.\nSono ideali per tracciare il progresso nel tempo o confrontare le prestazioni attraverso diverse valutazioni di un medesimo tratto.\n\n\nAd esempio, nel caso di Giovanni, che ha beneficiato di un programma di supporto alla lettura:\n\nPunteggi Norm-Referenced: Fornirebbero insight su come le capacità di lettura di Giovanni si confrontano con quelle dei suoi coetanei dopo l’intervento.\nPunteggi Rasch o IRT: Consentirebbero di valutare l’evoluzione precisa delle competenze di lettura di Giovanni, misurando il progresso a partire dal suo livello iniziale.\nPunteggi Grezzi: Darebbero indicazioni sul miglioramento assoluto, sebbene privi della capacità di riflettere le variazioni in termini di difficoltà degli item o di altri fattori.\nPunteggi Criterion-Referenced: Stabilirebbero se Giovanni ha raggiunto specifici obiettivi di competenza in lettura definiti a priori.\n\nIn contesti educativi, l’uso di punteggi norm-referenced standardizzati per età può essere preferibile per determinare se uno studente sta progredendo adeguatamente rispetto ai suoi pari. In contesti clinici, come nella gestione della depressione, i punteggi criterion-referenced possono offrire una valutazione mirata del raggiungimento di soglie di miglioramento clinico significativo.\nIn conclusione, la scelta del tipo di punteggio da utilizzare è guidata dal contesto di valutazione e dall’obiettivo specifico della misurazione. Diverse tipologie di punteggi illuminano aspetti distinti delle prestazioni, rendendoli più o meno adatti a seconda delle esigenze informative della valutazione.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#significato-e-applicazione-delle-norme-e-dei-punteggi-standardizzati",
    "href": "chapters/measurement/01_scores_scales.html#significato-e-applicazione-delle-norme-e-dei-punteggi-standardizzati",
    "title": "2  Punteggi e scale",
    "section": "2.9 Significato e Applicazione delle Norme e dei Punteggi Standardizzati",
    "text": "2.9 Significato e Applicazione delle Norme e dei Punteggi Standardizzati\nPer chiarire questi concetti, esaminiamo i dati della Tabella 2.1 di Bandalos (2018). Con degli esempi numerici, analizzeremo vari tipi di punteggi normativi, tra cui:\n\nPunteggi Percentili: Che indicano la posizione relativa di un individuo all’interno del gruppo normativo.\nPunteggi Standardizzati e Normalizzati: Che trasformano i punteggi grezzi in una scala standard per facilitare il confronto tra diversi individui o gruppi.\nStanini: Un metodo di punteggio che divide i punteggi in intervalli standardizzati.\nEquivalenti alla Curva Normale: Che adattano i punteggi a una distribuzione normale.\n\nNei capitoli successivi esamineremo come calcolare i punteggi basati sulla teoria IRT.\nIniziamo a leggere i dati.\n\nraw_score &lt;- c(\n    26, 25, 33, 31, 26, 34, 29, 36, 25, 29, 28, 32, 25,\n    30, 27, 31, 30, 30, 35, 30, 27, 26, 34, 32, 26, 34,\n    30, 28, 28, 31, 30, 27, 26, 29, 29, 33, 27, 35, 26,\n    27, 28, 29, 28, 27, 34, 36, 26, 26, 34, 30, 34, 27\n)\n\n\n2.9.1 Distribuzione di frequenze\n\nfreq &lt;- table(raw_score) # frequency\ncumfreq &lt;- cumsum(freq) # cumulative frequency\nperc &lt;- prop.table(freq) * 100 # percentage\ncumperc &lt;- cumsum(perc) # cumulative percentage\npr &lt;- (cumperc - 0.5 * perc) # percentile rank\ncbind(freq, cumfreq, perc, cumperc, pr)\n\n\nA matrix: 12 x 5 of type dbl\n\n\n\nfreq\ncumfreq\nperc\ncumperc\npr\n\n\n\n\n25\n3\n3\n5.769231\n5.769231\n2.884615\n\n\n26\n8\n11\n15.384615\n21.153846\n13.461538\n\n\n27\n7\n18\n13.461538\n34.615385\n27.884615\n\n\n28\n5\n23\n9.615385\n44.230769\n39.423077\n\n\n29\n5\n28\n9.615385\n53.846154\n49.038462\n\n\n30\n7\n35\n13.461538\n67.307692\n60.576923\n\n\n31\n3\n38\n5.769231\n73.076923\n70.192308\n\n\n32\n2\n40\n3.846154\n76.923077\n75.000000\n\n\n33\n2\n42\n3.846154\n80.769231\n78.846154\n\n\n34\n6\n48\n11.538462\n92.307692\n86.538462\n\n\n35\n2\n50\n3.846154\n96.153846\n94.230769\n\n\n36\n2\n52\n3.846154\n100.000000\n98.076923\n\n\n\n\n\n\n\n2.9.2 Punteggi Percentili\nI punteggi percentili sono un modo efficace per interpretare e confrontare i punteggi di un individuo con quelli di un campione normativo. Un punteggio percentile indica la posizione relativa di un individuo all’interno di un gruppo normativo. Più specificamente, un punteggio percentile mostra la percentuale di persone nel campione normativo che ha ottenuto un punteggio uguale o inferiore a quello dell’individuo in questione.\nPer esemplificare il concetto, consideriamo il calcolo di un quantile di ordine 0.74. Questo significa che stiamo cercando il valore al di sotto del quale si trova il 74% dei punteggi nel campione normativo. In altre parole, un individuo con un punteggio corrispondente a questo quantile ha superato il 74% delle persone nel gruppo normativo.\nIl calcolo dei punteggi percentili può essere effettuato attraverso l’analisi statistica dei dati di un campione rappresentativo. Questi dati vengono ordinati in modo crescente, e si identifica il punteggio che corrisponde al percentile desiderato. Nel caso del quantile 0.74, si cerca il punteggio che si trova alla posizione che corrisponde al 74% della lunghezza totale dell’elenco ordinato dei punteggi.\n\nquantile(raw_score, .74) \n\n74%: 31.74\n\n\n\n# Use a different type (see https://en.wikipedia.org/wiki/Quantile#Estimating_quantiles_from_a_sample)\nquantile(raw_score, .74, type = 6)\n\n74%: 32\n\n\nI punteggi percentili sono particolarmente utili perché offrono una comprensione intuitiva della posizione di un individuo rispetto agli altri. Tuttavia, è importante notare che essi rappresentano una scala ordinale e, pertanto, le differenze tra i punteggi percentili non sono necessariamente uniformi o proporzionali attraverso l’intera gamma di punteggi.\nIn conclusione, i punteggi percentili sono uno strumento fondamentale nella valutazione psicologica e educativa, poiché forniscono un modo diretto e facilmente interpretabile per valutare le prestazioni di un individuo in confronto a un campione normativo.\n\n\n2.9.3 Punteggi Standardizzati\nI punteggi standardizzati rappresentano una trasformazione essenziale nel campo della psicometria, che consente di convertire i punteggi grezzi ottenuti in un test in una scala unificata. Questa trasformazione permette di confrontare i risultati di individui o gruppi in maniera equa e coerente, superando le variazioni di scala o di difficoltà tra diversi test.\n\n2.9.3.1 Principi Fondamentali dei Punteggi Standardizzati\n\nMedia e Deviazione Standard Predefinite: I punteggi standardizzati sono calcolati in modo tale da avere una media e una deviazione standard specifiche, stabilite in anticipo. Per esempio, spesso si utilizza una media di 100 e una deviazione standard di 15 (come nei test di intelligenza) o una media di 0 e una deviazione standard di 1 (come negli z-score).\nRisultati Confrontabili: Attraverso questa standardizzazione, i punteggi diventano direttamente confrontabili. Un punteggio standardizzato rispetto a una media di 100 e una deviazione standard di 15, ad esempio, permette di valutare rapidamente se un punteggio è al di sopra, al di sotto o vicino alla media del campione normativo.\n\n\n\n2.9.3.2 Come Funziona la Trasformazione\nIl processo di standardizzazione implica la sottrazione della media del campione normativo dal punteggio grezzo di un individuo, seguita dalla divisione del risultato per la deviazione standard del campione normativo. In termini matematici, se $ X $ è un punteggio grezzo, $ $ è la media del campione normativo e $ $ è la deviazione standard del campione normativo, allora il punteggio standardizzato $ Z $ è calcolato come:\n\\[\nZ = \\frac{X - \\mu}{\\sigma}.\n\\]\n\n\n2.9.3.3 Utilità dei Punteggi Standardizzati\n\nComparabilità: Rendono i punteggi ottenuti da test diversi o da campioni diversi direttamente comparabili.\nInterpretazione Facilitata: Forniscono un modo semplice per interpretare i punteggi individuali in termini di posizione relativa rispetto alla media del campione normativo.\nAdattabilità: Sono utili in una varietà di contesti, da test educativi a valutazioni cliniche.\n\nIn conclusione, i punteggi standardizzati sono uno strumento cruciale nella psicometria e nella valutazione educativa. Trasformando i punteggi grezzi in una scala comune con media e deviazione standard specifiche, facilitano il confronto e l’interpretazione dei risultati dei test, rendendo più accessibile l’analisi e la valutazione delle prestazioni individuali e di gruppo.\nNel caso dell’esempio, i calcoli si svolgono in R nel modo seguente:\n\nz_score &lt;- (raw_score - mean(raw_score)) / sd(raw_score)\nc(mean = mean(z_score), sd = sd(z_score))\n\nmean-5.61516645146954e-16sd1\n\n\n\n\n2.9.3.4 Punteggi T\nI punteggi T sono una forma specifica di punteggi standardizzati, utilizzati frequentemente nella psicometria per rendere più accessibili e interpretabili i risultati dei test. A differenza dei punteggi z, che tipicamente hanno una media di 0 e una deviazione standard di 1, i punteggi T sono trasformati in modo da avere una media fissata a 50 e una deviazione standard di 10.\n\n\n2.9.3.5 Caratteristiche Principali dei Punteggi T\n\nMedia e Deviazione Standard: La media fissata a 50 e la deviazione standard di 10 sono scelte per offrire una scala più intuitiva e di facile lettura rispetto agli z-score. Questa trasformazione sposta la scala degli z-score in una gamma numericamente più familiare e più semplice da interpretare per la maggior parte delle persone.\nCalcolo dei Punteggi T: Il calcolo dei punteggi T avviene trasformando prima i punteggi grezzi in z-score e poi convertendo questi z-score nella scala dei punteggi T. Matematicamente, se $ Z $ è lo z-score, il punteggio T corrispondente $ T $ è calcolato come:\n\\[\nT = 50 + 10 \\times Z.\n\\]\nQuesta formula adatta lo z-score in una scala che inizia da 50 e si allarga in entrambe le direzioni con incrementi standard di 10 per ogni deviazione standard.\n\n\n\n2.9.3.6 Utilizzo dei Punteggi T\n\nFacilità di Interpretazione: I punteggi T sono particolarmente utili quando si desidera presentare i risultati dei test in un formato che sia immediatamente comprensibile, senza la necessità di ulteriori calcoli o trasformazioni.\nComparabilità: Consentono di confrontare i risultati di test diversi in modo più diretto, grazie alla loro scala standardizzata.\nAmpio Utilizzo: Sono ampiamente usati in vari ambiti della valutazione psicologica, inclusi l’educazione, la ricerca e la pratica clinica.\n\nIn sintesi, i punteggi T offrono un modo efficace e standardizzato per interpretare i risultati dei test, rendendo i dati più accessibili e immediatamente comprensibili. La loro trasformazione da z-score a una scala con media 50 e deviazione standard 10 facilita la comprensione e la comparazione dei punteggi tra diversi test e diversi individui.\nSvolgendo i calcoli in R otteniamo\n\nT_score &lt;- z_score * 10 + 50\nc(mean = mean(T_score), sd = sd(T_score))\n\nmean50sd10\n\n\n\n\n\n2.9.4 Punteggi Stanini\nI punteggi Stanini (dall’inglese “standard nine”) rappresentano un metodo standardizzato per categorizzare i risultati dei test in psicometria, dividendoli in nove intervalli. Questa scala, progettata per semplificare l’interpretazione dei dati, permette di valutare la posizione relativa di un individuo all’interno di un gruppo di riferimento.\nCome funzionano? Ogni intervallo Stanine corrisponde a un range di punteggi grezzi, con un’ampiezza che può variare leggermente a seconda della distribuzione dei dati. Un punteggio Stanine di 5 indica una prestazione media, mentre valori più alti o più bassi indicano prestazioni rispettivamente superiori o inferiori alla media. È importante notare che i punteggi Stanini sono principalmente utilizzati per confronti relativi all’interno di un gruppo, piuttosto che per misurazioni assolute.\nCalcolo dei Punteggi Stanini. Per calcolare i punteggi Stanini, è necessario seguire alcuni passaggi:\n\nDeterminare Media e Deviazione Standard: Inizialmente, si calcolano la media e la deviazione standard dei dati del campione normativo.\nApplicare la Formula dei Punteggi Stanini: Per ogni punteggio grezzo, si applica la seguente formula per calcolare il punteggio Stanine corrispondente:\n\\[\n\\text{Stanine} = \\left( \\frac{\\text{Punteggio Grezzo} - \\text{Media}}{\\text{Deviazione Standard}} \\right) \\times 2 + 5.\n\\]\nQuesta formula trasforma il punteggio grezzo in un valore sulla scala dei punteggi Stanini.\nArrotondare al Numero Intero Più Vicino: Infine, si arrotonda il risultato al numero intero più vicino per ottenere il punteggio Stanini finale.\n\nI punteggi Stanini offrono diversi vantaggi:\n\nSemplicità: La scala a nove punti è facile da comprendere e memorizzare.\nRapidità: Permettono una valutazione rapida della performance.\nStandardizzazione: Consentono di confrontare i risultati ottenuti in test diversi o da gruppi diversi.\n\nLimitazioni:\nSebbene i punteggi Stanini siano uno strumento utile, è importante considerarne anche i limiti: essi assumono una distribuzione normale dei dati e quindi non sono adatti a tutti i tipi di test.\nPer l’esempio presente abbiamo:\n\nmean_score &lt;- mean(raw_score)\nsd_score &lt;- sd(raw_score)\n\nstanine_scores &lt;- round((raw_score - mean_score) / sd_score * 2 + 5)\nprint(stanine_scores)\n\n [1] 3 2 7 6 3 8 5 9 2 5 4 7 2 5 3 6 5 5 8 5 3 3 8 7 3 8 5 4 4 6 5 3 3 5 5 7 3 8\n[39] 3 3 4 5 4 3 8 9 3 3 8 5 8 3\n\n\nÈ importante ricordare che la trasformazione in punti z non cambia la forma della distribuzione.\n\nplot(density(raw_score))\n\n\n\n\n\n\n\n\n\nplot(density(z_score))\n\n\n\n\n\n\n\n\n\nplot(density(T_score))\n\n\n\n\n\n\n\n\n\nplot(density(stanine_scores))\n\n\n\n\n\n\n\n\nLa seguente figura proposta da Petersen (2024) illustra le relazioni tra i punteggi stanini e altre tipologie di punteggi derivati.\n\n\n\n\n\n\n\n\n\n\n\n2.9.5 Equivalenti alla Curva Normale (NCE)\nGli Equivalenti alla Curva Normale, noti come NCE (dall’inglese “Normal Curve Equivalents”), sono un tipo di punteggio standardizzato utilizzato in ambito psicometrico. Questi punteggi vengono calcolati per trasformare i punteggi grezzi ottenuti in un test in una scala che rifletta una distribuzione approssimativamente normale. L’obiettivo principale dei punteggi NCE è quello di rendere i punteggi di diverse misure o test direttamente confrontabili, mantenendo una distribuzione che si allinea strettamente con una curva normale standard.\n\n2.9.5.1 Caratteristiche dei Punteggi NCE\n\nDistribuzione Normalizzata: I punteggi NCE sono progettati per aderire a una distribuzione normale. Ciò significa che, a differenza di altri tipi di punteggi, i NCE si allineano più da vicino con le caratteristiche di una curva di distribuzione gaussiana, con la maggior parte dei punteggi concentrati intorno alla media e una distribuzione simmetrica verso gli estremi.\nFacilità di Comparazione: Grazie alla loro standardizzazione, i punteggi NCE consentono un confronto diretto e significativo tra le prestazioni in diversi test o misure. Questo è particolarmente utile in contesti educativi e clinici dove è necessario interpretare e confrontare i risultati di diversi test.\n\n\n\n2.9.5.2 Calcolo e Utilizzo dei Punteggi NCE\nIl calcolo dei punteggi NCE si basa sulla trasformazione dei punteggi grezzi in modo che si adattino a una distribuzione normalizzata. Questo processo implica l’uso di formule matematiche che riallineano i dati grezzi su una scala standard, considerando la media e la deviazione standard del campione normativo.\nUna volta calcolati, i punteggi NCE offrono una visione chiara e immediata delle prestazioni relative di un individuo o di un gruppo, rispetto a un campione normativo. Questo tipo di punteggio è particolarmente utile quando i punteggi grezzi provengono da distribuzioni che non seguono una curva normale, consentendo così un’interpretazione più accurata e standardizzata dei risultati.\n\n\n2.9.5.3 Applicazioni Pratiche dei Punteggi NCE\nI punteggi NCE trovano impiego in una varietà di contesti, tra cui:\n\nValutazioni Educative: In ambito scolastico, per confrontare le prestazioni degli studenti in test diversi.\nRicerca Psicologica: Per analizzare e confrontare i risultati di diversi studi o misure psicometriche.\nPratica Clinica: Nella valutazione di clienti o pazienti utilizzando diversi strumenti diagnostici.\n\nIn conclusione, i punteggi Equivalenti alla Curva Normale rappresentano uno strumento psicometrico potente per standardizzare e confrontare efficacemente i risultati di diversi test o misure, assicurando che questi siano interpretati all’interno di un quadro coerente e comparabile.\nPer i dati dell’esempio abbiamo:\n\n# Using normal quantile\nqnorm_pr &lt;- qnorm(pr / 100)\n# Convert raw scores\nnormalized_zscore &lt;- as.vector(qnorm_pr[as.character(raw_score)])\n\nIn alternativa, è possibile usare la trasformazione di Box-Cox, che è una tecnica parametrica che cerca di correggere le asimmetrie e trasformare i dati in una forma che approssima una distribuzione normale. È efficace per i dati positivi. La trasformazione è definita come segue:\n\\[\ny(\\lambda) = \\begin{cases} \\frac{x^\\lambda - 1}{\\lambda} & \\text{se } \\lambda \\neq 0 \\\\ \\log(x) & \\text{se } \\lambda = 0 \\end{cases},\n\\]\ndove \\(x\\) è il valore originale e \\(\\lambda\\) è il parametro di trasformazione che viene spesso trovato attraverso la massimizzazione della verosimiglianza.\nSupponiamo di voler utilizzare la trasformazione di Box-Cox sul nostro set di dati. La procedura è la seguente. Questo codice utilizza la funzione boxcox dal pacchetto MASS per trovare il valore di \\(\\lambda\\) che massimizza la log-verosimiglianza della trasformazione di Box-Cox applicata ai dati. Poi, utilizza questo \\(\\lambda\\) per trasformare i dati.\n\n# Dati di esempio\nset.seed(123) # Per rendere l'esempio riproducibile\ndata &lt;- raw_score\n\n# Trova il miglior lambda per la trasformazione di Box-Cox\nbc &lt;- boxcox(data ~ 1, lambda = seq(-2, 2, by = 0.1))\n\n# Calcola la trasformazione di Box-Cox con il lambda ottimale\nlambda_opt &lt;- bc$x[which.max(bc$y)]\ndata_transformed &lt;- (data^lambda_opt - 1) / lambda_opt\n\n\n\n\n\n\n\n\nAvendo trovato i dati trasformati con la procedura Box-Cox, li confrontiamo con gli Equivalenti alla Curva Normale (NCE) calcolati con la procedura usuale.\n\nplot(normalized_zscore, data_transformed)\n\n\n\n\n\n\n\n\n\nplot(density(normalized_zscore)) # the shape will be closer to normal\n\n\n\n\n\n\n\n\n\nplot(density((data_transformed - mean(data_transformed)) / sd(data_transformed)))\n\n\n\n\n\n\n\n\nSi osservi che i dati NCE presentano una distribuzione più simile a una curva a forma campanulare rispetto ai dati grezzi.\n\nlillie.test(normalized_zscore)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  normalized_zscore\nD = 0.085801, p-value = 0.4426\n\n\n\nlillie.test(data_transformed)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  data_transformed\nD = 0.12456, p-value = 0.04274",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#riflessioni-finali-sui-metodi-di-trasformazione-dei-punteggi",
    "href": "chapters/measurement/01_scores_scales.html#riflessioni-finali-sui-metodi-di-trasformazione-dei-punteggi",
    "title": "2  Punteggi e scale",
    "section": "2.10 Riflessioni Finali sui Metodi di Trasformazione dei Punteggi",
    "text": "2.10 Riflessioni Finali sui Metodi di Trasformazione dei Punteggi\nLa trasformazione dei punteggi grezzi in formati più interpretabili è una pratica cruciale in psicometria. Due sono i principali approcci utilizzati per attribuire significato ai punteggi di un test: il riferimento normativo e il riferimento criteriale.\n\n2.10.1 Riferimento Normativo\nNel riferimento normativo, si confronta il punteggio di un individuo con quello medio del gruppo normativo, ovvero gli altri soggetti che hanno svolto lo stesso test. Ci sono diversi tipi di punteggi normati, ciascuno con i suoi specifici vantaggi e limitazioni:\n\nPunteggi Percentili: Questi punteggi sono intuitivi e offrono un’indicazione immediata della posizione relativa di un individuo all’interno di un gruppo. Tuttavia, sono una scala ordinale e non si prestano bene a calcoli matematici più complessi.\nPunteggi Standardizzati: Gli z-score e i T-scores rientrano in questa categoria. Sono scalari a intervallo, quindi adatti a operazioni matematiche. Mantengono la forma distributiva originale dei punteggi grezzi, rendendo più agevole la loro elaborazione statistica.\n\n\n\n2.10.2 Riferimento Criteriale\nAl contrario del riferimento normativo, il riferimento criteriale confronta i punteggi di un individuo con uno standard prestabilito o un criterio specifico, piuttosto che con i punteggi di altri individui.\n\n\n2.10.3 Trasformazioni per la Normalizzazione\nPer ottenere una distribuzione dei punteggi più vicina alla curva normale, si possono utilizzare trasformazioni come gli stanini o gli NCE (Normal Curve Equivalents). Questi metodi di normalizzazione aiutano a standardizzare la distribuzione dei punteggi, facilitando così l’interpretazione e l’analisi dei dati psicometrici.\nIn conclusione, la scelta del metodo di trasformazione dei punteggi dipende dagli obiettivi specifici della valutazione e dall’interpretazione desiderata. La comprensione di queste diverse tecniche è essenziale per una corretta interpretazione dei risultati dei test e per l’analisi psicometrica più generale.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#riflessioni-conclusive",
    "href": "chapters/measurement/01_scores_scales.html#riflessioni-conclusive",
    "title": "2  Punteggi e scale",
    "section": "2.11 Riflessioni Conclusive",
    "text": "2.11 Riflessioni Conclusive\nQuesto capitolo fornisce una panoramica sui diversi tipi di punteggi dei test e il loro significato. Iniziamo notando che i punteggi grezzi, sebbene facili da calcolare, di solito forniscono poche informazioni utili sul rendimento di un esaminando in un test. Di conseguenza, di solito trasformiamo i punteggi grezzi in punteggi derivati, che possono essere di riferimento normativo o al criterio. I punteggi di riferimento normativo confrontano il rendimento di un esaminando con quello di altre persone nel campione di standardizzazione, mentre quelli al criterio confrontano il rendimento con un livello di competenza specificato. È importante valutare l’adeguatezza del campione di standardizzazione quando si utilizzano punteggi di riferimento normativo.\nPer interpretazioni basate sui punteggi di riferimento normativo, è utile conoscere la distribuzione normale e i punteggi standard di riferimento. Questi ultimi hanno una media predefinita e una deviazione standard. Esistono anche punteggi normalizzati quando i punteggi non seguono una distribuzione normale. Altri tipi di punteggi di riferimento normativo includono il rango percentile e i punteggi basati su età o livello di scolarità. Tuttavia, questi ultimi sono da evitare, se possibile, a favore di punteggi standard e ranghi percentile.\nI punteggi al criterio confrontano il rendimento con un livello specifico di competenza. Sono utili per valutare abilità in domini specifici, ma richiedono una chiara definizione del dominio. A volte, un test può produrre entrambi i tipi di punteggi. Forniamo anche una panoramica dei punteggi basati sulla teoria della risposta agli item (IRT), che sono utili per misurare i cambiamenti nel tempo.\nIn conclusione, i diversi tipi di punteggi dei test forniscono informazioni per rispondere a diverse domande e devono essere scelti in base alle esigenze specifiche dell’analisi dei dati del test.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#esercizi",
    "href": "chapters/measurement/01_scores_scales.html#esercizi",
    "title": "2  Punteggi e scale",
    "section": "2.12 Esercizi",
    "text": "2.12 Esercizi\nBandalos, capitolo 2, E1, E2, E5, E6, E7, E8, E9, E12",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/01_scores_scales.html#session-info",
    "href": "chapters/measurement/01_scores_scales.html#session-info",
    "title": "2  Punteggi e scale",
    "section": "2.13 Session Info",
    "text": "2.13 Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] nortest_1.0-4     MASS_7.3-61       ggokabeito_0.1.0  viridis_0.6.5    \n [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n[13] lavaan_0.6-18     psych_2.4.6.26    scales_1.3.0      markdown_1.13    \n[17] knitr_1.48        lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n[21] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n[25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.16.0  jsonlite_1.8.9     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5.1 farver_2.1.2      \n  [7] nloptr_2.1.1       rmarkdown_2.28     vctrs_0.6.5       \n [10] minqa_1.2.8        base64enc_0.1-3    rstatix_0.7.2     \n [13] htmltools_0.5.8.1  broom_1.0.6        Formula_1.2-5     \n [16] htmlwidgets_1.6.4  plyr_1.8.9         sandwich_3.1-1    \n [19] emmeans_1.10.4     zoo_1.8-12         uuid_1.2-1        \n [22] igraph_2.0.3       mime_0.12          lifecycle_1.0.4   \n [25] pkgconfig_2.0.3    Matrix_1.7-0       R6_2.5.1          \n [28] fastmap_1.2.0      shiny_1.9.1        digest_0.6.37     \n [31] OpenMx_2.21.12     fdrtool_1.2.18     colorspace_2.1-1  \n [34] rprojroot_2.0.4    Hmisc_5.1-3        fansi_1.0.6       \n [37] timechange_0.3.0   abind_1.4-8        compiler_4.4.1    \n [40] withr_3.0.1        glasso_1.11        htmlTable_2.4.3   \n [43] backports_1.5.0    carData_3.0-5      ggsignif_0.6.4    \n [46] corpcor_1.6.10     gtools_3.9.5       tools_4.4.1       \n [49] pbivnorm_0.6.0     foreign_0.8-87     zip_2.3.1         \n [52] httpuv_1.6.15      nnet_7.3-19        glue_1.7.0        \n [55] quadprog_1.5-8     promises_1.3.0     nlme_3.1-166      \n [58] lisrelToR_0.3      grid_4.4.1         pbdZMQ_0.3-13     \n [61] checkmate_2.3.2    cluster_2.1.6      reshape2_1.4.4    \n [64] generics_0.1.3     gtable_0.3.5       tzdb_0.4.0        \n [67] data.table_1.16.0  hms_1.1.3          car_3.1-2         \n [70] utf8_1.2.4         sem_3.1-16         pillar_1.9.0      \n [73] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [76] splines_4.4.1      lattice_0.22-6     survival_3.7-0    \n [79] kutils_1.73        tidyselect_1.2.1   miniUI_0.1.1.1    \n [82] pbapply_1.7-2      stats4_4.4.1       xfun_0.47         \n [85] qgraph_1.9.8       arm_1.14-4         stringi_1.8.4     \n [88] boot_1.3-31        evaluate_1.0.0     codetools_0.2-20  \n [91] mi_1.1             cli_3.6.3          RcppParallel_5.1.9\n [94] IRkernel_1.3.2     rpart_4.1.23       xtable_1.8-4      \n [97] repr_1.1.7         munsell_0.5.1      Rcpp_1.0.13       \n[100] coda_0.19-4.1      png_0.1-8          XML_3.99-0.17     \n[103] parallel_4.4.1     jpeg_0.1-10        lme4_1.1-35.5     \n[106] mvtnorm_1.3-1      openxlsx_4.2.7.1   crayon_1.5.3      \n[109] rlang_1.1.4        multcomp_1.4-26    mnormt_2.1.1      \n\n\n\n\n\n\nBandalos, D. L. (2018). Measurement theory and applications for the social sciences. Guilford Publications.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Punteggi e scale</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E1_likert.html",
    "href": "chapters/measurement/E1_likert.html",
    "title": "3  ✏️ Esercizi",
    "section": "",
    "text": "3.1 Scaling Likert\nIn questo tutorial, ripreso da Brown (2023), esamineremo i dati di un questionario ordinale. In particolare, esamineremo il Strengths and Difficulties Questionnaire (SDQ), ovvero un breve questionario di screening comportamentale progettato per valutare i comportamenti di bambini e adolescenti tra i 3 e i 16 anni. Il SDQ è disponibile in diverse versioni per soddisfare le esigenze di ricercatori, clinici ed educatori. Per maggiori informazioni, è possibile consultare il sito ufficiale http://www.sdqinfo.org/, dove è possibile scaricare il questionario, insieme alle chiavi di scoring e alle norme pubblicate dal distributore del test.\nLa versione auto-compilata del questionario, destinata ai ragazzi stessi, include 25 item suddivisi in 5 scale (o dimensioni) che misurano specifici aspetti comportamentali. Ogni scala comprende 5 item:\nOgni item viene valutato dai partecipanti utilizzando le seguenti opzioni di risposta:\nAlcuni item nel SDQ rappresentano comportamenti che devono essere invertiti rispetto alla scala di appartenenza, ossia item a punteggio invertito. Questo significa che punteggi alti sulla scala corrispondono a punteggi bassi per questi specifici item. Ad esempio, l’item “Di solito faccio ciò che mi viene detto” (variabile “obeys”) è un item a punteggio invertito per la scala “Problemi di Condotta”.\nNel SDQ sono presenti 5 item di questo tipo, contrassegnati con un asterisco (*) nella tabella. Questi item devono essere codificati invertendo i punteggi (ad esempio, da 0 a 2 e viceversa) prima di calcolare il punteggio complessivo della scala.\nIn questo studio, i partecipanti sono studenti di prima media (Year 7) provenienti dalla stessa scuola, per un totale di 228 ragazzi. Si tratta di un campione della comunità scolastica, quindi non ci si aspetta che molti dei partecipanti superino le soglie cliniche indicate dal test.\nIl questionario SDQ è stato somministrato due volte:\nQuesta progettazione longitudinale consente di analizzare eventuali cambiamenti nei punteggi SDQ durante il passaggio tra il primo e il secondo anno di scuola secondaria.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E1_likert.html#scaling-likert",
    "href": "chapters/measurement/E1_likert.html#scaling-likert",
    "title": "3  ✏️ Esercizi",
    "section": "",
    "text": "Sintomi Emotivi: somatizzazione, preoccupazioni, infelicità, attaccamento, paura\n\nProblemi di Condotta: capricci, ubbidienza*, litigi, bugie, furti\n\nIperattività: irrequietezza, agitazione, distrazione, riflessione, attenzione\n\nProblemi con i Pari: solitudine, amicizia, popolarità, vittimismo, miglior amico più grande\n\nComportamento Prosociale: considerazione, condivisione, empatia, gentilezza, aiuto agli altri\n\n\n\n0 = “Non vero”\n\n1 = “Parzialmente vero”\n\n2 = “Assolutamente vero”\n\n\n\n\n\n\nLa prima somministrazione è avvenuta all’inizio della scuola secondaria, quando i ragazzi erano nel Year 7.\n\nLa seconda somministrazione è avvenuta un anno dopo, quando i partecipanti erano nel Year 8.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E1_likert.html#emotional-symptoms-scale",
    "href": "chapters/measurement/E1_likert.html#emotional-symptoms-scale",
    "title": "3  ✏️ Esercizi",
    "section": "3.2 Emotional Symptoms scale",
    "text": "3.2 Emotional Symptoms scale\nIniziamo ad esaminare la scala Emotional Symptoms. Questa scala non contiene item reverse. Importiamo i dati in R.\n\nload(\"../../data/data_sdq/SDQ.RData\")\nglimpse(SDQ)\n\nRows: 228\nColumns: 51\n$ Gender   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~\n$ consid   &lt;dbl&gt; 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, ~\n$ restles  &lt;dbl&gt; 2, 0, 0, 0, 1, 0, 2, 1, 2, 0, 1, 1, 0, 1, 0, 2, 0, 1, 1, ~\n$ somatic  &lt;dbl&gt; 2, 2, 0, 0, 2, 1, 0, 0, 1, 0, 0, 2, 0, 0, 1, 2, 1, 1, 1, ~\n$ shares   &lt;dbl&gt; 1, 1, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, ~\n$ tantrum  &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 2, 0, 2, 0, 0, 1, 0, 1, 1, 2, 0, 1, 1, ~\n$ loner    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, ~\n$ obeys    &lt;dbl&gt; 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, ~\n$ worries  &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 1, 2, 0, 1, 1, 2, ~\n$ caring   &lt;dbl&gt; 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, ~\n$ fidgety  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, ~\n$ friend   &lt;dbl&gt; 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, ~\n$ fights   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\n$ unhappy  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 2, ~\n$ popular  &lt;dbl&gt; 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, ~\n$ distrac  &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 0, 1, ~\n$ clingy   &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 2, 0, 0, 0, 0, 1, 0, 2, 2, 1, 2, 0, 2, ~\n$ kind     &lt;dbl&gt; 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, ~\n$ lies     &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 1, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, ~\n$ bullied  &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, ~\n$ helpout  &lt;dbl&gt; 2, 1, 2, 2, 0, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, ~\n$ reflect  &lt;dbl&gt; 1, 1, 2, 2, 0, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, ~\n$ steals   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, ~\n$ oldbest  &lt;dbl&gt; 1, 0, 2, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, ~\n$ afraid   &lt;dbl&gt; 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 2, 2, 0, 1, 1, 1, 0, 1, ~\n$ attends  &lt;dbl&gt; 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 1, ~\n$ consid2  &lt;dbl&gt; 1, 2, 2, 2, NA, 2, 2, 2, 2, 2, NA, 1, NA, 2, 2, NA, 1, 2,~\n$ restles2 &lt;dbl&gt; 0, 1, 2, 1, NA, 0, 1, 1, 0, 0, NA, 2, NA, 0, 1, NA, 1, 1,~\n$ somatic2 &lt;dbl&gt; 0, 1, 1, 0, NA, 0, 0, 0, 0, 0, NA, 2, NA, 0, 1, NA, 0, 1,~\n$ shares2  &lt;dbl&gt; 1, 2, 2, 1, NA, 2, 1, 2, 2, 2, NA, 2, NA, 2, 2, NA, 1, 2,~\n$ tantrum2 &lt;dbl&gt; 0, 1, 2, 0, NA, 0, 2, 0, 0, 0, NA, 2, NA, 0, 1, NA, 1, 0,~\n$ loner2   &lt;dbl&gt; 0, 0, 1, 0, NA, 0, 0, 0, 0, 0, NA, 1, NA, 1, 0, NA, 0, 0,~\n$ obeys2   &lt;dbl&gt; 2, 1, 2, 1, NA, 2, 2, 2, 2, 1, NA, 1, NA, 2, 1, NA, 1, 2,~\n$ worries2 &lt;dbl&gt; 0, 0, 1, 0, NA, NA, 1, 0, 0, 0, NA, 1, NA, 1, 2, NA, 0, 0~\n$ caring2  &lt;dbl&gt; 2, 2, 1, 2, NA, 2, 2, 2, 2, 2, NA, 2, NA, 2, 2, NA, 1, 2,~\n$ fidgety2 &lt;dbl&gt; 0, 1, 0, 0, NA, 0, 1, 0, 0, 0, NA, 2, NA, 0, 0, NA, 1, 0,~\n$ friend2  &lt;dbl&gt; 2, 2, 1, 2, NA, 2, 2, 2, 2, 2, NA, 2, NA, 1, 2, NA, 2, 2,~\n$ fights2  &lt;dbl&gt; 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA, 2, NA, 0, 0, NA, 0, 0,~\n$ unhappy2 &lt;dbl&gt; 0, 0, 1, 0, NA, 0, 0, 0, 0, 0, NA, 1, NA, 0, 0, NA, 0, 0,~\n$ popular2 &lt;dbl&gt; 2, 1, 1, 2, NA, 2, 1, 2, 2, 2, NA, 2, NA, 2, 2, NA, 1, 2,~\n$ distrac2 &lt;dbl&gt; 0, 0, 0, 2, NA, 0, 2, 1, 0, 0, NA, 1, NA, 0, 1, NA, 1, 0,~\n$ clingy2  &lt;dbl&gt; 1, 1, 1, 0, NA, 1, 1, 1, 0, 0, NA, 1, NA, 0, 0, NA, 2, 0,~\n$ kind2    &lt;dbl&gt; 2, 2, 2, 2, NA, 2, 2, 2, 2, 2, NA, 2, NA, 2, 2, NA, 1, 2,~\n$ lies2    &lt;dbl&gt; 1, 0, 0, 0, NA, 0, 1, 0, 1, 0, NA, 1, NA, 0, 0, NA, 1, 0,~\n$ bullied2 &lt;dbl&gt; 0, 0, 0, 0, NA, 0, 2, 0, 0, 0, NA, 0, NA, 0, 0, NA, 0, 0,~\n$ helpout2 &lt;dbl&gt; 1, 1, 1, 2, NA, 2, 2, 1, 2, 1, NA, 2, NA, 2, 1, NA, 0, 2,~\n$ reflect2 &lt;dbl&gt; 1, 1, 2, 1, NA, 2, 1, 2, 1, 2, NA, 1, NA, 2, 1, NA, 1, 2,~\n$ steals2  &lt;dbl&gt; 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA, 2, NA, 0, 0, NA, 0, 0,~\n$ oldbest2 &lt;dbl&gt; 0, 0, 1, 0, NA, 1, 0, 1, 1, 0, NA, 1, NA, 0, 0, NA, 0, 0,~\n$ afraid2  &lt;dbl&gt; 0, 1, 0, 0, NA, 0, 0, 0, 0, 0, NA, 2, NA, 0, 0, NA, 0, 0,~\n$ attends2 &lt;dbl&gt; 1, 1, 2, 0, NA, 2, 2, 2, 2, 1, NA, 1, NA, 2, 2, NA, 1, 1,~\n\n\nSelezioniamo solo gli item della Emotional Symptoms scale al tempo 1.\n\nitems_emotion &lt;- c(\"somatic\", \"worries\", \"unhappy\", \"clingy\", \"afraid\")\nsdq_emo &lt;- SDQ[, items_emotion]  \nsdq_emo |&gt;\n    head()\n\n\nA tibble: 6 x 5\n\n\nsomatic\nworries\nunhappy\nclingy\nafraid\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n2\n1\n0\n1\n0\n\n\n2\n0\n0\n1\n0\n\n\n0\n0\n0\n0\n1\n\n\n0\n0\n0\n1\n1\n\n\n2\n1\n0\n1\n0\n\n\n1\n0\n0\n1\n0\n\n\n\n\n\nCalcoliamo il punteggio della scala.\n\nrowSums(sdq_emo) |&gt; print()\n\n  [1]  4  3  1  2  4  2  4  0  1  1  0  8  2  3  7  4  5  2  8  6  1  4  9\n [24]  4  5  9  0  3  3  1  0  2  6  3  9  4  4  0  7  1  3  6  4  5  4  1\n [47]  4  1  0  5  1  2  2  4  4  4  6  1  8  3  2  2  4  1  1  0  2  2  7\n [70]  5  0 NA NA  1  1  7  4  1  8  3  5  0  5  4  0  1  1  5  3  6  1  3\n [93]  2  6  6  0  2  4  5  3  3  1  1  7  2  3  5  5 NA  0  4  0  4  1  1\n[116]  1  1  0  2  7  0  3  8  4  6 NA  2  4  7  1  0  0  1  0  4  3  0 10\n[139]  5  2  1  6  1  2  1  0  1 NA  4  4  2  4  7  5  6  1  0  5  3  1  3\n[162]  3  6  4  2  3  1  0  3  3  0  3  0  0  0  2  2  2  0  1  5  3  3  1\n[185]  4  3  1  6  2  4  2 NA  0  2  5  5  0  2  2  3  4  0  2  4  2  2  1\n[208]  3  2  0  1  0  0  8  1  1  2  1  2  2  4  0  0  1  2  2  1  6\n\n\nNotiamo che ci sono diversi punteggi mancanti, denotati da NA. Un primo metodo per affrontare i dati mancanti è semplicemente quello di ignorarli:\n\nrowSums(sdq_emo, na.rm = TRUE) |&gt; print()\n\n  [1]  4  3  1  2  4  2  4  0  1  1  0  8  2  3  7  4  5  2  8  6  1  4  9\n [24]  4  5  9  0  3  3  1  0  2  6  3  9  4  4  0  7  1  3  6  4  5  4  1\n [47]  4  1  0  5  1  2  2  4  4  4  6  1  8  3  2  2  4  1  1  0  2  2  7\n [70]  5  0  2  7  1  1  7  4  1  8  3  5  0  5  4  0  1  1  5  3  6  1  3\n [93]  2  6  6  0  2  4  5  3  3  1  1  7  2  3  5  5  4  0  4  0  4  1  1\n[116]  1  1  0  2  7  0  3  8  4  6  0  2  4  7  1  0  0  1  0  4  3  0 10\n[139]  5  2  1  6  1  2  1  0  1  4  4  4  2  4  7  5  6  1  0  5  3  1  3\n[162]  3  6  4  2  3  1  0  3  3  0  3  0  0  0  2  2  2  0  1  5  3  3  1\n[185]  4  3  1  6  2  4  2  4  0  2  5  5  0  2  2  3  4  0  2  4  2  2  1\n[208]  3  2  0  1  0  0  8  1  1  2  1  2  2  4  0  0  1  2  2  1  6\n\n\nTuttavia, questa non è una buona idea. Anche per il fatto che, in questo modo non verrà calcolato il punteggio totale di 7 partecipanti. Possiamo identificare le colonne in cui ci sono dei valori mancanti usando summary().\n\nsummary(sdq_emo)\n\n    somatic         worries         unhappy          clingy     \n Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   :0.000  \n 1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.000  \n Median :0.000   Median :0.000   Median :0.000   Median :1.000  \n Mean   :0.611   Mean   :0.621   Mean   :0.317   Mean   :0.842  \n 3rd Qu.:1.000   3rd Qu.:1.000   3rd Qu.:1.000   3rd Qu.:1.000  \n Max.   :2.000   Max.   :2.000   Max.   :2.000   Max.   :2.000  \n NA's   :2       NA's   :1       NA's   :1                      \n     afraid    \n Min.   :0.00  \n 1st Qu.:0.00  \n Median :0.00  \n Mean   :0.48  \n 3rd Qu.:1.00  \n Max.   :2.00  \n NA's   :3     \n\n\nUn approccio semplice per gestire il problema dei dati mancanti è l’imputazione, che consiste nel sostituire i valori mancanti con stime plausibili basate sulle informazioni disponibili nel dataset. Il metodo più elementare di imputazione prevede la sostituzione del valore mancante con la media della colonna corrispondente. Questo approccio è facile da implementare e può essere utile come soluzione preliminare, ma potrebbe non catturare correttamente la variabilità e le relazioni tra le variabili.\n\nsdq_emo &lt;- sdq_emo %&gt;%\n    mutate_at(vars(somatic:afraid), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .))\n\nQuesta istruzione utilizza la funzione mutate_at del pacchetto dplyr per applicare una trasformazione a colonne specifiche (da somatic a afraid). All’interno della funzione di trasformazione, essa controlla se ogni valore è mancante (NA). Se lo è, lo sostituisce con la media della colonna usando mean(., na.rm = TRUE), che calcola la media escludendo eventuali valori mancanti.\nPossiamo ora calcolare il punteggio della scala per ciascun partecipante.\n\nSDQ$s_emotion &lt;- rowSums(sdq_emo) |&gt; round()\nSDQ$s_emotion |&gt; print()\n\n  [1]  4  3  1  2  4  2  4  0  1  1  0  8  2  3  7  4  5  2  8  6  1  4  9\n [24]  4  5  9  0  3  3  1  0  2  6  3  9  4  4  0  7  1  3  6  4  5  4  1\n [47]  4  1  0  5  1  2  2  4  4  4  6  1  8  3  2  2  4  1  1  0  2  2  7\n [70]  5  0  2  8  1  1  7  4  1  8  3  5  0  5  4  0  1  1  5  3  6  1  3\n [93]  2  6  6  0  2  4  5  3  3  1  1  7  2  3  5  5  5  0  4  0  4  1  1\n[116]  1  1  0  2  7  0  3  8  4  6  0  2  4  7  1  0  0  1  0  4  3  0 10\n[139]  5  2  1  6  1  2  1  0  1  4  4  4  2  4  7  5  6  1  0  5  3  1  3\n[162]  3  6  4  2  3  1  0  3  3  0  3  0  0  0  2  2  2  0  1  5  3  3  1\n[185]  4  3  1  6  2  4  2  5  0  2  5  5  0  2  2  3  4  0  2  4  2  2  1\n[208]  3  2  0  1  0  0  8  1  1  2  1  2  2  4  0  0  1  2  2  1  6\n\n\nUn istogramma si ottiene nel modo seguente.\n\nSDQ |&gt;\n    ggplot(aes(x = s_emotion)) +\n    geom_histogram(bins = 10)\n\n\n\n\n\n\n\n\n\nhist(SDQ$s_emotion)\n\n\n\n\n\n\n\n\nPiù utile è un KDE plot.\n\nSDQ |&gt;\n    ggplot(aes(x = s_emotion)) +\n    geom_density()\n\n\n\n\n\n\n\n\nPossiamo ottenere le statistiche descrittive della scala usando la funzione describe del pacchetto psych.\n\ndescribe(SDQ$s_emotion)\n\n\nA psych: 1 x 13\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nX1\n1\n228\n2.87\n2.31\n2\n2.65\n2.97\n0\n10\n10\n0.722\n-0.143\n0.153\n\n\n\n\n\nCome si può vedere, la mediana (il punteggio al di sotto del quale si trova la metà del campione) di s_emotion è 2, mentre la media è più alta e pari a 2.87. Questo perché la distribuione dei punteggi è asimmetrica positiva; in questo caso, la mediana è più rappresentativa della tendenza centrale. Queste statistiche sono coerenti con la nostra osservazione dell’istogramma, che mostra un forte floor effect.\nDi seguito sono riportati i valori di soglia per i casi “Normali”, “Borderline” e “Anormali” per i Sintomi Emotivi forniti dal publisher del test (vedi https://sdqinfo.org/). Questi sono i punteggi che distinguono i casi probabilmente borderline e anormali dai casi “normali”.\nNormale: 0-5\nBorderline: 6\nAnormale: 7-10\n\ntable(SDQ$s_emotion &lt;= 5)\n\n\nFALSE  TRUE \n   32   196 \n\n\nIn questo campione, dunque, l’85% dei partecipanti è classificato nell’intervallo Normale.\n\ntable(SDQ$s_emotion &lt;= 5)[2] / length(SDQ$s_emotion)\n\nTRUE: 0.859649122807018\n\n\nIn maniera equivalente otteniamo i valori dei partecipanti “borderline”:\n\ntable(SDQ$s_emotion == 6)[2] / length(SDQ$s_emotion)\n\nTRUE: 0.0570175438596491\n\n\ne dei partecipanti “anormali”:\n\ntable(SDQ$s_emotion &gt;= 7)[2] / length(SDQ$s_emotion)\n\nTRUE: 0.0833333333333333",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E1_likert.html#item-reverse",
    "href": "chapters/measurement/E1_likert.html#item-reverse",
    "title": "3  ✏️ Esercizi",
    "section": "3.3 Item reverse",
    "text": "3.3 Item reverse\nIn un secondo esempio consideriamo la codifica delle risposte degli item SDQ che misurano i Problemi di Condotta. Alcuni item sono stati codificati usando una codifica inversa. Prima di calcolare il punteggio totale è dunque necessario invertire il punteggio degli item a codifica inversa.\n\nitems_conduct &lt;- c(\"tantrum\", \"obeys\", \"fights\", \"lies\", \"steals\")\n\nPer i Problemi di Condotta, abbiamo solo un item reverse, obeys.\ntantrum    obeys*      fights       lies       steals\nPer invertire il codice di questo item, useremo una funzione dedicata del pacchetto psych, reverse.code(). Questa funzione ha la forma generale reverse.code(keys, items,…). L’argomento keys è un vettore di valori 1 o -1, dove -1 implica l’inversione dell’item. L’argomento items sono i nomi delle variabili che vogliamo valutare.\n\nR_conduct &lt;- reverse.code(keys = c(1, -1, 1, 1, 1), SDQ[, items_conduct]) |&gt; as_tibble()\nR_conduct |&gt; head()\n\n\nA tibble: 6 x 5\n\n\ntantrum\nobeys-\nfights\nlies\nsteals\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n\n\n1\n2\n0\n2\n0\n\n\n0\n0\n0\n0\n0\n\n\n\n\n\n\nSDQ[, items_conduct] |&gt; head()\n\n\nA tibble: 6 x 5\n\n\ntantrum\nobeys\nfights\nlies\nsteals\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0\n2\n0\n0\n0\n\n\n0\n2\n0\n0\n0\n\n\n0\n2\n0\n0\n0\n\n\n0\n2\n0\n0\n0\n\n\n1\n0\n0\n2\n0\n\n\n0\n2\n0\n0\n0\n\n\n\n\n\nAnche in questo caso ci sono dei dati mancanti.\n\nsummary(R_conduct)\n\n    tantrum          obeys-          fights           lies      \n Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   :0.000  \n 1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.000  \n Median :0.000   Median :1.000   Median :0.000   Median :0.000  \n Mean   :0.571   Mean   :0.579   Mean   :0.193   Mean   :0.544  \n 3rd Qu.:1.000   3rd Qu.:1.000   3rd Qu.:0.000   3rd Qu.:1.000  \n Max.   :2.000   Max.   :2.000   Max.   :2.000   Max.   :2.000  \n NA's   :2                                       NA's   :2      \n     steals     \n Min.   :0.000  \n 1st Qu.:0.000  \n Median :0.000  \n Mean   :0.185  \n 3rd Qu.:0.000  \n Max.   :2.000  \n NA's   :1      \n\n\nUsiamo la stessa procedura descritta in precedenza:\n\nR_conduct &lt;- R_conduct %&gt;%\n    mutate_at(vars(tantrum:steals), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .))\n\nCalcoliamo ora il punteggio totale.\n\nSDQ$s_conduct &lt;- rowMeans(R_conduct)\n\n\nSDQ |&gt;\n    ggplot(aes(x = s_conduct)) +\n    geom_histogram(bins = 10)",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E1_likert.html#session-info",
    "href": "chapters/measurement/E1_likert.html#session-info",
    "title": "3  ✏️ Esercizi",
    "section": "3.4 Session Info",
    "text": "3.4 Session Info\n\nsessionInfo() \n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [5] ggExtra_0.10.1    bayesplot_1.11.1  gridExtra_2.3     patchwork_1.3.0  \n [9] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-18     psych_2.4.6.26   \n[13] scales_1.3.0      markdown_1.13     knitr_1.48        lubridate_1.9.3  \n[17] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[21] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n[25] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.16.0  jsonlite_1.8.9     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5.1 farver_2.1.2      \n  [7] nloptr_2.1.1       rmarkdown_2.28     vctrs_0.6.5       \n [10] minqa_1.2.8        base64enc_0.1-3    rstatix_0.7.2     \n [13] htmltools_0.5.8.1  broom_1.0.6        Formula_1.2-5     \n [16] htmlwidgets_1.6.4  plyr_1.8.9         sandwich_3.1-1    \n [19] emmeans_1.10.4     zoo_1.8-12         uuid_1.2-1        \n [22] igraph_2.0.3       mime_0.12          lifecycle_1.0.4   \n [25] pkgconfig_2.0.3    Matrix_1.7-0       R6_2.5.1          \n [28] fastmap_1.2.0      shiny_1.9.1        digest_0.6.37     \n [31] OpenMx_2.21.12     fdrtool_1.2.18     colorspace_2.1-1  \n [34] rprojroot_2.0.4    Hmisc_5.1-3        labeling_0.4.3    \n [37] fansi_1.0.6        timechange_0.3.0   abind_1.4-8       \n [40] compiler_4.4.1     withr_3.0.1        glasso_1.11       \n [43] htmlTable_2.4.3    backports_1.5.0    carData_3.0-5     \n [46] ggsignif_0.6.4     MASS_7.3-61        corpcor_1.6.10    \n [49] gtools_3.9.5       tools_4.4.1        pbivnorm_0.6.0    \n [52] foreign_0.8-87     zip_2.3.1          httpuv_1.6.15     \n [55] nnet_7.3-19        glue_1.7.0         quadprog_1.5-8    \n [58] promises_1.3.0     nlme_3.1-166       lisrelToR_0.3     \n [61] grid_4.4.1         pbdZMQ_0.3-13      checkmate_2.3.2   \n [64] cluster_2.1.6      reshape2_1.4.4     generics_0.1.3    \n [67] gtable_0.3.5       tzdb_0.4.0         data.table_1.16.0 \n [70] hms_1.1.3          car_3.1-2          utf8_1.2.4        \n [73] sem_3.1-16         pillar_1.9.0       IRdisplay_1.1     \n [76] rockchalk_1.8.157  later_1.3.2        splines_4.4.1     \n [79] lattice_0.22-6     survival_3.7-0     kutils_1.73       \n [82] tidyselect_1.2.1   miniUI_0.1.1.1     pbapply_1.7-2     \n [85] stats4_4.4.1       xfun_0.47          qgraph_1.9.8      \n [88] arm_1.14-4         stringi_1.8.4      boot_1.3-31       \n [91] evaluate_1.0.0     codetools_0.2-20   mi_1.1            \n [94] cli_3.6.3          RcppParallel_5.1.9 IRkernel_1.3.2    \n [97] rpart_4.1.23       xtable_1.8-4       repr_1.1.7        \n[100] munsell_0.5.1      Rcpp_1.0.13        coda_0.19-4.1     \n[103] png_0.1-8          XML_3.99-0.17      parallel_4.4.1    \n[106] jpeg_0.1-10        lme4_1.1-35.5      mvtnorm_1.3-1     \n[109] openxlsx_4.2.7.1   crayon_1.5.3       rlang_1.1.4       \n[112] multcomp_1.4-26    mnormt_2.1.1      \n\n\n\n\n\n\nBrown, A. (2023). Psychometrics in Exercises using R and RStudio: Textbook and Data Resource. https://bookdown.org/annabrown/psychometricsR",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E2_optimal_scoring.html",
    "href": "chapters/measurement/E2_optimal_scoring.html",
    "title": "4  ✏️ Esercizi",
    "section": "",
    "text": "4.1 Ottimizzazione dello scoring dei dati di questionari ordinali\nNell’Esercizio precedente, abbiamo calcolato i punteggi del Strength and Difficulties Questionnaire (SDQ) utilizzando il cosiddetto approccio della “scalatura Likert”. In questo metodo, alle categorie di risposta “Non vero”, “Parzialmente vero” e “Assolutamente vero” sono stati assegnati interi consecutivi, rispettivamente 0-1-2. Sebbene questo assegnamento rifletta apparentemente un grado crescente di accordo nelle opzioni di risposta, la scelta degli interi è stata arbitraria: non vi era un motivo particolare per assegnare 0-1-2 anziché, ad esempio, 1-2-3. Questo tipo di assegnazione arbitraria dei punteggi agli item è comunemente chiamato “misurazione per decreto” (measurement by fiat).\nIn questo secondo esercizio, cercheremo di individuare punteggi “ottimali” per le risposte ordinali al SDQ. Per “ottimali” intendiamo che i punteggi assegnati non siano semplicemente arbitrari, ma rappresentino la “migliore” scelta possibile in base a un determinato criterio statistico.\nEsistono diversi modi per “ottimizzare” i punteggi degli item. In questo caso, ci concentreremo sulla massimizzazione del rapporto tra la varianza del punteggio totale e la somma delle varianze dei punteggi degli item. In psicometria, soddisfare questo criterio significa massimizzare la somma delle correlazioni tra gli item e, di conseguenza, migliorare la consistenza interna del test, misurata dall’alfa di Cronbach.\nQuesto approccio consente di definire punteggi più informativi, che riflettono meglio la coerenza tra le risposte degli item e il punteggio totale del test, migliorando la qualità psicometrica della scala.\nPer fare un esempio, useremo di nuovo gli item della scala Sintomi Emotivi. Utilizzeremo il pacchetto aspect, che semplifica l’ottimizzazione della scalatura grazie a una gamma di opzioni utili e a funzioni grafiche integrate.\nsource(\"../../code/_common.R\")\nlibrary(\"aspect\")\nImportiamo i dati del Strengths and Difficulties Questionnaire (SDQ).\nload(\"../../data/data_sdq/SDQ.RData\")\nglimpse(SDQ)\n\nRows: 228\nColumns: 51\n$ Gender   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~\n$ consid   &lt;dbl&gt; 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, ~\n$ restles  &lt;dbl&gt; 2, 0, 0, 0, 1, 0, 2, 1, 2, 0, 1, 1, 0, 1, 0, 2, 0, 1, 1, ~\n$ somatic  &lt;dbl&gt; 2, 2, 0, 0, 2, 1, 0, 0, 1, 0, 0, 2, 0, 0, 1, 2, 1, 1, 1, ~\n$ shares   &lt;dbl&gt; 1, 1, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, ~\n$ tantrum  &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 2, 0, 2, 0, 0, 1, 0, 1, 1, 2, 0, 1, 1, ~\n$ loner    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, ~\n$ obeys    &lt;dbl&gt; 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, ~\n$ worries  &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 1, 2, 0, 1, 1, 2, ~\n$ caring   &lt;dbl&gt; 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, ~\n$ fidgety  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, ~\n$ friend   &lt;dbl&gt; 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, ~\n$ fights   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\n$ unhappy  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 2, ~\n$ popular  &lt;dbl&gt; 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, ~\n$ distrac  &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 0, 1, ~\n$ clingy   &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 2, 0, 0, 0, 0, 1, 0, 2, 2, 1, 2, 0, 2, ~\n$ kind     &lt;dbl&gt; 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, ~\n$ lies     &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 1, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, ~\n$ bullied  &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, ~\n$ helpout  &lt;dbl&gt; 2, 1, 2, 2, 0, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, ~\n$ reflect  &lt;dbl&gt; 1, 1, 2, 2, 0, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, ~\n$ steals   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, ~\n$ oldbest  &lt;dbl&gt; 1, 0, 2, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, ~\n$ afraid   &lt;dbl&gt; 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 2, 2, 0, 1, 1, 1, 0, 1, ~\n$ attends  &lt;dbl&gt; 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 1, ~\n$ consid2  &lt;dbl&gt; 1, 2, 2, 2, NA, 2, 2, 2, 2, 2, NA, 1, NA, 2, 2, NA, 1, 2,~\n$ restles2 &lt;dbl&gt; 0, 1, 2, 1, NA, 0, 1, 1, 0, 0, NA, 2, NA, 0, 1, NA, 1, 1,~\n$ somatic2 &lt;dbl&gt; 0, 1, 1, 0, NA, 0, 0, 0, 0, 0, NA, 2, NA, 0, 1, NA, 0, 1,~\n$ shares2  &lt;dbl&gt; 1, 2, 2, 1, NA, 2, 1, 2, 2, 2, NA, 2, NA, 2, 2, NA, 1, 2,~\n$ tantrum2 &lt;dbl&gt; 0, 1, 2, 0, NA, 0, 2, 0, 0, 0, NA, 2, NA, 0, 1, NA, 1, 0,~\n$ loner2   &lt;dbl&gt; 0, 0, 1, 0, NA, 0, 0, 0, 0, 0, NA, 1, NA, 1, 0, NA, 0, 0,~\n$ obeys2   &lt;dbl&gt; 2, 1, 2, 1, NA, 2, 2, 2, 2, 1, NA, 1, NA, 2, 1, NA, 1, 2,~\n$ worries2 &lt;dbl&gt; 0, 0, 1, 0, NA, NA, 1, 0, 0, 0, NA, 1, NA, 1, 2, NA, 0, 0~\n$ caring2  &lt;dbl&gt; 2, 2, 1, 2, NA, 2, 2, 2, 2, 2, NA, 2, NA, 2, 2, NA, 1, 2,~\n$ fidgety2 &lt;dbl&gt; 0, 1, 0, 0, NA, 0, 1, 0, 0, 0, NA, 2, NA, 0, 0, NA, 1, 0,~\n$ friend2  &lt;dbl&gt; 2, 2, 1, 2, NA, 2, 2, 2, 2, 2, NA, 2, NA, 1, 2, NA, 2, 2,~\n$ fights2  &lt;dbl&gt; 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA, 2, NA, 0, 0, NA, 0, 0,~\n$ unhappy2 &lt;dbl&gt; 0, 0, 1, 0, NA, 0, 0, 0, 0, 0, NA, 1, NA, 0, 0, NA, 0, 0,~\n$ popular2 &lt;dbl&gt; 2, 1, 1, 2, NA, 2, 1, 2, 2, 2, NA, 2, NA, 2, 2, NA, 1, 2,~\n$ distrac2 &lt;dbl&gt; 0, 0, 0, 2, NA, 0, 2, 1, 0, 0, NA, 1, NA, 0, 1, NA, 1, 0,~\n$ clingy2  &lt;dbl&gt; 1, 1, 1, 0, NA, 1, 1, 1, 0, 0, NA, 1, NA, 0, 0, NA, 2, 0,~\n$ kind2    &lt;dbl&gt; 2, 2, 2, 2, NA, 2, 2, 2, 2, 2, NA, 2, NA, 2, 2, NA, 1, 2,~\n$ lies2    &lt;dbl&gt; 1, 0, 0, 0, NA, 0, 1, 0, 1, 0, NA, 1, NA, 0, 0, NA, 1, 0,~\n$ bullied2 &lt;dbl&gt; 0, 0, 0, 0, NA, 0, 2, 0, 0, 0, NA, 0, NA, 0, 0, NA, 0, 0,~\n$ helpout2 &lt;dbl&gt; 1, 1, 1, 2, NA, 2, 2, 1, 2, 1, NA, 2, NA, 2, 1, NA, 0, 2,~\n$ reflect2 &lt;dbl&gt; 1, 1, 2, 1, NA, 2, 1, 2, 1, 2, NA, 1, NA, 2, 1, NA, 1, 2,~\n$ steals2  &lt;dbl&gt; 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, NA, 2, NA, 0, 0, NA, 0, 0,~\n$ oldbest2 &lt;dbl&gt; 0, 0, 1, 0, NA, 1, 0, 1, 1, 0, NA, 1, NA, 0, 0, NA, 0, 0,~\n$ afraid2  &lt;dbl&gt; 0, 1, 0, 0, NA, 0, 0, 0, 0, 0, NA, 2, NA, 0, 0, NA, 0, 0,~\n$ attends2 &lt;dbl&gt; 1, 1, 2, 0, NA, 2, 2, 2, 2, 1, NA, 1, NA, 2, 2, NA, 1, 1,~\nPer analizzare solo gli item che misurano i Sintomi Emotivi, è conveniente creare un nuovo data frame.\nitems_emotion &lt;- c(\"somatic\", \"worries\", \"unhappy\", \"clingy\", \"afraid\")\nsdq_emo &lt;- SDQ[, items_emotion]\nsdq_emo |&gt;\n    head()\n\n\nA tibble: 6 x 5\n\n\nsomatic\nworries\nunhappy\nclingy\nafraid\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n2\n1\n0\n1\n0\n\n\n2\n0\n0\n1\n0\n\n\n0\n0\n0\n0\n1\n\n\n0\n0\n0\n1\n1\n\n\n2\n1\n0\n1\n0\n\n\n1\n0\n0\n1\n0\nAffrontiamo il problema dei dati mancanti come discusso in precedenza.\nsdq_emo &lt;- sdq_emo %&gt;%\n    mutate_at(vars(somatic:afraid), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)) |&gt; round()\nEsaminiamo le modalità di ciascun item:\nemotional_symptoms &lt;- c(\"somatic\", \"worries\", \"unhappy\", \"clingy\", \"afraid\")\nresult &lt;- lapply(emotional_symptoms, function(x) sort(unique(sdq_emo[[x]])))\nresult |&gt; print()\n\n[[1]]\n[1] 0 1 2\n\n[[2]]\n[1] 0 1 2\n\n[[3]]\n[1] 0 1 2\n\n[[4]]\n[1] 0 1 2\n\n[[5]]\n[1] 0 1 2\nTrasformiamo il data frame in una matrice.\nM &lt;- sdq_emo |&gt; as.matrix()\nImplementiamo lo scaling ottimale con la funzione corAspect().\nopt &lt;- corAspect(M, aspect = \"aspectSum\", level = \"ordinal\")\nParametri principali della funzione:\nNel caso delle categorie di risposta del SDQ (“non vero”, “parzialmente vero”, “assolutamente vero”), queste riflettono chiaramente un ordine crescente di accordo. Vogliamo preservare questo ordine durante l’ottimizzazione, quindi impostiamo level=\"ordinal\".\nEsaminiamo il risultato ottenuto.\nattributes(opt) |&gt; print()\n\n$names\n [1] \"loss\"      \"catscores\" \"cormat\"    \"eigencor\"  \"indmat\"    \"scoremat\" \n [7] \"data\"      \"burtmat\"   \"niter\"     \"call\"     \n\n$class\n[1] \"aspect\"\nsummary(opt)\n\n\nCorrelation matrix of the scaled data:\n        somatic worries unhappy clingy afraid\nsomatic   1.000   0.339   0.369  0.254  0.313\nworries   0.339   1.000   0.468  0.397  0.378\nunhappy   0.369   0.468   1.000  0.367  0.454\nclingy    0.254   0.397   0.367  1.000  0.378\nafraid    0.313   0.378   0.454  0.378  1.000\n\n\nEigenvalues of the correlation matrix:\n[1] 2.497 0.757 0.634 0.610 0.502\n\nCategory scores:\nsomatic:\n    score\n0 -0.901\n1  0.586\n2  1.972\n\nworries:\n    score\n0 -0.854\n1  0.445\n2  2.096\n\nunhappy:\n    score\n0 -0.601\n1  1.393\n2  2.659\n\nclingy:\n    score\n0 -1.199\n1  0.237\n2  1.617\n\nafraid:\n    score\n0 -0.769\n1  1.009\n2  1.951\nQuesto approccio offre un metodo rigoroso per ottimizzare la misurazione degli item, migliorando la qualità psicometrica della scala e assicurando che l’interpretazione delle risposte rifletta al meglio la coerenza interna del test.\nI punteggi ottenuti si ottengono nel modo seguente:\nopt$scoremat\n\n\nA matrix: 228 x 5 of type dbl\n\n\n\nsomatic\nworries\nunhappy\nclingy\nafraid\n\n\n\n\n1\n1.972\n0.445\n-0.601\n0.237\n-0.769\n\n\n2\n1.972\n-0.854\n-0.601\n0.237\n-0.769\n\n\n3\n-0.901\n-0.854\n-0.601\n-1.199\n1.009\n\n\n4\n-0.901\n-0.854\n-0.601\n0.237\n1.009\n\n\n5\n1.972\n0.445\n-0.601\n0.237\n-0.769\n\n\n6\n0.586\n-0.854\n-0.601\n0.237\n-0.769\n\n\n7\n-0.901\n0.445\n1.393\n1.617\n-0.769\n\n\n8\n-0.901\n-0.854\n-0.601\n-1.199\n-0.769\n\n\n9\n0.586\n-0.854\n-0.601\n-1.199\n-0.769\n\n\n10\n-0.901\n-0.854\n-0.601\n-1.199\n1.009\n\n\n11\n-0.901\n-0.854\n-0.601\n-1.199\n-0.769\n\n\n12\n1.972\n2.096\n1.393\n0.237\n1.951\n\n\n13\n-0.901\n-0.854\n-0.601\n-1.199\n1.951\n\n\n14\n-0.901\n0.445\n-0.601\n1.617\n-0.769\n\n\n15\n0.586\n2.096\n1.393\n1.617\n1.009\n\n\n16\n1.972\n-0.854\n-0.601\n0.237\n1.009\n\n\n17\n0.586\n0.445\n-0.601\n1.617\n1.009\n\n\n18\n0.586\n0.445\n-0.601\n-1.199\n-0.769\n\n\n19\n0.586\n2.096\n2.659\n1.617\n1.009\n\n\n20\n0.586\n0.445\n1.393\n1.617\n1.009\n\n\n21\n0.586\n-0.854\n-0.601\n-1.199\n-0.769\n\n\n22\n1.972\n0.445\n-0.601\n-1.199\n1.009\n\n\n23\n1.972\n2.096\n2.659\n0.237\n1.951\n\n\n24\n-0.901\n0.445\n1.393\n0.237\n1.009\n\n\n25\n0.586\n2.096\n-0.601\n-1.199\n1.951\n\n\n26\n1.972\n2.096\n2.659\n1.617\n1.009\n\n\n27\n-0.901\n-0.854\n-0.601\n-1.199\n-0.769\n\n\n28\n0.586\n-0.854\n-0.601\n1.617\n-0.769\n\n\n29\n0.586\n-0.854\n1.393\n0.237\n-0.769\n\n\n30\n-0.901\n-0.854\n-0.601\n0.237\n-0.769\n\n\n...\n...\n...\n...\n...\n...\n\n\n199\n0.586\n-0.854\n-0.601\n0.237\n-0.769\n\n\n200\n0.586\n-0.854\n-0.601\n0.237\n1.009\n\n\n201\n0.586\n0.445\n-0.601\n0.237\n1.009\n\n\n202\n-0.901\n-0.854\n-0.601\n-1.199\n-0.769\n\n\n203\n-0.901\n2.096\n-0.601\n-1.199\n-0.769\n\n\n204\n0.586\n0.445\n1.393\n-1.199\n1.009\n\n\n205\n0.586\n-0.854\n-0.601\n0.237\n-0.769\n\n\n206\n0.586\n-0.854\n-0.601\n0.237\n-0.769\n\n\n207\n-0.901\n-0.854\n-0.601\n0.237\n-0.769\n\n\n208\n0.586\n-0.854\n-0.601\n1.617\n-0.769\n\n\n209\n-0.901\n0.445\n-0.601\n0.237\n-0.769\n\n\n210\n-0.901\n-0.854\n-0.601\n-1.199\n-0.769\n\n\n211\n-0.901\n0.445\n-0.601\n-1.199\n-0.769\n\n\n212\n-0.901\n-0.854\n-0.601\n-1.199\n-0.769\n\n\n213\n-0.901\n-0.854\n-0.601\n-1.199\n-0.769\n\n\n214\n0.586\n2.096\n1.393\n1.617\n1.951\n\n\n215\n-0.901\n0.445\n-0.601\n-1.199\n-0.769\n\n\n216\n-0.901\n-0.854\n-0.601\n0.237\n-0.769\n\n\n217\n-0.901\n-0.854\n-0.601\n0.237\n1.009\n\n\n218\n0.586\n-0.854\n-0.601\n-1.199\n-0.769\n\n\n219\n-0.901\n-0.854\n-0.601\n1.617\n-0.769\n\n\n220\n0.586\n-0.854\n-0.601\n-1.199\n1.009\n\n\n221\n0.586\n-0.854\n1.393\n0.237\n1.009\n\n\n222\n-0.901\n-0.854\n-0.601\n-1.199\n-0.769\n\n\n223\n-0.901\n-0.854\n-0.601\n-1.199\n-0.769\n\n\n224\n-0.901\n0.445\n-0.601\n-1.199\n-0.769\n\n\n225\n0.586\n-0.854\n-0.601\n-1.199\n1.009\n\n\n226\n-0.901\n0.445\n-0.601\n0.237\n-0.769\n\n\n227\n-0.901\n0.445\n-0.601\n-1.199\n-0.769\n\n\n228\n1.972\n0.445\n1.393\n0.237\n1.009\nEsaminiamo la relazione tra lo scoring basato sul metodo Likert con lo scoring ottimale.\nplot(opt$scoremat[, 1], sdq_emo$somatic)\nplot(opt$scoremat[, 4], sdq_emo$clingy)\nplot(opt$scoremat[, 3], sdq_emo$unhappy)\nplot(opt$scoremat[, 2], sdq_emo$worries)\nplot(opt$scoremat[, 5], sdq_emo$afraid)\nGuardando ai grafici ottenuti, si può notare che 1) i punteggi per le categorie successive aumentano quasi linearmente; 2) le categorie sono approssimativamente equidistanti. Concludiamo che per la valutazione degli item ordinali nella scala dei Sintomi Emotivi del SDQ, la scala Likert è appropriata, e l’ottimizzazione della scala rispetto alla semplice scala Likert di base produce cambiamenti minimi. Per altri dati, comunque, la situazione potrebbe essere molto diversa.\nIn conclusione, l’ottimizzazione dello scoring dei dati di questionari ordinali offre un metodo rigoroso per ottimizzare la misurazione degli item, migliorando la qualità psicometrica della scala e assicurando che l’interpretazione delle risposte rifletta al meglio la coerenza interna del test.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E2_optimal_scoring.html#ottimizzazione-dello-scoring-dei-dati-di-questionari-ordinali",
    "href": "chapters/measurement/E2_optimal_scoring.html#ottimizzazione-dello-scoring-dei-dati-di-questionari-ordinali",
    "title": "4  ✏️ Esercizi",
    "section": "",
    "text": "data\nQuesto argomento rappresenta il data frame che contiene i dati da analizzare. Nel nostro caso, si tratta degli item relativi alla scala che stiamo studiando (ad esempio, quelli che misurano i Sintomi Emotivi).\naspect\nQuesto parametro specifica il criterio da ottimizzare. Per impostazione predefinita, aspect=\"aspectSum\" massimizza la somma delle correlazioni tra gli item. Questo criterio è utile per migliorare la consistenza interna della scala, ad esempio incrementando l’alfa di Cronbach. Nel nostro caso, utilizziamo questa impostazione predefinita.\nlevel\nQuesto argomento definisce il livello di misura delle variabili analizzate:\n\nnominal (impostazione predefinita): suppone che le variabili rappresentino categorie nominali. In questo caso, non vi sono restrizioni sui punteggi risultanti.\n\nordinal: richiede che l’ordine dei punteggi venga preservato.\n\nnumerical: oltre a preservare l’ordine, richiede che le distanze tra i punteggi siano uguali.\n\n\n\n\n\n\n\nPunteggi ottimali per ogni item:\nLa funzione calcola i punteggi “ottimali” per ogni item, ovvero valori che massimizzano la somma delle correlazioni tra gli item. Questo migliora la coerenza interna della scala.\nPreservazione dell’ordine delle risposte:\nUtilizzando level=\"ordinal\", i punteggi ottimizzati mantengono l’ordine crescente delle categorie di risposta, come ad esempio:\n\n“non vero” &lt; “parzialmente vero” &lt; “assolutamente vero”.\nCiò assicura che la struttura ordinata delle risposte venga rispettata.\n\nCorrelazioni e punteggi trasformati:\nL’output include:\n\nLa matrice di correlazione dei punteggi trasformati, ovvero le correlazioni tra gli item dopo la scalatura ottimale.\n\nLe correlazioni possono essere confrontate con quelle calcolate sulle variabili originali utilizzando la funzione cor(items).\n\nAutovalori della matrice di correlazione:\nL’output mostra anche gli autovalori della matrice di correlazione, che rappresentano le varianze delle componenti principali (da un’Analisi delle Componenti Principali, PCA).\n\nGli autovalori sono utili per determinare il numero di dimensioni misurate dal set di item.\n\nAd esempio, se il primo autovalore è significativamente più grande degli altri, ciò suggerisce che gli item misurano una sola dimensione, come ci si aspettava.\n\nPunteggi delle categorie:\nLa funzione mostra i punteggi assegnati a ciascuna categoria di risposta dopo la scalatura ottimale. Ad esempio, per l’item somatic, i risultati potrebbero indicare:\n\n“non vero” → -0.886\n\n“parzialmente vero” → 0.584\n\n“assolutamente vero” → 2.045\n\nQuesti punteggi sono scelti in modo da:\n\nAvere una media pari a 0 nel campione analizzato.\n\nMassimizzare le correlazioni tra gli item, migliorando la coerenza interna della scala.\n\nGrafici delle trasformazioni:\nIl pacchetto aspect offre grafici utili che mostrano visivamente l’assegnazione dei punteggi alle categorie. Questi grafici aiutano a interpretare il risultato della scalatura ottimale in modo intuitivo.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E2_optimal_scoring.html#session-info",
    "href": "chapters/measurement/E2_optimal_scoring.html#session-info",
    "title": "4  ✏️ Esercizi",
    "section": "4.2 Session Info",
    "text": "4.2 Session Info\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] aspect_1.0-6      MASS_7.3-61       viridis_0.6.5     viridisLite_0.4.2\n [5] ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0  \n [9] bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19    \n[13] psych_2.4.6.26    scales_1.3.0      markdown_1.13     knitr_1.49       \n[17] lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n[21] purrr_1.0.2       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n[25] ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.17.1   jsonlite_1.8.9      magrittr_2.0.3     \n  [4] TH.data_1.1-2       estimability_1.5.1  farver_2.1.2       \n  [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n [10] Cairo_1.6-2         minqa_1.2.8         base64enc_0.1-3    \n [13] rstatix_0.7.2       htmltools_0.5.8.1   broom_1.0.7        \n [16] Formula_1.2-5       htmlwidgets_1.6.4   plyr_1.8.9         \n [19] sandwich_3.1-1      emmeans_1.10.5      zoo_1.8-12         \n [22] uuid_1.2-1          igraph_2.1.1        mime_0.12          \n [25] lifecycle_1.0.4     pkgconfig_2.0.3     Matrix_1.7-1       \n [28] R6_2.5.1            fastmap_1.2.0       shiny_1.9.1        \n [31] digest_0.6.37       OpenMx_2.21.13      fdrtool_1.2.18     \n [34] colorspace_2.1-1    rprojroot_2.0.4     Hmisc_5.2-0        \n [37] fansi_1.0.6         timechange_0.3.0    abind_1.4-8        \n [40] compiler_4.4.2      withr_3.0.2         glasso_1.11        \n [43] htmlTable_2.4.3     backports_1.5.0     carData_3.0-5      \n [46] ggsignif_0.6.4      corpcor_1.6.10      gtools_3.9.5       \n [49] tools_4.4.2         pbivnorm_0.6.0      foreign_0.8-87     \n [52] zip_2.3.1           httpuv_1.6.15       nnet_7.3-19        \n [55] glue_1.8.0          quadprog_1.5-8      promises_1.3.0     \n [58] nlme_3.1-166        lisrelToR_0.3       grid_4.4.2         \n [61] pbdZMQ_0.3-13       checkmate_2.3.2     cluster_2.1.6      \n [64] reshape2_1.4.4      generics_0.1.3      gtable_0.3.6       \n [67] tzdb_0.4.0          data.table_1.16.2   hms_1.1.3          \n [70] car_3.1-3           utf8_1.2.4          sem_3.1-16         \n [73] pillar_1.9.0        IRdisplay_1.1       rockchalk_1.8.157  \n [76] later_1.3.2         splines_4.4.2       cherryblossom_0.1.0\n [79] lattice_0.22-6      survival_3.7-0      kutils_1.73        \n [82] tidyselect_1.2.1    miniUI_0.1.1.1      pbapply_1.7-2      \n [85] airports_0.1.0      stats4_4.4.2        xfun_0.49          \n [88] qgraph_1.9.8        arm_1.14-4          stringi_1.8.4      \n [91] pacman_0.5.1        boot_1.3-31         evaluate_1.0.1     \n [94] codetools_0.2-20    mi_1.1              cli_3.6.3          \n [97] RcppParallel_5.1.9  IRkernel_1.3.2      rpart_4.1.23       \n[100] xtable_1.8-4        repr_1.1.7          munsell_0.5.1      \n[103] Rcpp_1.0.13-1       coda_0.19-4.1       png_0.1-8          \n[106] XML_3.99-0.17       parallel_4.4.2      usdata_0.3.1       \n[109] jpeg_0.1-10         lme4_1.1-35.5       mvtnorm_1.3-2      \n[112] openxlsx_4.2.7.1    crayon_1.5.3        openintro_2.5.0    \n[115] rlang_1.1.4         multcomp_1.4-26     mnormt_2.1.1",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E3_thurstone.html",
    "href": "chapters/measurement/E3_thurstone.html",
    "title": "5  ✏️ Esercizi",
    "section": "",
    "text": "5.1 Introduzione allo Scaling di Thurstone\nLo scaling di Thurstone, sviluppato da Louis Leon Thurstone nel 1931, è un approccio statistico che mira a modellare dati di ranking soggettivo. I dati di ranking soggettivo si producono quando le persone ordinano un insieme di elementi o stimoli secondo un criterio particolare. Questo tipo di dati è particolarmente utile quando è più semplice per i partecipanti esprimere una preferenza relativa piuttosto che stime quantitative precise.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E3_thurstone.html#introduzione-allo-scaling-di-thurstone",
    "href": "chapters/measurement/E3_thurstone.html#introduzione-allo-scaling-di-thurstone",
    "title": "5  ✏️ Esercizi",
    "section": "",
    "text": "5.1.1 Il Modello Thurstoniano\nIl modello Thurstoniano rappresenta un approccio statistico per analizzare e interpretare le preferenze o i ranking individuali rispetto a vari oggetti o stimoli. Questo modello si basa sull’idea che esista una scala latente, ovvero una dimensione non direttamente osservabile, attorno alla quale si distribuiscono i ranking individuali. In altre parole, ogni individuo assegna un punteggio ad ogni oggetto basandosi su criteri personali, ma queste valutazioni individuali sono influenzate da una percezione collettiva o aggregata che può essere descritta su una scala continua latente.\nIl principale obiettivo del modello Thurstoniano è di trasformare queste medie di ranking latenti aggregati, che esistono su una scala continua, in un ranking discreto che possiamo interpretare più facilmente. Per farlo, il modello si avvale di alcune ipotesi chiave:\n\nDistribuzione Gaussiana: Si assume che il ranking latente per ciascun oggetto possa essere descritto da una distribuzione gaussiana.\nMedia Differenziata, Varianza Costante: Il modello presuppone che le distribuzioni gaussiane dei ranking per ciascun oggetto differiscano tra loro solo per la media, mantenendo costante la varianza (scaling di Thurstone caso V). Questo implica che, sebbene gli oggetti possano avere livelli di preferenza medi diversi (alcuni potrebbero essere generalmente preferiti ad altri), la variabilità delle valutazioni (quanto le opinioni dei rispondenti differiscono tra loro) è la stessa per tutti gli oggetti.\n\nPer posizionare gli oggetti sulla scala di Thurstone, si procede nel seguente modo:\n\nSi calcola la proporzione di rispondenti che preferiscono un oggetto rispetto a ciascuno degli altri.\nSi determinano i corrispondenti percentile (z-scores) della distribuzione cumulativa normale, che ci dicono quante deviazioni standard un valore è distante dalla media.\nSi calcola la media di questi z-scores per ciascun oggetto.\n\nEsempio Pratico:\nImmaginiamo di avere tre oggetti: A, B e C. Dopo aver raccolto le preferenze, scopriamo che:\n\nIl 70% dei rispondenti preferisce A rispetto a B e C.\nIl 50% dei rispondenti preferisce B rispetto ad A, ma solo il 30% lo preferisce a C.\nIl 80% dei rispondenti preferisce C rispetto a B, ma solo il 50% lo preferisce ad A.\n\nTrasformando queste percentuali in z-scores, possiamo ottenere una misura della “distanza” di ciascun oggetto dalla media sulla scala latente. Mediando questi z-scores, possiamo creare un ranking discreto che riflette le preferenze medie aggregate, permettendoci di interpretare quali oggetti sono generalmente preferiti rispetto ad altri secondo il modello Thurstoniano.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E3_thurstone.html#studio-sulle-preferenze-riguardanti-le-caratteristiche-delloccupazione-ideale",
    "href": "chapters/measurement/E3_thurstone.html#studio-sulle-preferenze-riguardanti-le-caratteristiche-delloccupazione-ideale",
    "title": "5  ✏️ Esercizi",
    "section": "5.2 Studio sulle preferenze riguardanti le caratteristiche dell’occupazione ideale",
    "text": "5.2 Studio sulle preferenze riguardanti le caratteristiche dell’occupazione ideale\nI dati utilizzati in questo studio sono stati raccolti nell’ambito di una ricerca sulla motivazione lavorativa condotta da Ilke Inceoglu. Nel corso di questa indagine, 1079 partecipanti sono stati invitati a classificare nove aspetti lavorativi in base all’importanza che desideravano che fossero presenti nella loro occupazione ideale:\n\nAmbiente di Supporto (Supporto)\nLavoro Stimolante (Sfida)\nProgressione di Carriera (Carriera)\nLavoro Etico (Etica)\nControllo sul Lavoro, Impatto Personale (Autonomia)\nSviluppo (Sviluppo)\nInterazione Sociale (Interazione)\nAmbiente Competitivo (Competizione)\nAmbiente Piacevole e Sicuro (Sicurezza)\n\nUn punteggio di 1 attribuito a qualsiasi aspetto lavorativo indica che tale aspetto era il più importante per quel partecipante, mentre un punteggio di 9 indica che era il meno importante.\n\nJobFeatures &lt;- rio::import(\"../data/JobFeatures.txt\")\nglimpse(JobFeatures)\n\nRows: 1,079\nColumns: 9\n$ Support     &lt;int&gt; 8, 7, 5, 7, 1, 6, 5, 1, 1, 7, 6, 8, 5, 9, 8, 1, 6, 7, 4, 2~\n$ Challenge   &lt;int&gt; 3, 5, 8, 6, 4, 1, 4, 9, 3, 4, 2, 1, 4, 8, 6, 7, 4, 4, 1, 3~\n$ Career      &lt;int&gt; 4, 1, 1, 8, 8, 3, 7, 2, 7, 6, 3, 4, 6, 1, 3, 5, 8, 3, 5, 4~\n$ Ethics      &lt;int&gt; 5, 6, 9, 9, 3, 7, 2, 8, 4, 1, 9, 3, 7, 5, 9, 6, 7, 5, 9, 8~\n$ Autonomy    &lt;int&gt; 2, 2, 6, 3, 9, 8, 3, 7, 9, 8, 4, 6, 3, 7, 5, 2, 3, 8, 2, 1~\n$ Development &lt;int&gt; 6, 8, 2, 4, 2, 5, 6, 5, 2, 5, 1, 2, 2, 6, 1, 3, 1, 2, 6, 5~\n$ Interaction &lt;int&gt; 1, 3, 3, 2, 6, 2, 1, 4, 6, 9, 5, 5, 1, 4, 2, 8, 2, 6, 3, 6~\n$ Competition &lt;int&gt; 7, 9, 4, 5, 7, 4, 9, 6, 8, 3, 7, 7, 9, 2, 7, 9, 9, 1, 7, 9~\n$ Safety      &lt;int&gt; 9, 4, 7, 1, 5, 9, 8, 3, 5, 2, 8, 9, 8, 3, 4, 4, 5, 9, 8, 7~\n\n\nConsideriamo i dati del primo rispondente:\n\nJobFeatures[1, ]\n\n\nA data.frame: 1 x 9\n\n\n\nSupport\nChallenge\nCareer\nEthics\nAutonomy\nDevelopment\nInteraction\nCompetition\nSafety\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n8\n3\n4\n5\n2\n6\n1\n7\n9\n\n\n\n\n\nQuesto rispondente ha risposto assegnando la caratteristica più importante dell’impego a “Interaction”, seguita da “Autonomy”. L’ultima preferenza è “Safety”.\nEseguiamo lo scaling di Thurstone usando la funzione thurstone del pacchetto psych:\n\nscaling &lt;- psych::thurstone(JobFeatures, ranks = TRUE)\n\nGli attributi dell’oggetto scaling prodotto da thurstone() possono essere elencati nel modo seguente.\n\nattributes(scaling)\n\n\n    $names\n        \n'scale''GF''choice''residual''Call'\n\n    $class\n        \n'psych''thurstone'\n\n\n\n\nI risultati dello scaling si ottengono nel modo seguente. Sono elencati nell’ordine fornito sopra, ovvero Support, Challenge, Career, Ethics, Autonomy, Development, Interaction, Competition e Safety.\nUna media alta indica che i partecipanti attribuiscono un alto valore a questo aspetto lavorativo rispetto agli altri. Tuttavia, poiché le preferenze sono sempre relative, è impossibile identificare in maniera univoca tutte le medie. Pertanto, una delle medie deve essere fissata a un valore arbitrario. È consuetudine fissare la media dell’aspetto meno preferito a 0. Quindi, tutte le altre medie sono positive.\n\nscaling$scale |&gt; print()\n\n[1] 0.97 0.93 0.91 0.92 0.60 1.04 0.63 0.00 0.23\n\n\nLa media più bassa (0.0) corrisponde all’8° aspetto, Competizione, mentre la media più alta (1.04) corrisponde al 6° aspetto, Sviluppo. Ciò significa che l’ambiente competitivo era il meno desiderato, mentre le opportunità di sviluppo personale erano le più desiderate dalle persone nel loro lavoro ideale. Gli altri aspetti sono stati valutati come aventi un’importanza relativa intermedia a queste due, con Sicurezza che ha una media bassa (0.23) - appena superiore a 0 per la Competizione, mentre Supporto, Sfida, Carriera ed Etica hanno medie simili (intorno a 0.9). Autonomia e Interazione hanno medie moderate simili intorno a 0.6.\nL’istruzione seguente produce una matrice 9x9 contenente le proporzioni dei partecipanti nel campione che hanno preferito l’aspetto nella colonna rispetto all’aspetto nella riga. Nella matrice risultante, le righe e le colonne seguono l’ordine delle variabili nel file originale.\n\nscaling$choice |&gt; round(2)\n\n\nA matrix: 9 x 9 of type dbl\n\n\n0.50\n0.47\n0.47\n0.47\n0.36\n0.51\n0.36\n0.20\n0.23\n\n\n0.53\n0.50\n0.47\n0.49\n0.38\n0.53\n0.36\n0.17\n0.28\n\n\n0.53\n0.53\n0.50\n0.50\n0.38\n0.52\n0.39\n0.19\n0.26\n\n\n0.53\n0.51\n0.50\n0.50\n0.36\n0.52\n0.38\n0.19\n0.26\n\n\n0.64\n0.62\n0.62\n0.64\n0.50\n0.67\n0.51\n0.29\n0.34\n\n\n0.49\n0.47\n0.48\n0.48\n0.33\n0.50\n0.29\n0.15\n0.20\n\n\n0.64\n0.64\n0.61\n0.62\n0.49\n0.71\n0.50\n0.22\n0.30\n\n\n0.80\n0.83\n0.81\n0.81\n0.71\n0.85\n0.78\n0.50\n0.61\n\n\n0.77\n0.72\n0.74\n0.74\n0.66\n0.80\n0.70\n0.39\n0.50\n\n\n\n\n\nIl valore maggiore è\n\nmax(scaling$choice)\n\n0.852641334569045\n\n\nQuesto valore, 0.8526, rappresenta la proporzione di partecipanti che hanno preferito l’8° aspetto, Competizione, al 6° aspetto, Sviluppo, ed è il valore più grande nella matrice precedente: questa coppia di caratteristiche ha la preferenza più decisa per un aspetto rispetto all’altro.\nLa preferenza più decisa in termini di proporzioni di persone che scelgono un aspetto rispetto all’altro deve avere la maggiore distanza/differenza sulla scala delle preferenze soggettive (il 6° aspetto, Sviluppo, deve avere una preferenza percepita media molto più alta dell’8° aspetto, Competizione). Questo risultato è effettivamente in linea con i risultati per le medie di utilità, dove la media dello Sviluppo è la più alta con un valore di 1.04 e la Competizione è la più bassa con un valore di 0.\nConsideriamo i residui del modello:\n\nscaling$residual\n\n\nA matrix: 9 x 9 of type dbl\n\n\n0.000000000\n0.0133479521\n0.001732662\n0.0077151838\n-0.005872352\n0.020341170\n0.008084655\n-0.036657121\n0.007039854\n\n\n-0.013347952\n0.0000000000\n0.022201895\n0.0003878924\n-0.005625238\n0.011115950\n0.018649360\n0.009672656\n-0.040301391\n\n\n-0.001732662\n-0.0222018946\n0.000000000\n0.0073806440\n0.004543318\n0.028230409\n0.001106985\n-0.004628757\n-0.008671076\n\n\n-0.007715184\n-0.0003878924\n-0.007380644\n0.0000000000\n0.010887738\n0.027845520\n0.007400479\n-0.012284446\n-0.010646760\n\n\n0.005872352\n0.0056252384\n-0.004543318\n-0.0108877381\n0.000000000\n-0.005140288\n0.006785811\n-0.012984302\n0.017008932\n\n\n-0.020341170\n-0.0111159497\n-0.028230409\n-0.0278455199\n0.005140288\n0.000000000\n0.049380030\n0.002756144\n0.015651117\n\n\n-0.008084655\n-0.0186493603\n-0.001106985\n-0.0074004791\n-0.006785811\n-0.049380030\n0.000000000\n0.042051948\n0.041174300\n\n\n0.036657121\n-0.0096726558\n0.004628757\n0.0122844463\n0.012984302\n-0.002756144\n-0.042051948\n0.000000000\n-0.021145626\n\n\n-0.007039854\n0.0403013911\n0.008671076\n0.0106467596\n-0.017008932\n-0.015651117\n-0.041174300\n0.021145626\n0.000000000\n\n\n\n\n\nL’istruzione precedente produce una matrice 9x9 contenente le differenze tra le proporzioni osservate (la matrice delle scelte) e le proporzioni attese (proporzioni che preferiscono l’aspetto nella riga rispetto all’aspetto nella colonna, che sarebbe atteso in base alle distribuzioni normali standard delle preferenze soggettive intorno alle medie scalate come sopra). Gli scarti tra i valori attesi e quelli osservati sono il modo più diretto di misurare se un modello (in questo caso, il modello proposto da Thurstone) “si adatta” ai dati osservati. Gli scarti piccoli (vicini allo zero) indicano che ci sono piccole discrepanze tra le scelte osservate e le scelte previste dal modello; il che significa che il modello che abbiamo adottato è piuttosto buono.\nInfine, esaminiamo un indice di bontà di adattamento:\n\nscaling$GF\n\n0.998754828963899\n\n\nIl valore GF (Goodness of Fit) viene calcolato come 1 meno la somma dei residui al quadrato divisi per i valori osservati al quadrato. Quando i residui sono quasi zero, i loro rapporti al quadrato rispetto alle proporzioni osservate dovrebbero anch’essi avvicinarsi a zero. Di conseguenza, l’indice di bontà di adattamento di un modello ben adattato dovrebbe essere vicino a 1.\nNella nostra analisi, tutti i residui sono notevolmente piccoli, indicando una stretta corrispondenza tra le scelte osservate (proporzioni di preferenze per una caratteristica rispetto a un’altra). Questo allineamento preciso si riflette nell’indice GF, che è quasi 1, suggerendo che il modello di Thurstone cattura adeguatamente le proprietà dei dati relativi alle caratteristiche dell’occupazione ideale.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E3_thurstone.html#considerazioni-conclusive",
    "href": "chapters/measurement/E3_thurstone.html#considerazioni-conclusive",
    "title": "5  ✏️ Esercizi",
    "section": "5.3 Considerazioni conclusive",
    "text": "5.3 Considerazioni conclusive\nQuesta metodologia, introdotta da Louis Leon Thurstone negli anni ’20, rappresenta una delle forme più semplici e intuitive di scaling, dove per “scaling” si intende il processo di costruzione di un ordinamento di valori lungo un continuum psicologico. Lo scaling thurstoniano si basa sulla premessa che sia possibile ordinare stimoli o concetti secondo il grado in cui incarnano una certa proprietà psicologica, creando così una scala di misura che riflette le percezioni, le attitudini o i giudizi degli individui.\nUno degli aspetti centrali dello scaling di Thurstone, in particolare il caso V della sua legge del giudizio comparativo, è l’assunzione che le distribuzioni di ranking degli stimoli abbiano varianze uguali. Questa ipotesi, pur facilitando la modellizzazione matematica e l’interpretazione dei dati, è stata oggetto di critiche poiché difficilmente riscontrabile nella pratica. Le varianze possono variare significativamente tra gli stimoli a seconda della coerenza dei giudizi degli individui e della natura degli stimoli stessi. Questa limitazione ha stimolato lo sviluppo e l’adozione di metodi alternativi più flessibili per affrontare la complessità dello scaling psicologico.\nNel panorama contemporaneo, l’approccio più diffuso e metodologicamente avanzato per lo scaling psicologico deriva dalla Teoria della Risposta all’Item (IRT). L’IRT supera alcune delle limitazioni intrinseche allo scaling thurstoniano offrendo un quadro teorico e metodologico che considera la probabilità di una certa risposta a un item in funzione delle caratteristiche dell’item stesso e del livello dell’attributo psicologico del rispondente. Questo approccio permette di gestire in modo più efficace la varianza tra gli stimoli e di fornire stime più accurate delle proprietà psicometriche degli items e delle caratteristiche degli individui.\nIn conclusione, mentre lo scaling thurstoniano ha rappresentato un passo fondamentale nello sviluppo degli strumenti di misurazione in psicologia, l’evoluzione metodologica e teorica ha portato a preferire approcci basati sull’IRT. Questo non diminuisce il valore storico e didattico dello scaling di Thurstone, che continua a essere un esempio introduttivo prezioso per comprendere i concetti fondamentali dello scaling psicologico. Tuttavia, è nell’ambito della IRT che attualmente si trovano le soluzioni più robuste e sofisticate per affrontare le sfide della misurazione psicologica, guidando la ricerca e l’applicazione pratica nel campo della psicometria contemporanea.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/E3_thurstone.html#sesssion-info",
    "href": "chapters/measurement/E3_thurstone.html#sesssion-info",
    "title": "5  ✏️ Esercizi",
    "section": "5.4 Sesssion Info",
    "text": "5.4 Sesssion Info\n\nsessionInfo()\n\nR version 4.3.2 (2023-10-31)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.3.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] rio_1.0.1         ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2\n [5] ggpubr_0.6.0      ggExtra_0.10.1    bayesplot_1.11.0  gridExtra_2.3    \n [9] patchwork_1.2.0   semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-17    \n[13] psych_2.4.1       scales_1.3.0      markdown_1.12     knitr_1.45       \n[17] lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n[21] purrr_1.0.2       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n[25] ggplot2_3.4.4     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.4.1 nloptr_2.0.3      \n  [7] rmarkdown_2.25     vctrs_0.6.5        minqa_1.2.6       \n [10] base64enc_0.1-3    rstatix_0.7.2      htmltools_0.5.7   \n [13] broom_1.0.5        Formula_1.2-5      htmlwidgets_1.6.4 \n [16] plyr_1.8.9         sandwich_3.1-0     emmeans_1.10.0    \n [19] zoo_1.8-12         uuid_1.2-0         igraph_2.0.1.1    \n [22] mime_0.12          lifecycle_1.0.4    pkgconfig_2.0.3   \n [25] Matrix_1.6-5       R6_2.5.1           fastmap_1.1.1     \n [28] shiny_1.8.0        digest_0.6.34      OpenMx_2.21.11    \n [31] fdrtool_1.2.17     colorspace_2.1-0   rprojroot_2.0.4   \n [34] Hmisc_5.1-1        fansi_1.0.6        timechange_0.3.0  \n [37] abind_1.4-5        compiler_4.3.2     withr_3.0.0       \n [40] glasso_1.11        htmlTable_2.4.2    backports_1.4.1   \n [43] carData_3.0-5      R.utils_2.12.3     ggsignif_0.6.4    \n [46] MASS_7.3-60.0.1    corpcor_1.6.10     gtools_3.9.5      \n [49] tools_4.3.2        pbivnorm_0.6.0     foreign_0.8-86    \n [52] httpuv_1.6.14      zip_2.3.1          nnet_7.3-19       \n [55] R.oo_1.26.0        glue_1.7.0         quadprog_1.5-8    \n [58] promises_1.2.1     nlme_3.1-164       lisrelToR_0.3     \n [61] grid_4.3.2         pbdZMQ_0.3-11      checkmate_2.3.1   \n [64] cluster_2.1.6      reshape2_1.4.4     generics_0.1.3    \n [67] gtable_0.3.4       tzdb_0.4.0         R.methodsS3_1.8.2 \n [70] data.table_1.15.0  hms_1.1.3          car_3.1-2         \n [73] utf8_1.2.4         sem_3.1-15         pillar_1.9.0      \n [76] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [79] splines_4.3.2      lattice_0.22-5     survival_3.5-7    \n [82] kutils_1.73        tidyselect_1.2.0   miniUI_0.1.1.1    \n [85] pbapply_1.7-2      stats4_4.3.2       xfun_0.42         \n [88] qgraph_1.9.8       arm_1.13-1         stringi_1.8.3     \n [91] boot_1.3-28.1      evaluate_0.23      codetools_0.2-19  \n [94] mi_1.1             cli_3.6.2          RcppParallel_5.1.7\n [97] IRkernel_1.3.2     rpart_4.1.23       xtable_1.8-4      \n[100] repr_1.1.6         munsell_0.5.0      Rcpp_1.0.12       \n[103] coda_0.19-4.1      png_0.1-8          XML_3.99-0.16.1   \n[106] parallel_4.3.2     ellipsis_0.3.2     jpeg_0.1-10       \n[109] lme4_1.1-35.1      mvtnorm_1.2-4      openxlsx_4.2.5.2  \n[112] crayon_1.5.2       rlang_1.1.3        multcomp_1.4-25   \n[115] mnormt_2.1.1",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/02_development.html",
    "href": "chapters/measurement/02_development.html",
    "title": "6  Sviluppo dello strumento",
    "section": "",
    "text": "6.1 Introduzione\nLo sviluppo di un buon test psicologico non è semplice come potrebbe sembrare a prima vista. Si tratta di un processo articolato in più fasi, che richiede generalmente un notevole investimento di tempo, ricerca e, aspetto fondamentale, la disponibilità di partecipanti disposti a sottoporsi al test. Questo capitolo offre una panoramica del processo di sviluppo del test distinguendo quattro fasi principali: la concettualizzazione del test, la definizione della sua struttura e formato, la pianificazione delle standardizzazioni e degli studi psicometrici, e l’implementazione del piano (Reynolds & Livingston, 2021).",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sviluppo dello strumento</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/02_development.html#fasi-di-sviluppo",
    "href": "chapters/measurement/02_development.html#fasi-di-sviluppo",
    "title": "6  Sviluppo dello strumento",
    "section": "6.2 Fasi di sviluppo",
    "text": "6.2 Fasi di sviluppo\n\n6.2.1 Identificazione del costrutto\nLo sviluppo di un test psicologico o educativo è un processo che inizia con la chiara identificazione di una necessità specifica all’interno del campo. È imperativo che lo sviluppatore del test determini il costrutto che desidera misurare e dimostri la necessità di un nuovo metodo di misurazione. Con l’ampia varietà di test psicologici disponibili, diventa cruciale identificare una lacuna specifica o un bisogno non ancora soddisfatto.\nPrima di procedere con la creazione di un nuovo strumento di valutazione, è vitale condurre un’indagine approfondita per stabilire se esistano già misure valide e affidabili per il costrutto di interesse. Questo implica un’analisi meticolosa della letteratura scientifica e una revisione dei test psicometrici esistenti. Un tale approccio permette di scoprire se strumenti adeguati siano già disponibili, evitando così di duplicare inutilmente il lavoro già fatto. Inoltre, l’esistenza di misure preesistenti può servire da prezioso punto di riferimento per confrontare e validare il nuovo test in fase di sviluppo, assicurando che il nuovo strumento apporti un contributo significativo e unico al campo della psicologia e dell’educazione.\nCon il progredire della psicologia, nuovi costrutti vengono definiti e quelli esistenti vengono modificati. Ad esempio, la concezione dell’intelligenza è cambiata nel tempo, passando dalla misurazione del tempo di reazione e dell’acume sensoriale, all’enfasi sulla conoscenza, fino all’attuale enfasi sul problem solving in termini di intelligenza cristallizzata e fluida.\nIn alcuni casi, un clinico o ricercatore potrebbe avere la necessità di misurare una variabile ben definita, ma i test esistenti potrebbero essere di qualità dubbia o con qualità psicometriche obsolete. In tali situazioni, potrebbe essere necessario sviluppare un metodo di misurazione migliore o più esatto. Per esempio, le misurazioni del tempo di reazione, un tempo effettuate con l’osservazione umana e il cronometraggio, sono state sostituite da metodi elettronici molto più precisi.\nTuttavia, alcuni test possono valutare costrutti clinicamente utili ma essere impraticabili per l’applicazione clinica reale. Per esempio, la ricerca di sensazioni è stata misurata in modo lungo e complicato fino agli anni ’90, quando sono state sviluppate misurazioni rapide e affidabili che hanno reso il costrutto pratico per l’applicazione clinica.\nInoltre, i campioni standardizzati usati nei test possono diventare obsoleti e inapplicabili ai soggetti attuali, creando opportunità per lo sviluppo di nuovi test se persiste la necessità di misurare i costrutti in questione.\nNuovi costrutti vengono talvolta definiti anche nei campi della psicologia e dell’educazione. Questi costrutti, di solito derivati da osservazioni teoriche, devono essere studiati e manipolati per essere compresi, e ciò richiede regole per assegnare valori numerici alle osservazioni, ovvero la definizione stessa di un test. Di conseguenza, c’è sempre bisogno di nuovi strumenti di test, sia per modernizzare quelli esistenti sia per misurare nuovi costrutti. Gli sviluppatori e i ricercatori di test possono condurre ricerche letterarie dettagliate per determinare quali strumenti sono disponibili per valutare un costrutto e valutarne la qualità e l’applicabilità prima di stabilire un reale bisogno.\nIn conclusione, la decisione di sviluppare un nuovo test dovrebbe basarsi sulla domanda se questo migliorerebbe la pratica o la ricerca in un determinato campo, o se potrebbe migliorare la condizione umana o la nostra comprensione di essa.\nEsempio di Studio: Nello studio di Watson et al. (2007), una notevole parte dell’introduzione è dedicata all’analisi critica della letteratura esistente. Questa analisi mira a evidenziare i limiti degli strumenti di misurazione disponibili, esaminando le caratteristiche degli item utilizzati, la correlazione tra gli indicatori impiegati dagli strumenti esistenti e gli approcci teorici relativi alla depressione e all’ansia, nonché le soluzioni fattoriali risultanti dai dati raccolti attraverso tali strumenti. Viene inoltre considerata l’insufficiente esplorazione di alcune aree del costrutto da parte degli strumenti esistenti.\nWatson et al. (2007) si sono proposti di sviluppare uno strumento per la misurazione della depressione che superi i limiti di strumenti preesistenti come il Beck Depression Inventory-II (BDI-II; Beck, Steer, & Brown, 1996) e il Center for Epidemiological Studies Depression Scale (CES-D; Radloff, 1977). La scala sviluppata prende il nome di Inventory of Depression and Anxiety Symptoms (IDAS).\nPer rispondere alla prima questione, gli autori sottolineano che gli strumenti esistenti includono contenuti non specifici o non direttamente legati alla depressione. Sia il BDI-II sia il CES-D, per esempio, contengono item relativi a vari tipi di ansia, compromettendo così la loro validità discriminante. Gli strumenti esistenti non coprono inoltre l’intero dominio del costrutto della depressione maggiore come definito dal Diagnostic and Statistical Manual of Mental Disorders (4ª edizione). Presentano inoltre il limite di produrre un unico punteggio di severità dei sintomi, ignorando così l’eterogeneità e la multidimensionalità del fenomeno depressivo. Ciò si riflette nella struttura fattoriale poco chiara di tali strumenti, con diverse soluzioni fattoriali trovate da vari autori. L’IDAS, sviluppato da Watson et al. (2007), mira a superare queste difficoltà, creando sottoscale che riflettano direttamente gli aspetti distintivi della depressione.\nPer rispondere alla seconda questione, gli autori evidenziano come la depressione sia collocata all’interno di una rete nomologica di costrutti che include primariamente l’ansia. A differenza di strumenti preesistenti come BDI-II e CES-D, l’IDAS è stato progettato esplicitamente per creare scale che riflettano aspetti specifici della depressione, distinti dall’ansia. Questo è stato realizzato considerando un’ampia gamma di item che rappresentano sintomi associati all’ansia, al fine di esplorare la relazione tra i sintomi di ansia e quelli della depressione e creare scale distinte per queste dimensioni, aumentando così la validità discriminante dello strumento.\nInfine, per rispondere alla terza questione, Watson et al. (2007) dichiarano l’intento di sviluppare uno strumento che, nel suo punteggio complessivo, rifletta le caratteristiche generali della depressione e che, nelle sue sottoscale, misuri con precisione le varie dimensioni del costrutto esaminato.\n\n\n6.2.2 Obiettivo\nDopo aver identificato la necessità di un nuovo test per misurare un costrutto, è importante descrivere l’obiettivo primario nello sviluppo di una scala psicologica, gli usi previsti e le possibili interpretazioni dei risultati. Ci si dovrebbe chiedere: in quali contesti e per quali scopi verrà impiegato questo strumento? Quali interpretazioni dei risultati sono previste una volta che il test è stato somministrato e i risultati sono stati raccolti?\nLa risposta a queste domande dovrebbe scaturire logicamente dal passo precedente. Se risulta difficile rispondere, ciò potrebbe indicare che la concezione del test e dei costrutti da misurare è ancora troppo vaga. In tal caso, sarebbe opportuno ritornare al primo passo e sviluppare ulteriormente le idee e il concetto del test.\nComprendere come e in quale contesto un test può essere utilizzato è fondamentale per molti aspetti del suo sviluppo. Ad esempio, esistono numerosi test di personalità, ma ciascuno tende a enfatizzare aspetti diversi della personalità, delle emozioni e dell’affettività. Alcuni si concentrano sulla personalità normale, altri sugli stati psicopatologici. Il contesto (ad esempio, un ospedale psichiatrico rispetto alla selezione del personale) e l’uso previsto dei risultati determineranno contenuti e schemi interpretativi differenti.\nConoscere lo scopo del test e il contesto in cui verrà utilizzato influenza quindi la risposta a tutti gli altri fattori nel processo di sviluppo, dalla definizione dell’utente del test, al campione normativo appropriato, fino ai tipi di studi di validità necessari per convalidare le interpretazioni proposte dei punteggi.\n\n\n6.2.3 Utenti\nNello sviluppo di un test, è cruciale determinare chi lo utilizzerà e per quale motivo. I test dovrebbero essere progettati tenendo conto degli utenti specifici, ovvero individui che svolgono funzioni particolari facilitati dall’uso di test psicologici ed educativi. È importante considerare il tipo di formazione accademica formale e le esperienze supervisionate che potrebbero essere richieste agli utenti per applicare correttamente i risultati dei test. Ad esempio, per la maggior parte dei test psicometrici, esistono requisiti di formazione imposti dalle leggi che limitano l’uso dei test agli psicologi. Tuttavia, è anche possibile pensare di sviluppare un test che non sarà somministrato da psicologi.\nÈ anche utile, in questa fase, determinare quali individui in quali contesti troveranno il test proposto utile nel loro ruolo. Questo dovrebbe derivare direttamente dallo scopo del test e dalle interpretazioni proposte dei risultati. Ad esempio, se lo scopo del test è diagnosticare una condizione clinica come il disturbo bipolare pediatrico, l’utente target sarà probabilmente uno psicologo clinico, o forse un psichiatra. Tuttavia, un test progettato per esaminare un gran numero di bambini per verificare se presentano livelli di rischio elevati per disturbi emotivi e comportamentali potrebbe essere concepito in modo da poter essere somministrato e valutato da insegnanti o infermieri.\nLa conoscenza dell’utente previsto influenzerà le caratteristiche del test, in particolare riguardo alla complessità della sua somministrazione, valutazione e interpretazione, poiché diverse categorie di utenti possiedono diversi livelli di competenza.\n\n\n6.2.4 4. Definizione concettuale e operativa\nLa creazione di un test psicologico efficace inizia con l’identificazione e la definizione precisa del costrutto o della caratteristica psicologica che si intende misurare. Questo processo non è banale: spesso crediamo di comprendere pienamente costrutti come depressione, ansia, intelligenza cristallizzata, intelligenza fluida, aggressività, amabilità, fino a quando non tentiamo di esprimerli a parole. È in questo momento che possiamo renderci conto che la nostra comprensione del costrutto potrebbe non essere così chiara come inizialmente pensavamo.\nSi raccomanda di scrivere due tipi di definizioni: una concettuale e una operativa. Una definizione concettuale spiega il costrutto a livello teorico. Per esempio, una possibile definizione concettuale della depressione potrebbe essere: “La depressione è uno stato di malinconia, tristezza e bassi livelli di energia che porta all’anedonia, sentimenti di inutilità e stanchezza cronica.” Una definizione operativa, invece, dice esattamente come il nostro test definirà o misurerà il costrutto. Ad esempio: “Nella Scala di Valutazione della Depressione degli Studenti, la depressione sarà valutata sommando le valutazioni in direzione punteggiata su osservazioni di comportamento come espressioni di sentimenti di tristezza, sentimenti di solitudine, di sentirsi incompresi e non apprezzati, mancanza di coinvolgimento in attività piacevoli, troppo o troppo poco sonno, pianto in momenti inappropriati e lamentele di stanchezza.” Quindi, mentre la definizione concettuale ci dice ciò che vogliamo misurare in astratto, la definizione operativa ci informa in modo più diretto e specifico su come sarà derivato il punteggio del costrutto. Anche se questi sforzi possono sembrare tediosi per scale che hanno molti costrutti, più costrutti sono presenti in un test, più tali definizioni si rivelano utili.\nEsempio: Nel loro studio sulla depressione, Watson et al. (2007) si riferiscono al DSM-IV, che elenca nove criteri sintomatici per diagnosticare un episodio depressivo maggiore. Questi includono: (1) umore depresso per la maggior parte del giorno; (2) notevole riduzione di interesse o piacere per quasi tutte le attività; (3) variazione significativa del peso o dell’appetito; (4) insonnia o ipersonnia; (5) agitazione o rallentamento psicomotorio; (6) affaticamento o perdita di energia; (7) sentimenti di inutilità o eccessiva colpa; (8) difficoltà di concentrazione o indecisione; (9) pensieri di morte o ideazione suicidaria.\nPer ottimizzare l’utilità dell’Inventory of Depression and Anxiety Symptoms (IDAS), Watson et al. (2007) hanno incluso diversi item per ciascuno di questi nove criteri. Con l’intento di garantire una rappresentazione adeguata di ciascuna potenziale dimensione del costrutto, gli autori hanno inizialmente organizzato gli item in gruppi denominati homogeneous item composites (HIC). Tuttavia, hanno precisato che la formazione di questi HIC non implica necessariamente l’emergere di un fattore corrispondente, ma serve piuttosto a coprire l’intero dominio potenziale del costrutto di depressione.\n\n\n6.2.5 Dissimulazione\nDeterminare la necessità di misure per rilevare la dissimulazione. La dissimulazione è il comportamento di presentare se stessi in modo diverso dalla realtà. Per esempio, una persona che si sente triste ma non vuole ammetterlo potrebbe rispondere falsamente a una domanda di un test di personalità riguardo la tristezza per nascondere i propri sentimenti. La dissimulazione può verificarsi anche quando si valutano altre persone, ad esempio compilando una scala di valutazione per un bambino, coniuge o genitore anziano.\nLe persone possono impegnarsi nella dissimulazione per vari motivi, come negare sintomi in una valutazione di personalità o psicopatologia per non apparire con tratti indesiderati o comportamenti inaccettabili. Questo fenomeno si verifica anche tra coloro che cercano trattamento. Nell’ambito lavorativo, non è insolito che i candidati rispondano alle domande in modo da aumentare le possibilità di ottenere il lavoro.\nIn altri casi, le persone possono esagerare i sintomi per apparire più compromessi di quanto siano in realtà, un comportamento noto come simulazione, spesso motivato dal guadagno personale. Ad esempio, possono fingere problemi di personalità, comportamentali o cognitivi per ottenere benefici di invalidità, aumentare i risarcimenti in cause legali o evitare punizioni.\nNelle valutazioni cognitive, la dissimulazione può essere rilevata attraverso test di sforzo, ossia test che chiunque può completare correttamente se ci prova. Questi test sono raccomandati in quasi tutte le situazioni forensi e quando un paziente ha qualcosa da guadagnare fingendo deficit cognitivi.\nLa dissimulazione può manifestarsi in molti modi e talvolta si vedono definizioni più specifiche. Ad esempio, la validità dei sintomi si riferisce all’accuratezza o veridicità della presentazione comportamentale del soggetto, mentre il bias di risposta è un tentativo di ingannare l’esaminatore con risposte inesatte o incomplete. L’effort riguarda l’impegno nell’eseguire al meglio delle proprie capacità.\nCapire lo scopo del test, chi lo utilizzerà e in quali circostanze aiuta a determinare se includere scale per rilevare la dissimulazione. La psicologia ha sviluppato nel tempo metodi sofisticati per rilevare la dissimulazione, e le tecniche comuni verranno ora esaminate.\nLe scale più comuni per rilevare la dissimulazione (spesso note come scale di validità) nelle misure di personalità e comportamento includono le scale F, le scale L e gli indici di inconsistenza. Queste sono brevemente descritte qui sotto:\n\nScale F (Infrequency Scales): Queste scale sono progettate per rilevare la presentazione esagerata dei sintomi. Gli item delle scale F riflettono sintomi raramente segnalati anche da persone con livelli significativi di psicopatologia, o rappresentano approvazioni estreme di sintomi comuni che sono anch’essi rari. Poiché questi item rappresentano risposte infrequenti e scarsamente correlate tra loro, i soggetti che indicano di sperimentare un gran numero di questi sintomi forniscono probabilmente una presentazione esagerata della psicopatologia.\nScale L (Social Desirability Scales): Sono progettate per rilevare la negazione inaccurata di sintomi realmente presenti, rilevando il bias opposto della scala F. Le scale L includono item speciali che riflettono piccoli difetti comuni che quasi tutti sperimentano in qualche momento. Ad esempio, una risposta negativa a un item come “Talvolta mi sento triste” può indicare dissimulazione.\nScale di Inconsistenza: Sono progettate per rilevare incongruenze nelle risposte a item simili su scale di personalità e comportamento. Quando un rispondente non è coerente nel rispondere a item simili, i risultati non sono considerati affidabili. Per esempio, un rispondente che afferma sia “Sono pieno di energia” sia “Mi sento affaticato” potrebbe essere incoerente.\n\nPer alcuni tipi di test, possono essere utili altre scale di dissimulazione per scopi speciali. Ad esempio, nello sviluppo di misure per la selezione di personale, potrebbe essere utile includere indicatori più avanzati e sottili di bias di risposta.\nNonostante non siano misure di dissimulazione, gli autori dei test a volte includono anche scale che esaminano la comprensione degli item e il livello di cooperazione del soggetto nei test auto-somministrati e nelle scale di valutazione. Queste scale, a volte chiamate V-scale o scale di validità, contengono tipicamente item privi di senso dove la risposta è la stessa per tutti coloro che prendono il test seriamente e cooperano con il processo di esame.\nNelle valutazioni attitudinali e nei test di profitto, alcune persone potrebbero non impegnarsi adeguatamente nei test per ragioni di guadagno, un comportamento comunemente visto come simulazione. Tuttavia, ci sono altre ragioni plausibili, come la sindrome amotivazionale a seguito di infortuni cerebrali o la avolizione in disturbi psichiatrici come la schizofrenia.\nPochi test cognitivi hanno misure incorporate di dissimulazione o mancanza di sforzo. Tuttavia, alcuni costruiscono scale o item per rilevare la mancanza di sforzo. Spesso, la simulazione su misure cognitive è valutata attraverso modelli di prestazione indicativi di risposte non valide, incongruenze tra risultati del test e comportamenti osservati, e discrepanze tra i risultati dei test e le informazioni di background documentate.\nIn conclusione, le scale di dissimulazione sono strumenti essenziali nel processo di sviluppo dei test, aiutando a garantire che i risultati riflettano accuratamente le caratteristiche e le abilità del soggetto.\n\n\n6.2.6 Formato degli Item\nLa determinazione del formato degli item è un passo cruciale nello sviluppo di un test psicologico o educativo. Questa fase comporta la scelta della struttura e del formato degli item, i quali devono essere in linea con il costrutto da misurare e gli obiettivi specifici del test. I formati possono variare, includendo domande a scelta multipla, scale Likert o altri tipi di risposta. È importante sottolineare che la scelta del formato influisce sull’accuratezza e sull’affidabilità del test, richiedendo quindi un’attenta valutazione.\nIn questa fase, si devono considerare anche aspetti pratici come la modalità di somministrazione del test, che può essere individuale o di gruppo, e il formato del test, che può essere cartaceo o computerizzato. Un’ulteriore considerazione riguarda chi completerà effettivamente il test o il foglio delle risposte, che potrebbe essere l’esaminatore, l’esaminando o un informatore terzo, come nel caso delle scale di valutazione comportamentale.\nDifferenti tipi di item sono utili in diverse circostanze e possono misurare caratteristiche in modi diversi. Per esempio, la valutazione di sentimenti, pensieri e altri comportamenti non osservabili è solitamente meglio realizzata tramite autovalutazione. Una volta selezionato il formato di autovalutazione, è importante decidere il tipo di item da utilizzare, poiché esistono diverse opzioni di item per l’autovalutazione. Ad esempio, l’item “Mi sento triste” può variare significativamente a seconda del formato di risposta scelto, influenzando l’intento e l’interpretazione delle risposte.\nDopo aver scelto i formati degli item appropriati, è fondamentale scrivere esempi di item per ciascun formato che si prevede di utilizzare nel test. Questo include anche la redazione delle istruzioni per la somministrazione e la valutazione del test, che dovrebbero essere chiare e comprensibili anche per chi non è familiare con il test.\nInfine, è utile stimare il numero di item necessari per valutare in modo affidabile i costrutti. Come regola generale, si dovrebbe inizialmente scrivere almeno il doppio del numero di item che si prevede di utilizzare nel test finale. Questo perché molti item potrebbero essere scartati a causa di statistiche insufficienti, pregiudizi o ambiguità. La lunghezza del test dovrebbe essere adeguata alla popolazione bersaglio, tenendo conto della sensibilità di alcuni gruppi, come i giovani o gli anziani, al tempo richiesto per completare il test.\nUn altro aspetto fondamentale da considerare è chi completerà effettivamente il test o il foglio delle risposte. Questo può variare a seconda delle circostanze: potrebbe essere l’esaminatore, l’esaminato, o un terzo informatore, come nel caso delle scale di valutazione comportamentale, dove i rispondenti possono essere genitori, insegnanti o altre figure rilevanti. La scelta di chi completerà il test può influenzare non solo la logistica della somministrazione del test, ma anche la natura delle informazioni raccolte, e quindi la validità e l’utilità dei risultati.\nScala Likert. Una scala Likert è un tipo di scala ordinale che viene utilizzata per misurare gli atteggiamenti di una persona. Viene chiesto al rispondente di valutare il grado di accordo o disaccordo con un’affermazione utilizzando un’alternativa di risposta che di solito varia da cinque a sette punti. Tuttavia, poiché è una scala ordinale, le distanze tra i livelli della scala non sono quantificabili e non possiamo assumere che le differenze tra i livelli di risposta siano equidistanti. Pertanto, c’è una lunga controversia sulla possibilità di trattare i valori numerici di una scala ordinale come se provenissero da una scala ad intervalli. Alcuni autori ritengono problematico non potere trattare i dati provenienti da scale di tipo Likert come se fossero a livello di scala ad intervalli, mentre altri autori lo considerano giustificato in presenza di un’ampia numerosità campionaria e di una distribuzione approssimativamente normale dei dati. In ogni caso, la procedura che sta alla base delle scale Likert consiste nella somma dei punti attribuiti ad ogni singola domanda. I vantaggi della scala Likert sono la sua semplicità e applicabilità, mentre i suoi svantaggi sono il fatto che i suoi elementi vengono trattati come scale cardinali pur essendo ordinali e il fatto che il punteggio finale non rappresenta una variabile cardinale.\nItem a codifica inversa. In parole più semplici, ci sono alcune domande in un test che sono strettamente correlate in modo negativo con le altre domande e con il punteggio totale del test. Queste domande richiedono una risposta diversa rispetto alle altre domande. Ad esempio, in un questionario sull’ansia, una domanda potrebbe chiedere “Sono preoccupata” e la scala di risposta potrebbe essere “Per nulla”, “Un po’”, “Abbastanza” e “Moltissimo” con valori 1, 2, 3 e 4 rispettivamente. Tuttavia, un’altra domanda potrebbe chiedere “Mi sento bene” e la scala di risposta potrebbe essere la stessa, ma con valori 4, 3, 2 e 1 rispettivamente. Questo perché le proprietà contrarie si trovano sullo stesso continuum latente. Questo è importante nella costruzione di un test psicologico, dove è consigliato utilizzare sia domande orientate nella direzione del costrutto (chiamate “straight item”) sia nella direzione opposta (chiamate “reverse item”) per contrastare l’acquiescenza e ottenere risposte più accurate.\n\n\n6.2.7 Sviluppare una struttura del test\nLo sviluppo di una struttura per un test psicologico o educativo richiede passaggi ben definiti prima di procedere alla creazione dei singoli item della scala. Prima di tutto, è essenziale stabilire un’organizzazione strutturale del test, delineando gli obiettivi specifici che si intendono raggiungere attraverso la sua somministrazione. Questo passaggio comporta la definizione dei sottodomini o delle dimensioni che si vogliono misurare e di come questi contribuiranno alla valutazione complessiva del costrutto.\nUna questione fondamentale da risolvere è se il test produrrà un unico punteggio, come somma di tutte le risposte agli item, o se sarà necessario suddividerlo in sottoscale. La struttura del test è spesso determinata dai costrutti che si intendono misurare, quindi le definizioni precedentemente scritte sono di grande importanza in questa fase decisionale. Se il test include sottoscale o sotto-test, è importante considerare anche se ci saranno punteggi compositi che forniscono indici riassuntivi dei raggruppamenti dei sotto-test.\nNella fase di sviluppo del test, è cruciale spiegare come gli item e le sottoscale (se presenti) saranno organizzati. Se la logica alla base del test e la discussione sui costrutti sottostanti non rendono evidente la specifica organizzazione degli item e dei sotto-test, è importante affrontare il motivo per cui questa particolare organizzazione è la più appropriata. Naturalmente, è da tenere in considerazione che la ricerca condotta durante il processo di sviluppo del test potrebbe portare a modifiche della struttura intesa, in base ai dati effettivamente raccolti.\nEsempio: Nello studio di Watson et al. (2007), per garantire un campionamento esaustivo dell’intero ambito del costrutto, sono stati definiti 20 homogeneous item composites (HIC), che includono: Umore Depresso, Perdita di Interesse o Piacere, Disturbi dell’Appetito, Disturbi del Sonno, Problemi Psicomotori, Fatica/Anergia, Sentimenti di Inutilità o Colpa, Problemi Cognitivi, Ideazione Suicidaria, Senso di Speranza, Depressione Melanconica, Umore Arrabbiato/Irritabile, Alta Energia/Affetto Positivo, Umore Ansioso, Preoccupazione, Panico, Agorafobia, Ansia Sociale, Intrusioni Traumatiche, Sintomi Ossessivo-Compulsivi.\nDi questi, 13 HIC (comprendenti in totale 117 item) sono stati dedicati agli indicatori specifici della depressione. Nove di questi HIC (79 item in totale) coprono i sintomi fondamentali della depressione maggiore secondo il DSM-IV, tra cui umore depresso, perdita di interesse o piacere, disturbi dell’appetito, del sonno, problemi psicomotori, fatica/anergia, senso di inutilità e colpa, problemi cognitivi e ideazione suicidaria. I restanti quattro HIC trattano temi quali la disperazione (Hopelessness, secondo Abramson, Metalsky, & Alloy, 1989), sintomi specifici della depressione melanconica (Joiner et al., 2005), umore arrabbiato/irritabile (una forma alternativa di depressione negli adolescenti secondo il DSM-IV, American Psychiatric Association, 1994, p. 327), e indicatori di energia e affetto positivo (associati alla depressione secondo Mineka et al., 1998).\nI rimanenti sette HIC (63 item totali) sono stati introdotti per valutare sintomi associati all’ansia. Questi HIC includono categorie come umore ansioso, preoccupazione, panico, agorafobia, ansia sociale e intrusioni traumatiche associate al PTSD. Questa approfondita categorizzazione mira a una valutazione comprensiva e differenziata dei sintomi legati sia alla depressione sia all’ansia, fornendo così una visione più completa del panorama psicopatologico.\n\n\n6.2.8 Tabella delle Specifiche\nLa creazione di un test psicologico o educativo implica lo sviluppo di una Tabella delle Specifiche (TOS), che funge da “schema” per assicurare l’allineamento tra le definizioni dei costrutti, le definizioni operative e il contenuto del test stesso. La TOS è particolarmente utile nelle misure di rendimento, dove serve a garantire che il test sia in congruenza con un determinato curriculum o campo di studio. Questo strumento definisce in modo chiaro le aree di contenuto principali che il test intende esplorare, selezionate attraverso un’analisi accurata degli obiettivi educativi. Questo processo è fondamentale per assicurare che il contenuto del test sia direttamente collegato agli obiettivi di apprendimento e di valutazione, rendendo il test pertinente e focalizzato sui temi chiave.\nÈ importante che gli autori di test evitino di affidarsi eccessivamente a processi cognitivi di livello inferiore, come la memorizzazione meccanica, e che includano anche processi di livello superiore per garantire una valutazione equilibrata e completa delle capacità cognitive. Inoltre, la distribuzione degli item tra le varie aree di contenuto dovrebbe essere bilanciata per riflettere adeguatamente l’importanza di ciascuna area nel quadro complessivo del curriculum o dell’area di studio.\nLe TOS non sono utili solo per test di rendimento e attitudine, ma anche per quelli di personalità e comportamento. Ad esempio, nello sviluppo di un test sulla depressione, la TOS può aiutare a garantire che il test esamini accuratamente il vasto dominio dei sintomi depressivi, coprendo le diverse sfaccettature della depressione in proporzioni adeguate. La TOS può elencare le diverse aree o aspetti dei sintomi depressivi da valutare, includendo sia comportamenti osservabili sia pensieri e sentimenti interni, rilevanti per la diagnosi. In questo modo, la TOS funge da guida nella redazione degli item, assicurando che tutte le dimensioni rilevanti della depressione siano adeguatamente rappresentate nel test.\n\n\n6.2.9 Pool Iniziale degli Item\nCreare il pool iniziale di item. A questo punto, si procede a sviluppare una vasta gamma di item che coprano i diversi aspetti del costrutto di interesse. Questo pool iniziale di item dovrebbe essere variegato e ben bilanciato, rappresentando in modo adeguato la complessità del costrutto e i diversi livelli di abilità o atteggiamenti che si vogliono misurare. È importante anche scrivere le risposte corrette secondo ciò che si intende misurare e la direzione del punteggio del test.\n\n\n6.2.10 Revisione\nCondurre la revisione iniziale degli item (e apportare modifiche). Gli item raccolti nel pool iniziale vengono sottoposti a un esame attento da parte di esperti nel campo. Si valuta la pertinenza, la chiarezza, la coerenza e la validità dei singoli item. Sulla base dei feedback ricevuti dagli esperti, possono essere apportate modifiche o eliminati item problematici.\n\n\n6.2.11 Validazione\nNella fase di validazione di un test, l’implementazione di un ampio trial empirico degli item è un passaggio metodologico fondamentale. Tale processo prevede la somministrazione degli item a un campione rappresentativo della popolazione di riferimento. L’obiettivo è di valutare la discriminazione item-partecipante e di identificare eventuali anomalie o limitazioni strutturali della scala.\nLa definizione del campione target, ovvero la popolazione di riferimento per la quale il test è stato progettato, costituisce il primo passo nella pianificazione del campionamento. La selezione del campione di standardizzazione è cruciale in quanto determina il gruppo di riferimento per il confronto dei punteggi nei test normativi, oltre a stabilire le norme o i punteggi normativi del test. Questo è rilevante sia nei test normativi sia nei test basati su criteri, dove la performance delle popolazioni target è fondamentale nella definizione dei punteggi di taglio e nelle decisioni correlate alle prestazioni.\nUna volta identificata la popolazione target, si procede alla formulazione di un piano di campionamento. Idealmente, si ricercano campioni casuali veri della popolazione target, tuttavia, tale approccio è spesso impraticabile data l’impossibilità di conoscere e coinvolgere tutti i membri della popolazione. È quindi essenziale garantire la rappresentatività del campione attraverso un piano di campionamento stratificato proporzionale. Questo implica la definizione delle caratteristiche salienti (strati) della popolazione e la determinazione delle percentuali necessarie di soggetti con tali caratteristiche per assicurare la rappresentatività del campione.\nIn conclusione, il processo di campionamento deve essere accuratamente progettato per assicurare che il campione sia rappresentativo della popolazione target, superando le limitazioni pratiche e metodologiche incontrate nella selezione del campione stesso.\nNumero di soggetti. In ambito psicometrico non c’è un accordo univoco sulla dimensione del campione necessaria per condurre un’analisi fattoriale. Tuttavia, gli autori hanno fornito alcune indicazioni che possono essere utili come riferimento. Nunnally (1978) ha suggerito che il campione debba essere composto da almeno 10 soggetti per ogni item. Comrey e Lee (1992) hanno fornito una scala che valuta la qualità del campione in base alla dimensione: “molto scarsa” per 50, “scarso” per 100, “sufficiente” per 200, “buona” per 300, “molto buona” per 500 e “eccellente” per 1.000 o più. Altri autori hanno suggerito come regola generale di avere almeno 300 casi per l’analisi fattoriale (Tabachnick e Fidell, 2001). In ogni caso, è importante tenere presente che la scelta della dimensione del campione dipende anche dalla complessità del costrutto che si intende analizzare e dalla qualità degli item utilizzati nel test.\n\n\n6.2.12 Analisi degli item\nI dati raccolti dal test di campo vengono analizzati utilizzando metodi statistici adeguati. Questo processo mira a identificare item che non funzionano correttamente, che mostrano una bassa discriminazione o che potrebbero causare distorsioni nelle risposte. Gli item che superano questa fase sono considerati per la versione finale della scala.\n\n\n6.2.13 Revisione degli item\nSulla base dei risultati dell’analisi, gli item della scala possono essere rivisti o sostituiti, al fine di migliorarne l’accuratezza, la coerenza e l’affidabilità.\nNumero di item. La lunghezza del test dovrebbe essere adatta al suo scopo. Ad esempio, un test per valutare le abilità degli studenti delle scuole primarie non dovrebbe richiedere più di 30 minuti per essere completato, perché l’affaticamento e la noia possono influire sui risultati. Lo stesso vale per un test di personalità per adulti. In generale, un test dovrebbe essere il più breve possibile, ma deve raggiungere un livello accettabile di validità. Come regola generale, Kline (1986) suggerisce di avere almeno 50 domande nella versione finale del test.\n\n\n6.2.14 Calcolo dell’Affidabilità\nLa consistenza interna della scala viene valutata tramite il calcolo dell’affidabilità, ad esempio utilizzando il coefficiente alpha di Cronbach. Questo passaggio assicura che gli item scelti per la scala si correlino tra loro in modo coerente, riflettendo così la coerenza delle misure.\n\n\n6.2.15 Seconda Somministrazione\nUna volta apportate le revisioni agli item, viene eseguita una seconda somministrazione per confermare l’efficacia delle modifiche e per valutare l’affidabilità della versione rivista della scala su un nuovo campione.\n\n\n6.2.16 Ripetere i Passaggi Precedenti\nSe durante la seconda somministrazione emergono ancora problemi o se l’affidabilità della scala non raggiunge i livelli desiderati, è necessario ripetere i passaggi precedenti fino a raggiungere una versione della scala che soddisfi gli standard di qualità e affidabilità.\n\n\n6.2.17 Studi di validazione\nPer garantire la validità di una scala, è fondamentale condurre studi di validazione che dimostrino la capacità della scala di misurare con precisione il costrutto di interesse. Questi studi possono includere l’analisi delle relazioni tra i punteggi della scala e altre misure correlate, nonché il confronto tra gruppi con differenze note nel costrutto.\nCome per gli studi di affidabilità, è cruciale pianificare in anticipo gli studi di validità, assicurandosi che siano concettualizzati e progettati in modo da consentire la valutazione dell’adeguatezza delle interpretazioni proposte dei punteggi del test una volta completato. In linea con quanto discusso nel Capitolo 5, ciascuna delle cinque categorie fondamentali di evidenza di validità deve essere affrontata, pur variando l’enfasi e il livello di dettaglio a seconda dei costrutti valutati, delle interpretazioni proposte dei punteggi e degli scopi per i quali i punteggi saranno applicati.\nAd esempio, nei test di intelligenza progettati per predire il successo accademico, dovrebbero essere sottolineati e resi prioritari gli studi predittivi basati su criteri specifici che rappresentino il successo accademico, quali il tasso di laurea o i punteggi ACT. In maniera simile, i test destinati alla selezione del personale richiedono studi accurati sulla capacità predittiva dei punteggi in relazione al successo lavorativo, il quale dovrebbe essere definito in anticipo dall’ente o dall’individuo che utilizza il test. È importante riconoscere che la definizione di successo lavorativo può variare ampiamente; pertanto, è necessario chiarire e specificare le definizioni dei criteri utilizzati.\nPer una misura della psicopatologia dello sviluppo, ad esempio quella finalizzata a perfezionare l’accuratezza diagnostica nei Disturbi dello Spettro Autistico (ASD), l’accento dovrebbe essere posto sulla capacità dei punteggi del test di discriminare tra gruppi di soggetti diagnosticati (indipendentemente dal test) con ciascuno dei disturbi rilevanti. Gli studi di validità frequentemente si concentrano soltanto sulla distinzione tra gruppi diagnosticati e soggetti non diagnosticati o normodotati; tuttavia, questo approccio di ricerca risulta spesso di limitata utilità, in quanto la maggior parte dei test è in grado di differenziare tra normalità e patologia grazie ad un’alta affidabilità dei punteggi e a campioni di ampia dimensione. La sfida effettiva consiste nel distinguere tra le diverse diagnosi all’interno di un campione specifico, come ad esempio differenziare bambini con ASD da quelli con ADHD o depressione.\nIn definitiva, è fondamentale che gli studi di validità si concentrino sulle interpretazioni proposte dei punteggi e sulle loro applicazioni intenzionali. Non esistono limiti a ciò che può essere progettato come studi di validità; sono limitati solo dalla creatività del ricercatore e dalla loro conoscenza del costrutto e delle teorie pertinenti che incarnano il costrutto misurato.\n\n\n6.2.18 Linee Guida\nNel processo di sviluppo di un test, la fase finale si concentra sull’elaborazione di linee guida dettagliate, fondamentali per garantire una somministrazione appropriata e un’accurata valutazione dei punteggi. Questo passaggio si rivela cruciale per assicurare che il test sia utilizzato nel modo previsto e che i risultati siano interpretati correttamente.\nLe istruzioni per partecipare allo studio devono essere chiare e concise, fornendo un’idea generale degli obiettivi della ricerca e dei trattamenti previsti. I partecipanti devono essere informati dei benefici prevedibili e dei rischi, e della libertà di scegliere di non partecipare. Inoltre, la privacy dei partecipanti è protetta dalla legge sulla protezione dei dati personali e i loro dati verranno raccolti e conservati in forma anonima, tranne che per il nominativo. I partecipanti possono esercitare i propri diritti di protezione dei dati personali e interrompere la partecipazione in qualsiasi momento. Alla fine dello studio, i partecipanti possono ricevere i risultati della ricerca e possono rivolgersi al Comitato Etico dell’Università degli Studi di Firenze per segnalare qualsiasi problema. Prima di partecipare, i partecipanti devono firmare una dichiarazione di consenso informato per accettare di partecipare alla ricerca e di autorizzare il trattamento dei loro dati personali.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sviluppo dello strumento</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/02_development.html#considerazioni-finali",
    "href": "chapters/measurement/02_development.html#considerazioni-finali",
    "title": "6  Sviluppo dello strumento",
    "section": "6.3 Considerazioni Finali",
    "text": "6.3 Considerazioni Finali\nQuesto capitolo ha offerto un quadro pratico, così come discuso da Reynolds & Livingston (2021), riguardante le tappe cruciali e gli elementi fondamentali implicati nel processo di creazione di un test. Una particolare attenzione è stata rivolta alle fasi iniziali di concettualizzazione, vitali per la riuscita del progetto. Tali fasi includono l’identificazione della necessità di sviluppare un nuovo test e la formulazione di definizioni sia concettuali che operative dei costrutti da valutare.\nAbbiamo anche messo in evidenza l’importanza di delineare anticipatamente gli scopi specifici per cui il test è stato progettato, le modalità di interpretazione dei risultati e il pubblico target che ne farà uso. Questo approccio preliminare aiuta a definire chiaramente il contesto e le aspettative relative all’uso del test.\nUn altro aspetto trattato riguarda la stesura di una descrizione esauriente del test, che comprende la creazione di una tabella delle specifiche o di uno schema del test. Questo strumento si rivela essenziale per guidare sistematicamente lo sviluppatore attraverso le varie fasi di creazione del test, assicurando che tutti gli aspetti cruciali siano presi in considerazione.\n\n\n\n\nReynolds, C. R., & Livingston, R. (2021). Mastering modern psychological testing. Springer.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sviluppo dello strumento</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/03_equating.html",
    "href": "chapters/measurement/03_equating.html",
    "title": "7  Equating nei Test Psicologici",
    "section": "",
    "text": "7.1 Introduzione\nL’equating è una procedura statistica utilizzata quando somministrazioni su più occasioni e a diversi gruppi di esaminandi possono portare a una sovraesposizione degli item, compromettendo la sicurezza del test. Per limitare l’esposizione degli item, si possono utilizzare forme alternative del test. Tuttavia, l’uso di forme multiple genera scale di punteggio diverse, che misurano il costrutto di interesse a livelli di difficoltà differenti. L’obiettivo dell’equating è quindi correggere queste differenze di difficoltà tra le forme del test, rendendo le scale di punteggio comparabili.\nL’equating è il processo che stabilisce una relazione statistica tra le distribuzioni dei punteggi di diverse forme di un test e, di conseguenza, tra le scale di punteggio associate. Questo permette di rendere confrontabili i punteggi, anche se derivano da forme diverse dello stesso test. Quando le forme del test sono costruite seguendo le stesse specifiche (ad esempio, misurano lo stesso costrutto con uguale affidabilità) e hanno caratteristiche statistiche simili (difficoltà comparabile, distribuzione dei punteggi analoga), questa relazione è definita come funzione di equating. La funzione di equating consente di tradurre i punteggi da una scala all’altra, trattandole come scale parallele secondo la teoria classica dei test.\nSe, invece, le forme del test differiscono in aspetti significativi, come la lunghezza o il contenuto, la relazione tra le scale viene definita linking, e non equating. In questo caso, le scale risultano correlate ma non parallele né interscambiabili, poiché non garantiscono piena equivalenza nei punteggi. La relazione tra queste scale è descritta da una funzione di linking.\nPer ottenere una funzione di equating valida, devono essere soddisfatti alcuni requisiti fondamentali:\nIn sintesi, si può dire che l’equating si applica a scale parallele nel senso della teoria classica dei test (stessa media della distribuzione, identica varianza, medesima affidabilità, misurazione dello stesso costrutto, struttura e difficoltà degli item pressoché identiche), mentre il linking si utilizza per scale non parallele, che possono essere comparabili solo in modo parziale (lunghezza differente, contenuti parzialmente diversi, variabilità non perfettamente sovrapponibile, difficoltà degli item non completamente omogenee).",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Equating nei Test Psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/03_equating.html#introduzione",
    "href": "chapters/measurement/03_equating.html#introduzione",
    "title": "7  Equating nei Test Psicologici",
    "section": "",
    "text": "Le forme del test devono misurare lo stesso costrutto con la stessa affidabilità.\nI risultati dell’equating devono garantire equità, ovvero ogni esaminando dovrebbe ottenere lo stesso risultato indipendentemente dal fatto che gli venga somministrata la forma \\(X\\) o la forma \\(Y\\) (Lord, 1980).\nLa funzione di equating deve essere simmetrica: se un punteggio è tradotto da una scala all’altra, il processo inverso deve restituire il punteggio originale.\nLa funzione deve essere invariabile rispetto alla popolazione degli esaminandi: deve funzionare nello stesso modo per diversi gruppi di persone.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Equating nei Test Psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/03_equating.html#progettazione-dellequating",
    "href": "chapters/measurement/03_equating.html#progettazione-dellequating",
    "title": "7  Equating nei Test Psicologici",
    "section": "7.2 Progettazione dell’Equating",
    "text": "7.2 Progettazione dell’Equating\nLe procedure di equating e linking dei punteggi osservati richiedono dati raccolti in diverse somministrazioni del test. Il disegno di equating specifica come le forme del test e i gruppi di esaminandi differiscono tra le somministrazioni. Per semplicità, i disegni di equating possono essere classificati in tre tipi principali: gruppo unico, gruppi equivalenti e gruppi non equivalenti. Le forme del test vengono costruite in base al tipo di gruppo considerato.\n\n7.2.1 Disegno a gruppo unico\nIn questo disegno, un unico gruppo, campionato dalla popolazione target \\(T\\), sostiene due forme del test \\(X\\) e \\(Y\\). È possibile bilanciare l’ordine di somministrazione (metà del gruppo sostiene \\(X\\) prima di \\(Y\\) e l’altra metà viceversa). Eventuali differenze nelle distribuzioni dei punteggi tra \\(X\\) e \\(Y\\) vengono attribuite esclusivamente alle forme del test, poiché si assume che l’abilità del gruppo resti costante.\n\n\n7.2.2 Disegno a gruppi equivalenti\nQuesto disegno prevede che due campioni casuali della popolazione target \\(T\\) sostengano rispettivamente \\(X\\) e \\(Y\\). Anche in questo caso, si assume che l’abilità dei due gruppi sia costante, e le differenze nei punteggi riflettono le differenze di difficoltà tra le forme del test.\n\n\n7.2.3 Disegno a gruppi non equivalenti\nQuando i gruppi non sono equivalenti, sorgono due problemi:\n\nLa popolazione target deve essere definita indirettamente, utilizzando campioni da due popolazioni di esaminandi, \\(P\\) e \\(Q\\).\nLe differenze di abilità tra i gruppi devono essere considerate, poiché possono confondere la stima delle differenze di difficoltà tra le forme del test.\n\nPer affrontare questi problemi, si utilizza un anchor test, \\(V\\), un test comune somministrato a entrambi i gruppi per controllare o eliminare le differenze di abilità. In alternativa, si possono usare covariate esterne, come i punteggi ottenuti in altri test.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Equating nei Test Psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/03_equating.html#funzioni-di-equating",
    "href": "chapters/measurement/03_equating.html#funzioni-di-equating",
    "title": "7  Equating nei Test Psicologici",
    "section": "7.3 Funzioni di Equating",
    "text": "7.3 Funzioni di Equating\nLe procedure di equating utilizzano funzioni matematiche per mappare i punteggi di una forma del test su una scala comune. Queste funzioni possono essere classificate come lineari (ad esempio, identità, media e regressione lineare) o non lineari (ad esempio, equipercentile). Ogni tipo di funzione si basa su assunzioni specifiche riguardo le differenze di difficoltà tra le forme del test.\n\n7.3.1 Funzione di identità\nLa funzione di identità si applica quando le forme del test hanno la stessa scala. In questo caso, i punteggi non vengono modificati, e la relazione tra \\(X\\) e \\(Y\\) è semplicemente:\n\\[\nid_Y(x) = x.\n\\]\n\n\n7.3.2 Funzione della media\nNell’equating basato sulla media, si assume che la forma \\(X\\) differisca dalla forma \\(Y\\) per un valore costante lungo l’intera scala dei punteggi. Ad esempio, se la forma \\(X\\) è più facile di 2 punti rispetto alla forma \\(Y\\) per gli esaminandi con punteggi alti, si assume che sia ugualmente più facile di 2 punti anche per quelli con punteggi bassi. Sebbene questa ipotesi di differenza costante possa risultare troppo restrittiva in molte situazioni di testing, l’equating basato sulla media è utile per illustrare alcuni concetti fondamentali dell’equating.\nLa funzione basata sulla media calcola le differenze di difficoltà tra le due forme utilizzando la differenza tra le medie dei punteggi, \\(\\mu_Y - \\mu_X\\). La funzione è espressa come:\n\\[\nmean_Y(x) = x + (\\mu_Y - \\mu_X).\n\\]\n\n\n7.3.3 Funzione Lineare\nA differenza della funzione basata sulla media, che assume una differenza costante tra due forme del test, l’equating lineare consente che le differenze di difficoltà tra le forme \\(X\\) e \\(Y\\) varino lungo la scala dei punteggi. Ad esempio, l’equating lineare permette che la forma \\(X\\) sia più difficile della forma \\(Y\\) per esaminandi con punteggi bassi, ma meno difficile per esaminandi con punteggi alti.\nNell’equating lineare, i punteggi che si trovano a una distanza standardizzata equivalente (in unità di deviazione standard) dalle rispettive medie vengono resi uguali. In questo modo, l’equating lineare tiene conto sia delle differenze nei valori medi, sia delle differenze nelle unità di scala (deviazioni standard) delle due forme. Definendo \\(\\sigma_X\\) e \\(\\sigma_Y\\) come le deviazioni standard dei punteggi nelle forme \\(X\\) e \\(Y\\), l’equazione di conversione lineare si basa sull’uguaglianza degli z-score tra le due forme, espressa come:\n\\[\n\\frac{x - \\mu_X}{\\sigma_X} = \\frac{y - \\mu_Y}{\\sigma_Y}.\n\\]\nRisolvendo questa equazione per \\(y\\), si ottiene la funzione di equating lineare:\n\\[\nlin_Y(x) = \\frac{\\sigma_Y}{\\sigma_X}x + \\left(\\mu_Y - \\frac{\\sigma_Y}{\\sigma_X}\\mu_X\\right),\n\\]\ndove il coefficiente angolare (\\(\\text{slope}\\)) è dato da \\(\\frac{\\sigma_Y}{\\sigma_X}\\) e l’intercetta (\\(\\text{intercept}\\)) è \\(\\mu_Y - \\frac{\\sigma_Y}{\\sigma_X}\\mu_X\\).\nSe le deviazioni standard delle due forme (\\(\\sigma_X\\) e \\(\\sigma_Y\\)) fossero uguali, la funzione lineare si ridurrebbe alla funzione basata sulla media. Questo dimostra che la funzione lineare generalizza quella basata sulla media, includendo la possibilità di differenze nella dispersione dei punteggi tra le forme.\n\n\n7.3.4 Equipercentile Equating\nL’equating equipercentile è una tecnica avanzata utilizzata nei casi in cui le differenze tra le forme di un test non possono essere descritte da una relazione lineare. A differenza dell’equating lineare, che assume una variazione proporzionale lungo tutta la scala dei punteggi, l’equipercentile permette di modellare differenze complesse utilizzando una curva che descrive le variazioni di difficoltà tra le forme.\nAd esempio, con l’equipercentile, la forma \\(X\\) potrebbe risultare più difficile della forma \\(Y\\) per i punteggi estremamente alti e bassi, ma meno difficile per i punteggi intermedi. Questa flessibilità rende il metodo equipercentile più generale rispetto a quello lineare.\nLa funzione di equating è definita come equipepercentile quando la distribuzione dei punteggi della forma \\(X\\), convertita sulla scala della forma \\(Y\\), risulta identica alla distribuzione dei punteggi della forma \\(Y\\) nella popolazione. In altre parole, i punteggi di \\(X\\) vengono mappati sui punteggi di \\(Y\\) in modo che i ranghi percentili corrispondano tra le due forme.\nIl processo di sviluppo della funzione equipercentile consiste nell’identificare, per ciascun punteggio di \\(X\\), il punteggio di \\(Y\\) che ha lo stesso rango percentile. Questo metodo garantisce che la relazione tra le due scale tenga conto delle differenze lungo l’intera distribuzione, offrendo una maggiore precisione nei casi in cui i punteggi non si distribuiscono in modo simile tra le due forme del test.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Equating nei Test Psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/03_equating.html#equating-dei-test-basato-sulla-ctt",
    "href": "chapters/measurement/03_equating.html#equating-dei-test-basato-sulla-ctt",
    "title": "7  Equating nei Test Psicologici",
    "section": "7.4 Equating dei Test basato sulla CTT",
    "text": "7.4 Equating dei Test basato sulla CTT\nNelle sezioni successive, applicheremo i metodi di equating basati sulla Classical Test Theory (CTT) descritti in precedenza. Come esempio, analizzeremo il caso di un disegno a gruppi equivalenti utilizzando i dati forniti dal set ACTmath, disponibile nel pacchetto equate. Questi dati derivano da due somministrazioni del test di matematica ACT (Kolen e Brennan, 2014). Il dataset è organizzato in un data frame con tre colonne: la prima rappresenta la scala dei punteggi (da 0 a 40 punti), mentre la seconda e la terza colonna riportano il numero di esaminandi delle forme \\(X\\) e \\(Y\\) che hanno ottenuto ciascun punteggio.\nNel disegno a gruppi equivalenti, un campione casuale di esaminandi sostiene la forma \\(X\\), mentre un secondo campione casuale sostiene la forma \\(Y\\). Poiché entrambi i campioni sono estratti in modo casuale dalla stessa popolazione, eventuali differenze nella distribuzione dei punteggi sono attribuite esclusivamente alle differenze di difficoltà tra le due forme del test. La randomizzazione garantisce, infatti, che i due gruppi siano equivalenti in termini di abilità valutata.\nProcediamo ora all’estrazione dei dati per le forme \\(X\\) e \\(Y\\), ottenendo le rispettive distribuzioni di frequenza.\n\nform_x &lt;- as.freqtab(ACTmath[, 1:2])\nform_y &lt;- as.freqtab(ACTmath[, c(1, 3)])\n\n\nhead(form_x)\n\n\nA data.frame: 6 x 2\n\n\n\ntotal\ncount\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n0\n0\n\n\n2\n1\n1\n\n\n3\n2\n1\n\n\n4\n3\n3\n\n\n5\n4\n9\n\n\n6\n5\n18\n\n\n\n\n\n\nhead(form_y)\n\n\nA data.frame: 6 x 2\n\n\n\ntotal\ncount\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n0\n0\n\n\n2\n1\n1\n\n\n3\n2\n3\n\n\n4\n3\n13\n\n\n5\n4\n42\n\n\n6\n5\n59\n\n\n\n\n\nEsaminiamo le statistiche descrittive.\n\nrbind(x = summary(form_x), y = summary(form_y))\n\n\nA data.frame: 2 x 7\n\n\n\nmean\nsd\nskew\nkurt\nmin\nmax\nn\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nx\n19.9\n8.21\n0.375\n2.30\n1\n40\n4329\n\n\ny\n19.0\n8.94\n0.353\n2.15\n1\n40\n4152\n\n\n\n\n\nEsaminiamo le distribuzioni dei punteggi. Il metodo di equating basato sulla CTT richiede che le due forme del test abbiano la stessa distribuzione.\n\nplot(x = form_x, main = \"Bar plot of the test scores on form X\")\n\n\n\n\n\n\n\n\n\nplot(x = form_y, main = \"Bar plot of the test scores on form Y\")",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Equating nei Test Psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/03_equating.html#metodi-di-equating-lineare",
    "href": "chapters/measurement/03_equating.html#metodi-di-equating-lineare",
    "title": "7  Equating nei Test Psicologici",
    "section": "7.5 Metodi di Equating Lineare",
    "text": "7.5 Metodi di Equating Lineare\nI metodi di equating lineare comprendono diverse funzioni: identità (ossia, nessun equating), media, lineare semplice e nonlineare.\n\n7.5.1 Equating Basato sulla Media\nQuesto metodo corregge i punteggi della nuova forma (X) utilizzando la differenza tra le medie delle due forme (X e Y) come costante fissa. In altre parole, il punteggio sulla scala di X viene traslato di un valore pari alla differenza delle medie, per compensare eventuali discrepanze di difficoltà tra le due forme.\nTrasformiamo i punteggi X con il metodo di equating basato sulla media usando la funzione equate().\n\nmean_yx &lt;- equate(form_x, form_y, type = \"mean\")\nmean_yx\n\n\nMean Equating: form_x to form_y \n\nDesign: equivalent groups \n\nSummary Statistics:\n   mean   sd skew kurt  min  max    n\nx  19.9 8.21 0.38 2.30 1.00 40.0 4329\ny  19.0 8.94 0.35 2.15 1.00 40.0 4152\nyx 19.0 8.21 0.38 2.30 0.13 39.1 4329\n\nCoefficients:\nintercept     slope        cx        cy        sx        sy \n   -0.873     1.000    20.000    20.000    40.000    40.000 \n\n\nPossiamo anche consultare la tabella di concordanza per verificare come i punteggi della forma \\(X\\) siano cambiati dopo essere stati equiparati alla forma \\(Y\\). La tabella di concordanza indica che un esaminando che ha ottenuto un punteggio di 1 sulla forma \\(X\\) si aspetterebbe di ottenere un punteggio di 1-0.873 sulla forma \\(Y\\). In pratica, si tratta di una semplice trasformazione: il punteggio della forma \\(X\\) viene incrementato di -0.873.\n\nhead(mean_yx$concordance)\n\n\nA data.frame: 6 x 2\n\n\n\nscale\nyx\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n0\n-0.873\n\n\n2\n1\n0.127\n\n\n3\n2\n1.127\n\n\n4\n3\n2.127\n\n\n5\n4\n3.127\n\n\n6\n5\n4.127\n\n\n\n\n\nPossiamo unire la tabella di concordanza alla forma \\(X\\) in modo da visualizzare insieme sia i punteggi originali che quelli equiparati per ogni esaminando.\n\n# Save the concordance table\nform_yx &lt;- mean_yx$concordance\n\n# Rename the first column to total\ncolnames(form_yx)[1] &lt;- \"total\"\n\n# Merge the concordance table to form x\ndata_xy &lt;- merge(form_x, form_yx)\nhead(data_xy)\n\n\nA data.frame: 6 x 3\n\n\n\ntotal\ncount\nyx\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n0\n0\n-0.873\n\n\n2\n1\n1\n0.127\n\n\n3\n2\n1\n1.127\n\n\n4\n3\n3\n2.127\n\n\n5\n4\n9\n3.127\n\n\n6\n5\n18\n4.127\n\n\n\n\n\nSia form_x che form_xy sono tabelle di contingenza.\n\nform_yx |&gt; head()\n\n\nA data.frame: 6 x 2\n\n\n\ntotal\nyx\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n0\n-0.873\n\n\n2\n1\n0.127\n\n\n3\n2\n1.127\n\n\n4\n3\n2.127\n\n\n5\n4\n3.127\n\n\n6\n5\n4.127\n\n\n\n\n\n\nform_x |&gt; head()\n\n\nA data.frame: 6 x 2\n\n\n\ntotal\ncount\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n0\n0\n\n\n2\n1\n1\n\n\n3\n2\n1\n\n\n4\n3\n3\n\n\n5\n4\n9\n\n\n6\n5\n18\n\n\n\n\n\nPer ottenere i dati grezzi, prima le trasformiamo in un oggetto data.frame.\n\nform_x_df &lt;- form_x |&gt; as.data.frame()\nform_yx_df &lt;- form_yx |&gt; as.data.frame()\n\nPoi ricostruiamo i dati grezzi.\n\n# Ricostruire i dati grezzi per form_x\nraw_data_x &lt;- rep(form_x_df$total, form_x_df$count)\n\n# Ricostruire i dati grezzi per form_yx (usando i punteggi trasformati yx)\nraw_data_yx &lt;- rep(form_yx_df$yx, form_x_df$count) # Assumendo che i conteggi siano identici\n\nI dati trasformati X sono semplicemente una tralazione lineare dei dati originari.\n\nplot(raw_data_x, raw_data_yx, type = 'l')\n\n\n\n\n\n\n\n\n\n\n7.5.2 Equating Lineare\nQuesto metodo corregge i punteggi della nuova forma (\\(X\\)) utilizzando sia la differenza delle medie che quella delle deviazioni standard tra le due forme (\\(X\\) e \\(Y\\)), applicando un’equazione lineare.\n\nlinear_yx &lt;- equate(form_x, form_y, type = \"linear\")\nlinear_yx\n\n\nLinear Equating: form_x to form_y \n\nDesign: equivalent groups \n\nSummary Statistics:\n   mean   sd skew kurt   min  max    n\nx  19.9 8.21 0.38 2.30  1.00 40.0 4329\ny  19.0 8.94 0.35 2.15  1.00 40.0 4152\nyx 19.0 8.94 0.38 2.30 -1.54 40.9 4329\n\nCoefficients:\nintercept     slope        cx        cy        sx        sy \n    -2.63      1.09     20.00     20.00     40.00     40.00 \n\n\n\nplot(linear_yx)\n\n\n\n\n\n\n\n\nPossiamo anche eseguire un bootstrap per il processo di equating e creare un grafico degli errori standard:\n\nlinear_yx_boot &lt;- equate(form_x, form_y, type = \"linear\", boot = TRUE, reps = 5)\nplot(linear_yx_boot, out = \"se\")\n\n\n\n\n\n\n\n\n\n\n7.5.3 Equating Non Lineare\nI metodi di equating non lineare includono l’equipercentile, l’arco circolare e le funzioni composte.\n\n7.5.3.1 Equating Equipercentile\nL’equipercentile funziona suddividendo i punteggi delle forme \\(X\\) e \\(Y\\) in percentili. Successivamente, i percentili della forma \\(X\\) vengono abbinati ai percentili della forma \\(Y\\).\nAd esempio, possiamo applicare l’equipercentile con il seguente comando:\n\nequi_yx &lt;- equate(form_x, form_y, type = \"equipercentile\")\n\nOra possiamo verificare come i punteggi di \\(X\\) siano stati corretti in base ai ranghi percentili delle forme \\(X\\) e \\(Y\\). Il grafico seguente mostra che le maggiori discrepanze tra queste due funzioni si trovano alle estremità della distribuzione:\n\nplot(equi_yx$concordance$yx ~ equi_yx$concordance$scale,\n    type = \"p\",\n    xlab = \"Punteggi Forma X\",\n    ylab = \"Punteggi corretti su Forma Y\"\n)\npoints(linear_yx$concordance$yx ~ linear_yx$concordance$scale, pch = 4)\n\n\n\n\n\n\n\n\n\n\n7.5.3.2 Smoothing Loglineare\nPossiamo anche aggiungere uno smoothing loglineare al processo e confrontarlo con il metodo equipercentile standard. La linea curva indica l’equipercentile con smussamento loglineare, mentre la linea dritta (con un punto di rottura) rappresenta l’equipercentile standard.\nQuesto confronto consente di osservare come il processo di smussamento renda la relazione tra le due forme più fluida e meno sensibile alle variazioni estreme.\n\nequismooth_yx &lt;- equate(form_x, form_y, type = \"equipercentile\", smooth = \"loglin\", degree = 3)\n\n# Compare equating functions\nplot(equi_yx, equismooth_yx, addident = FALSE)\n\n\n\n\n\n\n\n\nIn questo caso, lo smoothing non ha praticamente effetto.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Equating nei Test Psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/03_equating.html#gruppi-non-equivalenti",
    "href": "chapters/measurement/03_equating.html#gruppi-non-equivalenti",
    "title": "7  Equating nei Test Psicologici",
    "section": "7.6 Gruppi Non Equivalenti",
    "text": "7.6 Gruppi Non Equivalenti\nIn un disegno a gruppi non equivalenti, non si può assumere che la popolazione di esaminandi che sostiene le diverse forme del test provenga dalla stessa popolazione. Pertanto, è necessario tenere conto delle potenziali differenze nelle loro abilità relative al costrutto misurato. Quando si utilizza un disegno a gruppi non equivalenti, è indispensabile specificare un metodo di equating adeguato.\nGeneralmente, questi metodi stabiliscono una relazione tra i punteggi totali della forma \\(X\\) e della forma \\(Y\\) attraverso i punteggi ottenuti su un insieme comune di item presenti in entrambe le forme (noti come anchor items) e la creazione di una popolazione sintetica ponderata. Per maggiori informazioni su questi metodi, si rimanda ad Albano (2016).\nI metodi di equating disponibili nel pacchetto equate per i gruppi non equivalenti includono:\n\nTucker\nNominal\nLevine true score\nBraun/Holland\nFrequency\nChained\n\nPer un approfondimento, si rimanda al manuale del pacchetto equate.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Equating nei Test Psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/03_equating.html#equating-basato-sulla-teoria-della-risposta-allitem",
    "href": "chapters/measurement/03_equating.html#equating-basato-sulla-teoria-della-risposta-allitem",
    "title": "7  Equating nei Test Psicologici",
    "section": "7.7 Equating Basato sulla Teoria della Risposta all’Item",
    "text": "7.7 Equating Basato sulla Teoria della Risposta all’Item\nL’equating basato sulla Item Response Theory (IRT) rappresenta un approccio avanzato per confrontare e rendere comparabili punteggi di diverse forme di un test. Questo metodo sfrutta la capacità della IRT di modellare la relazione tra le risposte degli esaminandi e le caratteristiche degli item. I metodi IRT di equating si dividono in due categorie principali:\n\nEquating attraverso item comuni\nEquating attraverso persone comuni\n\nIn entrambi i casi, gli item comuni o le persone comuni fungono da ponte tra le due forme del test, consentendo di stabilire una scala comune per il confronto dei punteggi.\n\n7.7.1 Equating Attraverso Item Comuni\nL’equating basato su item comuni utilizza item identici inclusi in entrambe le forme del test. Questi item condivisi, detti item ancorati (anchor items), permettono di calibrare le due forme su una scala comune. Le applicazioni tipiche includono:\n\nEquating tramite costanti di equating\nIn questo approccio, si utilizzano costanti per collegare i parametri degli item delle due forme. Queste costanti sono derivate dagli item comuni, che aiutano a compensare le differenze tra le due forme del test.\nEquating tramite curve caratteristiche del test\nLe curve caratteristiche del test (TCC, Test Characteristic Curves) rappresentano la relazione tra i punteggi latenti (\\(\\theta\\)) e i punteggi osservati. Con questo metodo, si utilizzano le TCC delle due forme per stabilire una corrispondenza tra i punteggi.\nEquating tramite calibrazione simultanea\nLa calibrazione simultanea prevede di stimare i parametri degli item di entrambe le forme del test in un’unica analisi. Gli item comuni agiscono come vincoli per collegare le due forme sulla stessa scala.\n\n\n\n7.7.2 Vantaggi dell’Equating IRT\n\nConsente di correggere le differenze tra le forme del test basandosi su parametri specifici degli item (difficoltà, discriminazione, probabilità di indovinare).\nNon richiede che le distribuzioni delle abilità degli esaminandi siano identiche per le due forme, rendendolo particolarmente utile per disegni a gruppi non equivalenti.\nPuò gestire forme di test con numero di item diverso o contenuti solo parzialmente sovrapposti.\n\nL’utilizzo dell’IRT garantisce una maggiore precisione rispetto ai metodi tradizionali, soprattutto in presenza di forme di test complesse. Le funzionalità necessarie per implementare questi metodi sono disponibili nel pacchetto R equateIRT, che offre strumenti avanzati per l’equating basato sulla teoria della risposta all’item. Per ulteriori approfondimenti, si consiglia di consultare la documentazione del pacchetto.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Equating nei Test Psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/03_equating.html#riflessioni-conclusive",
    "href": "chapters/measurement/03_equating.html#riflessioni-conclusive",
    "title": "7  Equating nei Test Psicologici",
    "section": "7.8 Riflessioni Conclusive",
    "text": "7.8 Riflessioni Conclusive\nIn questo capitolo abbiamo esplorato i metodi di equating basati sulla Classical Test Theory (CTT) e abbiamo fornito alcuni accenni sui metodi basati sulla Item Response Theory (IRT). Abbiamo analizzato come l’equating consenta di confrontare i punteggi di diverse forme di un test, garantendo equità e comparabilità anche in presenza di differenze di difficoltà tra le forme. In conclusione, la capacità di confrontare e convertire punteggi tra diverse forme di test non è solo un esercizio statistico, ma un elemento fondamentale per assicurare l’equità e la validità dei processi valutativi.",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Equating nei Test Psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/measurement/03_equating.html#session-info",
    "href": "chapters/measurement/03_equating.html#session-info",
    "title": "7  Equating nei Test Psicologici",
    "section": "7.9 Session Info",
    "text": "7.9 Session Info\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] equate_2.0.8      aspect_1.0-6      missForest_1.5    nortest_1.0-4    \n [5] MASS_7.3-61       viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [9] ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1 \n[13] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19     psych_2.4.6.26   \n[17] scales_1.3.0      markdown_1.13     knitr_1.49        lubridate_1.9.3  \n[21] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[25] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n[29] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.17.1    jsonlite_1.8.9       magrittr_2.0.3      \n  [4] TH.data_1.1-2        estimability_1.5.1   farver_2.1.2        \n  [7] nloptr_2.1.1         rmarkdown_2.29       vctrs_0.6.5         \n [10] Cairo_1.6-2          minqa_1.2.8          base64enc_0.1-3     \n [13] rstatix_0.7.2        itertools_0.1-3      htmltools_0.5.8.1   \n [16] broom_1.0.7          Formula_1.2-5        htmlwidgets_1.6.4   \n [19] plyr_1.8.9           sandwich_3.1-1       emmeans_1.10.5      \n [22] zoo_1.8-12           uuid_1.2-1           igraph_2.1.1        \n [25] iterators_1.0.14     mime_0.12            lifecycle_1.0.4     \n [28] pkgconfig_2.0.3      Matrix_1.7-1         R6_2.5.1            \n [31] fastmap_1.2.0        shiny_1.9.1          digest_0.6.37       \n [34] OpenMx_2.21.13       fdrtool_1.2.18       colorspace_2.1-1    \n [37] rprojroot_2.0.4      Hmisc_5.2-0          randomForest_4.7-1.2\n [40] fansi_1.0.6          timechange_0.3.0     abind_1.4-8         \n [43] compiler_4.4.2       rngtools_1.5.2       withr_3.0.2         \n [46] glasso_1.11          htmlTable_2.4.3      backports_1.5.0     \n [49] carData_3.0-5        ggsignif_0.6.4       corpcor_1.6.10      \n [52] gtools_3.9.5         tools_4.4.2          pbivnorm_0.6.0      \n [55] foreign_0.8-87       zip_2.3.1            httpuv_1.6.15       \n [58] nnet_7.3-19          glue_1.8.0           quadprog_1.5-8      \n [61] promises_1.3.0       nlme_3.1-166         lisrelToR_0.3       \n [64] grid_4.4.2           pbdZMQ_0.3-13        checkmate_2.3.2     \n [67] cluster_2.1.6        reshape2_1.4.4       generics_0.1.3      \n [70] gtable_0.3.6         tzdb_0.4.0           data.table_1.16.2   \n [73] hms_1.1.3            car_3.1-3            utf8_1.2.4          \n [76] sem_3.1-16           foreach_1.5.2        pillar_1.9.0        \n [79] IRdisplay_1.1        rockchalk_1.8.157    later_1.3.2         \n [82] splines_4.4.2        cherryblossom_0.1.0  lattice_0.22-6      \n [85] survival_3.7-0       kutils_1.73          tidyselect_1.2.1    \n [88] miniUI_0.1.1.1       pbapply_1.7-2        airports_0.1.0      \n [91] stats4_4.4.2         xfun_0.49            qgraph_1.9.8        \n [94] arm_1.14-4           stringi_1.8.4        pacman_0.5.1        \n [97] boot_1.3-31          evaluate_1.0.1       codetools_0.2-20    \n[100] mi_1.1               cli_3.6.3            RcppParallel_5.1.9  \n[103] IRkernel_1.3.2       rpart_4.1.23         xtable_1.8-4        \n[106] repr_1.1.7           munsell_0.5.1        Rcpp_1.0.13-1       \n[109] coda_0.19-4.1        png_0.1-8            XML_3.99-0.17       \n[112] parallel_4.4.2       usdata_0.3.1         doRNG_1.8.6         \n[115] jpeg_0.1-10          lme4_1.1-35.5        mvtnorm_1.3-2       \n[118] openxlsx_4.2.7.1     crayon_1.5.3         openintro_2.5.0     \n[121] rlang_1.1.4          multcomp_1.4-26      mnormt_2.1.1",
    "crumbs": [
      "Punteggi e scale",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Equating nei Test Psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html",
    "href": "chapters/ctt/01_ctt_1.html",
    "title": "8  Fondamenti teorici",
    "section": "",
    "text": "8.1 Introduzione\nLa CTT, originariamente sviluppata da Spearman (1904) e successivamente formalizzata da Lord & Novick (1968), è una teoria utilizzata in psicometria per valutare e misurare le caratteristiche psicologiche degli individui attraverso l’uso di test e questionari. Alla base della CTT c’è l’idea che il punteggio di un individuo in un test rifletta tanto il suo vero livello nella caratteristica misurata quanto un certo grado di errore casuale di misurazione. Questa teoria fornisce un quadro concettuale per comprendere come il punteggio osservato possa differire dal punteggio vero e per analizzare la precisione e l’affidabilità del test stesso. Sebbene siano stati sviluppati approcci più recenti, come la teoria della risposta all’item e la teoria della generalizzabilità, la CTT rimane una componente fondamentale nella psicometria e continua a guidare la costruzione, l’interpretazione e la valutazione dei test psicologici.\nSecondo questa teoria, il punteggio ottenuto da un individuo in un test è influenzato da due componenti: il punteggio vero dell’individuo sulla caratteristica misurata e l’errore casuale di misurazione.\nIl punteggio vero rappresenta la misura effettiva della caratteristica che si intende valutare nel soggetto. Tuttavia, a causa di vari fattori come l’errore di misurazione, le distrazioni o l’incertezza dell’individuo durante il test, il punteggio osservato può deviare dal punteggio vero. Questa discrepanza tra il punteggio vero e il punteggio osservato viene definita errore di misurazione.\nLa teoria classica dei test si focalizza sulla quantificazione della relazione tra il punteggio vero, il punteggio osservato e l’errore di misurazione. Attraverso l’uso di statistiche come la media, la deviazione standard e il coefficiente di affidabilità, questa teoria fornisce una base concettuale per la costruzione dei test, l’interpretazione dei risultati e l’analisi dell’affidabilità del test stesso.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#costrutti",
    "href": "chapters/ctt/01_ctt_1.html#costrutti",
    "title": "8  Fondamenti teorici",
    "section": "8.2 Costrutti",
    "text": "8.2 Costrutti\nUn costrutto è un concetto, spesso considerato una idea latente o fenomeno non direttamente osservabile (Petersen, 2024). Ad esempio, la depressione può essere un costrutto perché non possiamo misurare direttamente il livello di depressione di una persona, ma lo inferiamo attraverso indicatori indiretti come umore basso, perdita di interesse, difficoltà nel sonno, ecc. Gli indicatori sono misure che riflettono il costrutto.\nEsistono due tipi principali di costrutti: costrutti riflessivi e costrutti formativi.\n\nIn un costrutto riflessivo, il costrutto causa le misure e gli indicatori riflettono il costrutto (Bollen & Lennox, 1991). Ad esempio, l’estroversione è un costrutto riflessivo perché la risposta agli indicatori come “piacere di parlare con sconosciuti” o “andare a feste” riflette il livello di estroversione della persona. Gli indicatori sono correlati perché riflettono tutti un unico costrutto latente. La consistenza interna tra gli indicatori è quindi attesa.\nIn un costrutto formativo, invece, le misure causano il costrutto (Bollen & Lennox, 1991). Ad esempio, lo stato socioeconomico (SES) può essere formato dall’educazione, dal reddito e dal prestigio occupazionale di una persona. Gli indicatori formano il costrutto e potrebbero non essere correlati tra loro, in contrasto con i costrutti riflessivi.\n\nDifferenze tra costrutti riflessivi e formativi:\n\nCorrelazioni tra indicatori: Gli indicatori riflessivi sono correlati, mentre quelli formativi non lo devono necessariamente essere.\nCampionamento degli indicatori: Nei costrutti formativi è essenziale campionare tutti gli aspetti del costrutto, mentre nei riflessivi gli indicatori possono essere intercambiabili.\nCorrelazioni ottimali: Nei costrutti riflessivi, alte correlazioni tra indicatori sono desiderabili, mentre nei formativi correlazioni troppo alte possono creare multicollinearità.\n\nI costrutti riflessivi devono essere stimati con modelli latenti come SEM, analisi fattoriale o teoria della risposta dell’item (IRT). I costrutti formativi possono essere stimati con medie pesate o tramite SEM.\nIn sintesi, prima di stimare un costrutto, è importante comprendere la sua natura teorica. I costrutti riflessivi richiedono modelli che riflettano la varianza comune tra gli indicatori, mentre i costrutti formativi possono essere stimati con medie o punteggi sommativi. La teoria è essenziale per guidare la scelta del metodo di stima.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#lequazione-fondamentale-della-ctt",
    "href": "chapters/ctt/01_ctt_1.html#lequazione-fondamentale-della-ctt",
    "title": "8  Fondamenti teorici",
    "section": "8.3 L’Equazione Fondamentale della CTT",
    "text": "8.3 L’Equazione Fondamentale della CTT\nLa CTT descrive come i punteggi ottenuti da un test psicometrico siano legati a un costrutto latente. Uno dei concetti fondamentali all’interno della CTT riguarda l’affidabilità dei punteggi ottenuti dai test. L’affidabilità, in questo contesto, indica la capacità del test di produrre risultati coerenti e stabili in diverse occasioni. Questo concetto può essere compreso attraverso l’equazione fondamentale della CTT:\n\\[\nX = T + E,\n\\tag{8.1}\\]\ndove \\(X\\) rappresenta il punteggio osservato nel test, \\(T\\) è il punteggio vero (ovvero la rappresentazione della variabile latente di interesse), e \\(E\\) rappresenta l’errore di misurazione.\nUn aspetto di particolare rilevanza all’interno della CTT riguarda la varianza dell’errore. Maggiore è questa varianza, minore sarà la precisione con cui il punteggio vero si riflette nei punteggi osservati. In un contesto ideale, gli errori di misurazione sarebbero tutti nulli, garantendo punteggi esatti per ogni partecipante. Tuttavia, a causa delle inevitabili imperfezioni, si verifica una certa variazione negli errori. La deviazione standard associata a questi errori è chiamata errore standard di misurazione e viene indicata con \\(\\sigma_E\\). Uno degli obiettivi principali della CTT è stimare \\(\\sigma_E\\) al fine di valutare la qualità di una scala psicometrica.\n\n8.3.1 Le due componenti del punteggio osservato\nL’Equazione 8.1 rappresenta il cuore del modello, sottolineando che il punteggio osservato è il risultato dell’addizione del punteggio vero e dell’errore di misurazione.\nL’obiettivo principale della CTT è quantificare l’errore di misurazione (rappresentato da \\(\\sigma_E\\)) per valutare l’affidabilità del test e ottenere una stima dell’errore standard di misurazione. L’affidabilità del test riflette la precisione con cui il test può misurare il punteggio vero (Coaley, 2014). Si calcola come il rapporto tra la varianza dei punteggi veri e la varianza dei punteggi osservati. Un’alta affidabilità indica una ridotta incertezza dovuta all’errore di misurazione (\\(\\sigma_E\\)), indicando che il punteggio osservato (\\(X\\)) fornisce una misura accurata del punteggio vero (\\(T\\)). Al contrario, una bassa affidabilità indica un elevato errore di misurazione (\\(\\sigma_E\\)) e una significativa discrepanza tra il punteggio osservato e il punteggio vero.\nLa stima dell’errore standard di misurazione comporta il calcolo della deviazione standard della variabile casuale \\(E\\) (ossia \\(\\sigma_E\\)), che rappresenta l’errore di misurazione influente sui punteggi veri. Questa stima offre un’indicazione della dispersione dei punteggi osservati attorno ai punteggi veri, causata dall’errore di misurazione.\nNelle prossime sezioni, esploreremo come il concetto chiave di attendibilità nella CTT possa essere collegato al coefficiente di determinazione nel contesto del modello statistico di regressione lineare. Inoltre, vedremo come l’errore standard di misurazione della CTT possa essere associato all’errore standard nella regressione.\n\n\n8.3.2 Il punteggio vero\nL’Equazione 8.1 ci spiega che il punteggio osservato è il risultato della combinazione di due componenti: una componente sistematica (il punteggio vero) e una componente aleatoria (l’errore di misurazione). Ma cosa rappresenta esattamente il punteggio vero? La Teoria Classica dei Test (CTT) attribuisce diverse interpretazioni al concetto di punteggio vero.\n\nDa un punto di vista psicologico, la CTT considera il test come una selezione casuale di domande da un insieme più ampio di domande che riflettono il costrutto da misurare (Kline, 2013; Nunnally, 1994). In questo contesto, il punteggio vero rappresenta il punteggio che un partecipante otterrebbe se rispondesse a tutte le domande dell’insieme completo. L’errore di misurazione riflette quindi quanto le domande selezionate rappresentano l’intero insieme di domande relative al costrutto.\nIn modo equivalente, il punteggio vero può essere considerato come il punteggio non influenzato da fattori esterni al costrutto, come effetti di apprendimento, fatica, memoria, motivazione, e così via. Poiché è concepito come un processo completamente casuale, la componente aleatoria non introduce alcun bias nella tendenza centrale della misurazione (la media di \\(E\\) è assunta essere uguale a 0).\nDal punto di vista statistico, il punteggio vero è un punteggio inosservabile che rappresenta il valore atteso di infinite misurazioni del punteggio ottenute:\n\n\\[\nT = \\mathbb{E}(X) \\equiv \\mu_X \\equiv \\mu_{T}.\n\\]\nCombinando le definizioni presentate sopra, Lord & Novick (1968) concepiscono il punteggio vero come la media dei punteggi che un soggetto otterrebbe se il test venisse somministrato ripetutamente nelle stesse condizioni, senza effetti di apprendimento o fatica.\n\n\n8.3.3 Somministrazioni ripetute\nNella CTT, possiamo distinguere due tipi di esperimenti casuali: uno in cui l’unità di osservazione (l’individuo) è considerata una variabile campionaria, e un altro in cui il punteggio di un singolo individuo è trattato come una variabile casuale. La combinazione di questi due esperimenti consente di estendere i risultati della CTT, originariamente sviluppata assumendo somministrazioni ripetute immaginarie del test allo stesso individuo in condizioni identiche, al caso di una singola somministrazione su un campione di individui (Allen & Yen, 2001).\nQuesta estensione si fonda sull’assunzione ergodica, secondo cui è possibile interpretare la variabilità nelle misurazioni ripetute su un singolo individuo come rappresentativa della variabilità in un campione di individui, a condizione che siano soddisfatte le seguenti condizioni:\n\nOmogeneità: le proprietà fondamentali del costrutto misurato sono uguali per tutti gli individui nel campione. In altre parole, ogni individuo risponde in modo simile rispetto alla dimensione latente misurata dal test, e le differenze individuali sono esclusivamente dovute alla variabilità casuale o all’errore di misura, non a variazioni sostanziali nel costrutto.\nStabilità: le caratteristiche individuali misurate restano costanti nel tempo durante le somministrazioni ripetute. Ciò significa che, per uno stesso individuo, il costrutto misurato non cambia tra una somministrazione e l’altra, e le variazioni osservate riflettono soltanto la componente di errore o la variabilità casuale.\n\nQuando queste condizioni sono soddisfatte, le quantità fondamentali della CTT assumono un significato empirico valido anche per la somministrazione del test a una popolazione di individui. In questo contesto:\n\n\\(\\sigma^2_X\\) rappresenta la varianza del punteggio osservato nella popolazione,\n\\(\\sigma^2_T\\) rappresenta la varianza del punteggio vero nella popolazione,\n\\(\\sigma^2_E\\) rappresenta la varianza della componente d’errore nella popolazione.\n\nL’assunzione ergodica permette quindi di inferire proprietà della popolazione a partire da misurazioni su individui singoli e viceversa, in quanto la variabilità all’interno di un individuo e quella tra individui sono considerate comparabili nelle stesse condizioni.\n\n\n8.3.4 Le assunzioni sul punteggio ottenuto\nLa CTT assume che la media del punteggio osservato \\(X\\) sia uguale alla media del punteggio vero,\n\\[\n\\mu_X \\equiv \\mu_{T},\n\\tag{8.2}\\]\nin altri termini, assume che il punteggio osservato fornisca una stima statisticamente corretta dell’abilità latente (punteggio vero).\nIn pratica, il punteggio osservato non sarà mai uguale all’abilità latente, ma corrisponde solo ad uno dei possibili punteggi che il soggetto può ottenere, subordinatamente alla sua abilità latente. L’errore della misura è la differenza tra il punteggio osservato e il punteggio vero:\n\\[\nE \\equiv X - T.\n\\]\nIn base all’assunzione secondo cui il valore atteso dei punteggi è uguale alla media del valore vero, segue che\n\\[\n\\mathbb{E}(E) = \\mathbb{E}(X - T) = \\mathbb{E}(X) - \\mathbb{E}(T) = \\mu_{T} - \\mu_{T} = 0,\n\\]\novvero, il valore atteso degli errori è uguale a zero.\nPer dare un contenuto concreto alle affermazioni precedenti, consideriamo la seguente simulazione svolta in \\(\\textsf{R}\\). In tale simulazione il punteggio vero \\(T\\) e l’errore \\(E\\) sono creati in modo tale da soddisfare i vincoli della CTT: \\(T\\) e \\(E\\) sono variabili casuali gaussiane tra loro incorrelate. Nella simulazione generiamo 100 coppie di valori \\(X\\) e \\(T\\) con i seguenti parametri: \\(T \\sim \\mathcal{N}(\\mu_T = 12, \\sigma^2_T = 6)\\), \\(E \\sim \\mathcal{N}(\\mu_E = 0, \\sigma^2_T = 3)\\):\n\nset.seed(8394)\n\nn &lt;- 100\nSigma &lt;- matrix(c(6, 0, 0, 3), byrow = TRUE, ncol = 2)\nmu &lt;- c(12, 0)\ndat &lt;- mvrnorm(n, mu, Sigma, empirical = TRUE)\nT &lt;- dat[, 1]\nE &lt;- dat[, 2]\n\nLe istruzioni precedenti (empirical = TRUE) creano un campione di valori nei quali le medie e la matrice di covarianze assumono esattamente i valori richiesti. Possiamo dunque immaginare tale insieme di dati come la “popolazione”.\nSecondo la CTT, il punteggio osservato è \\(X = T + E\\). Simuliamo dunque il punteggio osservato \\(X\\) come:\n\nX &lt;- T + E\n\nLe prime 6 osservazioni così ottenute sono:\n\ntibble(X, T, E) |&gt; head()\n\n\nA tibble: 6 x 3\n\n\nX\nT\nE\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n15.70\n16.77\n-1.067\n\n\n13.66\n12.25\n1.409\n\n\n6.73\n7.85\n-1.120\n\n\n14.62\n14.23\n0.388\n\n\n10.61\n10.19\n0.420\n\n\n12.37\n13.33\n-0.960\n\n\n\n\n\n\nUn diagramma di dispersione è fornito nella figura seguente:\n\ntibble(X, T) |&gt;\nggplot(aes(T, X)) +\n    geom_point(position = position_jitter(w = .3, h = .3)) +\n    geom_abline(col = \"blue\")\n\n\n\n\n\n\n\n\nSecondo la CTT, il valore atteso di \\(T\\) è uguale al valore atteso di \\(X\\). Verifichiamo questa assunzione nei nostri dati\n\nmean(T) == mean(X)\n\nTRUE\n\n\n\nL’errore deve avere media zero:\n\nmean(E)\n\n-8.88178419700125e-17\n\n\n\nLe varianze dei punteggi veri, dei punteggi osservati e degli errori sono rispettivamente uguali a:\n\nc(var(T), var(X), var(E))  \n\n\n693",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#lerrore-standard-della-misurazione-sigma_e",
    "href": "chapters/ctt/01_ctt_1.html#lerrore-standard-della-misurazione-sigma_e",
    "title": "8  Fondamenti teorici",
    "section": "8.4 L’errore standard della misurazione \\(\\sigma_E\\)",
    "text": "8.4 L’errore standard della misurazione \\(\\sigma_E\\)\nLa radice quadrata della varianza degli errori di misurazione, ovvero la deviazione standard degli errori, \\(\\sigma_E\\), è la quantità fondamentale della CTT ed è chiamata errore standard della misurazione. La stima dell’errore standard della misurazione costituisce uno degli obiettivi più importanti della CTT.\nNel caso presente, abbiamo:\n\nsqrt(var(E))\n\n1.73205080756888\n\n\n\nRicordiamo che la deviazione standard indica quanto i dati di una distribuzione si discostano dalla media di quella distribuzione. È simile allo scarto tipico, ovvero la distanza media tra i valori della distribuzione e la loro media. Possiamo dunque utilizzare questa proprietà per descrivere il modo in cui la CTT interpreta la quantità \\(\\sigma_E\\): l’errore standard della misurazione \\(\\sigma_E\\) ci dice qual è, approssimativamente, la quantità attesa di variazione del punteggio osservato, se il test venisse somministrato ripetute volte al medesimo rispondente sotto le stesse condizioni (ovvero, in assenza di effetti di apprendimento o di fatica).",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#assiomi-della-teoria-classica",
    "href": "chapters/ctt/01_ctt_1.html#assiomi-della-teoria-classica",
    "title": "8  Fondamenti teorici",
    "section": "8.5 Assiomi della Teoria Classica",
    "text": "8.5 Assiomi della Teoria Classica\nLa CTT assume che gli errori siano delle variabili casuali incorrelate tra loro\n\\[\n\\rho(E_i, E_k \\mid T) = 0, \\qquad\\text{con}\\; i \\neq k,\n\\]\ne incorrelate con il punteggio vero,\n\\[\n\\rho(E, T) = 0,\n\\]\nle quali seguono una distribuzione gaussiana con media zero e deviazione standard pari a \\(\\sigma_E\\):\n\\[\nE \\sim \\mathcal{N}(0, \\sigma_E).\n\\]\nLa quantità \\(\\sigma_E\\) è appunto l’errore standard della misurazione. Sulla base di tali assunzioni la CTT deriva la formula dell’attendibilità di un test. Si noti che le assunzioni della CTT hanno una corrispondenza puntuale con le assunzioni su cui si basa il modello di regressione lineare.\nVerifichiamo le assunzioni per i dati dell’esempio.\n\ncor(E, T)\n\n-4.22920527591589e-17\n\n\n\nplot(density(E))\ncurve(dnorm(x, mean(E), sd(E)), add = TRUE, col = \"red\")",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#quattro-livelli-di-misurazione-nella-ctt",
    "href": "chapters/ctt/01_ctt_1.html#quattro-livelli-di-misurazione-nella-ctt",
    "title": "8  Fondamenti teorici",
    "section": "8.6 Quattro Livelli di Misurazione nella CTT",
    "text": "8.6 Quattro Livelli di Misurazione nella CTT\nNell’ambito della CTT, le misure della stessa entità (che possono essere item, sottoscale o test) possono essere classificate in base al loro livello di similarità. In questa sezione, verranno definiti quattro livelli di similarità: misure parallele, \\(\\tau\\)-equivalenti, essenzialmente \\(\\tau\\)-equivalenti e congeneriche.\nÈ importante notare che questi livelli sono gerarchici nel senso che il livello più alto (misure parallele) richiede la maggiore similarità, mentre i livelli inferiori nella gerarchia consentono una minore similarità nelle proprietà del test. Ad esempio, le misure parallele devono avere varianze di vero punteggio uguali, mentre le misure congenetiche non richiedono questa condizione.\nUn modo utile per comprendere questi livelli è riflettere sulle relazioni tra i punteggi veri di coppie di misure (Komaroff, 1997). Nella CTT, la relazione tra i punteggi veri su due misure (\\(t_i\\) e \\(t_j\\)) è espressa come:\n\\[\nt_i = a_{ij }+ b_{ij} t_{j}.\n\\]\n\n\\(a_{ij}\\): Rappresenta lo scarto medio tra i punteggi delle due misure. Se è diverso da zero, una misura tende a dare punteggi sistematicamente più alti o più bassi dell’altra.\n\\(b_{ij}\\): Rappresenta la scala con cui una misura misura il tratto latente rispetto all’altra. Se è diverso da uno, le misure non misurano lo stesso tratto con la stessa intensità.\n\nCosa significano questi parametri per i livelli di similarità?\n\nMisure parallele: Entrambe le misure sono identiche, sia nella scala che nello scarto medio.\nMisure τ-equivalenti: Le misure hanno la stessa scala, ma potrebbero avere uno scarto medio diverso.\nMisure essenzialmente τ-equivalenti: Le misure possono differire sia nella scala che nello scarto medio, ma entro certi limiti.\nMisure congeneriche: Le misure possono differire in modo sostanziale sia nella scala che nello scarto medio.\n\nIn sostanza, più i parametri \\(a_{ij}\\) e \\(b_{ij}\\) sono vicini a zero e uno, rispettivamente, più le misure sono simili e misurano lo stesso costrutto in modo più coerente.\n\n8.6.1 Misure parallele\nLe misure parallele rappresentano il livello più alto di similarità tra le misure. Ciò significa che due misure sono considerate parallele quando soddisfano le seguenti condizioni:\n\nUguaglianza delle medie dei punteggi veri: Il termine \\(a_{ij}\\) nell’equazione è uguale a zero per tutte le coppie di misure, indicando che non esiste alcuna differenza sistematica tra le medie dei punteggi veri delle due misure.\nUguaglianza delle varianze dei punteggi veri: Il termine \\(b_{ij}\\) è uguale a uno per tutte le coppie di misure, indicando che le varianze dei punteggi veri delle due misure sono identiche.\nUguaglianza delle varianze di errore: Le misure parallele presentano la stessa quantità di errore di misura.\n\nQueste condizioni implicano che:\n\nI punteggi osservati (ovvero, i punteggi effettivamente ottenuti dai soggetti) avranno medie, varianze e correlazioni uguali.\nGli item che costituiscono misure parallele hanno lo stesso potere discriminativo (carico fattoriale) rispetto al costrutto che misurano.\n\nIn sostanza, le misure parallele misurano esattamente lo stesso costrutto, con la stessa precisione e senza alcuna distorsione sistematica.\nSimuliamo i punteggi di due test paralleli in R.\n\nset.seed(2237) # setting the seed ensure reproducibility\nnum_person &lt;- 1000 # number of respondents\n# True scores for Test 1\nt1 &lt;- rnorm(num_person, mean = 20, sd = 5)\n# Error scores for Test 1\ne1 &lt;- rnorm(num_person, mean = 0, sd = 2)\n# Observed scores for Test 1\nx1 &lt;- t1 + e1\n# True scores for Test 2\nt2 &lt;- t1 # parallel tests have equal true scores\n# Error scores for Test 2\ne2 &lt;- rnorm(num_person, mean = 0, sd = 2)\n# Observed scores for Test 2\nx2 &lt;- t2 + e2\n\n\n# Merge into a data frame\ntest_df &lt;- data.frame(x1, x2)\n# Get means and variances\nmv &lt;- datasummary(x1 + x2 ~ Mean + Var,\n    data = test_df,\n    output = \"data.frame\"\n)\nmv\n\n\nA data.frame: 2 x 3\n\n\n\nMean\nVar\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\nx1\n20.41\n29.20\n\n\nx2\n20.31\n30.27\n\n\n\n\n\n\ncor(test_df$x1, test_df$x2)\n\n0.865310361839848\n\n\n\nNel caso di due test paralleli, le medie e le varianze dei punteggi osservati sono (teoricamente) uguali; la correlazione descrive l’affidabilità del test.\n\n\n8.6.2 Misure \\(\\tau\\)-equivalenti\nLe misure τ-equivalenti rappresentano un livello di similarità leggermente inferiore rispetto alle misure parallele.\nCaratteristiche delle misure τ-equivalenti:\n\nUguaglianza delle varianze dei punteggi veri: Come le misure parallele, anche le misure τ-equivalenti hanno lo stesso valore del punteggio vero per ogni individuo, indipendentemente dal test utilizzato. Ciò implica che il parametro \\(b_{ij}\\) è sempre uguale a 1.\nPossibile differenza nelle varianze di errore: A differenza delle misure parallele, le misure τ-equivalenti possono presentare diversi livelli di errore di misura. Questo significa che, pur misurando lo stesso costrutto, un test potrebbe essere più preciso di un altro.\n\nIn sintesi, le misure τ-equivalenti misurano lo stesso costrutto sullo stesso scala, ma possono differire nella precisione con cui lo misurano. In altre parole, i punteggi veri sono uguali per tutti gli item, ma gli errori di misura possono variare.\nConseguenze:\n\nCovarianze uguali: Le misure τ-equivalenti presentano le stesse covarianze tra i punteggi veri e tra i punteggi osservati.\nVariazioni nelle varianze osservate: A causa delle possibili differenze nelle varianze di errore, le varianze dei punteggi osservati possono differire tra le misure τ-equivalenti.\n\nSimuliamo due misure \\(\\tau\\)-equivalenti.\n\nset.seed(2237) # setting the seed ensure reproducibility\nnum_person &lt;- 1000 # number of respondents\n# True scores for Test 1\nt1 &lt;- rnorm(num_person, mean = 20, sd = 5)\n# Error scores for Test 1\ne1 &lt;- rnorm(num_person, mean = 0, sd = 2)\n# Observed scores for Test 1\nx1 &lt;- t1 + e1\n# True scores for Test 2\nt2 &lt;- t1 # parallel tests have equal true scores\n# Error scores for Test 2\ne2 &lt;- rnorm(num_person, mean = 0, sd = 2)\n# Observed scores for Test 2\nx2 &lt;- t2 + e2\n\nSe conosciamo i punteggi veri, le stime dell’affidabilità di x1 e x2 sono:\n\n# Reliability for x1\nvar(t1) / var(x1)\n\n0.878424313030747\n\n\n\n# Reliability for x2\nvar(t2) / var(x2)\n\n0.847351804948915\n\n\n\n# Merge into a data frame\ntest_df &lt;- data.frame(x1, x2)\n# Get means and variances\nmv &lt;- datasummary(x1 + x2 ~ Mean + Var,\n    data = test_df,\n    output = \"data.frame\"\n)\nmv\n\n\nA data.frame: 2 x 3\n\n\n\nMean\nVar\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\nx1\n20.41\n29.20\n\n\nx2\n20.31\n30.27\n\n\n\n\n\n\ncor(test_df$x1, test_df$x2)\n\n0.865310361839848\n\n\n\nIn conclusione, nel caso di due test \\(\\tau\\)-equivalenti, le medie e le varianze dei punteggi osservati sono (teoricamente) uguali. Anche in questo caso, la correlazione descrive l’affidabilità del test.\n\n\n8.6.3 Misure essenzialmente \\(\\tau\\)-equivalenti\nLe misure essenzialmente \\(\\tau\\)-equivalenti rappresentano una forma di misurazione in cui i punteggi veri possono differire di una costante additiva. Ciò significa che, pur misurando lo stesso costrutto, le medie dei punteggi veri di diversi item possono variare leggermente.\nCaratteristiche delle misure essenzialmente \\(\\tau\\)-equivalenti:\n\nUguaglianza delle varianze dei punteggi veri: Come nelle misure \\(\\tau\\)-equivalenti, la varianza del punteggio vero è la stessa per tutti gli item.\nPossibili differenze nelle medie dei punteggi veri: Il parametro \\(a_{ij}\\) può essere diverso da zero, indicando che le medie dei punteggi veri possono variare.\nPossibili differenze nelle varianze di errore: Come nelle misure \\(\\tau\\)-equivalenti, gli item possono avere diversi livelli di precisione, ovvero diverse varianze di errore.\n\nImplicazioni:\n\nCorrelazione perfetta tra i punteggi veri: Nonostante le differenze nelle medie, i punteggi veri sono perfettamente correlati linearmente.\nCovarianze uguali tra i punteggi veri: Le covarianze tra i punteggi veri sono uguali per tutte le coppie di item.\nPossibili differenze nelle varianze e nelle covarianze dei punteggi osservati: A causa delle differenze nelle varianze di errore, le varianze e le covarianze dei punteggi osservati possono variare.\n\nIn sintesi, le misure essenzialmente \\(\\tau\\)-equivalenti sono utili quando si desidera confrontare diversi item che misurano lo stesso costrutto, ma si ammette la possibilità di piccole differenze sistematiche nelle medie dei punteggi.\n\n# True scores for Test 3\nt3 &lt;- 5 + t1 # essentially tau-equivalent tests\n# Error scores for Test 3 (larger error SDs)\ne3 &lt;- rnorm(num_person, mean = 0, sd = 4)\n# Observed scores for Test 2\nx3 &lt;- t3 + e3\n\n\n# Merge into a data frame\ntest_df2 &lt;- data.frame(x1, x3)\n# Get means and variances\nmv &lt;- datasummary(x1 + x3 ~ Mean + Var,\n    data = test_df2,\n    output = \"data.frame\"\n)\nmv\n\n\nA data.frame: 2 x 3\n\n\n\nMean\nVar\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\nx1\n20.41\n29.20\n\n\nx3\n25.41\n41.50\n\n\n\n\n\nSe conosciamo i punteggi veri, la stima dell’affidabilità di x3 è:\n\n# Reliability for x3\nvar(t3) / var(x3)\n\n0.618012243898734\n\n\n\nIn conclusione, nel caso di test essenzialmente \\(\\tau\\)-equivalenti, le medie e le varianze dei punteggi osservati sono diverse; la correlazione non è uguale all’affidabilità.\n\n\n8.6.4 Misure Congeneriche\nLe misure congeneriche rappresentano il livello di similarità più basso tra le diverse tipologie di misure.\nCaratteristiche delle misure congeneriche:\n\nNessuna restrizione: A differenza delle misure parallele, τ-equivalenti ed essenzialmente τ-equivalenti, le misure congeneriche non sono soggette a restrizioni specifiche sui parametri \\(a_{ij}\\) e \\(b_{ij}\\). Ciò significa che:\n\nLe medie dei punteggi veri possono differire significativamente.\nLe varianze dei punteggi veri possono essere diverse.\nLe varianze di errore possono variare notevolmente.\n\nUnidimensionalità: Nonostante queste differenze, si assume che tutte le misure congeneriche misurino lo stesso costrutto latente sottostante.\n\nImplicazioni:\n\nFlessibilità: Le misure congeneriche offrono la massima flessibilità in termini di differenze tra gli item.\nMinor comparabilità: A causa delle numerose differenze, confrontare direttamente i punteggi ottenuti con misure congeneriche può essere più complesso.\n\nIn sintesi, le misure congeneriche rappresentano un modello molto generale, che consente di includere una vasta gamma di situazioni. Tuttavia, la loro flessibilità comporta una minore comparabilità tra gli item.\n\n# True scores for Test 4\nt4 &lt;- 2 + 0.8 * t1\n# Error scores for Test 4 (larger error SDs)\ne4 &lt;- rnorm(num_person, mean = 0, sd = 3)\n# Observed scores for Test 2\nx4 &lt;- t4 + e4\n\n\n# Merge into a data frame\ntest_df3 &lt;- data.frame(x1, x4)\n# Get means and variances\nmv &lt;- datasummary(x1 + x4 ~ Mean + Var,\n    data = test_df3,\n    output = \"data.frame\"\n)\nmv\n\n\nA data.frame: 2 x 3\n\n\n\nMean\nVar\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\nx1\n20.41\n29.20\n\n\nx4\n18.27\n24.23\n\n\n\n\n\nSe conosciamo i punteggi veri, la stima dell’affidabilità di x4 è:\n\n# Reliability for x4\nvar(t4) / var(x4)\n\n0.677398252481377\n\n\n\nNel caso di test congenerici, le medie e le varianze dei punteggi osservati sono diverse; la correlazione non è uguale all’affidabilità. Per distinguere test congenerici dai test essenzialmente \\(\\tau\\)-equivalenti sono necessari più di due test.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#riflessioni-conclusive",
    "href": "chapters/ctt/01_ctt_1.html#riflessioni-conclusive",
    "title": "8  Fondamenti teorici",
    "section": "8.7 Riflessioni Conclusive",
    "text": "8.7 Riflessioni Conclusive\nQuesto capitolo ha offerto una panoramica dei concetti chiave della teoria classica dei test (CTT) e ha introdotto quattro tipi di misure psicometriche. Le misure parallele si distinguono per l’elevata somiglianza nei punteggi veri, garantendo che le varianze siano uguali per tutte le misure. Le misure τ-equivalenti condividono questa equivalenza nelle varianze dei punteggi veri, ma non richiedono una somiglianza così stretta come le misure parallele. Le misure essenzialmente τ-equivalenti tollerano una maggiore variabilità nei punteggi veri, pur mantenendo la coerenza dei risultati. Infine, le misure congeneriche presentano le minori restrizioni tra le quattro tipologie, consentendo differenze sia nelle medie sia nelle varianze dei punteggi veri.\nComprendere le differenze tra queste tipologie di misure è fondamentale per valutare l’affidabilità e la validità di un test e per interpretare in modo accurato i risultati. Nelle prossime sezioni della dispensa, approfondiremo l’applicazione pratica della CTT nello sviluppo e nella valutazione dei test psicometrici. Per un’esplorazione più dettagliata, si rimanda alle letture di riferimento: McDonald (2013) e Lord & Novick (1968).",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/01_ctt_1.html#session-info",
    "href": "chapters/ctt/01_ctt_1.html#session-info",
    "title": "8  Fondamenti teorici",
    "section": "8.8 Session Info",
    "text": "8.8 Session Info\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] modelsummary_2.2.0 MASS_7.3-61        viridis_0.6.5     \n [4] viridisLite_0.4.2  ggpubr_0.6.0       ggExtra_0.10.1    \n [7] gridExtra_2.3      patchwork_1.3.0    bayesplot_1.11.1  \n[10] semTools_0.5-6     semPlot_1.1.6      lavaan_0.6-19     \n[13] psych_2.4.6.26     scales_1.3.0       markdown_1.13     \n[16] knitr_1.49         lubridate_1.9.3    forcats_1.0.0     \n[19] stringr_1.5.1      dplyr_1.1.4        purrr_1.0.2       \n[22] readr_2.1.5        tidyr_1.3.1        tibble_3.2.1      \n[25] ggplot2_3.5.1      tidyverse_2.0.0    here_1.0.1        \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.17.1  jsonlite_1.8.9     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5.1 farver_2.1.2      \n  [7] nloptr_2.1.1       rmarkdown_2.29     vctrs_0.6.5       \n [10] Cairo_1.6-2        minqa_1.2.8        base64enc_0.1-3   \n [13] rstatix_0.7.2      htmltools_0.5.8.1  broom_1.0.7       \n [16] Formula_1.2-5      htmlwidgets_1.6.4  plyr_1.8.9        \n [19] sandwich_3.1-1     emmeans_1.10.5     zoo_1.8-12        \n [22] uuid_1.2-1         igraph_2.1.1       mime_0.12         \n [25] lifecycle_1.0.4    pkgconfig_2.0.3    Matrix_1.7-1      \n [28] R6_2.5.1           fastmap_1.2.0      shiny_1.9.1       \n [31] digest_0.6.37      OpenMx_2.21.13     fdrtool_1.2.18    \n [34] colorspace_2.1-1   rprojroot_2.0.4    Hmisc_5.2-0       \n [37] labeling_0.4.3     fansi_1.0.6        timechange_0.3.0  \n [40] abind_1.4-8        compiler_4.4.2     withr_3.0.2       \n [43] glasso_1.11        htmlTable_2.4.3    backports_1.5.0   \n [46] carData_3.0-5      ggsignif_0.6.4     corpcor_1.6.10    \n [49] gtools_3.9.5       tools_4.4.2        pbivnorm_0.6.0    \n [52] foreign_0.8-87     zip_2.3.1          httpuv_1.6.15     \n [55] nnet_7.3-19        glue_1.8.0         quadprog_1.5-8    \n [58] promises_1.3.0     nlme_3.1-166       lisrelToR_0.3     \n [61] grid_4.4.2         pbdZMQ_0.3-13      checkmate_2.3.2   \n [64] cluster_2.1.6      reshape2_1.4.4     generics_0.1.3    \n [67] gtable_0.3.6       tzdb_0.4.0         data.table_1.16.2 \n [70] hms_1.1.3          car_3.1-3          utf8_1.2.4        \n [73] tables_0.9.31      sem_3.1-16         pillar_1.9.0      \n [76] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [79] splines_4.4.2      lattice_0.22-6     survival_3.7-0    \n [82] kutils_1.73        tidyselect_1.2.1   miniUI_0.1.1.1    \n [85] pbapply_1.7-2      stats4_4.4.2       xfun_0.49         \n [88] qgraph_1.9.8       arm_1.14-4         stringi_1.8.4     \n [91] pacman_0.5.1       boot_1.3-31        evaluate_1.0.1    \n [94] codetools_0.2-20   mi_1.1             cli_3.6.3         \n [97] RcppParallel_5.1.9 IRkernel_1.3.2     rpart_4.1.23      \n[100] xtable_1.8-4       repr_1.1.7         munsell_0.5.1     \n[103] Rcpp_1.0.13-1      coda_0.19-4.1      png_0.1-8         \n[106] XML_3.99-0.17      parallel_4.4.2     jpeg_0.1-10       \n[109] lme4_1.1-35.5      mvtnorm_1.3-2      insight_0.20.5    \n[112] openxlsx_4.2.7.1   crayon_1.5.3       rlang_1.1.4       \n[115] multcomp_1.4-26    mnormt_2.1.1      \n\n\n\n\n\n\nAllen, M. J., & Yen, W. M. (2001). Introduction to measurement theory. Waveland Press.\n\n\nBollen, K., & Lennox, R. (1991). Conventional wisdom on measurement: A structural equation perspective. Psychological bulletin, 110(2), 305–314.\n\n\nKline, P. (2013). Handbook of psychological testing. Routledge.\n\n\nKomaroff, E. (1997). Effect of simultaneous violations of essential/g= t/-equivalence and uncorrelated error on coefficient/g= a. Applied Psychological Measurement, 21(4), 337–348.\n\n\nLord, F. M., & Novick, M. R. (1968). Statistical theories of mental test scores. Addison-Wesley.\n\n\nMcDonald, R. P. (2013). Test theory: A unified treatment. Psychology Press.\n\n\nNunnally, J. C. (1994). Psychometric theory. McGraw-Hill.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.\n\n\nSpearman, C. (1904). General intelligence objectively determined and measured. American Journal of Psychology, 15, 201–293.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fondamenti teorici</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html",
    "href": "chapters/ctt/02_ctt_2.html",
    "title": "9  L’affidabilità del test",
    "section": "",
    "text": "9.1 Introduzione\nUno degli obiettivi principali della CTT è quello di suddividere la varianza di un insieme di punteggi osservati in varianza del punteggio vero e varianza dell’errore. Per definire l’attendibilità, la CTT si basa su due informazioni chiave:\nVedremo come ottenere queste informazioni utilizzando le assunzioni del modello statistico alla base della CTT. Queste assunzioni includono:",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#introduzione",
    "href": "chapters/ctt/02_ctt_2.html#introduzione",
    "title": "9  L’affidabilità del test",
    "section": "",
    "text": "La varianza dei punteggi osservati.\nLa correlazione tra il punteggio osservato e il punteggio vero.\n\n\n\nErrore medio nullo: Si assume che l’errore di misurazione abbia una media pari a zero, cioè \\(E(e) = 0\\). Questo implica che l’errore è casuale e distribuito uniformemente attorno al punteggio vero.\nIndipendenza tra punteggio vero e errore: La CTT assume che non ci sia correlazione tra il punteggio vero e l’errore di misurazione (\\(r_{T,e} = 0\\)).\nIndipendenza dell’errore nel tempo: Si assume che l’errore di misurazione in un determinato momento non sia correlato con l’errore in un altro momento (\\(r_{e1,e2} = 0\\)).",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#la-varianza-del-punteggio-osservato",
    "href": "chapters/ctt/02_ctt_2.html#la-varianza-del-punteggio-osservato",
    "title": "9  L’affidabilità del test",
    "section": "9.2 La varianza del punteggio osservato",
    "text": "9.2 La varianza del punteggio osservato\nLa varianza del punteggio osservato \\(X\\) è uguale alla somma della varianza del punteggio vero e della varianza dell’errore di misurazione:\n\\[\n\\sigma^2_X =   \\sigma_T^2 + \\sigma_E^2.\n\\tag{9.1}\\]\nLa dimostrazione è la seguente. La varianza del punteggio osservato è uguale a\n\\[\n\\sigma^2_X =  \\mathbb{V}(T+E) =  \\sigma_T^2 + \\sigma_E^2 + 2 \\sigma_{TE}.\n\\tag{9.2}\\]\nDato che \\(\\sigma_{TE}=\\rho_{TE}\\sigma_T \\sigma_E=0\\), in quanto \\(\\rho_{TE}=0\\), ne segue che\n\\[\n\\sigma^2_X =   \\sigma_T^2 + \\sigma_E^2.\n\\]\nPer fare un esempio concreto, riprendiamo la simulazione del capitolo precedente.\n\nset.seed(8394)\n\nn &lt;- 100\nSigma &lt;- matrix(c(6, 0, 0, 3), byrow = TRUE, ncol = 2)\nmu &lt;- c(12, 0)\ndat &lt;- mvrnorm(n, mu, Sigma, empirical = TRUE)\nT &lt;- dat[, 1]\nE &lt;- dat[, 2]\nX &lt;- T + E\n\ntibble(X, T, E) |&gt; head()\n\n\nA tibble: 6 × 3\n\n\nX\nT\nE\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n15.698623\n16.765359\n-1.0667358\n\n\n13.657503\n12.248096\n1.4094073\n\n\n6.731979\n7.852136\n-1.1201563\n\n\n14.621813\n14.233699\n0.3881133\n\n\n10.606647\n10.187035\n0.4196115\n\n\n12.370288\n13.329971\n-0.9596831\n\n\n\n\n\n\nvar(X) == var(T) + var(E)\n\nTRUE",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#la-covarianza-tra-punteggio-osservato-e-punteggio-vero",
    "href": "chapters/ctt/02_ctt_2.html#la-covarianza-tra-punteggio-osservato-e-punteggio-vero",
    "title": "9  L’affidabilità del test",
    "section": "9.3 La covarianza tra punteggio osservato e punteggio vero",
    "text": "9.3 La covarianza tra punteggio osservato e punteggio vero\nLa covarianza tra punteggio osservato \\(X\\) e punteggio vero \\(T\\) è uguale alla varianza del punteggio vero:\n\\[\n\\sigma_{X T} = \\sigma_T^2.\n\\tag{9.3}\\]\nLa dimostrazione è la seguente. La covarianza tra punteggio osservato e punteggio vero è uguale a\n\\[\n\\begin{aligned}\n\\sigma_{X T} &= \\mathbb{E}(XT) - \\mathbb{E}(X)\\mathbb{E}(T)\\notag\\\\\n&=  \\mathbb{E}[(T+E)T] - \\mathbb{E}(T+E)\\mathbb{E}(T)\\notag\\\\\n&=  \\mathbb{E}(T^2) + \\underbrace{\\mathbb{E}(ET)}_{=0} - [\\mathbb{E}(T)]^2 -  \\underbrace{\\mathbb{E}(E)}_{=0} \\mathbb{E}(T)\\notag\\\\\n&=\\mathbb{E}(T^2) - [\\mathbb{E}(T)]^2\\notag \\\\\n&= \\sigma_T^2.\n\\end{aligned}\n\\]\nVerifichiamo per i dati dell’esempio.\n\ncov(X, T) == var(T)\n\nTRUE",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#correlazione-tra-punteggio-osservato-e-punteggio-vero",
    "href": "chapters/ctt/02_ctt_2.html#correlazione-tra-punteggio-osservato-e-punteggio-vero",
    "title": "9  L’affidabilità del test",
    "section": "9.4 Correlazione tra punteggio osservato e punteggio vero",
    "text": "9.4 Correlazione tra punteggio osservato e punteggio vero\nLa correlazione tra punteggio osservato \\(X\\) e punteggio vero \\(T\\) è uguale al rapporto tra la covarianza tra \\(X\\) e \\(T\\) divisa per il prodotto delle due deviazioni standard:\n\\[\n\\rho_{XT} = \\frac{\\sigma_{XT}}{\\sigma_X \\sigma_T} = \\frac{\\sigma^2_{T}}{\\sigma_X \\sigma_T} = \\frac{\\sigma_{T}}{\\sigma_X}.\n\\tag{9.4}\\]\nDunque, la correlazione tra il punteggio osservato e il punteggio vero è uguale al rapporto tra la deviazione standard dei punteggi veri e la deviazione standard dei punteggi osservati.\nVerifichiamo per i dati dell’esempio.\n\ncor(X, T) \n\n0.816496580927726\n\n\n\nsd(T) / sd(X)\n\n0.816496580927726",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#definizione-e-significato-dellattendibilità",
    "href": "chapters/ctt/02_ctt_2.html#definizione-e-significato-dellattendibilità",
    "title": "9  L’affidabilità del test",
    "section": "9.5 Definizione e significato dell’attendibilità",
    "text": "9.5 Definizione e significato dell’attendibilità\nSulla base dell’Equazione 9.4, possiamo giungere alla definizione di attendibilità. La Teoria della Misurazione Classica (CTT) definisce l’attendibilità di un test (o di un singolo elemento) come il rapporto tra la varianza del punteggio vero e la varianza del punteggio osservato. In altre parole, l’attendibilità rappresenta il quadrato della correlazione tra il punteggio osservato \\(X\\) e il punteggio vero \\(T\\):\n\\[\n\\begin{equation}\n\\rho_{XT}^2 = \\frac{\\sigma_{T}^2}{\\sigma_{X}^2}.\n\\end{equation}\n\\]\nQuesta formula è il concetto fondamentale della CTT e misura il livello di variazione del punteggio vero rispetto alla variazione del punteggio osservato.\nAdesso possiamo procedere a verificare questa relazione utilizzando i dati forniti nell’esempio.\n\ncor(X, T)^2\n\n0.666666666666667\n\n\n\nvar(T) / var(X)\n\n0.666666666666667\n\n\nDato che \\(\\sigma^2_X = \\sigma_T^2 + \\sigma_E^2\\), in base alla {ref}eq-reliability-1 possiamo scrivere\n\\[\n\\begin{equation}\n\\rho_{XT}^2 = \\frac{\\sigma_{T}^2}{\\sigma_X^2} =\\frac{\\sigma_{X}^2 - \\sigma^2_E}{\\sigma_X^2} = 1-\\frac{\\sigma_{E}^2}{\\sigma_{X}^2}.\n\\end{equation}\n\\tag{9.5}\\]\n\n1 - (var(E) / var(X))\n\n0.666666666666667\n\n\nDall’Equazione 9.5, possiamo dedurre che il coefficiente di affidabilità assume il valore di \\(1\\) quando la varianza degli errori \\(\\sigma_{E}^2\\) è nulla, e assume il valore di \\(0\\) quando la varianza degli errori è uguale alla varianza del punteggio osservato. Quindi, il coefficiente di affidabilità è un valore assoluto situato nell’intervallo tra \\(0\\) e \\(1\\).",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#attendibilità-e-modello-di-regressione-lineare",
    "href": "chapters/ctt/02_ctt_2.html#attendibilità-e-modello-di-regressione-lineare",
    "title": "9  L’affidabilità del test",
    "section": "9.6 Attendibilità e modello di regressione lineare",
    "text": "9.6 Attendibilità e modello di regressione lineare\nIn parole semplici, la CTT si basa sul modello di regressione lineare, dove i punteggi osservati sono considerati come variabile dipendente e i punteggi veri come variabile indipendente. Il coefficiente di attendibilità \\(\\rho_{XT}^2\\) rappresenta la proporzione di varianza nella variabile dipendente spiegata dalla variabile indipendente in un modello di regressione lineare con una pendenza unitaria e un’intercetta di zero. In altre parole, il coefficiente di attendibilità è equivalente al coefficiente di determinazione del modello di regressione.\nPer rendere questo concetto più chiaro, possiamo tornare a considerare i dati simulati come esempio.\nLa motivazione di questa simulazione è quella di mettere in relazione il coefficiente di attendibilità, calcolato con la formula della CTT (come abbiamo fatto sopra), con il modello di regressione lineare. Analizziamo dunque i dati della simulazione mediante il seguente modello di regressione lineare:\n\\[\nX = a + b T + E.\n\\]\nUsando \\(\\textsf{R}\\) otteniamo:\n\nfm &lt;- lm(X ~ T)\nsummary(fm)\n\n\nCall:\nlm(formula = X ~ T)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.4343 -0.9720 -0.0865  1.0803  3.7347 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 9.948e-15  8.746e-01       0        1    \nT           1.000e+00  7.143e-02      14   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.741 on 98 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6633 \nF-statistic:   196 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nSi noti che la retta di regressione ha intercetta 0 e pendenza 1. Questo è coerente con l’assunzione \\(\\mathbb{E}(X) = \\mathbb{E}(T)\\). Ma il risultato più importante di questa simulazione è che il coefficiente di determinazione (\\(R^2\\) = 0.67) del modello di regressione \\(X = 0 + 1 \\times T + E\\) è identico al coefficiente di attendibilità calcolato con la formula \\(\\rho_{XT}^2 = \\frac{\\sigma_{T}^2}{\\sigma_X^2}\\):\n\nvar(T) / var(X)\n\n0.666666666666667\n\n\nQuesti risultati ci permettono di interpretare il coefficiente di affidabilità nel seguente modo: l’affidabilità di un test rappresenta la porzione di varianza presente nel punteggio osservato \\(X\\) che viene spiegata dalla regressione di \\(X\\) rispetto al punteggio vero \\(T\\). Questo risultato è stato ottenuto mediante una regressione lineare, dove il coefficiente angolare \\(\\beta\\) è uguale a 1 e l’intercetta \\(\\alpha\\) è uguale a 0.\nInoltre, ricordiamo che la radice quadrata della varianza degli errori è l’errore standard della misurazione, \\(\\sigma_E\\). La quantità \\(\\sqrt{\\sigma_E^2}\\) fornisce una misura della dispersione del punteggio osservato attorno al valore vero, nella condizione ipotetica di ripetute somministrazioni del test:\n\nsqrt(var(E) * 99 / 98)\n\n1.74086537242199\n\n\nL’output della funzione lm() rende chiaro che l’errore standard della misurazione della CTT è identico all’errore standard della regressione nel caso di un modello di regressione definito come abbiamo fatto sopra.\nNel codice precedente è stato incluso il termine correttivo 99/98. Questa correzione è necessaria poiché, mentre R calcola la deviazione standard con \\(n-1\\) al denominatore, l’errore standard della regressione richiede \\(n-2\\) al denominatore.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#misurazioni-parallele-e-affidabilità",
    "href": "chapters/ctt/02_ctt_2.html#misurazioni-parallele-e-affidabilità",
    "title": "9  L’affidabilità del test",
    "section": "9.7 Misurazioni parallele e affidabilità",
    "text": "9.7 Misurazioni parallele e affidabilità\nL’equazione \\(\\rho_{XT}^2 = \\frac{\\sigma_{T}^2}{\\sigma_X^2}\\) definisce il coefficiente di affidabilità, ma non ci fornisce gli strumenti pratici per calcolarlo direttamente. Questo perché la varianza del punteggio reale \\(\\sigma_{T}^2\\) rappresenta un valore sconosciuto. Il metodo utilizzato dalla CTT per ottenere una stima empirica dell’attendibilità è quello delle forme parallele del test. In pratica, se è possibile creare versioni alternative del test che siano equivalenti in termini di contenuto, modalità di risposta e caratteristiche statistiche, allora diventa possibile ottenere una stima empirica del coefficiente di affidabilità.\nSecondo la CTT, due test \\(X=T+E\\) e \\(X^\\prime=T^\\prime+E^\\prime\\) sono considerati misurazioni parallele della stessa abilità latente quando:\n\n\\(T = T^\\prime\\),\n\\(\\mathbb{V}(E) = \\mathbb{V}(E^\\prime)\\).\n\nQueste premesse implicano che \\(\\mathbb{E}(X) = \\mathbb{E}(X^\\prime)\\).\nLa dimostrazione procede come segue. Considerando che \\(\\mathbb{E}(X) = T\\) e \\(\\mathbb{E}(X^\\prime) = T\\), è evidente che \\(\\mathbb{E}(X) =\\mathbb{E}(X^\\prime)\\) poiché \\(\\mathbb{E}(E) = \\mathbb{E}(E^\\prime) = 0\\).\nIn modo analogo, l’uguaglianza delle varianze nei punteggi osservati delle due misurazioni parallele deve essere verificata, cioè \\(\\mathbb{V}(X) = \\mathbb{V}(X^\\prime)\\).\nQuesta dimostrazione si sviluppa come segue. Per \\(X\\), possiamo scrivere\n\\[\\mathbb{V}(X) = \\mathbb{V}(T + E) = \\mathbb{V}(T) + \\mathbb{V}(E);\\]\nmentre per \\(X^\\prime\\) possiamo scrivere\n\\[\\mathbb{V}(X^\\prime) = \\mathbb{V}(T^\\prime + E^\\prime) = \\mathbb{V}(T^\\prime) + \\mathbb{V}(E^\\prime).\\]\nPoiché sappiamo che \\(\\mathbb{V}(E) = \\mathbb{V}(E^\\prime)\\) e che \\(T = T^\\prime\\), possiamo dedurre che \\(\\mathbb{V}(X) = \\mathbb{V}(X^\\prime)\\).\nIn aggiunta, è importante notare che per costruzione gli errori \\(E\\) e \\(E^\\prime\\) sono incorrelati sia con \\(T\\) che tra di loro.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#la-correlazione-tra-due-forme-parallele-del-test",
    "href": "chapters/ctt/02_ctt_2.html#la-correlazione-tra-due-forme-parallele-del-test",
    "title": "9  L’affidabilità del test",
    "section": "9.8 La correlazione tra due forme parallele del test",
    "text": "9.8 La correlazione tra due forme parallele del test\nOra procediamo a dimostrare che, secondo le ipotesi della Teoria della CTT, la correlazione tra due versioni parallele di un test è effettivamente equivalente al rapporto tra la varianza del punteggio reale e la varianza del punteggio osservato. Come discusso nel capitolo precedente, le misurazioni parallele rappresentano il grado più elevato di somiglianza tra due diverse versioni di un test.\nLa dimostrazione è la seguente. Consideriamo, senza perdita di generalità, che \\(\\mathbb{E}(X) = \\mathbb{E}(X') = \\mathbb{E}(T) = 0\\). Questa scelta ci consente di scrivere:\n\\[\n\\begin{aligned}\n\\rho_{X X^\\prime} &= \\frac{\\sigma(X, X^\\prime)}{\\sigma(X) \\sigma(X^\\prime)} \\\\\n&= \\frac{\\mathbb{E}(XX^\\prime)}{\\sigma(X) \\sigma(X^\\prime)} \\\\\n&= \\frac{\\mathbb{E}[(T+E)(T+E^\\prime)]}{\\sigma(X) \\sigma(X^\\prime)} \\\\\n&= \\frac{\\mathbb{E}(T^2) + \\mathbb{E}(TE^\\prime) + \\mathbb{E}(TE) + \\mathbb{E}(EE^\\prime)}{\\sigma(X) \\sigma(X^\\prime)}.\n\\end{aligned}\n\\]\nTuttavia, sappiamo che \\(\\mathbb{E}(TE) = \\mathbb{E}(TE^\\prime) = \\mathbb{E}(EE^\\prime) = 0\\). Inoltre, \\(\\sigma(X) = \\sigma(X^\\prime) = \\sigma_X\\). Pertanto, giungiamo a:\n\\[\n\\rho_{X X^\\prime} = \\frac{\\mathbb{E}(T^2)}{\\sigma_X \\sigma_X} = \\frac{\\sigma^2_T}{\\sigma^2_X}.\n\\] {#eq:3-3-5}\nNotiamo che il risultato ottenuto, insieme all’equazione che definisce il coefficiente di affidabilità \\(\\rho_{XT}^2 = \\frac{\\sigma_{T}^2}{\\sigma_X^2}\\), presentano entrambi la stessa espressione a destra del segno di uguale. Questo conduce a un risultato cruciale: il coefficiente di affidabilità, ossia il quadrato della correlazione tra il punteggio osservato e il punteggio reale, è identico alla correlazione tra i punteggi osservati di due versioni parallele del test:\n\\[\n\\rho^2_{XT} =  \\rho_{XX^\\prime}.\n\\tag{9.6}\\]\nQuesta conclusione è di notevole importanza in quanto consente di esprimere la variabile inosservabile \\(\\rho^2_{XT}\\) in termini della variabile osservabile \\(\\rho_{XX^\\prime}\\), la quale può essere calcolata in base ai punteggi osservati delle due forme parallele del test. Fondamentalmente, la stima di \\(\\rho^2_{XT}\\) si semplifica nella stima di \\(\\rho^2_{XX^\\prime}\\). Questo spiega l’importanza dell’equazione {eq}eq-rho2xt-rhoxx nella CTT. Inoltre, è da sottolineare che l’equazione {ref}eq:rho2xt-rhoxx fornisce una giustificazione per l’utilizzo della correlazione split-half come misura di affidabilità.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#la-correlazione-tra-punteggio-osservato-e-punteggio-vero",
    "href": "chapters/ctt/02_ctt_2.html#la-correlazione-tra-punteggio-osservato-e-punteggio-vero",
    "title": "9  L’affidabilità del test",
    "section": "9.9 La correlazione tra punteggio osservato e punteggio vero",
    "text": "9.9 La correlazione tra punteggio osservato e punteggio vero\nEsaminiamo adesso la correlazione tra il punteggio osservato e il punteggio reale. L’Equazione 9.6 può essere riformulata come segue:\n\\[\n\\rho_{XT} = \\sqrt{\\rho_{XX^\\prime}}.\n\\]\nIn altre parole, la radice quadrata del coefficiente di affidabilità equivale alla correlazione tra il punteggio osservato e il punteggio reale.\nProcediamo ora a verificare questa relazione utilizzando i dati dell’esempio.\n\nsqrt(var(T) / var(X))\n\n0.816496580927726\n\n\n\ncor(X, T)\n\n0.816496580927726",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#i-fattori-che-influenzano-lattendibilità",
    "href": "chapters/ctt/02_ctt_2.html#i-fattori-che-influenzano-lattendibilità",
    "title": "9  L’affidabilità del test",
    "section": "9.10 I fattori che influenzano l’attendibilità",
    "text": "9.10 I fattori che influenzano l’attendibilità\nConsiderando le tre equazioni:\n\\[\n\\rho^2_{XT} = \\rho_{XX'},\\quad\n\\rho_{XT}^2 = \\frac{\\sigma_{T}^2}{\\sigma_X^2}, \\quad\n\\rho_{XT}^2 = 1-\\frac{\\sigma_{E}^2}{\\sigma_X^2},\n\\]\npossiamo affermare che esistono tre modi equivalenti per giungere alla conclusione che l’attendibilità di un test è elevata. L’attendibilità di un test è considerata alta quando si verificano le seguenti condizioni:\n\nLa correlazione tra le forme parallele del test è elevata.\nLa varianza del punteggio vero è ampia rispetto alla varianza del punteggio osservato.\nLa varianza dell’errore di misurazione è ridotta rispetto alla varianza del punteggio osservato.\n\nQueste considerazioni rivestono un’importanza fondamentale nella progettazione di un test. In particolare, l’equazione \\(\\rho^2_{XT} = \\rho_{XX'}\\) fornisce un criterio per la selezione degli item da includere nel test. Se interpretiamo \\(\\rho_{XX'}\\) come la correlazione tra due item, allora gli item che presentano la correlazione più elevata tra di loro dovrebbero essere inclusi nel test. In questo modo, l’attendibilità del test aumenta, poiché gli item selezionati risultano fortemente correlati con il punteggio vero.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#riflessioni-conclusive",
    "href": "chapters/ctt/02_ctt_2.html#riflessioni-conclusive",
    "title": "9  L’affidabilità del test",
    "section": "9.11 Riflessioni conclusive",
    "text": "9.11 Riflessioni conclusive\nL’affidabilità costituisce un concetto fondamentale all’interno della teoria della misurazione, poiché si riferisce alla coerenza dei punteggi in varie situazioni, come diverse configurazioni di item, versioni del test o momenti di somministrazione. Nel corso di questo capitolo, abbiamo esplorato le basi teoriche dell’affidabilità. All’interno della CTT, l’affidabilità è definita come la correlazione tra il punteggio vero e il punteggio osservato, oppure, equivalentemente, come uno meno la correlazione tra il punteggio di errore e il punteggio osservato. Dal momento che il punteggio vero non è direttamente osservabile, è necessario ricorrere a metodi alternativi per stimare l’affidabilità. Il metodo proposto dalla CTT per ottenere tale stima è quello della correlazione dei punteggi ottenuti da due test paralleli.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/02_ctt_2.html#session-info",
    "href": "chapters/ctt/02_ctt_2.html#session-info",
    "title": "9  L’affidabilità del test",
    "section": "9.12 Session Info",
    "text": "9.12 Session Info\n\nsessionInfo()\n\nR version 4.3.2 (2023-10-31)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.3.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] MASS_7.3-60.0.1    modelsummary_1.4.5 ggokabeito_0.1.0   viridis_0.6.5     \n [5] viridisLite_0.4.2  ggpubr_0.6.0       ggExtra_0.10.1     bayesplot_1.11.1  \n [9] gridExtra_2.3      patchwork_1.2.0    semTools_0.5-6     semPlot_1.1.6     \n[13] lavaan_0.6-17      psych_2.4.1        scales_1.3.0       markdown_1.12     \n[17] knitr_1.45         lubridate_1.9.3    forcats_1.0.0      stringr_1.5.1     \n[21] dplyr_1.1.4        purrr_1.0.2        readr_2.1.5        tidyr_1.3.1       \n[25] tibble_3.2.1       ggplot2_3.4.4      tidyverse_2.0.0    here_1.0.1        \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5   nloptr_2.0.3      \n  [7] rmarkdown_2.25     vctrs_0.6.5        minqa_1.2.6       \n [10] base64enc_0.1-3    rstatix_0.7.2      htmltools_0.5.7   \n [13] broom_1.0.5        Formula_1.2-5      htmlwidgets_1.6.4 \n [16] plyr_1.8.9         sandwich_3.1-0     emmeans_1.10.0    \n [19] zoo_1.8-12         uuid_1.2-0         igraph_2.0.2      \n [22] mime_0.12          lifecycle_1.0.4    pkgconfig_2.0.3   \n [25] Matrix_1.6-5       R6_2.5.1           fastmap_1.1.1     \n [28] shiny_1.8.0        digest_0.6.34      OpenMx_2.21.11    \n [31] fdrtool_1.2.17     colorspace_2.1-0   rprojroot_2.0.4   \n [34] Hmisc_5.1-1        fansi_1.0.6        timechange_0.3.0  \n [37] abind_1.4-5        compiler_4.3.2     withr_3.0.0       \n [40] glasso_1.11        htmlTable_2.4.2    backports_1.4.1   \n [43] carData_3.0-5      ggsignif_0.6.4     corpcor_1.6.10    \n [46] gtools_3.9.5       tools_4.3.2        pbivnorm_0.6.0    \n [49] foreign_0.8-86     zip_2.3.1          httpuv_1.6.14     \n [52] nnet_7.3-19        glue_1.7.0         quadprog_1.5-8    \n [55] nlme_3.1-164       promises_1.2.1     lisrelToR_0.3     \n [58] grid_4.3.2         pbdZMQ_0.3-11      checkmate_2.3.1   \n [61] cluster_2.1.6      reshape2_1.4.4     generics_0.1.3    \n [64] gtable_0.3.4       tzdb_0.4.0         data.table_1.15.0 \n [67] hms_1.1.3          car_3.1-2          utf8_1.2.4        \n [70] tables_0.9.17      sem_3.1-15         pillar_1.9.0      \n [73] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [76] splines_4.3.2      lattice_0.22-5     survival_3.5-8    \n [79] kutils_1.73        tidyselect_1.2.0   miniUI_0.1.1.1    \n [82] pbapply_1.7-2      stats4_4.3.2       xfun_0.42         \n [85] qgraph_1.9.8       arm_1.13-1         stringi_1.8.3     \n [88] boot_1.3-29        evaluate_0.23      codetools_0.2-19  \n [91] mi_1.1             cli_3.6.2          RcppParallel_5.1.7\n [94] IRkernel_1.3.2     rpart_4.1.23       xtable_1.8-4      \n [97] repr_1.1.6         munsell_0.5.0      Rcpp_1.0.12       \n[100] coda_0.19-4.1      png_0.1-8          XML_3.99-0.16.1   \n[103] parallel_4.3.2     ellipsis_0.3.2     jpeg_0.1-10       \n[106] lme4_1.1-35.1      mvtnorm_1.2-4      insight_0.19.8    \n[109] openxlsx_4.2.5.2   crayon_1.5.2       rlang_1.1.3       \n[112] multcomp_1.4-25    mnormt_2.1.1",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L'affidabilità del test</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html",
    "href": "chapters/ctt/03_ctt_3.html",
    "title": "10  Metodi di stima dell’affidabilità",
    "section": "",
    "text": "10.1 Approcci per Stimare l’Affidabilità\nPer stimare l’affidabilità (\\(\\rho_{TT'}\\)), ci troviamo di fronte alla sfida di dover stimare una delle due componenti non direttamente osservabili: il punteggio vero o la varianza dell’errore. Ma come possiamo affrontare questa sfida? La risposta è complessa e dipende da come intendiamo concettualizzare la varianza dell’errore (\\(\\sigma^2_E\\)).\nIn sostanza, le equazioni dell’affidabilità presentate in precedenza possono essere applicate a ciascuno dei tre tipi di affidabilità descritti sopra. La differenza fondamentale risiede nella nostra concezione e nel calcolo di \\(\\sigma^2_E\\), che varia a seconda del contesto e degli obiettivi specifici dell’analisi.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#approcci-per-stimare-laffidabilità",
    "href": "chapters/ctt/03_ctt_3.html#approcci-per-stimare-laffidabilità",
    "title": "10  Metodi di stima dell’affidabilità",
    "section": "",
    "text": "Affidabilità delle Forme Parallele: Se il nostro interesse principale è misurare quanto accuratamente possiamo stimare il punteggio vero dai dati osservati, potrebbe essere più appropriato considerare \\(\\sigma^2_E\\) come l’incertezza nella nostra stima attraverso ripetute somministrazioni di una misura equivalente. Questo approccio ci porta alla definizione di affidabilità delle forme parallele.\nConsistenza Interna: Se invece vogliamo valutare se più elementi su una scala riflettono lo stesso costrutto sottostante, possiamo utilizzare un concetto simile all’Alpha di Cronbach (\\(\\alpha\\)). Questo ci porta alla definizione di affidabilità come consistenza interna.\nCoerenza Temporale (Affidabilità Test-Retest): Se ci interessa la coerenza di una misura nel tempo, allora \\(\\sigma^2_E\\) potrebbe essere meglio interpretato come la varianza non comune attraverso diverse somministrazioni della stessa misura su un periodo di tempo arbitrario. Questo concetto ci conduce alla definizione di coerenza temporale o affidabilità test-retest.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#affidabilità-come-consistenza-interna",
    "href": "chapters/ctt/03_ctt_3.html#affidabilità-come-consistenza-interna",
    "title": "10  Metodi di stima dell’affidabilità",
    "section": "10.2 Affidabilità come Consistenza Interna",
    "text": "10.2 Affidabilità come Consistenza Interna\nIniziamo esaminando tre scenari distinti che illustrano le possibili relazioni tra gli item di un test: quelli con indicatori congenerici, tau-equivalenti e paralleli. Nell’ambito della CTT, sono disponibili due indicatori principali per valutare l’affidabilità in termini di coerenza interna, a seconda del tipo di relazione tra gli item presunta: l’indice alpha di Cronbach per gli item tau-equivalenti e l’indice di Spearman-Brown per gli item paralleli.\nOltre alla consistenza interna, esistono altre misure di affidabilità, tra cui la affidabilità test-retest, la affidabilità tra forme alternative, la affidabilità tra valutatori, la affidabilità dei punteggi compositi e la affidabilità dei punteggi delle differenze.\nAl centro della misurazione dell’affidabilità c’è l’errore di misurazione, e in precedenza abbiamo esaminato come lo standard error of measurement sia uno dei metodi per valutare l’errore di misurazione.\nVa notato che ci riferiamo all’affidabilità come una stima, poiché l’affidabilità assoluta o precisa dei risultati della valutazione non può essere conosciuta con certezza. Proprio come ci sono sempre degli errori nei punteggi dei test, ci sono anche degli errori nei nostri tentativi di misurare l’affidabilità. Tuttavia, i metodi di stima dell’affidabilità che discuteremo sono considerati stime conservative e rappresentano il limite inferiore della vera affidabilità dei punteggi dei test. In altre parole, l’affidabilità effettiva dei punteggi dei test è almeno altrettanto alta, se non superiore, rispetto all’affidabilità stimata (Reynolds, 1999).\n\n10.2.0.1 Coefficienti di consistenza interna\nLa CTT presenta il metodo delle forme parallele come un approccio parziale per stimare l’attendibilità dei test. Questo metodo prevede la somministrazione di due test distinti, indicati come \\(X\\) e \\(X^\\prime\\), che valutano lo stesso costrutto, a un campione di individui nello stesso momento. In questo contesto, la correlazione tra i punteggi totali dei due test, \\(\\rho^2_{XT} = \\rho_{XX^\\prime}\\), rappresenta l’indicatore principale dell’attendibilità. Tuttavia, è cruciale che le due versioni del test siano effettivamente parallele, secondo la definizione fornita dalla teoria classica dei test, affinché questa relazione sia valida.\nNella pratica, risulta impraticabile somministrare lo stesso test due volte agli stessi partecipanti “nelle stesse condizioni”, come richiesto dal metodo delle forme parallele. Di conseguenza, la stima dell’attendibilità deve basarsi sui dati raccolti attraverso una singola somministrazione del test. La CTT risponde a questa sfida introducendo specifici indicatori di coerenza interna, mirati a valutare l’affidabilità.\nQuesti indicatori di coerenza interna costituiscono la soluzione proposta dalla CTT per affrontare tale problematica. La loro logica si basa sull’idea che una correlazione tra i punteggi di diversi item che misurano lo stesso costrutto rifletta la varianza condivisa del punteggio reale, anziché la varianza condivisa dell’errore. Considerando che gli errori casuali dovrebbero mancare di una varianza condivisa, i coefficienti di coerenza interna riflettono la correlazione tra gli item all’interno del test, offrendo così un’indicazione dell’affidabilità generale della scala di misurazione.\nOltre a questo, gli item stessi possono rappresentare una fonte di errore nei punteggi dei test. Problemi come formulazioni confuse, item non coerenti con il costrutto, linguaggio poco comprensibile o item con risposte ambigue possono emergere quando gli item non sono formulati in modo adeguato. Tali problemi possono portare a risposte inconsistenti per due ragioni: innanzitutto, i partecipanti potrebbero reagire in modi diversi agli item problematici; in secondo luogo, tali item interferiscono con la capacità dei partecipanti di esprimere il loro reale livello del costrutto.\nPer valutare la coerenza delle risposte tra gli item all’interno di una scala, vengono impiegati i coefficienti di consistenza interna. Questi coefficienti si basano sull’assunto che una correlazione tra due punteggi osservati, che misurano lo stesso costrutto, rifletta la varianza condivisa del punteggio reale, non la varianza condivisa dell’errore. Dal momento che gli errori casuali dovrebbero mancare di varianza condivisa, i coefficienti di consistenza interna riflettono la correlazione tra gli item del test e forniscono un’indicazione dell’affidabilità complessiva della scala.\nQuando si valuta l’attendibilità con una singola somministrazione del test, sono disponibili vari approcci. In questo capitolo, esamineremo due metodi proposti dalla CTT: l’indice \\(\\alpha\\) di Cronbach e il metodo di Spearman-Brown. L’indice \\(\\alpha\\) è l’indicatore più comunemente usato per valutare l’attendibilità in termini di coerenza interna o omogeneità. Analizzeremo come questo indice rappresenta il valore minimo possibile dell’attendibilità di un test, sotto determinate ipotesi soddisfatte, e come, allo stesso tempo, può fornire una valutazione distorta dell’attendibilità se le assunzioni che delineeremo non sono rispettate.\nTuttavia, prima di esplorare dettagliatamente questi due diversi metodi di stima dell’attendibilità come coerenza interna, è essenziale distinguere tra tre diverse tipologie di relazioni tra gli item: item congenerici, item \\(\\tau\\)-equivalenti e item paralleli.\n\n\n10.2.0.2 Test paralleli\nSimuliamo i punteggi di due test paralleli.\n\nset.seed(2237) # setting the seed ensure reproducibility\nnum_person &lt;- 1000 # number of respondents\n# True scores for Test 1\nt1 &lt;- rnorm(num_person, mean = 20, sd = 5)\n# Error scores for Test 1\ne1 &lt;- rnorm(num_person, mean = 0, sd = 2)\n# Observed scores for Test 1\nx1 &lt;- t1 + e1\n# True scores for Test 2\nt2 &lt;- t1 # parallel tests have equal true scores\n# Error scores for Test 2\ne2 &lt;- rnorm(num_person, mean = 0, sd = 2)\n# Observed scores for Test 2\nx2 &lt;- t2 + e2\n\n# Merge into a data frame\ntest_df &lt;- data.frame(x1, x2)\n\nmv &lt;- datasummary(x1 + x2 ~ Mean + Var,\n    data = test_df,\n    output = \"data.frame\"\n)\nmv\n\n\nA data.frame: 2 x 3\n\n\n\nMean\nVar\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\nx1\n20.41\n29.20\n\n\nx2\n20.31\n30.27\n\n\n\n\n\n\n# Correlation\ncor(test_df) |&gt;\n    round(2)\n\n\nA matrix: 2 x 2 of type dbl\n\n\n\nx1\nx2\n\n\n\n\nx1\n1.00\n0.87\n\n\nx2\n0.87\n1.00\n\n\n\n\n\n\nvar(t1) / var(x1)\n\n0.878424313030747\n\n\n\nvar(t2) / var(x2)\n\n0.847351804948915\n\n\nIn conclusione, per test paralleli: - le medie e le varianze dei punteggi osservati sono statisticamente uguali; - la correlazione è uguale all’attendibilità.\n\n\n10.2.0.3 Test \\(\\tau\\)-equivalenti\n\n# True scores for Test 3\nt3 &lt;- 5 + t1 # essentially tau-equivalent tests\n# Error scores for Test 3 (larger error SDs)\ne3 &lt;- rnorm(num_person, mean = 0, sd = 4)\n# Observed scores for Test 2\nx3 &lt;- t3 + e3\n\n# Merge into a data frame\ntest_df2 &lt;- data.frame(x1, x3)\n# Get means and variances\nmv &lt;- datasummary(x1 + x3 ~ Mean + Var,\n    data = test_df2,\n    output = \"data.frame\"\n)\nmv\n\n\nA data.frame: 2 x 3\n\n\n\nMean\nVar\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\nx1\n20.41\n29.20\n\n\nx3\n25.41\n41.50\n\n\n\n\n\n\n# Correlation\ncor(test_df2) |&gt;\n    round(2)\n\n\nA matrix: 2 x 2 of type dbl\n\n\n\nx1\nx3\n\n\n\n\nx1\n1.00\n0.72\n\n\nx3\n0.72\n1.00\n\n\n\n\n\nSe conosciamo i punteggi veri, l’attendibilità di X3 si trova come\n\n# Reliability for x3\nvar(t3) / var(x3)\n\n0.618012243898734\n\n\nIn conclusione, per test tau-equivalenti: - le medie e le varianze dei punteggi osservati sono diverse; - correlazione \\(\\neq\\) attendibilità.\n\n\n10.2.0.4 Test congenerici\n\n# True scores for Test 4\nt4 &lt;- 2 + 0.8 * t1\n# Error scores for Test 4 (larger error SDs)\ne4 &lt;- rnorm(num_person, mean = 0, sd = 3)\n# Observed scores for Test 2\nx4 &lt;- t4 + e4\n\n# Merge into a data frame\ntest_df3 &lt;- data.frame(x1, x4)\n# Get means and variances\nmv &lt;- datasummary(x1 + x4 ~ Mean + Var,\n    data = test_df3,\n    output = \"data.frame\"\n)\nmv\n\n\nA data.frame: 2 x 3\n\n\n\nMean\nVar\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\nx1\n20.41\n29.20\n\n\nx4\n18.27\n24.23\n\n\n\n\n\n\n# Correlation\ncor(test_df3) |&gt;\n    round(2)\n\n\nA matrix: 2 x 2 of type dbl\n\n\n\nx1\nx4\n\n\n\n\nx1\n1.00\n0.73\n\n\nx4\n0.73\n1.00\n\n\n\n\n\nSe conosciamo i punteggi veri, l’attendibilità di X4 si trova come\n\n# Reliability for x4\nvar(t4) / var(x4)\n\n0.677398252481377\n\n\nIn conclusione, per test congenerici: - le medie e le varianze dei punteggi osservati sono diverse; - correlazione \\(\\neq\\) attendibilità; - sono necessari più di due test per distinguere test congenerici e test \\(\\tau\\)-equivalenti.\n\n\n10.2.0.5 Coefficiente \\(\\alpha\\) di Cronbach\nIl coefficiente \\(\\alpha\\) consente la stima dell’affidabilità nel contesto di indicatori \\(\\tau\\)-equivalenti. In queste circostanze, l’attendibilità viene valutata utilizzando l’equazione:\n\\[\n\\alpha = \\frac{{k}}{{k-1}} \\left( 1 - \\frac{{\\sum_{i=1}^{k} \\sigma_{X_i}^{2}}}{{\\sigma_{X}^{2}}} \\right)\n\\]\ndove: - \\(k\\) è il numero di item nel test, - \\(\\sigma_{i}^{2}\\) rappresenta la varianza del punteggio dell’item \\(i\\), - \\(\\sigma_{X}^{2}\\) è la varianza totale dei punteggi del test.\nUna derivazione della formula del coefficiente alpha di Cronbach è fornita nel capitolo {ref}reliability-fa-notebook.\nFu Guttman nel 1945 a scoprire questo coefficiente, anche se erroneamente attribuito a Cronbach. È spesso noto come coefficiente \\(\\alpha\\) di Guttman-Cronbach o G-C \\(\\alpha\\).\nQuando il modello di \\(\\tau\\)-equivalenza è applicabile, il coefficiente \\(\\alpha\\) costituisce un limite inferiore dell’affidabilità, in altri termini, il coefficiente \\(\\alpha\\) offre una stima prudente dell’affidabilità. Questa caratteristica è considerata uno dei principali vantaggi di questo indice. Tuttavia, è fondamentale notare che questa natura conservativa del coefficiente \\(\\alpha\\) vale solo se le ipotesi del modello \\(\\tau\\)-equivalente sono rispettate.\nIl coefficiente di attendibilità \\(\\alpha\\) è ampiamente utilizzato nell’ambito della psicometria. Tuttavia, come menzionato in precedenza, quando l’assunzione di \\(\\tau\\)-equivalenza non è valida, \\(\\alpha\\) può perdere la sua proprietà conservativa e sovrastimare l’attendibilità del test (Sijtsma, 2009). In tal caso, è necessario valutare attentamente l’adeguatezza dell’utilizzo del coefficiente \\(\\alpha\\) come indicatore di affidabilità.\nEsempio. Per illustrare la procedura di calcolo del coefficiente \\(\\alpha\\), useremo i dati bfi contenuti nel pacchetto psych. Il dataframe bfi comprende 25 item di autovalutazione della personalità. Sono riportati i dati di 2800 soggetti. Ci concentreremo qui sulla sottoscala Openness. - O1: Am full of ideas; - O2: Avoid difficult reading material; - O3: Carry the conversation to a higher level; - O4: Spend time reflecting on things; - O5: Will not probe deeply into a subject.\nLeggiamo i dati in R.\n\ndata(bfi, package = \"psych\")\nhead(bfi[c(\"O1\", \"O2\", \"O3\", \"O4\", \"O5\")])\n\n\nA data.frame: 6 x 5\n\n\n\nO1\nO2\nO3\nO4\nO5\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n61617\n3\n6\n3\n4\n3\n\n\n61618\n4\n2\n4\n3\n3\n\n\n61620\n4\n2\n5\n5\n2\n\n\n61621\n3\n3\n4\n3\n5\n\n\n61622\n3\n3\n4\n3\n3\n\n\n61623\n4\n3\n5\n6\n1\n\n\n\n\n\nEsaminiamo la correlazione tra gli item della sottoscale Openness.\n\ncor(bfi[c(\"O1\", \"O2\", \"O3\", \"O4\", \"O5\")], use = \"pairwise.complete.obs\") |&gt;\n    round(2)\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nO1\nO2\nO3\nO4\nO5\n\n\n\n\nO1\n1.00\n-0.21\n0.40\n0.18\n-0.24\n\n\nO2\n-0.21\n1.00\n-0.26\n-0.07\n0.32\n\n\nO3\n0.40\n-0.26\n1.00\n0.19\n-0.31\n\n\nO4\n0.18\n-0.07\n0.19\n1.00\n-0.18\n\n\nO5\n-0.24\n0.32\n-0.31\n-0.18\n1.00\n\n\n\n\n\nÈ necessario ricodificare due item.\n\nbfi$O2r &lt;- 7 - bfi$O2\nbfi$O5r &lt;- 7 - bfi$O5\n\n\ncor(bfi[c(\"O1\", \"O2r\", \"O3\", \"O4\", \"O5r\")], use = \"pairwise.complete.obs\") |&gt;\n    round(2)\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nO1\nO2r\nO3\nO4\nO5r\n\n\n\n\nO1\n1.00\n0.21\n0.40\n0.18\n0.24\n\n\nO2r\n0.21\n1.00\n0.26\n0.07\n0.32\n\n\nO3\n0.40\n0.26\n1.00\n0.19\n0.31\n\n\nO4\n0.18\n0.07\n0.19\n1.00\n0.18\n\n\nO5r\n0.24\n0.32\n0.31\n0.18\n1.00\n\n\n\n\n\nConsideriamo la matrice di varianze e covarianze della sottoscala Openness.\n\nC &lt;- cov(bfi[c(\"O1\", \"O2r\", \"O3\", \"O4\", \"O5r\")], use = \"pairwise.complete.obs\")\nC |&gt; round(2)\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nO1\nO2r\nO3\nO4\nO5r\n\n\n\n\nO1\n1.28\n0.38\n0.54\n0.25\n0.36\n\n\nO2r\n0.38\n2.45\n0.50\n0.13\n0.67\n\n\nO3\n0.54\n0.50\n1.49\n0.29\n0.50\n\n\nO4\n0.25\n0.13\n0.29\n1.49\n0.29\n\n\nO5r\n0.36\n0.67\n0.50\n0.29\n1.76\n\n\n\n\n\nCalcoliamo alpha:\n\np &lt;- 5\nalpha &lt;- (p / (p - 1)) * (1 - tr(C) / sum(C))\nalpha\n\n0.600172514820215\n\n\nLo stesso risultato si ottiene utilizzando la funzione alpha() contenuta nel pacchetto psych:\n\npsych::alpha(C)\n\n\nReliability analysis   \nCall: psych::alpha(x = C)\n\n  raw_alpha std.alpha G6(smc) average_r S/N median_r\n       0.6      0.61    0.57      0.24 1.5     0.23\n\n    95% confidence boundaries \n      lower alpha upper\nFeldt -0.49   0.6  0.95\n\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r S/N  var.r med.r\nO1       0.53      0.53    0.48      0.22 1.1 0.0092  0.23\nO2r      0.57      0.57    0.51      0.25 1.3 0.0076  0.22\nO3       0.50      0.50    0.44      0.20 1.0 0.0071  0.20\nO4       0.61      0.62    0.56      0.29 1.6 0.0044  0.29\nO5r      0.51      0.53    0.47      0.22 1.1 0.0115  0.20\n\n Item statistics \n       r r.cor r.drop\nO1  0.65  0.52   0.39\nO2r 0.60  0.43   0.33\nO3  0.69  0.59   0.45\nO4  0.52  0.29   0.22\nO5r 0.66  0.52   0.42\n\n\n\n\n10.2.0.6 Metodi alternativi per la stima del coefficiente di attendibilità\nCi sono altri coefficienti di consistenza interna oltre al coefficiente alpha di Cronbach. Alcuni esempi includono il coefficiente KR-20 e il coefficiente KR-21, che vengono utilizzati con item dicotomici (ossia con risposte a due alternative, come vero/falso).\n\n\n10.2.0.7 Coefficiente KR-20\nLa formula di Kuder-Richardson-20 (KR-20) è un caso particolare del coefficiente α. Se ogni item è dicotomico, il coefficiente α diventa il KR-20. Il coefficiente Coefficiente KR-20 si calcola con la formula:\n\\[\nKR\\_20 = \\frac{{k}}{{k-1}} \\left( 1 - \\frac{{p(1-p)}}{{\\sigma_{X}^{2}}} \\right)\n\\]\ndove: - \\(k\\) è il numero di item nel test, - \\(p\\) è la proporzione di individui che rispondono correttamente all’item, - \\(\\sigma_{X}^{2}\\) è la varianza totale dei punteggi del test.\nEsempio. Per fare un esempio, consideriamo il data-set LSAT contenuto nel pacchetto ltm.\n\nKR20 &lt;- function(responses) {\n    # Get number of items (N) and individuals\n    n.items &lt;- ncol(responses)\n    n.persons &lt;- nrow(responses)\n    # get p_j for each item\n    p &lt;- colMeans(responses)\n    # Get total scores (X)\n    x &lt;- rowSums(responses)\n    # observed score variance\n    var.x &lt;- var(x) * (n.persons - 1) / n.persons\n    # Apply KR-20 formula\n    rel &lt;- (n.items / (n.items - 1)) * (1 - sum(p * (1 - p)) / var.x)\n    return(rel)\n}\n\n\ndata(LSAT)\nhead(LSAT)\n\n\nA data.frame: 6 x 5\n\n\n\nItem 1\nItem 2\nItem 3\nItem 4\nItem 5\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n0\n1\n\n\n5\n0\n0\n0\n0\n1\n\n\n6\n0\n0\n0\n0\n1\n\n\n\n\n\n\nKR20(LSAT)\n\n0.294997192215955\n\n\n\n\n10.2.0.8 Coefficiente KR-21\nIl coefficiente Coefficiente KR-21 si calcola con la formula:\n\\[\nKR\\_21 = \\frac{{k}}{{k-1}} \\left( 1 - \\frac{{\\frac{{\\sum_{i=1}^{k} p_{i}(1-p_{i})}}{{\\sigma_{X}^{2}}}}}{{1 - \\frac{{\\sum_{i=1}^{k} p_{i}}}{k}}} \\right)\n\\]\ndove: - \\(k\\) è il numero di item nel test, - \\(p_{i}\\) è la proporzione di individui che rispondono correttamente all’item \\(i\\), - \\(\\sigma_{X}^{2}\\) è la varianza totale dei punteggi del test.\n\n\n10.2.0.9 La formula “profetica” di Spearman-Brown\nL’indice di Spearman-Brown stima l’attendibilità nel caso di \\(p\\) indicatori paralleli:\n\\[\n\\begin{equation}\n  \\rho_p = \\frac{p \\rho_1}{(p-1)\\rho_1 + 1},\n\\end{equation}\n\\] (eq-spearman-brown-der)\ndove \\(\\rho_1\\) rappresenta l’attendibilità di un singolo elemento.\nUna derivazione della formula Spearman-Brown è fornita nel capitolo {ref}reliability-fa-notebook.\nL’equazione {eq}eq-spearman-brown-der esprime l’attendibilità \\(\\rho_p\\) di un test composto da \\(p\\) elementi paralleli in termini dell’attendibilità di un singolo elemento. Questa equazione è universalmente riconosciuta come la formula “profetica” di Spearman-Brown (Spearman-Brown prophecy formula).\nPer fare un esempio concreto, poniamoci il problema di calcolare l’attendibilità della sottoscala Openness utilizzando la formula di Spearman-Brown. Ipotizziamo dunque che gli item della scala Openness siano paralleli. La matrice di correlazione è:\n\nR &lt;- cor(bfi[c(\"O1\", \"O2r\", \"O3\", \"O4\", \"O5r\")], use = \"pairwise.complete.obs\")\nR |&gt; round(2)\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nO1\nO2r\nO3\nO4\nO5r\n\n\n\n\nO1\n1.00\n0.21\n0.40\n0.18\n0.24\n\n\nO2r\n0.21\n1.00\n0.26\n0.07\n0.32\n\n\nO3\n0.40\n0.26\n1.00\n0.19\n0.31\n\n\nO4\n0.18\n0.07\n0.19\n1.00\n0.18\n\n\nO5r\n0.24\n0.32\n0.31\n0.18\n1.00\n\n\n\n\n\nSupponiamo di calcolare l’attendibilità di un singolo item (\\(\\rho_1\\)) come la correlazione media tra gli item:\n\nrr &lt;- NULL\np &lt;- 5\nk &lt;- 1\nfor (i in 1:p) {\n    for (j in 1:p) {\n        if (j != i) {\n            rr[k] &lt;- R[i, j]\n        }\n        k &lt;- k + 1\n    }\n}\nro_1 &lt;- mean(rr, na.rm = TRUE)\nro_1\n\n0.236538319550858\n\n\nApplicando la formula di Spearman-Brown, la stima dell’attendibilità del test diventa pari a\n\n(p * ro_1) / ((p - 1) * ro_1 + 1)\n\n0.607707322439719",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#forme-parallele-del-test",
    "href": "chapters/ctt/03_ctt_3.html#forme-parallele-del-test",
    "title": "10  Metodi di stima dell’affidabilità",
    "section": "10.3 Forme parallele del test",
    "text": "10.3 Forme parallele del test\nIn alcune situazioni, è possibile avere a disposizione diverse versioni di un test che sono progettate per essere interscambiabili, in modo tale che la specifica versione del test non influenzi i punteggi ottenuti dai partecipanti. Queste forme alternative del test sono comuni soprattutto nel campo dell’educazione, dove spesso vengono preparate diverse versioni al fine di prevenire frodi o imbrogli. Inoltre, anche i ricercatori possono adottare forme alternative in studi che coinvolgono pre-test e post-test, al fine di evitare che i partecipanti beneficiino degli effetti di pratica o memoria. Tuttavia, è di fondamentale importanza determinare se i punteggi ottenuti da queste diverse versioni sono coerenti, poiché la mancanza di equivalenza tra le forme potrebbe condurre a conclusioni errate riguardo alle variazioni dei punteggi.\nLe principali fonti di errore di misurazione per le forme alternative di test cognitivi derivano dalle differenze nei contenuti, nella difficoltà e nella complessità cognitiva degli item. Per quanto riguarda i test non-cognitivi, le differenze nei contenuti e nell’intensità degli item sono motivo di attenzione. Gli sviluppatori di forme alternative adottano diverse procedure al fine di garantire l’equivalenza tra le varie versioni, basandosi sulla stessa tabella di specifiche che stabilisce la proporzione di item per i diversi domini di contenuto e i livelli cognitivi o non-cognitivi. Inoltre, vengono appaiati gli item in base alla loro difficoltà e alla loro capacità discriminante.\nI coefficienti di equivalenza, noti anche come affidabilità delle forme alternative, valutano la similitudine tra due o più versioni di un test. Per calcolare questi coefficienti, le diverse forme vengono somministrate agli stessi partecipanti e i punteggi ottenuti vengono correlati. Tuttavia, vi sono alcune considerazioni legate alla somministrazione dei test e alla possibile fatica dei partecipanti. Al fine di affrontare tali problematiche, possono essere adottate strategie come il bilanciamento dell’ordine di somministrazione e l’introduzione di un breve intervallo di tempo tra le diverse versioni. Inoltre, è importante considerare gli effetti della pratica o della memoria, i quali potrebbero influenzare i punteggi ottenuti nel secondo test somministrato. L’impiego del bilanciamento tra gruppi può contribuire a controllare tali effetti.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#attendibilità-test-retest",
    "href": "chapters/ctt/03_ctt_3.html#attendibilità-test-retest",
    "title": "10  Metodi di stima dell’affidabilità",
    "section": "10.4 Attendibilità test-retest",
    "text": "10.4 Attendibilità test-retest\nInfine, esaminiamo il concetto di “affidabilità test-retest”, che si riferisce alla coerenza o stabilità dei punteggi di un test in diverse occasioni nel corso del tempo. Questo tipo di affidabilità riveste una particolare importanza nelle situazioni in cui i punteggi vengono ottenuti in momenti diversi e confrontati, come nel caso di test effettuati prima e dopo un intervento. Inoltre, è di rilievo quando i punteggi del test vengono utilizzati per prendere decisioni diagnostiche, di selezione o di collocazione. Tuttavia, è importante sottolineare che l’affidabilità test-retest non è adatta per valutare costrutti che non sono noti per la loro stabilità nel tempo. Ciò deriva dal fatto che l’analisi della stabilità di un test potrebbe essere influenzata da effettivi cambiamenti nei livelli veri del costrutto tra i partecipanti. Di conseguenza, è essenziale che i ricercatori siano consapevoli in anticipo della stabilità del costrutto che intendono misurare. È importante notare che molti costrutti di interesse nelle scienze sociali sono generalmente considerati stabili nel tempo, come ad esempio la creatività, l’abilità cognitiva e alcune caratteristiche della personalità.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#affidabilità-dei-punteggi-compositi",
    "href": "chapters/ctt/03_ctt_3.html#affidabilità-dei-punteggi-compositi",
    "title": "10  Metodi di stima dell’affidabilità",
    "section": "10.5 Affidabilità dei punteggi compositi",
    "text": "10.5 Affidabilità dei punteggi compositi\nL’affidabilità dei punteggi compositi si riferisce alla misura in cui più punteggi ottenuti da diverse fonti possono essere combinati per creare un punteggio complessivo. Ad esempio, nella valutazione educativa, la determinazione delle votazioni spesso si basa su un punteggio complessivo ottenuto da diverse prove e altre valutazioni somministrate durante un periodo di valutazione o un semestre. Molti test psicologici standardizzati includono diverse sottoscale che vengono combinate per formare un punteggio complessivo.\nIl vantaggio dei punteggi compositi è che la loro affidabilità è generalmente maggiore rispetto a quella dei punteggi individuali delle sottoscale (o item) che contribuiscono al punteggio composto. Più precisamente, l’affidabilità di un punteggio composto è il risultato del numero di punteggi inclusi nel composto, dell’affidabilità dei punteggi individuali e della correlazione tra questi punteggi. Più punteggi sono inclusi nel composto, più alta è la correlazione tra di essi e maggiore è l’affidabilità individuale, maggiore è l’affidabilità del composto. Come abbiamo notato in precedenza, i test rappresentano semplicemente dei campioni del dominio che si intende misurare, e la combinazione di misurazioni multiple è analoga all’aumento del numero di osservazioni o della dimensione del campione.\nPer fare un esempio, supponiamo di avere due variabili aleatorie, $ X $ e $ Y $, che rappresentano i punteggi di due subtest diversi. L’affidabilità (indicata come $ $) di un test è legata alla varianza del test stesso. Un modo per esprimere l’affidabilità è attraverso il rapporto tra la varianza del vero punteggio (quello che il test intende misurare) e la varianza totale del test. Supponendo che il vero punteggio e l’errore di misura siano indipendenti, la varianza totale del test è la somma della varianza del vero punteggio e della varianza dell’errore.\nQuando combiniamo più subtest in un punteggio composito, stiamo in effetti aumentando la varianza del vero punteggio (poiché stiamo combinando più misurazioni del costrutto che vogliamo misurare) mentre l’errore di misura, supposto indipendente tra i subtest, si somma meno che proporzionalmente.\nPer rendere queste affermazioni più concrete, consideriamo un esempio numerico nel quale supponiamo che i subtest siano correlati (il che è spesso il caso in psicometria, dove diversi subtest possono misurare aspetti correlati di un costrutto più ampio).\n\n10.5.1 Calcolo per il Puniteggio Composito\nPer esempio, dati due subtest con una varianza del vero punteggio di 25 ciascuno e una covarianza di 15 (dovuta al vero punteggio), la varianza del vero punteggio nel composito è data da:\n\\[ \\text{Var}(Z_{vero}) = 25 + 25 + 2 \\cdot 15 = 80 \\]\nLa varianza totale nel composito, tenendo conto anche della varianza dell’errore di misura, sarà:\n\\[ \\text{Var}(Z_{totale}) = 35 + 35 + 2 \\cdot 15 = 100 \\]\nIl rapporto tra la varianza del vero punteggio e la varianza totale nel composito è:\n\\[ \\text{Rapporto} = \\frac{\\text{Var}(Z_{vero})}{\\text{Var}(Z_{totale})} = \\frac{80}{100} = 0.8 \\]\n\n\n10.5.2 Confronto con un Singolo Subtest\nLa varianza del vero punteggio in un singolo subtest è data (come da ipotesi) da 25.\nLa varianza totale in un singolo subtest è la somma della varianza del vero punteggio e quella dell’errore di misura, quindi 35 (25 di vero punteggio + 10 di errore).\nIl rapporto tra la varianza del vero punteggio e la varianza totale in un singolo subtest è:\n\\[ \\text{Rapporto} = \\frac{\\text{Var}(X_{vero})}{\\text{Var}(X_{totale})} = \\frac{25}{35} \\approx 0.714 \\]\nIl confronto mostra che l’affidabilità del punteggio composito (0.8) è maggiore di quella di un singolo subtest (circa 0.714). Questo esemplifica come la correlazione positiva tra i subtest possa effettivamente aumentare l’affidabilità del punteggio composito rispetto ai subtest individuali.\nQuindi, il vantaggio di combinare i punteggi dai subtest in un punteggio composito emerge principalmente quando i subtest sono in qualche modo correlati e/o quando la varianza dell’errore di misura è ridotta rispetto alla varianza del vero punteggio. In pratica, l’uso di punteggi compositi è spesso giustificato dall’idea che essi forniscono una misura più completa e rappresentativa del costrutto di interesse, riducendo l’impatto dell’errore di misura specifico di ciascun subtest.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#laffidabilità-dei-punteggi-differenza",
    "href": "chapters/ctt/03_ctt_3.html#laffidabilità-dei-punteggi-differenza",
    "title": "10  Metodi di stima dell’affidabilità",
    "section": "10.6 L’affidabilità dei Punteggi Differenza",
    "text": "10.6 L’affidabilità dei Punteggi Differenza\nCi sono numerose situazioni in cui ricercatori e clinici vogliono considerare la differenza tra due punteggi. Qui, la variabile di interesse è un punteggio differenza che viene calcolato come:\n\\[ D = X - Y, \\]\ndove X è il punteggio su un test e Y su un altro. Ad esempio, un approccio alla diagnosi delle difficoltà di apprendimento prevede il calcolo dei punteggi differenza sottraendo il punteggio di un esaminando in un test di rendimento (ad esempio, comprensione della lettura) dal suo QI. Si presume che se la discrepanza è negativa e sufficientemente ampia (ad esempio, due o più deviazioni standard), l’esaminando non sta dimostrando un rendimento accademico commisurato all’attitudine. Se ulteriori valutazioni escludono una serie di spiegazioni come opportunità educative inadeguate o problemi sensoriali (ad esempio, problemi visivi o uditivi), la discrepanza potrebbe riflettere una difficoltà di apprendimento intrinseca.\nUn altro esempio comune dell’utilizzo dei punteggi differenza si ha quando uno psicologo vuole considerare i guadagni (o le perdite) nella performance di un test nel tempo. Ad esempio, un ricercatore potrebbe voler determinare se un trattamento specifico ha portato a un miglioramento nelle prestazioni su un determinato compito. Ciò è spesso realizzato somministrando test prima e dopo l’intervento.\nIn queste situazioni, la variabile di interesse è un punteggio differenza. Quando si trattano punteggi differenza, è però importante ricordare che l’affidabilità dei punteggi differenza è tipicamente considerevolmente inferiore rispetto alle affidabilità dei punteggi individuali. Come regola generale, l’affidabilità dei punteggi differenza diminuisce all’aumentare della correlazione tra le misure individuali.\nLa formula per l’affidabilità dei punteggi differenza è data da:\n\\[\nr_{dd} = \\frac{0.5 (r_{xx} + r_{yy}) - r_{xy}}{1 - r_{xy}}\n\\],\ndove \\(r_{xx}\\) e \\(r_{yy}\\) sono le affidabilità delle due componenti della differenza e \\(r_{xy}\\) è la loro correlazione. Facciamo un esempio numerico varianza la correlazione tra le due componenti.\n\nrdd &lt;- function(rxx, ryy, rxy) {\n    (0.5 * (rxx + ryy) - rxy) / (1 - rxy)\n}\n\nseq(0.01, 0.81, by = 0.1)\n\n\n0.010.110.210.310.410.510.610.710.81\n\n\n\nrxx &lt;- 0.9\nryy &lt;- 0.8\n\nrdd(rxx, ryy, seq(0.01, 0.81, by = 0.1))\n\n\n0.8484848484848490.8314606741573040.8101265822784810.7826086956521740.7457627118644070.6938775510204080.6153846153846160.4827586206896550.210526315789474\n\n\nSi vede che, all’aumentare di \\(r_{xy}\\), l’affidabilità del punteggio differenza diminuisce.\nIn sintesi, si dovrebbe essere cauti nell’interpretare i punteggi differenza. L’affidabilità dei punteggi differenza è tipicamente considerevolmente inferiore rispetto alle affidabilità dei punteggi individuali. Per aggravare il problema, i punteggi differenza sono spesso calcolati utilizzando punteggi che hanno correlazioni piuttosto forti tra loro (ad esempio, punteggi di QI e di rendimento; punteggi pre e post test).",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#scelta-del-coefficiente-di-affidabilità-in-funzione-del-contesto",
    "href": "chapters/ctt/03_ctt_3.html#scelta-del-coefficiente-di-affidabilità-in-funzione-del-contesto",
    "title": "10  Metodi di stima dell’affidabilità",
    "section": "10.7 Scelta del Coefficiente di Affidabilità in Funzione del Contesto",
    "text": "10.7 Scelta del Coefficiente di Affidabilità in Funzione del Contesto\nLa selezione di un coefficiente di affidabilità adeguato dipende da diversi fattori, tra cui la natura del costrutto psicologico misurato e l’uso che si intende fare dei risultati del test. È fondamentale considerare il contesto specifico in cui verrà applicato il test per identificare l’indice di affidabilità più appropriato.\n\n10.7.1 Affidabilità Test-Retest\nL’affidabilità test-retest è utile per test che vengono somministrati più volte agli stessi individui e misura la stabilità dei punteggi nel tempo. Questa misura è particolarmente importante per i test che potrebbero essere influenzati da errori di misurazione temporali. Ad esempio, in un test utilizzato per prevedere il comportamento futuro, l’affidabilità test-retest può fornire una stima affidabile della variabilità legata al tempo.\n\n\n10.7.2 Affidabilità della Coerenza Interna\nPer test somministrati una sola volta, è più rilevante considerare la coerenza interna. Si distinguono principalmente due metodi:\n\nAffidabilità Split-Half: Questa stima dell’affidabilità valuta l’errore dovuto alla varianza del campionamento del contenuto, risultando utile in test con contenuti eterogenei. Ad esempio, in un test che misura costrutti multipli (depressione, ansia, rabbia, impulsività), l’approccio split-half può essere preferito, poiché divide idealmente il test in due parti equilibrate per ciascun costrutto.\nCoefficienti Alfa e KR-20: Questi coefficienti stimano l’errore associato sia al campionamento del contenuto sia all’eterogeneità del costrutto misurato, risultando appropriati quando il test copre un singolo ambito di conoscenza o un unico tratto psicologico. Ad esempio, per un test sull’umore depressivo, l’alfa o il KR-20 sono indicati in quanto mirano a un dominio specifico e omogeneo.\n\n\n\n10.7.3 Affidabilità delle Forme Alternate\nPer test con diverse versioni, è necessario stimare l’affidabilità delle forme alternate per garantire la coerenza dei punteggi tra le varie versioni, assicurandosi che esse siano equivalenti e affidabili.\n\n\n10.7.4 Affidabilità Inter-Valutatori\nQuando il test richiede giudizi soggettivi da parte dei valutatori, diventa essenziale considerare l’affidabilità inter-valutatori. Questo tipo di affidabilità valuta la consistenza tra giudizi di diversi valutatori, assicurando che le valutazioni siano oggettive e riducendo la dipendenza dalle interpretazioni individuali.\nIn sintesi, la scelta del coefficiente di affidabilità dipende dal contesto del test, dalla natura del costrutto e dallo scopo del test. Una selezione accurata è cruciale per garantire la validità e l’accuratezza delle misurazioni psicologiche.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#linee-guida-sulla-scelta-e-valutazione-dei-coefficienti-di-affidabilità",
    "href": "chapters/ctt/03_ctt_3.html#linee-guida-sulla-scelta-e-valutazione-dei-coefficienti-di-affidabilità",
    "title": "10  Metodi di stima dell’affidabilità",
    "section": "10.8 Linee Guida sulla Scelta e Valutazione dei Coefficienti di Affidabilità",
    "text": "10.8 Linee Guida sulla Scelta e Valutazione dei Coefficienti di Affidabilità\nLa valutazione dei coefficienti di affidabilità in ambito psicometrico è influenzata da molteplici fattori.\n\n10.8.1 Significato e Importanza dei Coefficienti di Affidabilità\nI coefficienti di affidabilità rappresentano la proporzione della varianza dei punteggi attribuibile a differenze reali tra gli individui nel costrutto misurato. Ideale sarebbe raggiungere un valore di 1.0, suggerendo che tutta la varianza dei punteggi è legata a differenze effettive tra gli individui. Tuttavia, a causa dell’inevitabile errore di misurazione, una misura perfettamente affidabile è irrealizzabile. Un livello “accettabile” di affidabilità varia in base a costrutto, tempo disponibile, uso dei punteggi e metodo di stima.\n\n\n10.8.2 Fattori da Considerare nella Valutazione dell’Affidabilità\n\nCostrutto: Costrutti complessi come quelli legati alla personalità possono essere più difficili da misurare rispetto alle abilità cognitive. Un livello di affidabilità accettabile per una scala di “dipendenza” potrebbe non essere adeguato per una misura di intelligenza.\nTempo per il Test: Il tempo limitato influisce sull’affidabilità, poiché meno item aumentano l’errore di campionamento. Test brevi, come quelli per lo screening, richiedono standard di affidabilità diversi rispetto a quelli più lunghi.\nUso dei Punteggi del Test: Test diagnostici che influenzano decisioni cruciali richiedono standard di affidabilità elevati. Ad esempio, test sull’intelligenza utilizzati per diagnosi necessitano di alta affidabilità rispetto ai test usati per ricerche di gruppo o screening.\nMetodo di Stima dell’Affidabilità: I metodi di stima influenzano la grandezza dei coefficienti. Ad esempio, KR-20 e alfa tendono a stimare affidabilità più bassa rispetto al metodo split-half.\n\n\n\n10.8.3 Linee Guida Generali per i Coefficienti di Affidabilità\nEcco alcune linee guida generali:\n\nDecisioni importanti: Coefficienti ≥ 0.90, o persino 0.95, sono consigliabili.\nTest di rendimento e personalità: Coefficienti ≥ 0.80 sono generalmente accettabili.\nTest didattici o di screening: Coefficienti ≥ 0.70.\nRicerca di gruppo: Coefficienti ≥ 0.60 possono essere accettabili, ma con cautela se sotto 0.70.\n\nIn conclusione, la valutazione dell’affidabilità di un test psicometrico richiede considerazioni dettagliate dei vari fattori chiave, con standard di accettabilità che variano in base al contesto del test e al suo scopo specifico.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#riflessioni-conclusive",
    "href": "chapters/ctt/03_ctt_3.html#riflessioni-conclusive",
    "title": "10  Metodi di stima dell’affidabilità",
    "section": "10.9 Riflessioni Conclusive",
    "text": "10.9 Riflessioni Conclusive\nIn conclusione, la valutazione dell’affidabilità di un test richiede l’impiego di diversi coefficienti che tengono conto delle varie fonti di errore. I coefficienti di consistenza interna si concentrano sull’errore derivante dalle fluttuazioni delle risposte tra gli item, mentre quelli di equivalenza esaminano la coerenza dei punteggi tra diverse versioni del test. I coefficienti di stabilità misurano la coerenza dei punteggi nel corso del tempo. È di fondamentale importanza selezionare il tipo di affidabilità appropriato in base allo scopo del test, al fine di ottenere informazioni affidabili e utili per le decisioni basate sui punteggi ottenuti dal test.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/03_ctt_3.html#session-info",
    "href": "chapters/ctt/03_ctt_3.html#session-info",
    "title": "10  Metodi di stima dell’affidabilità",
    "section": "10.10 Session Info",
    "text": "10.10 Session Info\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ltm_1.2-0          polycor_0.8-1      msm_1.7.1          MASS_7.3-60.0.1   \n [5] modelsummary_1.4.5 ggokabeito_0.1.0   viridis_0.6.5      viridisLite_0.4.2 \n [9] ggpubr_0.6.0       ggExtra_0.10.1     bayesplot_1.11.1   gridExtra_2.3     \n[13] patchwork_1.2.0    semTools_0.5-6     semPlot_1.1.6      lavaan_0.6-17     \n[17] psych_2.4.1        scales_1.3.0       markdown_1.12      knitr_1.45        \n[21] lubridate_1.9.3    forcats_1.0.0      stringr_1.5.1      dplyr_1.1.4       \n[25] purrr_1.0.2        readr_2.1.5        tidyr_1.3.1        tibble_3.2.1      \n[29] ggplot2_3.5.0      tidyverse_2.0.0    here_1.0.1        \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     magrittr_2.0.3    \n  [4] nloptr_2.0.3       rmarkdown_2.26     vctrs_0.6.5       \n  [7] minqa_1.2.6        base64enc_0.1-3    rstatix_0.7.2     \n [10] htmltools_0.5.7    broom_1.0.5        Formula_1.2-5     \n [13] htmlwidgets_1.6.4  plyr_1.8.9         uuid_1.2-0        \n [16] admisc_0.35        igraph_2.0.2       mime_0.12         \n [19] lifecycle_1.0.4    pkgconfig_2.0.3    Matrix_1.6-5      \n [22] R6_2.5.1           fastmap_1.1.1      shiny_1.8.0       \n [25] digest_0.6.34      OpenMx_2.21.11     fdrtool_1.2.17    \n [28] colorspace_2.1-0   rprojroot_2.0.4    Hmisc_5.1-1       \n [31] fansi_1.0.6        timechange_0.3.0   abind_1.4-5       \n [34] compiler_4.3.3     withr_3.0.0        glasso_1.11       \n [37] htmlTable_2.4.2    backports_1.4.1    carData_3.0-5     \n [40] ggsignif_0.6.4     corpcor_1.6.10     gtools_3.9.5      \n [43] tools_4.3.3        pbivnorm_0.6.0     foreign_0.8-86    \n [46] zip_2.3.1          httpuv_1.6.14      nnet_7.3-19       \n [49] glue_1.7.0         quadprog_1.5-8     nlme_3.1-164      \n [52] promises_1.2.1     lisrelToR_0.3      grid_4.3.3        \n [55] pbdZMQ_0.3-11      checkmate_2.3.1    cluster_2.1.6     \n [58] reshape2_1.4.4     generics_0.1.3     gtable_0.3.4      \n [61] tzdb_0.4.0         data.table_1.15.2  hms_1.1.3         \n [64] car_3.1-2          utf8_1.2.4         tables_0.9.17     \n [67] sem_3.1-15         pillar_1.9.0       IRdisplay_1.1     \n [70] rockchalk_1.8.157  later_1.3.2        splines_4.3.3     \n [73] lattice_0.22-5     survival_3.5-8     kutils_1.73       \n [76] tidyselect_1.2.0   miniUI_0.1.1.1     pbapply_1.7-2     \n [79] stats4_4.3.3       xfun_0.42          expm_0.999-9      \n [82] qgraph_1.9.8       arm_1.13-1         stringi_1.8.3     \n [85] boot_1.3-29        evaluate_0.23      mi_1.1            \n [88] cli_3.6.2          RcppParallel_5.1.7 IRkernel_1.3.2    \n [91] rpart_4.1.23       xtable_1.8-4       repr_1.1.6        \n [94] munsell_0.5.0      Rcpp_1.0.12        coda_0.19-4.1     \n [97] png_0.1-8          XML_3.99-0.16.1    parallel_4.3.3    \n[100] ellipsis_0.3.2     jpeg_0.1-10        lme4_1.1-35.1     \n[103] mvtnorm_1.2-4      insight_0.19.8     openxlsx_4.2.5.2  \n[106] crayon_1.5.2       rlang_1.1.3        mnormt_2.1.1",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodi di stima dell'affidabilità</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html",
    "href": "chapters/ctt/04_err_std_mis.html",
    "title": "11  L’errore standard della misurazione",
    "section": "",
    "text": "11.1 Introduzione\nI coefficienti di affidabilità che abbiamo discusso nel capitolo precedente rappresentano una misura proporzionale della varianza osservata di un test che è attribuibile alla varianza reale. Questi coefficienti sono fondamentali per confrontare l’affidabilità dei punteggi ottenuti da diverse procedure di valutazione. In generale, preferiremo selezionare il test che produce i punteggi con la migliore affidabilità. Tuttavia, una volta scelto il test, il nostro focus si sposta sull’interpretazione dei punteggi.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html#errore-standard-della-misurazione",
    "href": "chapters/ctt/04_err_std_mis.html#errore-standard-della-misurazione",
    "title": "11  L’errore standard della misurazione",
    "section": "11.2 Errore Standard della Misurazione",
    "text": "11.2 Errore Standard della Misurazione\nL’Errore Standard della Misurazione (SEM) diventa una statistica più pratica quando l’attenzione è rivolta all’interpretazione dei punteggi di un test. Il SEM è definito come la deviazione standard della distribuzione dei punteggi che un individuo otterrebbe se fosse sottoposto a un numero infinito di forme parallele del test, costituite da item campionati casualmente dallo stesso dominio di contenuto.\nPer comprendere meglio, immaginiamo di creare un numero infinito di forme parallele di un test e di far svolgere queste forme alla stessa persona, senza che vi siano effetti di trasferimento. La presenza dell’errore di misurazione impedirebbe alla persona di ottenere sempre lo stesso punteggio. Anche se ogni test rappresenta ugualmente bene il dominio di contenuto, il candidato potrebbe ottenere risultati migliori in alcuni test e peggiori in altri, semplicemente a causa di errori casuali (ad esempio, la fortuna nel conoscere le risposte agli item selezionati per una versione del test ma non per un’altra). Prendendo i punteggi ottenuti in tutti questi test, si otterrebbe una distribuzione di punteggi. La media di questa distribuzione rappresenta il punteggio vero (T) dell’individuo, mentre il SEM è la deviazione standard di questa distribuzione di punteggi di errore.\nOvviamente, non è possibile attuare questi procedimenti nella realtà, quindi dobbiamo stimare il SEM utilizzando le informazioni disponibili. Esamineremo qui l’approccio utilizzato dalla Teoria Classica dei Test (CTT) per raggiungere questo obiettivo.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html#stima-di-sem",
    "href": "chapters/ctt/04_err_std_mis.html#stima-di-sem",
    "title": "11  L’errore standard della misurazione",
    "section": "11.3 Stima di SEM",
    "text": "11.3 Stima di SEM\nSecondo Lord (1968), l’errore \\(E = X - T\\) rappresenta la variabile aleatoria di interesse primario nella CTT. L’obiettivo della CTT è stimare il punteggio vero di ogni rispondente e confrontare le stime ottenute per rispondenti diversi. La grandezza dell’errore \\(E\\) fornisce informazioni essenziali in questo contesto. La discrepanza tra il punteggio osservato e il punteggio vero può essere misurata utilizzando la deviazione standard degli errori \\(E\\), conosciuta appunto come “Errore Standard della Misurazione” o SEM. Il SEM è quindi lo strumento impiegato dalla CTT per stimare in che misura un punteggio osservato differisce dal punteggio vero.\nNel presente capitolo esploreremo come sia possibile stimare la deviazione standard dell’errore (\\(\\sigma_E\\)) in un campione di osservazioni. Questo consente di comprendere meglio la precisione dei punteggi ottenuti attraverso un test psicometrico e di interpretare in modo più accurato i risultati.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html#lincertezza-della-misura",
    "href": "chapters/ctt/04_err_std_mis.html#lincertezza-della-misura",
    "title": "11  L’errore standard della misurazione",
    "section": "11.4 L’incertezza della misura",
    "text": "11.4 L’incertezza della misura\nIn base alla CTT, è possibile stimare l’errore standard della misurazione utilizzando una formula che dipende dalla deviazione standard della distribuzione dei punteggi del test e dall’attendibilità del test. Mediante questa formula, è possibile ottenere una stima dell’errore standard associato a un singolo punteggio, il quale indica quanto il punteggio osservato può variare rispetto al vero punteggio di un individuo:\n\\[\n\\sigma_E = \\sigma_X \\sqrt{1 -\\rho_{XX^\\prime}},\n\\tag{11.1}\\]\ndove \\(\\sigma_X\\) rappresenta la deviazione standard dei punteggi ottenuti da un campione di soggetti e \\(\\rho_{XX^\\prime}\\) è il coefficiente di attendibilità. Attraverso questo calcolo, si ottiene l’errore standard della misurazione sottraendo l’attendibilità del test da 1, quindi calcolando la radice quadrata del risultato e moltiplicandolo per la deviazione standard dei punteggi del test.\nLa logica alla base dell’errore standard della misurazione si fonda sull’assunzione che se una persona dovesse sostenere numerosi test equivalenti, i punteggi ottenuti seguirebbero una distribuzione normale con il vero punteggio dell’individuo come media. In altre parole, possiamo immaginare che l’individuo affronti ripetutamente versioni identiche del test, in circostanze simili e senza ricordare le risposte precedenti. In tale contesto ipotetico, l’errore standard della misurazione rappresenterebbe la deviazione standard tra queste misurazioni ripetute.\nLa formula sopra indicata evidenzia come l’errore standard della misurazione (\\(\\sigma_E\\)) sia strettamente correlato all’attendibilità del test: all’aumentare dell’attendibilità del test, l’errore standard della misurazione diminuisce. Se l’attendibilità del test si avvicina a 0, l’errore standard della misurazione tende a diventare uguale alla deviazione standard dei punteggi osservati del test. In contrasto, se l’attendibilità del test raggiunge 1, l’errore standard della misurazione si riduce a zero: in una situazione di perfetta affidabilità, in cui non vi è alcun errore di misurazione, \\(\\sigma_E\\) assume valore zero.\n\n11.4.1 Interpretazione\nLa Teoria Classica dei Test (CTT) postula che, se un individuo dovesse ripetere un test un numero infinito di volte, mantenendo inalterate le condizioni di somministrazione, i punteggi ottenuti si distribuirebbero in maniera normale attorno al suo vero punteggio. L’errore standard di misura (SEM) viene quindi definito come la stima della deviazione standard di questa distribuzione ipotetica di punteggi. Di conseguenza, un SEM elevato indica una maggiore incertezza nell’utilizzo del test per valutare l’abilità latente dell’individuo.\nSecondo McDonald, invece, il termine di errore (E) segue una distribuzione di propensione, che riflette le variazioni casuali nelle prestazioni di un individuo nel tempo a causa di test. Queste variazioni possono essere influenzate da fattori quali lo stato d’animo, la motivazione e altre variabili contestuali. L’errore standard di misura, in questo contesto, fornisce una quantificazione della deviazione standard dei punteggi attesi per un individuo, se fosse possibile testarlo un numero infinito di volte (o attraverso test equivalenti) in condizioni identiche, assumendo che il suo vero punteggio rimanga invariato.\nIl coefficiente di attendibilità, la varianza dell’errore e l’errore standard di misura rappresentano metriche che riflettono la precisione di un test psicometrico, ciascuna fornendo un tipo di insight specifico sulla precisione:\n\nL’errore standard di misura (SEM) offre una stima della precisione di un punteggio osservato per un individuo, offrendo una base per inferenze riguardo l’affidabilità di quel punteggio specifico. Al contrario, il coefficiente di attendibilità non si presta a una interpretazione così diretta in relazione ai punteggi individuali.\nIl SEM è calcolato nell’unità di misura dei punteggi del test, facilitando la comprensione e l’interpretazione della variabilità attorno al punteggio osservato di un individuo. Diversamente, la varianza dell’errore è espressa come il quadrato delle unità di misura del punteggio, rendendola meno intuitiva per interpretazioni dirette riguardanti la precisione del punteggio.\nIl coefficiente di attendibilità quantifica il rapporto tra la varianza dei punteggi veri e la varianza totale dei punteggi osservati, risultando in un indice senza unità di misura (adimensionale). Questo lo distingue dal SEM e dalla varianza dell’errore, in quanto l’attendibilità valuta la consistenza relativa dei punteggi all’interno dell’intero test piuttosto che la precisione di un singolo punteggio osservato.\n\nEsempio 1. Consideriamo un esempio in cui un test di intelligenza fornisce un punteggio medio di 100 con una deviazione standard di 15. Supponiamo inoltre che l’attendibilità di questo test sia pari a 0.73. Vogliamo calcolare l’errore standard della misurazione.\nUtilizzando la formula dell’errore standard della misurazione, otteniamo:\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\sigma_E &= \\sigma_X \\sqrt{1 -\\rho_{XX^\\prime}} \\notag\\\\\n&= 15 \\sqrt{1 - 0.73} \\notag\\\\\n&= 7.79.\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nIl valore 7.79 rappresenta l’errore standard atteso nei punteggi ottenuti da un singolo individuo se il test fosse somministrato più volte sotto identiche condizioni. In altre parole, ci aspettiamo che i punteggi variino in media di circa 8 punti tra diverse somministrazioni del test.\nInoltre, possiamo utilizzare l’errore standard della misurazione per calcolare un intervallo di confidenza intorno al vero punteggio del rispondente. Utilizzando la proprietà della distribuzione gaussiana, possiamo stimare che il 95% dei punteggi ottenuti da ripetute somministrazioni del test si troveranno nell’intervallo:\n\\[\n\\text{punteggio vero del rispondente} \\pm 1.96 \\cdot \\text{errore standard della misurazione}.\n\\]\nNel nostro caso, questo intervallo sarebbe pari a \\(2 \\cdot 1.96 \\cdot 7.79 = 30.54\\) punti. Quindi, ci aspettiamo che i punteggi del QI di un singolo rispondente varino all’interno di un intervallo di 30 punti se il test fosse somministrato molte volte sotto le stesse condizioni.\nQuesto esempio dimostra che se un test ha un’attendibilità di 0.73 e una deviazione standard dei punteggi di 15, la misurazione del test su un singolo individuo risulterebbe poco affidabile a causa dell’ampio errore di misurazione. A titolo di confronto, la Full Scale IQ (FSIQ) della WAIS-IV Wechsler (2008) ha un’attendibilità split-half di 0.98 e un errore standard di misurazione di 2.16.\nL’errore standard della misurazione può anche essere calcolato utilizzando la funzione SE.Means() del pacchetto psychometric.\n\nSE.Meas(15, .73)\n\n7.79422863405995\n\n\nEsempio 2. Continuando con l’esempio precedente, per gli ipotetici dati riportati sopra, poniamoci ora la seguente domanda: qual è la probabilità che un rispondente ottenga un punteggio minore o uguale a 116 nel test, se il suo punteggio vero fosse uguale a 120?\nIl problema si risolve rendendosi conto che i punteggi del rispondente si distribuiscono normalmente attorno al punteggio vero di 120, con una deviazione standard uguale a 7.79. Dobbiamo dunque trovare l’area sottesa alla normale \\(\\mathcal{N}(120, 7.79)\\) nell’intervallo \\([-\\infty, 116]\\). Utilizzando R, la soluzione si trova nel modo seguente:\n\npnorm(116, 120, 7.79)\n\n0.303808211691303\n\n\nSe la variabile aleatoria che corrisponde al punteggio osservato segue una distribuzione \\(\\mathcal{N}(120, 7.79)\\), la probabilità che il rispondente ottenga un punteggio minore o uguale a 116 è dunque uguale a 0.30.\nEsempio 3. Sempre per l’esempio discusso, poniamoci ora la seguente domanda: quale intervallo di valori centrato sul punteggio vero contiene, con una probabilità di 0.95, i punteggi che il rispondente otterrebbe in ipotetiche somministrazioni ripetute del test sotto le stesse identiche condizioni?\nDobbiamo trovare i quantili della distribuzione \\(\\mathcal{N}(120, 7.79)\\) a cui sono associate le probabilità di 0.025 e 0.975. La soluzione è data da:\n\nqnorm(c(.025, .975), 120, 7.79)\n\n\n104.731880560433135.268119439567\n\n\nL’intervallo cercato è dunque \\([104.7, 135.3]\\).\nEsempio 4. Calcoliamo ora l’errore standard di misurazione utilizzando un campione di dati grezzi. Esamineremo un set di dati discusso da Brown (2015). Il set di dati grezzi contiene 9 indicatori utilizzati per misurare la depressione maggiore così come è definita nel DSM-IV:\n\nMDD1: depressed mood;\nMDD2: loss of interest in usual activities;\nMDD3: weight/appetite change;\nMDD4: sleep disturbance;\nMDD5: psychomotor agitation/retardation;\nMDD6: fatigue/loss of energy;\nMDD7: feelings of worthlessness/guilt;\nMDD8: concentration difficulties;\nMDD9: thoughts of death/suicidality.\n\nImportiamo i dati:\n\ndf &lt;- readRDS(\n    here::here(\"data\", \"mdd_sex.RDS\")\n) |&gt;\n    dplyr::select(-sex)\n\nCi sono 750 osservazioni:\n\ndim(df) |&gt; print()\n\n[1] 750   9\n\n\n\nhead(df)\n\n\nA data.frame: 6 x 9\n\n\n\nmdd1\nmdd2\nmdd3\nmdd4\nmdd5\nmdd6\nmdd7\nmdd8\nmdd9\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n5\n4\n1\n6\n5\n6\n5\n4\n2\n\n\n2\n5\n5\n5\n5\n4\n5\n4\n5\n4\n\n\n3\n4\n5\n4\n2\n6\n6\n0\n0\n0\n\n\n4\n5\n5\n3\n3\n5\n5\n6\n4\n0\n\n\n5\n5\n5\n0\n5\n0\n4\n6\n0\n0\n\n\n6\n6\n6\n4\n6\n4\n6\n5\n6\n2\n\n\n\n\n\nCalcoliamo il coefficiente di attendibilità \\(\\alpha\\) di Cronbach con la funzione alpha() del pacchetto psych.\n\nres &lt;- psych::alpha(df)\nalpha &lt;- res$total$raw_alpha\nalpha\n\n0.753150463775787\n\n\nCalcoliamo un vettore che contiene il punteggio totale del test per ciascun individuo:\n\ntotal_score &lt;- rowSums(df)\n\nTroviamo l’errore standard di misurazione:\n\nsd(total_score) * sqrt(1 - alpha)\n\n5.29643177867088\n\n\nConfrontiamo il risultato con quello ottenuto con la funzione SE.Meas():\n\nSE.Meas(sd(total_score), alpha)\n\n5.29643177867088",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html#dimostrazione",
    "href": "chapters/ctt/04_err_std_mis.html#dimostrazione",
    "title": "11  L’errore standard della misurazione",
    "section": "11.5 Dimostrazione",
    "text": "11.5 Dimostrazione\nEsaminiamo ora la derivazione della formula per l’errore standard di misurazione, \\(\\sigma_E = \\sigma_X \\sqrt{1 - \\rho_{XX^\\prime}}\\). Per arrivare a questa formula, seguiremo due passaggi chiave: innanzitutto, calcoleremo la varianza del punteggio vero e successivamente rappresenteremo il punteggio osservato come la somma della varianza del punteggio vero e la varianza dell’errore.\nIniziamo definendo il coefficiente di attendibilità come \\(\\rho_{XX^\\prime} = \\frac{\\sigma^2_T}{\\sigma^2_X}\\), in cui \\(\\sigma^2_T\\) è la varianza del punteggio vero e \\(\\sigma^2_X\\) è la varianza del punteggio osservato. Utilizzando questa definizione, possiamo riscrivere \\(\\sigma^2_T\\) come \\(\\sigma^2_T = \\rho_{XX^\\prime} \\sigma^2_X\\), considerando che \\(X\\) e \\(X^\\prime\\) sono forme parallele di un test.\nDato che \\(\\sigma_X = \\sigma_{X^\\prime}\\), possiamo scrivere l’equazione precedente come \\(\\sigma^2_T = \\rho_{XX^\\prime} \\sigma_X \\sigma_{X^\\prime}\\). Inoltre, la covarianza tra \\(X\\) e \\(X^\\prime\\) è definita come \\(\\sigma_{XX^\\prime} = \\rho_{XX^\\prime} \\sigma_X \\sigma_{X^\\prime}\\). Da qui, possiamo affermare che \\(\\sigma^2_T = \\sigma_{XX^\\prime}\\), ovvero che la varianza del punteggio vero equivale alla covarianza tra due misurazioni parallele.\nOra, passiamo a calcolare la varianza dell’errore, \\(\\sigma^2_E\\). La varianza del punteggio osservato è espressa come \\(\\sigma^2_X = \\sigma^2_T + \\sigma^2_E\\). Utilizzando la definizione di attendibilità, possiamo riscrivere questa equazione come \\(\\sigma^2_X = \\rho_{XX^\\prime} \\sigma^2_X + \\sigma^2_E\\), da cui otteniamo:\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\sigma^2_E &= \\sigma^2_X - \\sigma^2_X \\rho_{XX^\\prime} \\\\\n&= \\sigma^2_X (1 - \\rho_{XX^\\prime}).\n\\end{aligned}\n\\end{equation}\n\\]\nDi conseguenza, la varianza dell’errore di misurazione, \\(\\sigma^2_E\\), può essere espressa come il prodotto di due fattori: il primo rappresenta la varianza del punteggio osservato, mentre il secondo equivale a uno meno la correlazione tra le due forme parallele del test (\\(\\rho_{XX^\\prime}\\)). In conclusione, abbiamo calcolato l’incognita \\(\\sigma^2_E\\) in termini di due quantità osservabili, \\(\\sigma^2_X\\) e \\(\\rho_{XX^\\prime}\\).",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html#relazione-tra-affidabilità-e-sem",
    "href": "chapters/ctt/04_err_std_mis.html#relazione-tra-affidabilità-e-sem",
    "title": "11  L’errore standard della misurazione",
    "section": "11.6 Relazione tra Affidabilità e SEM",
    "text": "11.6 Relazione tra Affidabilità e SEM\nSi osserva che, all’aumentare dell’affidabilità di un test, l’Errore Standard di Misurazione (SEM) diminuisce. Questa relazione inversa è coerente con il fatto che il coefficiente di affidabilità riflette la proporzione della varianza dei punteggi osservati dovuta alla varianza dei punteggi veri, e il SEM è una stima dell’errore presente nei punteggi del test. Quindi, maggiore è l’affidabilità dei punteggi di un test, minore è il SEM, e maggior fiducia possiamo avere nella precisione dei punteggi del test. Viceversa, minore è l’affidabilità di un test, maggiore è il SEM, e minore è la nostra fiducia nella precisione dei punteggi del test.\nPer esempio, con un coefficiente di affidabilità perfetto pari a 1.0, il SEM sarebbe uguale a 0, indicando l’assenza di errore nella misurazione e che il punteggio ottenuto rappresenta il punteggio vero. Un coefficiente di affidabilità pari a 0, invece, produrrebbe un SEM uguale alla deviazione standard (SD) dei punteggi ottenuti, indicando che tutta la varianza dei punteggi del test è dovuta a errori.\nIl SEM è tradizionalmente utilizzato nel calcolo di intervalli o bande intorno ai punteggi osservati, all’interno dei quali ci si aspetta che cada il punteggio vero. Ora passeremo a questa applicazione del SEM.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html#intervallo-di-confidenza-e-errore-standard-di-misurazione-sem",
    "href": "chapters/ctt/04_err_std_mis.html#intervallo-di-confidenza-e-errore-standard-di-misurazione-sem",
    "title": "11  L’errore standard della misurazione",
    "section": "11.7 Intervallo di Confidenza e Errore Standard di Misurazione (SEM)",
    "text": "11.7 Intervallo di Confidenza e Errore Standard di Misurazione (SEM)\nL’intervallo di confidenza rappresenta un range di punteggi che include il vero punteggio di un individuo con una probabilità prescritta. Generalmente, utilizziamo il SEM per calcolare gli intervalli di confidenza. Il SEM fornisce informazioni sulla distribuzione dei punteggi osservati intorno ai punteggi veri.\nAd esempio, se un individuo ha un punteggio vero di 70 in un test con un SEM di 3, ci aspetteremmo che ottenga punteggi tra 67 e 73 due terzi delle volte, a patto che non ci siano cambiamenti nelle prestazioni a causa della ripetizione del test.\n\n# Definiamo il punteggio vero e il SEM\npunteggio_vero &lt;- 70\nSEM &lt;- 3\n\npnorm(73, 70, 3) - pnorm(67, 70, 3)\n\n0.682689492137086\n\n\nPer ottenere un intervallo di confidenza del 95%, determiniamo il numero di deviazioni standard che comprendono il 95% dei punteggi in una distribuzione. Con un punteggio vero di 70 e un SEM di 3, l’intervallo di confidenza del 95% sarebbe 70 ± 3(1.96), ovvero 70 ± 5.88. Quindi, in questa situazione, ci aspetteremmo che il punteggio osservato dell’individuo sia tra 64.12 e 75.88, il 95% delle volte.\n\n# Calcoliamo il valore critico Z per il livello di confidenza del 95%\nlivello_confidenza &lt;- 0.95\nz_critico &lt;- qnorm((1 + livello_confidenza) / 2)\n\n# Calcoliamo l'errore standard dell'intervallo\nerrore_standard_intervallo &lt;- SEM * z_critico\n\n# Calcoliamo l'intervallo di confidenza\nintervallo_confidenza_inf &lt;- punteggio_vero - errore_standard_intervallo\nintervallo_confidenza_sup &lt;- punteggio_vero + errore_standard_intervallo\n\n# Stampiamo l'intervallo di confidenza\ncat(\"L'intervallo di confidenza al 95% e' [\", intervallo_confidenza_inf, \", \", intervallo_confidenza_sup, \"]\\n\")\n\nL'intervallo di confidenza al 95% e' [ 64.12011 ,  75.87989 ]\n\n\n\n11.7.1 Relazione tra Affidabilità, SEM e Intervalli di Confidenza\nÈ utile notare la relazione tra l’affidabilità di un punteggio di test, il SEM e gli intervalli di confidenza. Ricordiamo che all’aumentare dell’affidabilità dei punteggi, il SEM diminuisce. La stessa relazione esiste tra l’affidabilità dei punteggi di test e gli intervalli di confidenza. Man mano che l’affidabilità dei punteggi di test aumenta (denotando meno errore di misurazione), gli intervalli di confidenza diventano più piccoli (denotando maggiore precisione nella misurazione).\n\n\n11.7.2 Vantaggio del SEM e dell’Uso degli Intervalli di Confidenza\nIl SEM e l’utilizzo degli intervalli di confidenza forniscono ci ricordano che l’errore di misurazione è un elemento intrinseco a tutti i punteggi e che dovremmo interpretare tali punteggi con cautela. Troppo spesso, si tende a interpretare un singolo punteggio numerico come se fosse assolutamente preciso, trascurando la presenza di errori associati.\nPer esempio, se si riporta che Alice ha un QI totale di 113, i suoi genitori potrebbero essere inclini a interpretare questo dato come un’indicazione precisa del QI di Alice, assumendo che sia esattamente 113. Tuttavia, anche quando si utilizzano test di alta qualità per misurare il QI, i punteggi ottenuti non sono privi di errore. Il SEM e gli intervalli di confidenza sono strumenti utili che ci consentono di quantificare e illustrare questa inevitabile incertezza associata ai punteggi di misurazione. Essi ci avvertono che ogni punteggio contiene una certa dose di errore e ci invitano a considerare i risultati con una visione più prudente e completa.\n\n\n11.7.3 Problema nel Calcolare l’Intervallo di Confidenza\nUn problema potenziale con l’approccio descritto sopra è che non conosciamo il vero punteggio dell’esaminato, ma solo il punteggio osservato. È comune usare il SEM per stabilire intervalli di confidenza intorno ai punteggi ottenuti. Tuttavia, è importante sottolineare che questa pratica non è corretta Charter (1996).\n\nIn spite of Dudek (1979)’s reminder that the SEM should not be used to construct confidence intervals, many test manuals, computer-scoring programs, and texts in psychology and education continue to do so. Because authors of many textbooks and manuals make these errors, it is understandable that those who learned from and look to these sources for guidance also make these errors. In summary, the SEM should not be used to construct confidence intervals for test scores (p. 1141).\n\nÈ invece possibile costruire gli intervalli di confidenza basati su punteggi veri stimati e sull’errore standard della stima (SEE). Questo approccio verrà descritto nel prossimo capitolo.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html#riflessioni-conclusive",
    "href": "chapters/ctt/04_err_std_mis.html#riflessioni-conclusive",
    "title": "11  L’errore standard della misurazione",
    "section": "11.8 Riflessioni Conclusive",
    "text": "11.8 Riflessioni Conclusive\nNel contesto della CTT, le stime di affidabilità si rivelano uno strumento fondamentale per valutare la coerenza dei test. Tuttavia, quando si affrontano decisioni relative al singolo individuo, come ad esempio determinare se un candidato supera un esame, diventa più vantaggioso fare riferimento all’errore standard di misurazione (SEM). Il SEM rende evidente quanto i punteggi di un test siano suscettibili di fluttuazioni casuali se lo stesso test venisse ripetuto più volte dallo stesso esaminando. In generale, un SEM più ridotto corrisponde a un intervallo di fluttuazioni casuali più stretto. Ciò implica che, grazie a un SEM più basso, i punteggi rifletteranno in modo più coerente le vere capacità dell’esaminando.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/04_err_std_mis.html#session-info",
    "href": "chapters/ctt/04_err_std_mis.html#session-info",
    "title": "11  L’errore standard della misurazione",
    "section": "11.9 Session Info",
    "text": "11.9 Session Info\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] psychometric_2.4  multilevel_2.7    nlme_3.1-166      MASS_7.3-61      \n [5] viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1   \n [9] gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6   \n[13] semPlot_1.1.6     lavaan_0.6-19     psych_2.4.6.26    scales_1.3.0     \n[17] markdown_1.13     knitr_1.49        lubridate_1.9.3   forcats_1.0.0    \n[21] stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2       readr_2.1.5      \n[25] tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n[29] here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.17.1   jsonlite_1.8.9      magrittr_2.0.3     \n  [4] TH.data_1.1-2       estimability_1.5.1  farver_2.1.2       \n  [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n [19] emmeans_1.10.5      zoo_1.8-12          uuid_1.2-1         \n [22] igraph_2.1.1        mime_0.12           lifecycle_1.0.4    \n [25] pkgconfig_2.0.3     Matrix_1.7-1        R6_2.5.1           \n [28] fastmap_1.2.0       shiny_1.9.1         digest_0.6.37      \n [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n [34] rprojroot_2.0.4     Hmisc_5.2-0         fansi_1.0.6        \n [37] timechange_0.3.0    abind_1.4-8         compiler_4.4.2     \n [40] withr_3.0.2         glasso_1.11         htmlTable_2.4.3    \n [43] backports_1.5.0     carData_3.0-5       ggsignif_0.6.4     \n [46] corpcor_1.6.10      gtools_3.9.5        tools_4.4.2        \n [49] pbivnorm_0.6.0      foreign_0.8-87      zip_2.3.1          \n [52] httpuv_1.6.15       nnet_7.3-19         glue_1.8.0         \n [55] quadprog_1.5-8      promises_1.3.0      lisrelToR_0.3      \n [58] grid_4.4.2          pbdZMQ_0.3-13       checkmate_2.3.2    \n [61] cluster_2.1.6       reshape2_1.4.4      generics_0.1.3     \n [64] gtable_0.3.6        tzdb_0.4.0          data.table_1.16.2  \n [67] hms_1.1.3           car_3.1-3           utf8_1.2.4         \n [70] sem_3.1-16          pillar_1.9.0        IRdisplay_1.1      \n [73] rockchalk_1.8.157   later_1.3.2         splines_4.4.2      \n [76] cherryblossom_0.1.0 lattice_0.22-6      survival_3.7-0     \n [79] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n [82] pbapply_1.7-2       airports_0.1.0      stats4_4.4.2       \n [85] xfun_0.49           qgraph_1.9.8        arm_1.14-4         \n [88] stringi_1.8.4       pacman_0.5.1        boot_1.3-31        \n [91] evaluate_1.0.1      codetools_0.2-20    mi_1.1             \n [94] cli_3.6.3           RcppParallel_5.1.9  IRkernel_1.3.2     \n [97] rpart_4.1.23        xtable_1.8-4        repr_1.1.7         \n[100] munsell_0.5.1       Rcpp_1.0.13-1       coda_0.19-4.1      \n[103] png_0.1-8           XML_3.99-0.17       parallel_4.4.2     \n[106] usdata_0.3.1        jpeg_0.1-10         lme4_1.1-35.5      \n[109] mvtnorm_1.3-2       openxlsx_4.2.7.1    crayon_1.5.3       \n[112] openintro_2.5.0     rlang_1.1.4         multcomp_1.4-26    \n[115] mnormt_2.1.1       \n\n\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.\n\n\nCharter, R. A. (1996). Revisiting the standard errors of measurement, estimate, and prediction and their application to test scores. Perceptual and Motor Skills, 82(3), 1139–1144.\n\n\nDudek, F. J. (1979). The continuing misinterpretation of the standard error of measurement. Psychological Bulletin, 86(2), 335--337.\n\n\nWechsler, D. (2008). Wechsler adult intelligence scale–Fourth Edition (WAIS–IV). San Antonio, TX: NCS Pearson, 22(498), 816–827.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L'errore standard della misurazione</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/05_err_std_stima.html",
    "href": "chapters/ctt/05_err_std_stima.html",
    "title": "12  La stima del punteggio vero",
    "section": "",
    "text": "12.1 Introduzione\nUno dei principali scopi della valutazione psicologica è stimare il punteggio vero del soggetto. Il punteggio osservato \\(X\\) differisce dal punteggio vero \\(T\\) a causa dell’errore di misurazione: \\(X = T + E\\). Ora poniamoci l’obiettivo di utilizzare i concetti della Teoria Classica per stimare il punteggio vero di un soggetto, utilizzando il suo punteggio osservato e l’affidabilità del test. Questa stima risulta particolarmente utile quando è necessario costruire un intervallo di confidenza per il punteggio vero del soggetto.\nPer costruire l’intervallo di confidenza del vero punteggio, sono necessarie due misurazioni:\nCominciamo affrontando il problema della stima del vero punteggio.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>La stima del punteggio vero</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/05_err_std_stima.html#introduzione",
    "href": "chapters/ctt/05_err_std_stima.html#introduzione",
    "title": "12  La stima del punteggio vero",
    "section": "",
    "text": "Una stima del vero punteggio.\nL’errore standard della stima (ossia, una stima della deviazione standard della distribuzione delle stime del punteggio vero che si otterrebbero se il test venisse somministrato infinite volte nelle stesse condizioni).",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>La stima del punteggio vero</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/05_err_std_stima.html#il-paradosso-di-kelley",
    "href": "chapters/ctt/05_err_std_stima.html#il-paradosso-di-kelley",
    "title": "12  La stima del punteggio vero",
    "section": "12.2 Il paradosso di Kelley",
    "text": "12.2 Il paradosso di Kelley\nNegli anni ’20, Kelly ha dimostrato come sia possibile stimare il punteggio vero del rispondente utilizzando un modello di regressione. La formula di Kelley si fonda sull’equivalenza algebrica che lega l’attendibilità al quadrato del coefficiente di correlazione tra i punteggi osservati e quelli veri. Pertanto, la stima del punteggio vero di un rispondente può essere calcolata nel seguente modo:\n\\[\n\\begin{equation}\n\\hat{T} = \\mu_x + \\rho  (X - \\mu_x),\n\\end{equation}\n\\tag{12.1}\\]\ndove \\(X\\) rappresenta il punteggio osservato, \\(\\mu_x\\) è la media dei punteggi ottenuti da tutti i partecipanti nel campione, e \\(\\rho\\) è l’attendibilità del test.\nQuando l’attendibilità è perfetta (\\(\\rho = 1\\)), il punteggio vero coincide con il punteggio osservato. Nel caso di attendibilità nulla (dove tutta la varianza è attribuibile all’errore di misurazione), la stima più accurata del punteggio vero è semplicemente la media del campione. Per valori di \\(\\rho\\) compresi tra 0 e 1, la stima del punteggio vero si discosta dal punteggio osservato in direzione della media campionaria. In questo modo, la stima del punteggio vero illustra il concetto di regressione verso la media dei punteggi osservati, considerando l’attendibilità del test.\nLa formula del punteggio vero può essere interpretata nel modo seguente: per stimare il vero punteggio di un individuo, si parte dalla media della distribuzione di tutti i partecipanti e si procede in direzione del punteggio osservato. Tuttavia, il punteggio osservato non viene raggiunto completamente; l’entità dello spostamento è proporzionale all’attendibilità del test. Ciò implica che la stima del punteggio vero di un individuo, a seconda del valore di \\(\\rho\\), tiene conto anche della sua posizione rispetto alla media del gruppo. Se il soggetto si trova al di sotto della media, la stima del punteggio vero sarà maggiorata e viceversa. Questo fenomeno è noto come il “paradosso di Kelley”.\nÈ cruciale mettere in evidenza una discrepanza tra la formula di Kelley e l’intuizione comune che suggerisce il punteggio osservato come una stima accurata del vero punteggio (cioè \\(\\hat{T} = X\\)). Tuttavia, questo punto di vista è valido solo quando la misura è perfettamente attendibile (\\(\\rho = 1\\)). Al contrario, quando \\(\\rho = 0\\), la formula di Kelley suggerisce di utilizzare la media dei punteggi osservati come stima del vero punteggio, implicando che il punteggio osservato non rifletta necessariamente il vero punteggio, ma solo l’errore di misurazione.\nIn pratica, è estremamente improbabile che \\(\\rho\\) sia esattamente uguale a zero. Invece, con valori di \\(\\rho\\) compresi tra 0 e 1, la stima del punteggio vero si troverà in una posizione intermedia tra il punteggio osservato e la media della popolazione. Per una comprensione più dettagliata di questo concetto, possiamo fare riferimento a Kelley (1947), il quale ha osservato che:\n\nThis is an interesting equation in that it expresses the estimate of true ability as the weighted sum of two separate estimates, – one based upon the individual’s observed score, \\(X_1\\) (\\(X\\) nella notazione corrente) and the other based upon the mean of the group to which he belongs, \\(M_1\\) (\\(\\mu_x\\) nella notazione corrente). If the test is highly reliable, much weight is given to the test score and little to the group mean, and vice versa.\n\nPer chiarire l’Equazione 12.1 e la sua derivazione in termini di predizione del punteggio vero a partire dal punteggio osservato, iniziamo esaminando il modello di base di regressione lineare semplice. Questo modello stabilisce una relazione diretta tra il punteggio osservato \\(X\\) e il punteggio vero \\(T\\), una relazione che inizialmente abbiamo descritto con la formula \\(X = 0 + 1 \\cdot T + E\\). Il nostro interesse specifico qui, però, si sposta verso la predizione del punteggio vero \\(T\\) utilizzando il punteggio osservato \\(X\\), attraverso un modello di regressione. La formula per questa predizione assume la forma:\n\\[\nT = \\alpha + \\beta X + \\varepsilon.\n\\]\nRiorganizzando le variabili in termini di deviazioni dalla loro media (\\(x = X - \\bar{X}\\) e \\(\\tau = T - \\mathbb{E}(T)\\)), e considerando l’intercetta \\(\\alpha\\) come 0, il modello si semplifica in \\(\\hat{\\tau} = \\beta x\\), dove \\(\\hat{\\tau}\\) rappresenta la nostra stima del punteggio vero come deviazione dalla media. La questione centrale diventa quindi il calcolo di \\(\\beta\\), che è la pendenza della retta di regressione nel nostro modello semplificato.\nIl valore di \\(\\beta\\) viene definito come \\(\\beta = \\frac{\\sigma_{\\tau x}}{\\sigma^2_x}\\), che ci permette di esprimere il modello come:\n\\[\n\\hat{\\tau} = \\frac{\\sigma_{\\tau x}}{\\sigma^2_x} x.\n\\]\nIntroducendo la correlazione tra \\(x\\) (o \\(X\\)) e \\(\\tau\\) (o \\(T\\)), denotata \\(\\rho_{\\tau x}\\), e sostituendo la covarianza con il prodotto della correlazione per le deviazioni standard dei punteggi, riscriviamo l’equazione come:\n\\[\n\\hat{\\tau} = \\rho_{\\tau x}\\frac{\\sigma_{\\tau}}{\\sigma_x} x.\n\\]\nQuesto ci porta a considerare la definizione di attendibilità, secondo cui la varianza del punteggio vero può essere espressa come \\(\\sigma^2_{\\tau} = \\sigma^2_x \\rho_{xx^\\prime}\\). La stima del punteggio vero, in termini di deviazioni dalla media, diventa quindi una funzione del coefficiente di attendibilità e della deviazione del punteggio osservato dalla sua media:\n\\[\n\\hat{\\tau} = \\rho_{\\tau x} \\sqrt{\\rho_{xx^\\prime}} x.\n\\]\nAvendo dimostrato che \\(\\rho^2_{\\tau x} = \\rho_{xx^\\prime}\\), possiamo ulteriormente semplificare la nostra stima del punteggio vero come:\n\\[\n\\hat{\\tau} = \\rho_{xx^\\prime} x.\n\\]\nQuesto ci indica che la stima del punteggio vero (in termini di deviazioni dalla media) si ottiene moltiplicando il punteggio osservato, anch’esso espresso come deviazione dalla media, per il coefficiente di attendibilità.\nPer ritornare alla formula in termini di punteggi grezzi, aggiungiamo la media dei punteggi osservati \\(\\bar{X}\\) alla nostra equazione, ottenendo così la stima del punteggio vero grezzo \\(\\hat{T}\\):\n\\[\n\\hat{T} = \\rho_{XX^\\prime} (X - \\bar{X}) + \\bar{X}.\n\\]\nEspandendo e riorganizzando l’equazione, arriviamo a una forma che chiarisce la relazione tra la media dei punteggi osservati, il coefficiente di attendibilità, e il punteggio osservato grezzo:\n\\[\n\\hat{T} = \\bar{X} + \\rho_{XX^\\prime} (X - \\bar{X}).\n\\]\nQuesta equazione finale dimostra come la stima del punteggio vero grezzo possa essere calcolata regolando il punteggio osservato per la media dei punteggi e il coefficiente di attendibilità.\nNel contesto di dati campionari, dove il coefficiente di attendibilità popolazionale \\(\\rho_{XX^\\prime}\\) viene sostituito con il suo corrispettivo campionario \\(r_{XX^\\prime}\\), la formula diventa:\n\\[\n\\hat{T} = \\bar{X} + r_{XX^\\prime} (X - \\bar{X}),\n\\]\noffrendo un metodo pratico per stimare il punteggio vero di un individuo a partire dal suo punteggio osservato, con l’aggiunta della media dei punteggi osservati e il coefficiente di attendibilità campionario.\nEsercizio. Posto un coefficiente di attendibilità pari a 0.80 e una media del test pari a \\(\\bar{X} = 100\\), si trovi una stima del punteggio vero per un rispondente con un punteggio osservato uguale a \\(X\\) = 115.\nLa stima del punteggio vero \\(\\hat{T}\\) è uguale a\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{T} &= \\bar{X} + r_{XX^\\prime}  (X - \\bar{X})\\notag\\\\\n&= 100 + 0.80 \\cdot (115 - 100) = 112.\n\\end{aligned}\n\\end{equation}\n\\]\nIn alternativa, possiamo usare la funzione Est.true del pacchetto psychometric.\n\nEst.true(115, 100, .8)\n\n112",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>La stima del punteggio vero</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/05_err_std_stima.html#lerrore-standard-della-stima",
    "href": "chapters/ctt/05_err_std_stima.html#lerrore-standard-della-stima",
    "title": "12  La stima del punteggio vero",
    "section": "12.3 L’Errore Standard della Stima",
    "text": "12.3 L’Errore Standard della Stima\nNell’ambito del modello di regressione di Kelley, uno strumento fondamentale per valutare la precisione delle stime del punteggio vero ottenute dai punteggi osservati è l’errore standard della stima. Questo indice quantifica quanto le nostre stime del punteggio vero possano variare se ripetessimo il test più volte sotto le stesse condizioni. Denotiamo l’errore standard della stima con \\(\\sigma_{\\hat{T}}\\), dove \\(\\hat{T}\\) rappresenta la stima del valore vero.\nL’errore standard della stima è cruciale per comprendere quanto sia affidabile la stima del punteggio vero. Un errore standard più piccolo indica una stima più precisa. Matematicamente, l’errore standard della stima si calcola come:\n\\[\n\\sigma_{\\hat{T}} = \\sigma_X \\sqrt{\\rho_{XX^\\prime} (1 -\\rho_{XX^\\prime})},\n\\tag{12.2}\\]\ndove \\(\\sigma_X\\) è la deviazione standard dei punteggi osservati, e \\(\\rho_{XX^\\prime}\\) rappresenta il coefficiente di correlazione tra i punteggi osservati e i punteggi veri. Questa formula assume una distribuzione normale dei punteggi e una relazione lineare tra i punteggi osservati e i punteggi veri.\nPer dati campionari, utilizziamo una formula leggermente modificata per calcolare l’errore standard della stima:\n\\[\ns_{\\hat{T}} = s_X \\sqrt{r_{XX^\\prime} (1-r_{XX^\\prime})},\n\\]\nqui \\(s_X\\) indica la deviazione standard campionaria, e \\(r_{XX^\\prime}\\) è il coefficiente di affidabilità campionario.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>La stima del punteggio vero</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/05_err_std_stima.html#derivazione-dellerrore-standard-della-stima",
    "href": "chapters/ctt/05_err_std_stima.html#derivazione-dellerrore-standard-della-stima",
    "title": "12  La stima del punteggio vero",
    "section": "12.4 Derivazione dell’Errore Standard della Stima",
    "text": "12.4 Derivazione dell’Errore Standard della Stima\nPer derivare l’Equazione 12.2, iniziamo con la definizione dell’errore \\(\\varepsilon\\), che rappresenta la discrepanza tra il punteggio reale \\(T\\) e il punteggio stimato \\(\\hat{T}\\), come illustrato nella formula:\n\\[\n\\varepsilon = T - \\hat{T}.\n\\]\nSi sottolinea la distinzione tra l’errore di misurazione, indicato con \\(E = X - T\\) (dove \\(E\\) quantifica l’errore di misurazione, ovvero la differenza tra il punteggio osservato \\(X\\) e il punteggio reale \\(T\\)), e l’errore \\(\\varepsilon = T - \\hat{T}\\) (che esprime la discrepanza tra il punteggio reale \\(T\\) e la sua stima \\(\\hat{T}\\)).\nAdottando la formula \\(\\hat{T} = \\bar{X} + \\rho_{XX^\\prime} (X - \\bar{X})\\) per esprimere \\(\\hat{T}\\), si può calcolare la varianza di \\(\\varepsilon\\) come segue:\n\\[\n\\begin{aligned}\n\\mathbb{V}(\\varepsilon) &=  \\mathbb{V}(T - \\hat{T}) \\\\\n&= \\mathbb{V}(T - \\bar{X} - \\rho_{XX^\\prime} X + \\rho_{XX^\\prime}\\bar{X}).\n\\end{aligned}\n\\]\nDato che l’aggiunta di una costante non altera la varianza di una variabile aleatoria, possiamo semplificare l’espressione a:\n\\[\n\\mathbb{V}(\\varepsilon) = \\mathbb{V}(T - \\rho_{XX^\\prime}X).\n\\]\nSfruttando la regola della varianza per la somma di variabili aleatorie, incluso il caso in cui una variabile è moltiplicata per una costante, arriviamo a:\n\\[\n\\begin{aligned}\n\\mathbb{V}(\\varepsilon) &= \\mathbb{V}(T) + \\rho_{XX^\\prime}^2 \\mathbb{V}(X) - 2 \\rho_{XX^\\prime} \\mbox{Cov}(X,T) \\\\\n&= \\sigma^2_T + \\rho_{XX^\\prime}^2 \\sigma^2_X - 2 \\rho_{XX^\\prime} \\sigma_{XT}.\n\\end{aligned}\n\\]\nCon \\(\\sigma_{XT} = \\sigma^2_T\\), possiamo ridurre ulteriormente l’espressione a:\n\\[\n\\sigma^2_{\\varepsilon} = \\sigma^2_T + \\left(\\frac{\\sigma_T^2}{\\sigma_X^2}\\right)^2 \\sigma^2_X - 2 \\frac{\\sigma_T^2}{\\sigma_X^2} \\sigma_{XT}.\n\\]\nSemplificando, otteniamo:\n\\[\n\\begin{aligned}\n\\sigma^2_{\\varepsilon} &= \\sigma^2_T + \\frac{\\sigma_T^4}{\\sigma_X^4} \\sigma^2_X - 2 \\frac{\\sigma_T^2}{\\sigma_X^2} \\sigma_{XT} \\\\\n&= \\sigma^2_T \\left(1 + \\frac{\\sigma_T^2}{\\sigma_X^2} - 2 \\frac{\\sigma_{XT}}{\\sigma_X^2}\\right).\n\\end{aligned}\n\\]\nInfine, con \\(\\sigma_{XT} = \\sigma^2_T\\), semplifichiamo a:\n\\[\n\\begin{aligned}\n\\sigma^2_{\\varepsilon} &= \\sigma^2_T \\left(1 - \\frac{\\sigma_{T}^2}{\\sigma_X^2}\\right).\n\\end{aligned}\n\\]\nQuesto ci porta alla formula dell’errore standard della stima \\(\\sigma_{\\varepsilon}\\):\n\\[\n\\begin{aligned}\n\\sigma_{\\varepsilon} &= \\sigma_T \\sqrt{1 - \\frac{\\sigma^2_T}{\\sigma^2_X}} \\\\\n&= \\sigma_T \\sqrt{\\frac{\\sigma^2_X - \\sigma^2_T}{\\sigma^2_X}} \\\\\n&= \\frac{\\sigma_T}{\\sigma_X} \\sqrt{\\sigma^2_X - \\sigma^2_T}.\n\\end{aligned}\n\\]\nConsiderando che \\(\\sigma^2_X = \\sigma^2_T + \\sigma^2_E\\), l’errore standard di stima diventa:\n\\[\n\\begin{aligned}\n\\sigma_{\\varepsilon} &= \\frac{\\sigma_T}{\\sigma_X} \\sqrt{\\sigma^2_E } \\\\\n&= \\frac{\\sigma_T}{\\sigma_X} \\sigma_E \\\\\n&= \\sqrt{\\rho_{XX^\\prime}} \\sigma_E.\n\\end{aligned}\n\\]\nDato che l’errore standard di misurazione è definito come \\(\\sigma_E = \\sigma_X \\sqrt{1 - \\rho_{XX^\\prime}}\\), possiamo concludere che:\n\\[\n\\begin{aligned}\n\\sigma_{\\varepsilon} &= \\sqrt{\\rho_{XX^\\prime}} \\sigma_E \\\\\n&= \\sqrt{\\rho_{XX^\\prime}} \\sigma_X \\sqrt{1-\\rho_{XX^\\prime}} \\\\\n&= \\sigma_X \\sqrt{\\rho_{XX^\\prime} (1 - \\rho_{XX^\\prime})}.\n\\end{aligned}\n\\]\nQuest’ultima espressione dimostra come l’errore standard della stima sia determinato dalla deviazione standard dei punteggi osservati, modulata dal coefficiente di correlazione tra i punteggi osservati e i punteggi veri.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>La stima del punteggio vero</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/05_err_std_stima.html#intervallo-di-confidenza-per-il-punteggio-vero",
    "href": "chapters/ctt/05_err_std_stima.html#intervallo-di-confidenza-per-il-punteggio-vero",
    "title": "12  La stima del punteggio vero",
    "section": "12.5 Intervallo di confidenza per il punteggio vero",
    "text": "12.5 Intervallo di confidenza per il punteggio vero\nSiamo ora finalmente nelle condizioni di potere calcolare l’intervallo di confidenza per il punteggio vero. Conoscendo l’errore standard della stima \\(\\sigma_{\\hat{T}}\\), l’intervallo di confidenza per il punteggio vero è dato da:\n\\[\n\\hat{T} \\pm z  \\sigma_{\\hat{T}},\n\\]\nladdove \\(\\hat{T}\\) è la stima del punteggio vero e \\(z\\) è il quantile della normale standardizzata al livello di probabilità desiderato. Se il campione è piccolo (minore di 30) è opportuno usare \\(t\\) anziché \\(z\\).\n\n12.5.1 Interpretazione\nNotiamo che l’intervallo \\(\\hat{T} \\pm z \\sigma_{\\hat{T}}\\) è centrato sulla stima puntuale del valore vero e la sua ampiezza dipende sia dal livello di confidenza desiderato (rappresentato dal quantile \\(z_{\\frac{\\alpha}{2}}\\)), sia dal grado di precisione dello stimatore, misurato dall’errore standard della stima, \\(\\sigma_{\\hat{T}} = \\sigma_X \\sqrt{\\rho_{XX^\\prime} (1 -\\rho_{XX^\\prime})}\\). È importante notare che l’errore standard della stima diventa sempre più grande man mano che diminuisce l’attendibilità \\(\\rho_{XX^\\prime}\\) del test.\nL’intervallo di confidenza indica quanto l’imprecisione della misura influisce sull’interpretazione dei dati. Più l’intervallo di confidenza è ampio, maggiore è l’incertezza nella valutazione dei risultati.\nEsercizio. Charter (1996) ha esaminato l’effetto della variazione dell’attendibilità del test sull’ampiezza dell’intervallo di confidenza per il punteggio vero. Utilizzando come esempio i punteggi di QI (\\(\\mu\\) = 100, \\(\\sigma\\) = 15), Charter ha immaginato di variare il coefficiente di attendibilità del test utilizzato per la misurazione del QI. I valori presi in considerazione sono 0.55, 0.65, 0.75, 0.85 e 0.95. Ad esempio, supponiamo di avere un punteggio osservato pari a QI = 120 e un coefficiente di attendibilità del test \\(\\rho_{xx^\\prime}\\) pari a 0.65. In tali circostanze, la stima del punteggio vero è pari a\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{T} &= \\bar{X} + r_{XX^\\prime}  (X - \\bar{X}) \\notag\\\\\n&= 100 + 0.65 (120 - 100)\\notag\\\\\n&= 113.\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nL’errore standard della stima è uguale a\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\sigma_{\\hat{T}} &= \\sigma_{X} \\sqrt{r_{XX^\\prime} (1 - r_{XX^\\prime})} \\notag\\\\\n&= 15 \\sqrt{0.65 (1 - 0.65)}\\notag\\\\\n&= 7.15.\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nL’intervallo di confidenza al 95% per la stima del punteggio vero diventa pertanto uguale a\n\\[\n113 \\pm 1.96 \\cdot 7.15 = [98.98, 127.02].\n\\]\nSi noti che si può calcolare l’errore standard della stima con la funzione SE.Est() del pacchetto psychometric.\n\nSE.Est(15, .65)\n\n7.15454401062709\n\n\n\nInoltre, la funzione CI.tscore() restituisce sia la stima del punteggio vero sia l’intervallo di fiducia al livello desiderato di significatività.\n\nCI.tscore(120, 100, 15, 0.65, level = 0.95)\n\n\nA data.frame: 1 x 4\n\n\nSE.Est\nLCL\nT.Score\nUCL\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n7.154544\n98.97735\n113\n127.0226",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>La stima del punteggio vero</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/05_err_std_stima.html#cut-off",
    "href": "chapters/ctt/05_err_std_stima.html#cut-off",
    "title": "12  La stima del punteggio vero",
    "section": "12.6 Cut-off",
    "text": "12.6 Cut-off\nGli intervalli di confidenza per il punteggio vero possono essere utilizzati per confrontare i limiti dell’intervallo con un cut-off. Ci sono tre possibili esiti: il limite inferiore dell’intervallo di confidenza è maggiore del cut-off, il limite superiore dell’intervallo è minore del cut-off, o il valore del cut-off è compreso all’interno dell’intervallo. Nel primo caso, lo psicologo può affermare, con un grado di certezza \\(1-\\alpha\\), che il valore vero del rispondente è superiore al cut-off. Nel secondo caso, lo psicologo può affermare, con un grado di certezza \\(1-\\alpha\\), che il valore vero del rispondente è inferiore al cut-off. Nel terzo caso, lo psicologo non può concludere né che il valore vero sia inferiore né che sia superiore al cut-off, con un certo grado di incertezza.\n\nSi considerino i punteggi del QI, per cui \\(\\bar{X}\\) = 100 e \\(s_X\\) = 15. Sia l’attendibilità del test \\(\\rho_{XX^\\prime}\\) = 0.95. Supponiamo che il rispondente abbia un QI = 130. Poniamo che il cut-off per ammettere il rispondente ad un corso avanzato sia 120. Ci sono tre alternative: il valore vero del rispondente è sicuramente maggiore di 120; il valore vero del rispondente è sicuramente inferiore di 120; le evidenze disponibili ci lasciano in dubbio se il punteggio vero sia maggiore o minore di 120. Svolgiamo i calcoli per trovare l’intervallo di confidenza al livello di certezza del 95%:\n\nxm &lt;- 100\nsx &lt;- 15\nrho &lt;- .95\nx &lt;- 130\nt.hat &lt;- xm + rho * (x - xm)\nt.hat\nse.t &lt;- sx * sqrt(rho * (1 - rho))\nse.t\nt.hat + c(1, -1) * qnorm(.025, 0, 1) * se.t\n\n128.5\n\n\n3.26917420765551\n\n\n\n122.092536293808134.907463706192\n\n\nDato che il limite inferiore dell’intervallo di confidenza è maggiore del cut-off, lo psicologo conclude che il punteggio vero del rispondente è maggiore di 120. Quindi, raccomanda che il rispondente sia ammesso al corso avanzato.\nContinuando con l’esempio precedente, supponiamo che l’attendibilità del test abbia un valore simile a quello che solitamente si ottiene empiricamente, ovvero 0.80.\n\nxm &lt;- 100\nsx &lt;- 15\nrho &lt;- .8\nx &lt;- 130\nt.hat &lt;- xm + rho * (x - xm)\nt.hat\nse.t &lt;- sx * sqrt(rho * (1 - rho))\nse.t\nt.hat + c(1, -1) * qnorm(.025, 0, 1) * se.t\n\n124\n\n\n6\n\n\n\n112.24021609276135.75978390724\n\n\nIn questo secondo esempio, l’intervallo di confidenza al 95% è \\([112.24,\n135.76]\\) e contiene il valore del cut-off. Dunque, la decisione dello psicologo è che non vi sono evidenze sufficienti che il vero valore del rispondente sia superiore al cut-off. Si noti come la diminuzione dell’attendibilità del test porta all’aumento delle dimensioni dell’intervallo di confidenza.\n\n12.7 Riflessioni Conclusive\nLa teoria classica del punteggio vero si basa su un modello additivo, in cui il punteggio osservato \\(X\\) è considerato come la somma di due componenti: il punteggio vero stabile \\(T\\) e il punteggio di errore casuale \\(E\\). Si suppone che i punteggi di errore all’interno di un test non siano correlati né con i punteggi veri di quel test, né con i punteggi veri o di errore di altri test. I test paralleli hanno gli stessi punteggi veri e le stesse varianze di errore. I test che sono considerati “sostanzialmente equivalenti” o \\(\\tau\\)-equivalenti, differiscono solo per una costante additiva nei punteggi veri. Tuttavia, queste assunzioni possono essere violate in presenza di diverse condizioni che influenzano i punteggi dei test. Tuttavia, poiché non possiamo osservare direttamente \\(T\\) ed \\(E\\), non possiamo verificare direttamente l’adeguatezza di queste assunzioni, e possiamo solo fare delle supposizioni su quando sarebbero appropriate.\nÈ importante tenere a mente che i punteggi veri e quelli di errore sono concetti teorici e non osservabili. Ciò che possiamo osservare sono solamente i punteggi \\(X\\). Quando parliamo di punteggi veri, è essenziale considerare che il “punteggio vero”, cioè la media dei punteggi su ripetuti test indipendenti con lo stesso test, è un’astrazione teorica. Questo punteggio potrebbe non riflettere completamente l’attributo “vero” di interesse, a meno che il test non abbia una precisione perfetta, cioè che misuri esattamente ciò che afferma di misurare.\nL’approccio della teoria classica dei test (CTT) nel processo di sviluppo dei test presenta diversi vantaggi. In primo luogo, i concetti della CTT sono ampiamente diffusi e comprensibili. Inoltre, sono relativamente accessibili sia per l’apprendimento che per l’applicazione. Le statistiche descrittive dei test (come la media, la deviazione standard, l’intervallo, ecc.) e le analisi degli item (in particolare la facilità e la discriminazione degli item) possono essere calcolate facilmente. Inoltre, il modello CTT risponde a varie esigenze di misurazione, specialmente nello sviluppo di valutazioni di competenze e collocazione, utili per decisioni di ammissione, confronti tra programmi e valutazioni in vari contesti lavorativi. Infine, il modello CTT permette l’interpretazione dei punteggi degli esaminati sia al 0% che al 100% e delle stime di facilità degli item da 0.0 a 1.0, riflettendo risultati realistici. Tuttavia, queste interpretazioni non sono comuni nei modelli di teoria della risposta agli item (IRT).\nTuttavia, l’adozione della CTT presenta anche alcune limitazioni. In primo luogo, i test basati sulla CTT tendono a essere lunghi e composti da elementi omogenei. In secondo luogo, gli individui che svolgono test sviluppati con il metodo CTT potrebbero essere confrontati con item troppo facili o troppo difficili per le loro abilità. In terzo luogo, i risultati dei test CTT si applicano solo al campione considerato o a campioni molto simili. In quarto luogo, tali risultati si applicano solo alla selezione corrente di item. In quinto luogo, a causa della dipendenza dalla distribuzione normale, la CTT è adatta solo per lo sviluppo di test normativi. In sesto luogo, a causa della correlazione tra discriminazione degli item, affidabilità e alcune stime di validità, gli item e i test basati sulla CTT possono risultare sensibili alle differenze agli estremi della scala. Infine, sebbene gli errori di misurazione nei test CTT varino lungo tutto il range dei possibili punteggi (ossia, l’errore standard di misurazione è minore vicino alla media e aumenta man mano che i punteggi si discostano dalla media in entrambe le direzioni), l’errore standard di misurazione stimato nei CTT rappresenta una media su tutto questo intervallo.\n\n\n12.8 Session Info\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] psychometric_2.4  multilevel_2.7    MASS_7.3-60.0.1   nlme_3.1-164     \n [5] ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [9] ggExtra_0.10.1    bayesplot_1.11.1  gridExtra_2.3     patchwork_1.2.0  \n[13] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-17     psych_2.4.1      \n[17] scales_1.3.0      markdown_1.12     knitr_1.45        lubridate_1.9.3  \n[21] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[25] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.0    \n[29] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     magrittr_2.0.3    \n  [4] nloptr_2.0.3       rmarkdown_2.26     vctrs_0.6.5       \n  [7] minqa_1.2.6        base64enc_0.1-3    rstatix_0.7.2     \n [10] htmltools_0.5.7    broom_1.0.5        Formula_1.2-5     \n [13] htmlwidgets_1.6.4  plyr_1.8.9         uuid_1.2-0        \n [16] igraph_2.0.2       mime_0.12          lifecycle_1.0.4   \n [19] pkgconfig_2.0.3    Matrix_1.6-5       R6_2.5.1          \n [22] fastmap_1.1.1      shiny_1.8.0        digest_0.6.34     \n [25] OpenMx_2.21.11     fdrtool_1.2.17     colorspace_2.1-0  \n [28] rprojroot_2.0.4    Hmisc_5.1-1        fansi_1.0.6       \n [31] timechange_0.3.0   abind_1.4-5        compiler_4.3.3    \n [34] withr_3.0.0        glasso_1.11        htmlTable_2.4.2   \n [37] backports_1.4.1    carData_3.0-5      ggsignif_0.6.4    \n [40] corpcor_1.6.10     gtools_3.9.5       tools_4.3.3       \n [43] pbivnorm_0.6.0     foreign_0.8-86     zip_2.3.1         \n [46] httpuv_1.6.14      nnet_7.3-19        glue_1.7.0        \n [49] quadprog_1.5-8     promises_1.2.1     lisrelToR_0.3     \n [52] grid_4.3.3         pbdZMQ_0.3-11      checkmate_2.3.1   \n [55] cluster_2.1.6      reshape2_1.4.4     generics_0.1.3    \n [58] gtable_0.3.4       tzdb_0.4.0         data.table_1.15.2 \n [61] hms_1.1.3          car_3.1-2          utf8_1.2.4        \n [64] sem_3.1-15         pillar_1.9.0       IRdisplay_1.1     \n [67] rockchalk_1.8.157  later_1.3.2        splines_4.3.3     \n [70] lattice_0.22-5     kutils_1.73        tidyselect_1.2.0  \n [73] miniUI_0.1.1.1     pbapply_1.7-2      stats4_4.3.3      \n [76] xfun_0.42          qgraph_1.9.8       arm_1.13-1        \n [79] stringi_1.8.3      boot_1.3-29        evaluate_0.23     \n [82] mi_1.1             cli_3.6.2          RcppParallel_5.1.7\n [85] IRkernel_1.3.2     rpart_4.1.23       xtable_1.8-4      \n [88] repr_1.1.6         munsell_0.5.0      Rcpp_1.0.12       \n [91] coda_0.19-4.1      png_0.1-8          XML_3.99-0.16.1   \n [94] parallel_4.3.3     ellipsis_0.3.2     jpeg_0.1-10       \n [97] lme4_1.1-35.1      openxlsx_4.2.5.2   crayon_1.5.2      \n[100] rlang_1.1.3        mnormt_2.1.1      \n\n\n\n\n\n\n\n\nCharter, R. A. (1996). Revisiting the standard errors of measurement, estimate, and prediction and their application to test scores. Perceptual and Motor Skills, 82(3), 1139–1144.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>La stima del punteggio vero</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/05_err_std_stima.html#riflessioni-conclusive",
    "href": "chapters/ctt/05_err_std_stima.html#riflessioni-conclusive",
    "title": "12  La stima del punteggio vero",
    "section": "12.7 Riflessioni Conclusive",
    "text": "12.7 Riflessioni Conclusive\nLa teoria classica del punteggio vero si basa su un modello additivo, in cui il punteggio osservato \\(X\\) è considerato come la somma di due componenti: il punteggio vero stabile \\(T\\) e il punteggio di errore casuale \\(E\\). Si suppone che i punteggi di errore all’interno di un test non siano correlati né con i punteggi veri di quel test, né con i punteggi veri o di errore di altri test. I test paralleli hanno gli stessi punteggi veri e le stesse varianze di errore. I test che sono considerati “sostanzialmente equivalenti” o \\(\\tau\\)-equivalenti, differiscono solo per una costante additiva nei punteggi veri. Tuttavia, queste assunzioni possono essere violate in presenza di diverse condizioni che influenzano i punteggi dei test. Tuttavia, poiché non possiamo osservare direttamente \\(T\\) ed \\(E\\), non possiamo verificare direttamente l’adeguatezza di queste assunzioni, e possiamo solo fare delle supposizioni su quando sarebbero appropriate.\nÈ importante tenere a mente che i punteggi veri e quelli di errore sono concetti teorici e non osservabili. Ciò che possiamo osservare sono solamente i punteggi \\(X\\). Quando parliamo di punteggi veri, è essenziale considerare che il “punteggio vero”, cioè la media dei punteggi su ripetuti test indipendenti con lo stesso test, è un’astrazione teorica. Questo punteggio potrebbe non riflettere completamente l’attributo “vero” di interesse, a meno che il test non abbia una precisione perfetta, cioè che misuri esattamente ciò che afferma di misurare.\nL’approccio della teoria classica dei test (CTT) nel processo di sviluppo dei test presenta diversi vantaggi. In primo luogo, i concetti della CTT sono ampiamente diffusi e comprensibili. Inoltre, sono relativamente accessibili sia per l’apprendimento che per l’applicazione. Le statistiche descrittive dei test (come la media, la deviazione standard, l’intervallo, ecc.) e le analisi degli item (in particolare la facilità e la discriminazione degli item) possono essere calcolate facilmente. Inoltre, il modello CTT risponde a varie esigenze di misurazione, specialmente nello sviluppo di valutazioni di competenze e collocazione, utili per decisioni di ammissione, confronti tra programmi e valutazioni in vari contesti lavorativi. Infine, il modello CTT permette l’interpretazione dei punteggi degli esaminati sia al 0% che al 100% e delle stime di facilità degli item da 0.0 a 1.0, riflettendo risultati realistici. Tuttavia, queste interpretazioni non sono comuni nei modelli di teoria della risposta agli item (IRT).\nTuttavia, l’adozione della CTT presenta anche alcune limitazioni. In primo luogo, i test basati sulla CTT tendono a essere lunghi e composti da elementi omogenei. In secondo luogo, gli individui che svolgono test sviluppati con il metodo CTT potrebbero essere confrontati con item troppo facili o troppo difficili per le loro abilità. In terzo luogo, i risultati dei test CTT si applicano solo al campione considerato o a campioni molto simili. In quarto luogo, tali risultati si applicano solo alla selezione corrente di item. In quinto luogo, a causa della dipendenza dalla distribuzione normale, la CTT è adatta solo per lo sviluppo di test normativi. In sesto luogo, a causa della correlazione tra discriminazione degli item, affidabilità e alcune stime di validità, gli item e i test basati sulla CTT possono risultare sensibili alle differenze agli estremi della scala. Infine, sebbene gli errori di misurazione nei test CTT varino lungo tutto il range dei possibili punteggi (ossia, l’errore standard di misurazione è minore vicino alla media e aumenta man mano che i punteggi si discostano dalla media in entrambe le direzioni), l’errore standard di misurazione stimato nei CTT rappresenta una media su tutto questo intervallo.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>La stima del punteggio vero</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/05_err_std_stima.html#session-info",
    "href": "chapters/ctt/05_err_std_stima.html#session-info",
    "title": "12  La stima del punteggio vero",
    "section": "12.8 Session Info",
    "text": "12.8 Session Info\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] psychometric_2.4  multilevel_2.7    MASS_7.3-60.0.1   nlme_3.1-164     \n [5] ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [9] ggExtra_0.10.1    bayesplot_1.11.1  gridExtra_2.3     patchwork_1.2.0  \n[13] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-17     psych_2.4.1      \n[17] scales_1.3.0      markdown_1.12     knitr_1.45        lubridate_1.9.3  \n[21] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[25] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.0    \n[29] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     magrittr_2.0.3    \n  [4] nloptr_2.0.3       rmarkdown_2.26     vctrs_0.6.5       \n  [7] minqa_1.2.6        base64enc_0.1-3    rstatix_0.7.2     \n [10] htmltools_0.5.7    broom_1.0.5        Formula_1.2-5     \n [13] htmlwidgets_1.6.4  plyr_1.8.9         uuid_1.2-0        \n [16] igraph_2.0.2       mime_0.12          lifecycle_1.0.4   \n [19] pkgconfig_2.0.3    Matrix_1.6-5       R6_2.5.1          \n [22] fastmap_1.1.1      shiny_1.8.0        digest_0.6.34     \n [25] OpenMx_2.21.11     fdrtool_1.2.17     colorspace_2.1-0  \n [28] rprojroot_2.0.4    Hmisc_5.1-1        fansi_1.0.6       \n [31] timechange_0.3.0   abind_1.4-5        compiler_4.3.3    \n [34] withr_3.0.0        glasso_1.11        htmlTable_2.4.2   \n [37] backports_1.4.1    carData_3.0-5      ggsignif_0.6.4    \n [40] corpcor_1.6.10     gtools_3.9.5       tools_4.3.3       \n [43] pbivnorm_0.6.0     foreign_0.8-86     zip_2.3.1         \n [46] httpuv_1.6.14      nnet_7.3-19        glue_1.7.0        \n [49] quadprog_1.5-8     promises_1.2.1     lisrelToR_0.3     \n [52] grid_4.3.3         pbdZMQ_0.3-11      checkmate_2.3.1   \n [55] cluster_2.1.6      reshape2_1.4.4     generics_0.1.3    \n [58] gtable_0.3.4       tzdb_0.4.0         data.table_1.15.2 \n [61] hms_1.1.3          car_3.1-2          utf8_1.2.4        \n [64] sem_3.1-15         pillar_1.9.0       IRdisplay_1.1     \n [67] rockchalk_1.8.157  later_1.3.2        splines_4.3.3     \n [70] lattice_0.22-5     kutils_1.73        tidyselect_1.2.0  \n [73] miniUI_0.1.1.1     pbapply_1.7-2      stats4_4.3.3      \n [76] xfun_0.42          qgraph_1.9.8       arm_1.13-1        \n [79] stringi_1.8.3      boot_1.3-29        evaluate_0.23     \n [82] mi_1.1             cli_3.6.2          RcppParallel_5.1.7\n [85] IRkernel_1.3.2     rpart_4.1.23       xtable_1.8-4      \n [88] repr_1.1.6         munsell_0.5.0      Rcpp_1.0.12       \n [91] coda_0.19-4.1      png_0.1-8          XML_3.99-0.16.1   \n [94] parallel_4.3.3     ellipsis_0.3.2     jpeg_0.1-10       \n [97] lme4_1.1-35.1      openxlsx_4.2.5.2   crayon_1.5.2      \n[100] rlang_1.1.3        mnormt_2.1.1",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>La stima del punteggio vero</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/06_ctt_applications.html",
    "href": "chapters/ctt/06_ctt_applications.html",
    "title": "13  Applicazioni della CTT",
    "section": "",
    "text": "13.1 Introduzione\nQuesto capitolo si focalizza sull’esplorazione di diverse applicazioni della Teoria Classica dei Test (CTT). Innanzitutto, verrà analizzato il metodo per determinare il numero di item necessari al fine di ottenere un livello specifico di affidabilità. Successivamente, si approfondirà il concetto di correlazione disattenuata e si esaminerà il metodo proposto per mitigare tale disattenuazione. Infine, verrà presentato l’utilizzo del metodo di Kelly per migliorare la stima dei punteggi reali a livello individuale, e sarà esaminato come i modelli bayesiani gerarchici rappresentino un’alternativa più moderna a tale approccio.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Applicazioni della CTT</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/06_ctt_applications.html#stimare-un-cambiamento-clinicamente-significativo",
    "href": "chapters/ctt/06_ctt_applications.html#stimare-un-cambiamento-clinicamente-significativo",
    "title": "13  Applicazioni della CTT",
    "section": "13.2 Stimare un Cambiamento Clinicamente Significativo",
    "text": "13.2 Stimare un Cambiamento Clinicamente Significativo\nIn psicologia clinica, uno dei principali problemi è determinare se si sia verificato un cambiamento clinicamente significativo in un individuo. Le metodologie utilizzate per questa valutazione sono generalmente suddivise in due categorie: i metodi “basati su ancoraggi” e quelli “basati sulla distribuzione” (Blampied, 2022).\nI metodi basati su ancoraggi definiscono un cambiamento clinicamente significativo come una variazione nei punteggi che corrisponde a un evento clinico rilevante. In altre parole, si considera che un cambiamento sia significativo se riflette un miglioramento o peggioramento percepito in seguito a un evento clinico importante, come ad esempio il miglioramento dopo un intervento terapeutico. I metodi basati sulla distribuzione, invece, utilizzano gli errori di misurazione psicometrica come parametro per valutare la significatività clinica del cambiamento. Uno dei primi metodi di questo tipo è stato l’indice di cambiamento affidabile, o Reliable Change Index (RCI), proposto da Jacobson e Truax. Questo indice si basa sull’errore standard della differenza (SED), che a sua volta è derivato dall’errore standard di misura (SEM). Quest’ultimo è calcolato come:\n\\[\nSEM = s_x \\sqrt{1 - r_{xx'}},\n\\]\ndove \\(s_x\\) è la deviazione standard dei punteggi al pre-test e \\(r_{xx'}\\) è la affidabilità dello strumento di misura.\nUn cambiamento è considerato clinicamente significativo se supera l’errore di misura intrinseco, ossia la variabilità casuale che può essere attribuita all’errore di misurazione e non a un cambiamento reale.\n\n13.2.1 Calcolo del Reliable Change Index (RCI)\nPer comprendere come si calcola il Reliable Change Index, è utile ricordare che qualsiasi misurazione può essere scomposta in due componenti: il punteggio reale (o vero) e un componente di errore. Possiamo rappresentare qualsiasi misurazione come:\n\\[\nX = T \\pm E,\n\\]\ndove \\(X\\) è il punteggio osservato, \\(T\\) il punteggio vero e \\(E\\) l’errore di misurazione. Se un cambiamento effettivo si è verificato nel tempo per un individuo nella dimensione catturata dalla variabile dipendente e all’interno della sensibilità dello strumento di misurazione, allora il punteggio vero al tempo \\(t1\\) sarà diverso dal punteggio vero al tempo \\(t2\\). Tuttavia, data la variabilità e l’errore intrinseci alle misurazioni psicologiche, è possibile che si osservi una differenza tra i punteggi \\(t1\\) e \\(t2\\) anche in assenza di un reale cambiamento. Pertanto, come possiamo determinare quanto debba essere grande un cambiamento osservato affinché si possa concludere che un cambiamento reale sia effettivamente avvenuto, piuttosto che semplicemente una variazione insignificante?\nLa derivazione dell’RCI si basa sulla distribuzione di frequenza degli errori di misurazione. La deviazione standard della distribuzione degli errori di misurazione è data dal SEM, e Jacobson e colleghi (1984) hanno utilizzato questa conoscenza di base per definire l’RCI. Tuttavia, per comprendere questa definizione, è necessario prima comprendere la distribuzione degli errori dei punteggi di differenza.\n\n\n13.2.2 Calcolo della Differenza tra i Punteggi e la sua Distribuzione di Errore\nIl modo più semplice per identificare se un cambiamento si è verificato è calcolare un punteggio di differenza non nullo, rappresentato come un punteggio di cambiamento grezzo (\\(C\\)) dato dalla differenza tra il punteggio al tempo \\(t2\\) e quello al tempo \\(t1\\) per un individuo:\n\\[\nC_i = X_{t1} - X_{t2},\n\\]\ndove \\(C_i\\) è il punteggio di cambiamento per l’individuo \\(i\\), e \\(X\\) rappresenta il punteggio misurato in due momenti temporali distinti \\(t1\\) e \\(t2\\). Poiché ogni misurazione contiene sia un punteggio vero che un errore di misurazione, il punteggio di cambiamento grezzo comprenderà il vero cambiamento più o meno un errore di misurazione. Poiché l’errore è il risultato della combinazione degli errori presenti in ciascuna misurazione, esso sarà più grande rispetto agli errori associati ai singoli punteggi.\nLa distribuzione degli errori dei punteggi di differenza segue anch’essa una distribuzione normale con media pari a zero ma con una deviazione standard più ampia, chiamata deviazione standard della differenza (\\(SD_{Diff}\\)), calcolata come:\n\\[\nSD_{Diff} = \\sqrt{2 \\times SEM^2}.\n\\]\nQuesto perché, secondo la proprietà della varianza di una differenza tra due variabili indipendenti (o scorrelate), la varianza della differenza è data dalla somma delle varianze individuali, ossia:\n\\[\n\\sigma_{X1 - X2} = \\sigma_1^2 + \\sigma_2^2 - 2 \\cdot \\text{cov}(X1, X2).\n\\]\nNel caso in cui le misurazioni siano indipendenti o non correlate, \\(\\text{cov}(X1, X2) = 0\\), e quindi la deviazione standard della differenza diventa \\(\\sqrt{2 \\cdot SEM^2}\\).\nAnche se i punteggi osservati prima e dopo l’intervento riflettono lo stesso individuo e sono quindi correlati nei loro “punteggi veri” (cioè il cambiamento reale), si considera che gli errori di misurazione associati a ciascun punteggio siano indipendenti. Questo è un presupposto comune nelle analisi psicometriche, poiché ogni volta che si ripete una misurazione su uno stesso individuo, le fonti di errore possono variare (per esempio, variazioni casuali nel modo in cui risponde al test in un determinato giorno, piccole differenze nel contesto della misurazione, ecc.).\n\n\n13.2.3 Definizione dell’RCI\nJacobson e Truax (1991) hanno definito l’RCI come il punteggio di cambiamento standardizzato, ottenuto dividendo il punteggio di differenza per \\(SD_{Diff}\\):\n\\[\nC_i (\\text{Standardized}) = \\frac{C_i}{SD_{Diff}}.\n\\]\nQuesta trasformazione converte il punteggio di cambiamento grezzo in unità di deviazione standard, analogamente a come uno z-score rappresenta la differenza tra un punteggio individuale e la media, standardizzata tramite la deviazione standard. Un valore di RCI clinicamente significativo suggerisce che il cambiamento osservato è sufficientemente grande da non poter essere attribuito al solo errore di misurazione.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Applicazioni della CTT</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/06_ctt_applications.html#affidabilità-e-lunghezza-del-test",
    "href": "chapters/ctt/06_ctt_applications.html#affidabilità-e-lunghezza-del-test",
    "title": "13  Applicazioni della CTT",
    "section": "13.3 Affidabilità e lunghezza del test",
    "text": "13.3 Affidabilità e lunghezza del test\nL’affidabilità può essere utilizzata per determinare la lunghezza di un test. La formula di Spearman-Brown può essere adattata per calcolare il numero di item necessari al fine di raggiungere una specifica affidabilità:\n\\[\n\\begin{equation}\np = \\frac{\\rho_p(1 - \\rho_1)}{\\rho_1(1 - \\rho_p)},\n\\end{equation}\n\\tag{13.1}\\]\ndove \\(\\rho_1\\) rappresenta l’affidabilità stimata di un “item medio”, \\(\\rho_p\\) è il livello desiderato di affidabilità complessiva del test, e \\(p\\) è il numero di item nel test esteso.\nPer esempio, supponiamo che l’attendibilità di un test composto da 5 item sia 0.824, e che \\(\\rho_1\\) sia 0.479. Possiamo chiederci quanti item debbano essere aggiunti per raggiungere un livello di affidabilità pari a 0.95.\nPonendo \\(\\rho_p\\) a 0.95 e \\(\\rho_1\\) a 0.479, in base all’equazione Equazione 13.1, otteniamo che:\n\nrho_1 &lt;- 0.479\n(.95 * (1 - rho_1)) / (rho_1 * (1 - .95))\n\n20.6659707724426\n\n\nPertanto, per ottenere un livello di affidabilità pari a 0.95 sono necessari almeno 21 item.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Applicazioni della CTT</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/06_ctt_applications.html#attenuazione",
    "href": "chapters/ctt/06_ctt_applications.html#attenuazione",
    "title": "13  Applicazioni della CTT",
    "section": "13.4 Attenuazione",
    "text": "13.4 Attenuazione\n\n13.4.1 Attenuazione e Correlazioni Disattenuate\nUn aspetto cruciale nell’analisi statistica riguarda il fenomeno dell’attenuazione, che si verifica quando l’incremento dell’errore di misurazione porta a una riduzione della correlazione osservata tra due variabili. Questo errore di misurazione tende a “nascondere” la vera associazione esistente tra le variabili, generando quello che è noto come effetto di attenuazione.\nLord e Novick (1967) hanno sottolineato che, nel tentativo di esplorare la relazione tra due costrutti, gli psicologi spesso ricorrono allo sviluppo di scale di misura. Se esiste una relazione lineare tra queste scale, è possibile calcolare il grado di correlazione attraverso il coefficiente di correlazione. Tuttavia, dato che le scale includono inevitabilmente un certo livello di errore, la correlazione empiricamente osservata tra di esse risulta inferiore rispetto alla correlazione “vera” tra i costrutti. In queste circostanze, è possibile ricorrere a formule specifiche per stimare la correlazione corretta tra i tratti latenti.\nSi può dimostrare che la correlazione tra i punteggi veri di due costrutti, \\(T_X\\) e \\(T_Y\\), può essere calcolata utilizzando la correlazione \\(\\rho_{XY}\\) tra i punteggi osservati \\(X\\) e \\(Y\\), e i coefficienti di affidabilità \\(\\rho_{XX'}\\) e \\(\\rho_{YY'}\\) dei due test, come segue:\n\\[\n\\begin{equation}\n\\rho(T_X, T_Y)  = \\frac{\\rho_{XY}}{\\sqrt{\\rho_{XX^\\prime} \\rho_{YY^\\prime}}}.\n\\end{equation}\n\\](eq-3-9-6)\nAnalogamente, la correlazione tra i punteggi osservati di un test e i punteggi veri di un secondo test può essere espressa attraverso la correlazione tra i punteggi osservati dei due test e il coefficiente di affidabilità del secondo test:\n\\[\n\\begin{equation}\n\\rho(X, T_Y)  = \\frac{\\rho_{XY}}{\\sqrt{\\rho_{YY^\\prime}}}.\n\\end{equation}\n\\](eq-3-9-7)\nQueste equazioni forniscono gli strumenti per calcolare le correlazioni disattenuate secondo la Teoria Classica dei Test (CTT).\nIl calcolo degli intervalli di confidenza per la correlazione corretta richiede un approccio che tenga conto dell’attenuazione dell’affidabilità. Applicando la formula di disattenuazione agli estremi dell’intervallo di confidenza osservato, possiamo ottenere stime più precise degli intervalli di confidenza per la correlazione tra i punteggi veri.\nPer fare un esempio, supponiamo di avere una correlazione osservata di 0.5 tra due misure, con affidabilità di 0.7 per la prima misura e 0.8 per la seconda misura. Vogliamo calcolare la correlazione disattenuata e il relativo intervallo di confidenza.\n\n# Parametri\nr_osservata &lt;- 0.5\nrho_X &lt;- 0.7\nrho_Y &lt;- 0.8\n\n# Calcolo della correlazione disattenuata\nr_corretta &lt;- r_osservata / sqrt(rho_X * rho_Y)\n\n# Stampa della correlazione disattenuata\nprint(paste(\"Correlazione disattenuata:\", r_corretta))\n\n# Calcolo approssimativo dell'intervallo di confidenza (per semplificazione)\n# NOTA: Questo è un esempio semplificato e non riflette il calcolo preciso degli intervalli di confidenza.\nCI_lower_observed &lt;- 0.4 # Limite inferiore osservato\nCI_upper_observed &lt;- 0.6 # Limite superiore osservato\n\nCI_lower_corrected &lt;- CI_lower_observed / sqrt(rho_X * rho_Y)\nCI_upper_corrected &lt;- CI_upper_observed / sqrt(rho_X * rho_Y)\n\n# Stampa dell'intervallo di confidenza corretto\nprint(paste(\"Intervallo di confidenza corretto: da\", CI_lower_corrected, \"a\", CI_upper_corrected))\n\n[1] \"Correlazione disattenuata: 0.668153104781061\"\n[1] \"Intervallo di confidenza corretto: da 0.534522483824849 a 0.801783725737273\"\n\n\n\n\n13.4.2 L’impiego delle Correlazioni Disattenuate\nL’uso delle correlazioni disattenuate risale al 1904 con Spearman, che le applicò in uno studio in cui \\(X\\) misurava la discriminazione dell’altezza del suono e \\(Y\\) l’intelligenza valutata da un insegnante. La correlazione tra queste due misure era \\(\\hat{\\rho}_{XY} = 0.38\\), con affidabilità di \\(\\hat{\\rho}_{XX'} = 0.25\\) e \\(\\hat{\\rho}_{YY'} = 0.55\\). Utilizzando le formule sopra citate, la correlazione predetta tra i punteggi veri di discriminazione del suono e l’intelligenza risultava essere \\(\\hat{\\rho}(X, T_Y) = 0.76\\), mentre tra i punteggi veri dei due costrutti era \\(\\hat{\\rho}(T_X, T_Y) = 1.025\\).\nQuesto esempio evidenzia come l’uso delle correlazioni disattenuate possa portare a stime eccessive, una problematica già rilevata nell’interazione tra Spearman e Karl Pearson. Spearman, attraverso l’applicazione della sua formula, sottolineò come le correlazioni empiriche basse proposte da Pearson potessero essere sottostimate a causa dell’errore di misurazione. Tuttavia, Pearson non accolse queste osservazioni, rimanendo scettico riguardo alla possibilità che la formula di Spearman generasse correlazioni superiori a 1 e rigettando l’idea di quantità non osservabili.\nNonostante queste controversie, Spearman proseguì nello studio delle variabili psicologiche, trovando in numerosi casi che le correlazioni disattenuate si avvicinavano all’unità, suggerendo un’associazione stretta tra variabili indicative dello stesso fenomeno. Queste osservazioni lo portarono a sviluppare ulteriormente l’analisi fattoriale.\nMcDonald (1999) avverte sull’utilizzo delle correlazioni disattenuate, evidenziando la necessità di cautela. Propone come alternativa più affidabile l’uso di modelli di equazioni strutturali per calcolare le correlazioni tra variabili latenti, ovvero quelle non influenzate da errori di misurazione, consentendo un’esplorazione diretta e più accurata delle ipotesi, inclusa la correlazione tra variabili latenti.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Applicazioni della CTT</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/06_ctt_applications.html#usare-laffidabilità-per-migliorare-linferenza-a-livello-individuale",
    "href": "chapters/ctt/06_ctt_applications.html#usare-laffidabilità-per-migliorare-linferenza-a-livello-individuale",
    "title": "13  Applicazioni della CTT",
    "section": "13.5 Usare l’Affidabilità per Migliorare l’Inferenza a Livello Individuale",
    "text": "13.5 Usare l’Affidabilità per Migliorare l’Inferenza a Livello Individuale\nUn altro uso importante dell’affidabilità è quello che ci consente di migliorare la nostra inferenza sui punteggi veri a livello individuale.\nKelley ha dimostrato – già nel 1920 (vedi Kelley, 1947) – che possiamo stimare i punteggi veri per ciascun individuo, regredendo i punteggi osservati sulla stima dell’affidabilità:\n\\[ \\hat{T} = \\bar{X} + r_{xx'}(X - \\bar{X}). \\]\nQui, \\(\\bar{X}\\) è la media dei punteggi osservati su tutti i soggetti, data da:\n\\[ \\bar{X} = \\frac{1}{N} \\sum_{i=1}^{N} X_i. \\]\nIntuitivamente, il punteggio vero per ciascun soggetto è stimato avvicinando il loro punteggio osservato verso la media dei punteggi a livello di gruppo in proporzione all’affidabilità della stima a livello individuale.\nIn aggiunta alla sua teoria che i punteggi osservati tendono ad essere regolati verso la media del gruppo quando si stima il vero punteggio, Kelley ha evidenziato come l’errore standard della stima del vero punteggio sia ridotto secondo la formula:\n\\[\n\\sigma_{\\hat{T}} = \\sigma_X \\sqrt{\\rho_{XX^\\prime} (1 -\\rho_{XX^\\prime})}.\n\\]\nQui, \\(\\sigma_{\\hat{T}}\\) rappresenta l’errore standard della stima del vero punteggio, \\(\\sigma_X\\) è la deviazione standard dei punteggi osservati, e \\(\\rho_{XX^\\prime}\\) indica il coefficiente di affidabilità tra i punteggi osservati e quelli veri. Questo errore standard per le stime dei punteggi veri è inferiore rispetto all’errore standard dei punteggi osservati, espresso come:\n\\[\nSE_{X} = \\sigma_{X} \\sqrt{1 - \\rho_{XX'}}.\n\\]\nIl confronto tra le due formule rivela che l’errore standard della stima del vero punteggio include un fattore aggiuntivo, \\(\\rho_{XX'}\\), che rappresenta il coefficiente di affidabilità. Questo evidenzia l’importanza del coefficiente di affidabilità nell’influenzare la precisione della stima del vero punteggio: un alto coefficiente di affidabilità contribuisce a ridurre l’errore standard della stima, migliorando così la precisione della stima del vero punteggio.\nLe equazioni di Kelley, scoperte nel 1920, anticipano di molti anni i principi alla base degli stimatori di James-Stein, che analogamente aggiustano le stime individuali avvicinandole alla media del gruppo. Questa affinità storica evidenzia un precedente significativo alla comprensione moderna di come le stime possano essere migliorate mediante l’incorporazione di informazioni aggiuntive.\nLa relazione tra le equazioni di Kelley e i concetti bayesiani offre una prospettiva ancora più profonda. Assumendo che i punteggi veri seguano una distribuzione a priori normale e che esista una distribuzione normale dei punteggi veri intorno ai punteggi osservati, l’approccio bayesiano empirico genera medie posteriori che corrispondono alle stime di Kelley dei punteggi veri. Questa equivalenza, come discussa da de Gruijter e van der Kamp nel 2008, stabilisce un ponte concettuale tra la psicometria classica e l’inferenza bayesiana, sottolineando come l’incorporazione di presupposti a priori possa affinare le nostre stime.\nQuesta connessione è ulteriormente rafforzata dall’uso di tecniche simili alla stima bayesiana empirica nei software di modellazione multilivello, come ad esempio il pacchetto lmer in R. Questi software si avvalgono della potenza dell’inferenza bayesiana per integrare informazioni di gruppo, migliorando così la precisione delle inferenze a livello individuale. La pratica di utilizzare informazioni a livello di gruppo per affinare le stime individuali non solo ha radici storiche profonde ma continua a essere una componente essenziale nell’evoluzione delle tecniche statistiche, dimostrando il suo valore nell’arricchire l’accuratezza e l’affidabilità delle inferenze statistiche.\nPer illustrare in modo pratico come avviene la stima dei punteggi veri, ossia il processo di pooling, eseguiremo una simulazione basata sul codice R di Nathaniel Haines. Questa simulazione genera dati seguendo una distribuzione binomiale per 20 soggetti, con una probabilità media di successo di 0.7. La simulazione considera tre diversi set di item: 10, 30 e 100, al fine di esaminare come le variazioni nel numero di item influenzino l’affidabilità ottenuta e, di conseguenza, gli effetti del pooling.\nIl codice inizia definendo il numero di soggetti e la varietà delle dimensioni degli item. Successivamente, genera un campione casuale di “punteggi veri” intorno a 0.7 per ogni soggetto. Viene poi definita una funzione per stimare l’errore standard della misurazione (al quadrato), basata sulla probabilità di successo per ogni item.\nPer ogni set di item, il codice simula i dati osservati per ogni soggetto utilizzando il suo “punteggio vero”. Calcola quindi la media del gruppo per i punteggi osservati, la affidabilità, e l’errore standard di misurazione, utilizzando l’approccio basato sulla varianza. Infine, stima i punteggi veri e gli errori standard associati sia per i punteggi osservati sia per quelli stimati.\nI risultati della simulazione vengono visualizzati in un grafico, che confronta i punteggi veri, osservati e stimati per ogni soggetto, evidenziando come la precisione della stima vari in funzione del numero di item. Il grafico include anche intervalli di confidenza al 95% per i punteggi osservati e stimati, e una linea orizzontale che rappresenta la media del gruppo per i punteggi osservati, offrendo una rappresentazione visiva dell’efficacia del processo di pooling nel recuperare i punteggi veri a partire da dati osservati affetti da errore di misurazione.\n\nset.seed(43202)\n\n# Number of subjects and items\nn_subj &lt;- 20\nn_items &lt;- c(10, 30, 100)\n\n# Random sample of \"true\" scores around .7\ntheta &lt;- rnorm(n_subj, .7, .1)\n\n# Estimate standard error of measurement (squared)\nest_se2 &lt;- function(x) {\n    # Success and failure probability\n    n &lt;- length(x)\n    p &lt;- mean(x)\n    q &lt;- 1 - p\n\n    sig2_ep_i &lt;- (p * q) / (n - 1)\n\n    return(sig2_ep_i)\n}\n\n# Estimate observed and true score\ndis_dat &lt;- foreach(i = seq_along(n_items), .combine = \"rbind\") %do% {\n    # Generate observed data for each subject using \"true\" score\n    X_all &lt;- foreach(t = seq_along(theta), .combine = \"rbind\") %do% {\n        rbinom(n_items[i], 1, prob = theta[t])\n    }\n\n    # group average observed score\n    X_bar &lt;- mean(rowMeans(X_all))\n\n    # Reliability\n    X &lt;- rowMeans(X_all)\n\n    # Standard arror of measurement approach\n    sig2_ep &lt;- mean(apply(X_all, 1, est_se2))\n    sig2_X &lt;- var(X)\n    rho &lt;- 1 - (sig2_ep / sig2_X)\n\n    foreach(t = seq_along(theta), .combine = \"rbind\") %do% {\n        # Using observed scores from parallel form 1\n        X_obs &lt;- X_all[t, ]\n        X_i &lt;- mean(X_obs)\n\n        data.frame(\n            subj_num = t,\n            n_items = n_items[i],\n            theta = theta[t],\n            rho = rho,\n            X = X_i,\n            se_obs = sd(X) * sqrt(1 - rho),\n            se_hat = sd(X) * sqrt(1 - rho) * sqrt(rho),\n            theta_hat = (1 - rho) * X_bar + rho * X_i\n        )\n    }\n}\n\n# Plot true, observed, and estimated true scores\ndis_dat %&gt;%\n    mutate(subj_num = reorder(subj_num, theta)) %&gt;%\n    ggplot(aes(x = subj_num, y = theta)) +\n    geom_point(color = I(\"black\")) +\n    geom_point(aes(x = subj_num, y = X),\n        color = I(\"#DCBCBC\"),\n        position = position_jitter(width = .2, height = 0, seed = 1)\n    ) +\n    geom_linerange(\n        aes(\n            x = subj_num,\n            ymin = X - 1.96 * se_obs,\n            ymax = X + 1.96 * se_obs\n        ),\n        color = I(\"#DCBCBC\"),\n        position = position_jitter(width = .2, height = 0, seed = 1)\n    ) +\n    geom_point(aes(x = subj_num, y = theta_hat),\n        color = I(\"#8F2727\"),\n        position = position_jitter(width = .2, height = 0, seed = 2)\n    ) +\n    geom_linerange(\n        aes(\n            x = subj_num,\n            ymin = theta_hat - 1.96 * se_hat,\n            ymax = theta_hat + 1.96 * se_hat\n        ),\n        color = I(\"#8F2727\"),\n        position = position_jitter(width = .2, height = 0, seed = 2)\n    ) +\n    geom_hline(yintercept = X_bar, linetype = 2, color = I(\"gray\")) +\n    annotate(\"text\",\n        x = 15, y = .4, label = expression(\"True\" ~ theta[i]),\n        color = \"black\", size = 5\n    ) +\n    annotate(\"text\",\n        x = 15, y = .3, label = expression(\"Obs\" ~ X[i]),\n        color = \"#DCBCBC\", size = 5\n    ) +\n    annotate(\"text\",\n        x = 15, y = .2, label = expression(\"Est\" ~ hat(theta)[i]),\n        color = \"#8F2727\", size = 5\n    ) +\n    facet_wrap(c(\"n_items\"), nrow = 1) +\n    ggtitle(\"Regression-Based True Score Estimates\") +\n    xlab(\"Subject\") +\n    ylab(\"Value\") +\n    theme_minimal(base_size = 15) +\n    theme(\n        panel.grid = element_blank(),\n        axis.text.x.bottom = element_blank()\n    )\n\n\n\n\n\n\n\n\nSi notino tre risultati di questa simulazione:\n\nle stime puntuali basate sulla regressione di Kelley (i punti neri nel grafico) risultano più vicine alla media a livello di gruppo (rappresentata dalla linea tratteggiata grigia orizzontale) di quanto lo siano le stime individuali “non corrette” (punti grigi);\nquesto effetto di “pooling” è tanto maggiore quanto minore è l’attendibilità (in questa simulazione l’attendibilità è stata manipolata variando il numero di item);\ngli intervalli di confidenza per i punteggi veri stimati sono più stretti rispetto a quelli dei punteggi osservati.\n\n\n13.5.1 Approccio Bayesiano\nNella seguente simulazione mostreremo come i risultati raggiunti con la regressione di Kelley possano essere replicati se i dati vengono analizzati con un modello gerarchico bayesiano.\nQuando si analizzano dati provenienti da questionari con risposte dicotomiche (ad esempio, vero/falso o corretto/errato), è possibile applicare la distribuzione di Bernoulli. In questo contesto, ogni risposta data a un item del questionario può essere vista come il risultato di un esperimento di Bernoulli. Se indichiamo con \\(X\\) una variabile casuale che segue tale distribuzione, la probabilità di ottenere un successo (ad esempio, una risposta corretta) è espressa come:\n\\[\n\\Pr(X=1) = p, \\quad \\text{e quindi} \\quad \\Pr(X=0) = 1 - p = q,\n\\]\ndove \\(p\\) indica la probabilità di successo e \\(q\\) quella di insuccesso.\nIntroduciamo il modello di Bernoulli tramite l’equazione logistica:\n\\[\np = \\frac{1}{1 + e^{-\\theta}}.\n\\]\nQuesta formula ci permette di modellare \\(p\\) in termini di \\(\\theta\\), un parametro che riflette una caratteristica o “abilità” dell’individuo. Il modello logistico assicura che \\(p\\), la probabilità di successo, sia sempre compresa nell’intervallo \\([0, 1]\\). Il parametro \\(\\theta\\) viene definito come:\n\\[\n\\theta = \\log\\left(\\frac{p}{1-p}\\right),\n\\]\ne può variare tra \\(-\\infty\\) e \\(+\\infty\\). Attraverso la trasformazione logistica, \\(\\theta\\) viene mappato in un valore di \\(p\\) che rispetta i limiti di una probabilità. Questa funzione di collegamento permette di interpretare il legame tra \\(\\theta\\) e \\(p\\).\nIl modello descritto sopra può essere considerato una forma estremamente semplificata della Teoria della Risposta all’Item (IRT), dove ogni persona è caratterizzata da un unico parametro di abilità (\\(\\theta\\)), e tutti gli item del test sono assunti avere uguale difficoltà e capacità di discriminazione, fissate convenzionalmente a 1.\nIl nostro obiettivo principale nell’analisi dei dati è quindi stimare il parametro \\(\\theta\\) per ogni individuo. La relazione tra \\(\\theta\\) e \\(p\\) è fondamentale: \\(\\theta\\) determina il valore di \\(p\\) attraverso la funzione logistica, che trasforma i valori di \\(\\theta\\) in probabilità \\(p\\) comprese tra 0 e 1. La stima di \\(\\theta\\) ci fornisce, di conseguenza, una misura della probabilità di successo di un individuo in risposta agli item del questionario.\nPer approfondire la nostra comprensione su come emergono le risposte osservate, è fondamentale definire la modalità con cui i parametri \\(\\theta\\) vengono generati per ogni individuo. Similmente a quanto avviene nella teoria classica dei test, dove si presume l’esistenza di una distribuzione di campionamento a livello di popolazione, nell’ambito della modellazione generativa bayesiana si postula una distribuzione generativa per il gruppo. In termini pratici, possiamo ipotizzare che i parametri \\(\\theta\\) individuali derivino da una distribuzione normale standardizzata:\n\\[\n\\theta \\sim \\mathcal{N}(0, 1)\n\\]\nNel contesto bayesiano, questa distribuzione di gruppo viene comunemente identificata come una distribuzione a priori per \\(\\theta\\). In alternativa, possiamo dedurre questi parametri direttamente dai dati:\n\\[\n\\theta \\sim \\mathcal{N}(\\mu, \\sigma)\n\\]\nDi conseguenza, si introduce un’ipotesi generativa riguardante i parametri di media \\(\\mu\\) e deviazione standard \\(\\sigma\\) del gruppo, che potrebbero essere descritti, in termini bayesiani tradizionali, come a priori del gruppo. Nel nostro esempio, supponiamo \\(\\mu = 0\\) e \\(\\sigma \\sim \\text{HalfNormal}(1)\\), dove \\(\\text{HalfNormal}(1)\\) rappresenta una distribuzione normale limitata ai valori positivi, coerente con il principio che le deviazioni standard debbano essere positive.\nQuesto approccio introduce un modello gerarchico: durante l’adattamento del modello, i parametri individuali influenzano quelli di gruppo, che a loro volta modellano nuovamente quelli individuali. Analogamente alle stime dei punteggi “veri” ottenuti tramite regressione nella teoria classica dei test, i nostri parametri individuali verranno regolati (“pooled”) verso la media di gruppo, portando a una riduzione degli intervalli di incertezza per le stime individuali.\nPer facilitare la comprensione di come queste assunzioni generative si traducano in pratica, eseguiamo la seguente simulazione.\n\nfile &lt;- file.path(\"hbern.stan\")\n\n\nmod &lt;- cmdstan_model(file)\n\n\nmod$print()\n\ndata {\n  int&lt;lower=0&gt; N;      // Number of subjects\n  int&lt;lower=0&gt; N_items; // Number of timepoints\n  array[N, N_items] int Y; // Binary responses for each subject and item\n}\n\nparameters {\n  real&lt;lower=0&gt; sigma_theta; // SD of individual effects\n  real mu_theta; // Mean of individual effects\n  \n  vector[N] theta_pr; // Non-centered individual-level parameters\n}\n\ntransformed parameters {\n  vector[N] theta = mu_theta + sigma_theta * theta_pr; // Individual-level effects\n}\n\nmodel {\n  // Priors\n  mu_theta ~ normal(0, 1);\n  sigma_theta ~ normal(0, 1);\n  theta_pr ~ normal(0, 1);\n  \n  // Likelihood\n  for (i in 1:N) {\n    for (j in 1:N_items) {\n      Y[i, j] ~ bernoulli_logit(theta[i]);\n    }\n  }\n}\n\ngenerated quantities {\n  array[N] real p; // Success probability estimate for each individual\n  \n  for (i in 1:N) {\n    p[i] = inv_logit(theta[i]);\n  }\n}\n\n\nIl codice Stan presentato adotta una parametrizzazione non centrata (non-centered parameterization) per la parte di modello a livello di gruppo, una scelta motivata per migliorare l’efficienza computazionale e facilitare la convergenza degli algoritmi di stima, come il campionamento Hamiltoniano Monte Carlo (HMC) usato da Stan. Questa scelta di design è matematicamente equivalente al modello generativo descritto dalle equazioni precedenti, pur offrendo vantaggi pratici significativi in fase di implementazione.\nLa parametrizzazione non centrata è una strategia avanzata nella modellazione bayesiana, specialmente utile nei modelli gerarchici o multilivello. Essa differisce dalla parametrizzazione centrata, nella quale i parametri di gruppo sono direttamente definiti dai parametri individuali. Invece, con la parametrizzazione non centrata, i parametri individuali sono inizialmente espressi come variazioni indipendenti rispetto alla media e deviazione standard di gruppo, per poi essere trasformati.\nImplementazione nel codice Stan:\n\nDefinizione dei Parametri:\n\nsigma_theta denota la deviazione standard degli effetti individuali, indicando la variabilità dei parametri \\(\\theta\\) a livello personale.\nmu_theta rappresenta la media degli effetti individuali.\ntheta_pr corrisponde ai parametri individuali nella forma non centrata, esprimendo le deviazioni rispetto alla media di gruppo in unità standardizzate.\n\nTrasformazione dei Parametri:\n\nGli effetti individuali effettivi (theta) sono ottenuti trasformando theta_pr per allinearli attorno a mu_theta e adattarli alla scala definita da sigma_theta. Questo processo è sintetizzato dall’equazione theta = mu_theta + sigma_theta * theta_pr, che trasla e scala theta_pr per ottenere valori centrati e proporzionati correttamente.\n\nApplicazione nel Modello:\n\nAll’interno del modello, sia mu_theta che sigma_theta sono sottoposti a priori normali (normal(0, 1)), presupponendo una distribuzione iniziale per questi parametri a livello di gruppo. Anche theta_pr è assoggettato a una distribuzione normale standard come priori, rispecchiando l’approccio di considerare le variazioni in termini standardizzati.\nLa verosimiglianza del modello è calcolata usando una distribuzione di Bernoulli con una funzione di collegamento logit, basata sui valori di theta trasformati, per analizzare le risposte binarie Y fornite da ogni soggetto per ogni item.\n\n\nAttraverso questa struttura, il modello mira a una stima più stabile e accurata dei parametri, beneficiando della maggiore efficienza computazionale e della riduzione dei problemi di convergenza che spesso accompagnano la modellazione bayesiana gerarchica.\nSimuliamo i dati di un singolo soggetto.\n\n# Initialize parameters for a single subject\nn_subj &lt;- 1\nn_items &lt;- 30 # Example with 30 items for simplicity\n\n# Generate \"true\" theta for the subject\ntheta &lt;- rnorm(n_subj, .7, .1)\n\n# Generate observed data for the subject using \"true\" theta\nY &lt;- rbinom(n_items, 1, prob = theta)\n\nAdattiamo il modello gerarchico bayesiano ai dati.\n\nfit_bernoulli &lt;- mod$sample(\n    data = list(\n        N = n_subj,\n        N_items = n_items,\n        Y = matrix(Y, nrow = 1) # Ensure Y is a matrix even for a single subject\n    ),\n    iter_sampling = 2500,\n    iter_warmup = 500,\n    chains = 4,\n    parallel_chains = 4,\n    seed = 43202\n)\n\nCalcoliamo la media a posteriori di \\(\\theta\\) e l’intervallo di confidenza al 95%:\n\n# Extract posterior samples for parameter 'p'\nbayes_est &lt;- fit_bernoulli$draws(variables = \"p\")\n\nbayes_est_p &lt;- as.vector(bayes_est)\n\n# Calculate the mean of the Bayesian estimates for 'p'\nbayes_theta_est &lt;- mean(bayes_est_p)\n\n# Calculate the 95% HDI using quantiles for the flattened vector\nhdi_bounds &lt;- quantile(bayes_est_p, probs = c(0.025, 0.975))\n\n# Prepare the results with a single HDI for 'p'\nresults &lt;- data.frame(\n    subj_num = 1,\n    n_items = n_items,\n    theta = theta,\n    bayes_theta = bayes_theta_est,\n    bayes_lo = hdi_bounds[1], # Lower bound of HDI\n    bayes_hi = hdi_bounds[2] # Upper bound of HDI\n)\n\n# Print the corrected results\nprint(results)\n\n     subj_num n_items theta bayes_theta bayes_lo bayes_hi\n2.5%        1      30 0.699       0.712    0.547    0.849\n\n\nAdesso svolgiamo la stessa simulazione considerando però 20 soggetti e facendo variare il numero di item del questionario (10, 30, 100).\n\nset.seed(43202)\n\nn_subj &lt;- 20\nn_items_vec &lt;- c(10, 30, 100)\n\n# Placeholder for results\nresults &lt;- list()\n\nfor (n_items in n_items_vec) {\n    for (subj in 1:n_subj) {\n        # Generate \"true\" theta for the subject\n        theta &lt;- rnorm(1, .7, .1)\n\n        # Generate observed data for the subject using \"true\" theta\n        Y &lt;- rbinom(n_items, 1, prob = theta)\n\n        # Fit the model\n        fit_bernoulli &lt;- mod$sample(\n            data = list(\n                N = 1,\n                N_items = n_items,\n                Y = matrix(Y, nrow = 1) # Ensure Y is a matrix\n            ),\n            iter_sampling = 2500,\n            iter_warmup = 500,\n            chains = 4,\n            parallel_chains = 4,\n            seed = 43202\n        )\n\n        # Extract and process posterior samples for 'p'\n        bayes_est_p &lt;- as.vector(fit_bernoulli$draws(variables = \"p\"))\n        bayes_theta_est &lt;- mean(bayes_est_p)\n        hdi_bounds &lt;- quantile(bayes_est_p, probs = c(0.025, 0.975))\n\n        # Collect results\n        results[[paste(subj, n_items)]] &lt;- data.frame(\n            subj_num = subj,\n            n_items = n_items,\n            theta = theta,\n            bayes_theta = bayes_theta_est,\n            bayes_lo = hdi_bounds[1],\n            bayes_hi = hdi_bounds[2]\n        )\n    }\n}\n\nCombiniamo tutti i risultati in un singolo data frame.\n\nall_results &lt;- bind_rows(results)\nall_results |&gt; head()\n\n\nA data.frame: 6 x 6\n\n\n\nsubj_num\nn_items\ntheta\nbayes_theta\nbayes_lo\nbayes_hi\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n2.5%...1\n1\n10\n0.637\n0.576\n0.311\n0.819\n\n\n2.5%...2\n2\n10\n0.717\n0.500\n0.241\n0.754\n\n\n2.5%...3\n3\n10\n0.741\n0.731\n0.472\n0.926\n\n\n2.5%...4\n4\n10\n0.637\n0.577\n0.315\n0.821\n\n\n2.5%...5\n5\n10\n0.690\n0.499\n0.237\n0.760\n\n\n2.5%...6\n6\n10\n0.693\n0.654\n0.385\n0.878\n\n\n\n\n\nCreiamo un grafico con i risultati ottenuti.\n\nggplot(all_results, aes(x = theta, y = bayes_theta)) +\n    geom_point() +\n    geom_errorbar(aes(ymin = bayes_lo, ymax = bayes_hi), width = 0.02) +\n    geom_hline(yintercept = 0.7, linetype = \"dashed\", color = \"gray\") + # Add dashed line at y = 0.7\n    # facet_wrap(~n_items, scales = \"free_x\", ncol = 1) + # Separate panels for each n_items, with a common y-axis\n    facet_wrap(c(\"n_items\"), nrow = 1) +\n    labs(x = \"True Theta\", y = \"Estimated p\") +\n    ggtitle(\"Estimated p vs. True Theta \\nfor Different Numbers of Items\") \n\n\n\n\n\n\n\n\nLo scopo di questa simulazione è quello di confrontare i risultati del modello gerarchico bayesiano con i risultati ottenuti mediante la tecnica di Kelly. Per gli stessi dati utilizzati nel modello gerarchico bayesiamo, calcoliamo dunque la stima dei punteggi veri e gli intervalli di confidenza al 95% secondo il metodo di Kelley.\nLa formula di Kelley per stimare i punteggi veri dai punteggi osservati coinvolge l’affidabilità del test e la media e la deviazione standard dei punteggi osservati:\n\\[\n\\text{Punteggio Vero} = \\text{Media} + (\\text{Affidabilità}) \\times (\\text{Punteggio Osservato} - \\text{Media}).\n\\]\nPer calcolare il CI al 95% per i punteggi veri, dobbiamo tener conto dell’errore standard di misurazione, che deriva dall’affidabilità del test:\n\\[\n\\text{SEM} = \\sigma \\times \\sqrt{1 - \\text{Affidabilità}},\n\\]\ndove $ $ è la deviazione standard dei punteggi osservati.\nDate la stima di SEM, l’intervallo di confidenza al 95% per il punteggio vero di un individuo può essere calcolato come segue:\n\\[\n\\text{CI} = \\text{Punteggio Vero} \\pm (1.96 \\times \\text{SEM}).\n\\]\nSvolgiamo ora i calcoli in R.\n\n# Assuming a reliability coefficient\nr_xx &lt;- 0.8\nZ_alpha &lt;- qnorm(0.975) # For a 95% CI\n\n# Calculate estimated true scores and CIs\nall_results$kelley_true_score &lt;- all_results$bayes_theta\nall_results$kelley_lo &lt;- all_results$bayes_theta - (Z_alpha * sqrt(1 - r_xx) * sd(all_results$bayes_theta))\nall_results$kelley_hi &lt;- all_results$bayes_theta + (Z_alpha * sqrt(1 - r_xx) * sd(all_results$bayes_theta))\n\nA questo punto possiamo generare un grarico che contiene sia la stima del punteggio vero basata sul metodo di Kelley, insieme all’intervallo di confidenza al 95% (colore grigio), sia le stime bayesiane trovate in precedenza (colore blue).\nPer semplicità, ho solo considerato il caso in cui la stima di Kelley si riferisce al caso di 100 items.\n\nggplot() +\n    geom_point(data = all_results, aes(x = theta - 0.02, y = bayes_theta, color = \"Bayesian Estimate\")) +\n    geom_errorbar(data = all_results, aes(x = theta - 0.02, ymin = bayes_lo, ymax = bayes_hi, color = \"Bayesian Estimate\"), width = 0.02) +\n    geom_point(data = all_results, aes(x = theta, y = kelley_true_score, color = \"Kelley's Estimate\")) +\n    geom_errorbar(data = all_results, aes(x = theta, ymin = kelley_lo, ymax = kelley_hi, color = \"Kelley's Estimate\"), width = 0.02) +\n    geom_hline(yintercept = 0.7, linetype = \"dashed\", color = \"gray\") +\n    facet_wrap(c(\"n_items\"), nrow = 1) +\n    labs(x = \"True Theta\", y = \"Estimated Score\") +\n    ggtitle(\"Estimated Scores vs. True Theta\\nfor Different Numbers of Items\") +\n    scale_color_manual(values = c(\"Bayesian Estimate\" = \"blue\", \"Kelley's Estimate\" = \"darkgray\"))\n\n\n\n\n\n\n\n\nI risultati della simulazione completa sono riportati nella figura seguente.\n\n\n\n\n\n\nFigura 13.1\n\n\n\nI risultati della simulazione indicano che le stime medie a posteriori del modello bayesiano, così come gli intervalli di credibilità al 95% (definiti come intervalli di densità di probabilità più elevata), mostrano una notevole congruenza con le stime corrispondenti dei punteggi veri ottenute mediante la regressione di Kelley, insieme ai relativi intervalli di confidenza al 95%. Le stime puntuali prodotte da entrambi i metodi risultano quasi sovrapponibili. Considerando che i punteggi veri derivanti dalla regressione di Kelley posseggono un’interpretazione bayesiana, la similitudine tra i risultati non dovrebbe sorprendere eccessivamente. Tuttavia, una conferma empirica di questa corrispondenza fornisce una validazione più robusta.\nQuesto esempio illustra come i modelli bayesiani gerarchici siano capaci di generare stime dei “punteggi veri” comparabili a quelle prodotte dalla teoria classica dei test, offrendo l’ulteriore vantaggio di non richiedere il calcolo dell’affidabilità per giungere a tali stime. Al contrario, l’approccio bayesiano si basa sull’adozione di assunzioni generative e distribuzionali riguardo le relazioni sia tra i parametri del modello a diversi livelli (ad esempio, la struttura gerarchica delinea le connessioni tra i parametri individuali e quelli di gruppo) sia con i dati osservati. In questo modo, adottando la media posteriore come stima dell’aspettativa dei parametri a livello individuale, siamo in grado di ottenere le stime più accurate dei parametri reali che sottendono la generazione dei dati osservati.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Applicazioni della CTT</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/06_ctt_applications.html#riflessioni-conclusive",
    "href": "chapters/ctt/06_ctt_applications.html#riflessioni-conclusive",
    "title": "13  Applicazioni della CTT",
    "section": "13.6 Riflessioni Conclusive",
    "text": "13.6 Riflessioni Conclusive\nIn questo capitolo, abbiamo analizzato diverse applicazioni pratiche della CTT. Ci siamo concentrati sulla comprensione dei concetti di attenuazione e sul metodo per determinare il numero di item necessari per ottenere un livello desiderato di affidabilità. Inoltre, abbiamo esaminato come stimare i punteggi veri individuali utilizzando due approcci differenti: la regressione di Kelley basata sulla CTT e la regressione gerarchica bayesiana. Approfondire questi argomenti ci ha permesso di ottenere una visione più completa e concreta sull’utilizzo e sull’applicazione della CTT, migliorando la nostra comprensione dei concetti chiave e delle implicazioni pratiche della teoria.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Applicazioni della CTT</span>"
    ]
  },
  {
    "objectID": "chapters/ctt/06_ctt_applications.html#session-info",
    "href": "chapters/ctt/06_ctt_applications.html#session-info",
    "title": "13  Applicazioni della CTT",
    "section": "13.7 Session Info",
    "text": "13.7 Session Info\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] parallel  stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] doParallel_1.0.17   iterators_1.0.14    cmdstanr_0.8.1.9000\n [4] truncnorm_1.0-9     ggridges_0.5.6      foreach_1.5.2      \n [7] modelsummary_2.2.0  MASS_7.3-61         viridis_0.6.5      \n[10] viridisLite_0.4.2   ggpubr_0.6.0        ggExtra_0.10.1     \n[13] gridExtra_2.3       patchwork_1.3.0     bayesplot_1.11.1   \n[16] semTools_0.5-6      semPlot_1.1.6       lavaan_0.6-19      \n[19] psych_2.4.6.26      scales_1.3.0        markdown_1.13      \n[22] knitr_1.49          lubridate_1.9.3     forcats_1.0.0      \n[25] stringr_1.5.1       dplyr_1.1.4         purrr_1.0.2        \n[28] readr_2.1.5         tidyr_1.3.1         tibble_3.2.1       \n[31] ggplot2_3.5.1       tidyverse_2.0.0     here_1.0.1         \n\nloaded via a namespace (and not attached):\n  [1] tensorA_0.36.2.1     rstudioapi_0.17.1    jsonlite_1.8.9      \n  [4] magrittr_2.0.3       TH.data_1.1-2        estimability_1.5.1  \n  [7] farver_2.1.2         nloptr_2.1.1         rmarkdown_2.29      \n [10] vctrs_0.6.5          Cairo_1.6-2          minqa_1.2.8         \n [13] base64enc_0.1-3      rstatix_0.7.2        htmltools_0.5.8.1   \n [16] distributional_0.5.0 broom_1.0.7          Formula_1.2-5       \n [19] htmlwidgets_1.6.4    plyr_1.8.9           sandwich_3.1-1      \n [22] emmeans_1.10.5       zoo_1.8-12           uuid_1.2-1          \n [25] igraph_2.1.1         mime_0.12            lifecycle_1.0.4     \n [28] pkgconfig_2.0.3      Matrix_1.7-1         R6_2.5.1            \n [31] fastmap_1.2.0        shiny_1.9.1          digest_0.6.37       \n [34] OpenMx_2.21.13       fdrtool_1.2.18       colorspace_2.1-1    \n [37] ps_1.8.1             rprojroot_2.0.4      Hmisc_5.2-0         \n [40] labeling_0.4.3       fansi_1.0.6          timechange_0.3.0    \n [43] abind_1.4-8          compiler_4.4.2       withr_3.0.2         \n [46] glasso_1.11          htmlTable_2.4.3      backports_1.5.0     \n [49] carData_3.0-5        ggsignif_0.6.4       corpcor_1.6.10      \n [52] gtools_3.9.5         tools_4.4.2          pbivnorm_0.6.0      \n [55] foreign_0.8-87       zip_2.3.1            httpuv_1.6.15       \n [58] nnet_7.3-19          glue_1.8.0           quadprog_1.5-8      \n [61] promises_1.3.0       nlme_3.1-166         lisrelToR_0.3       \n [64] grid_4.4.2           pbdZMQ_0.3-13        checkmate_2.3.2     \n [67] cluster_2.1.6        reshape2_1.4.4       generics_0.1.3      \n [70] gtable_0.3.6         tzdb_0.4.0           data.table_1.16.2   \n [73] hms_1.1.3            car_3.1-3            utf8_1.2.4          \n [76] tables_0.9.31        sem_3.1-16           pillar_1.9.0        \n [79] IRdisplay_1.1        rockchalk_1.8.157    posterior_1.6.0     \n [82] later_1.3.2          splines_4.4.2        lattice_0.22-6      \n [85] survival_3.7-0       kutils_1.73          tidyselect_1.2.1    \n [88] miniUI_0.1.1.1       pbapply_1.7-2        stats4_4.4.2        \n [91] xfun_0.49            qgraph_1.9.8         arm_1.14-4          \n [94] stringi_1.8.4        pacman_0.5.1         boot_1.3-31         \n [97] evaluate_1.0.1       codetools_0.2-20     mi_1.1              \n[100] cli_3.6.3            RcppParallel_5.1.9   IRkernel_1.3.2      \n[103] rpart_4.1.23         xtable_1.8-4         processx_3.8.4      \n[106] repr_1.1.7           munsell_0.5.1        Rcpp_1.0.13-1       \n[109] coda_0.19-4.1        png_0.1-8            XML_3.99-0.17       \n[112] jpeg_0.1-10          lme4_1.1-35.5        mvtnorm_1.3-2       \n[115] insight_0.20.5       openxlsx_4.2.7.1     crayon_1.5.3        \n[118] rlang_1.1.4          multcomp_1.4-26      mnormt_2.1.1        \n\n\n\n\n\n\nBlampied, N. M. (2022). Reliable change and the reliable change index: still useful after all these years? The Cognitive Behaviour Therapist, 15, e50.",
    "crumbs": [
      "CTT",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Applicazioni della CTT</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html",
    "href": "chapters/raters/01_multilevel.html",
    "title": "14  Modelli multilivello",
    "section": "",
    "text": "14.1 Introduzione\nIn psicologia i modelli multilivello assumono un’importanza cruciale. Gli psicologi utilizzano frequentemente dati raccolti in contesti complessi, dove gli effetti individuali e contestuali si intrecciano. Ad esempio, nella valutazione delle prestazioni cognitive o della risposta emotiva, i modelli multilivello consentono di distinguere tra variazioni dovute a caratteristiche individuali (come l’abilità cognitiva o la personalità) e quelle derivanti da fattori esterni (come l’ambiente scolastico o familiare).\nI modelli multilivello sono particolarmente utili per:",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#introduzione",
    "href": "chapters/raters/01_multilevel.html#introduzione",
    "title": "14  Modelli multilivello",
    "section": "",
    "text": "Analizzare dati longitudinali, dove le misurazioni ripetute sugli stessi soggetti introducono correlazioni naturali.\nStudiare l’impatto di fattori contestuali su variabili psicologiche, consentendo di esaminare come l’ambiente influenzi i comportamenti o gli stati mentali.\nGestire la variabilità intra-individuale e inter-individuale in modo più efficace, offrendo una rappresentazione più realistica della complessità dei fenomeni psicologici.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#un-esempio-concreto",
    "href": "chapters/raters/01_multilevel.html#un-esempio-concreto",
    "title": "14  Modelli multilivello",
    "section": "14.2 Un Esempio Concreto",
    "text": "14.2 Un Esempio Concreto\nIn questo capitolo, ci focalizzeremo sull’analisi di un’indagine sperimentale condotta per studiare l’impatto della deprivazione del sonno sulle prestazioni psicomotorie. I dati utilizzati provengono dallo studio di Belenky et al. (2003) sugli effetti della deprivazione del sonno.\n\nI dati sono accessibili nel dataset sleepstudy, incluso nel pacchetto lme4 di R (Bates et al., 2014).\n\n\ndata(sleepstudy)\n\nIl data frame comprende 180 righe (osservazioni) e tre variabili:\n\nReaction: Average reaction time (ms)\nDays: Number of days of sleep deprivation\nSubject: Subject number on which the observation was made.\n\nQuesti dati forniscono un esempio di dati multilivello, caratterizzati da misurazioni ripetute su una stessa variabile dipendente - in questo caso, il tempo medio di reazione (RT) - raccolte dai medesimi partecipanti per un periodo di dieci giorni. Tale struttura di dati è molto diffusa in psicologia, dove spesso si valutano le variazioni delle risposte o dei comportamenti di individui nel tempo.\nIl dataset esaminato focalizza su diciotto partecipanti, sottoposti a una condizione di sonno limitato a tre ore. Nell’arco di dieci giorni, questi partecipanti hanno partecipato quotidianamente a un “test di vigilanza psicomotoria” della durata di dieci minuti. Durante il test, era richiesto loro di monitorare uno schermo e di premere un pulsante quanto più rapidamente possibile alla comparsa di uno stimolo. La variabile dipendente principale dello studio è il tempo medio di risposta (RT) di ciascun partecipante.\nPer analizzare questi dati, è utile iniziare con una rappresentazione grafica. Se ci concentriamo sui dati di un singolo soggetto, questo ci permette di osservare le tendenze e le variazioni nel tempo di reazione di quel particolare individuo, fornendo insight su come la restrizione del sonno possa influire sulle sue prestazioni nel corso dei dieci giorni dello studio.\n\njust_308 &lt;- sleepstudy |&gt;\n    filter(Subject == \"308\")\n\n\nggplot(just_308, aes(x = Days, y = Reaction)) +\n    geom_point(size = 2.5) +\n    scale_x_continuous(breaks = 0:9) \n\n\n\n\n\n\n\n\nEsaminiamo ora i dati di tutti i 18 soggetti.\n\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n    geom_point() +\n    scale_x_continuous(breaks = 0:9) +\n    facet_wrap(~Subject)\n\n\n\n\n\n\n\n\n\n14.2.1 Descrizione del Disegno Sperimentale\n\nFase di Adattamento e Baseline: I primi tre giorni dello studio (T1, T2 e B) sono stati utilizzati per l’adattamento e l’addestramento (T1 e T2), seguiti da una misurazione baseline (B). Durante questo periodo, ai soggetti è stato chiesto di rimanere a letto per 8 ore (dalle 23:00 alle 07:00).\nCondizioni di Sonno: Dal quarto giorno in poi, per sette giorni (E1-E7), i soggetti hanno sperimentato diverse condizioni di sonno, variando la durata del tempo a letto (TIB) da 3 a 9 ore.\n\nI primi due giorni (codificati come 0 e 1) sono stati dedicati all’adattamento e all’addestramento, mentre il terzo giorno (codificato come 2) ha visto la misurazione baseline. L’analisi dovrebbe idealmente partire dal giorno di baseline per riflettere l’effetto della restrizione del sonno sulle prestazioni. Per evitare che l’adattamento influenzi i risultati, i giorni 0 e 1 devono dunque essere esclusi dall’analisi, poiché qualsiasi variazione di prestazione in questi giorni è attribuibile all’addestramento piuttosto che alla restrizione del sonno.\n\n\n14.2.2 Preparazione dei Dati\n\nRimozione delle Osservazioni Iniziali: Dal dataset, eliminiamo le osservazioni dove la variabile Days è codificata come 0 o 1.\nCreazione di una Nuova Variabile: Creiamo una nuova variabile days_deprived basata su Days, iniziando la sequenza dal giorno 2. In questa nuova variabile, il giorno 2 viene ricodificato come 0, il giorno 3 come 1, e così via. Questa variabile rappresenta il numero di giorni di privazione del sonno. Salviamo il dataset modificato con il nome sleep2, che ora riflette accuratamente il periodo di restrizione del sonno per l’analisi.\n\n\nsleep2 &lt;- sleepstudy |&gt;\n    filter(Days &gt;= 2L) |&gt;\n    mutate(days_deprived = Days - 2L)\n\n\nsleep2 |&gt;\n    count(days_deprived, Days)\n\n\nA data.frame: 8 x 3\n\n\ndays_deprived\nDays\nn\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n\n\n\n\n0\n2\n18\n\n\n1\n3\n18\n\n\n2\n4\n18\n\n\n3\n5\n18\n\n\n4\n6\n18\n\n\n5\n7\n18\n\n\n6\n8\n18\n\n\n7\n9\n18\n\n\n\n\n\n\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n    geom_point() +\n    scale_x_continuous(breaks = 0:7) +\n    facet_wrap(~Subject) +\n    labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#relazione-tra-tempo-di-reazione-e-privazione-del-sonno",
    "href": "chapters/raters/01_multilevel.html#relazione-tra-tempo-di-reazione-e-privazione-del-sonno",
    "title": "14  Modelli multilivello",
    "section": "14.3 Relazione tra Tempo di Reazione e Privazione del Sonno",
    "text": "14.3 Relazione tra Tempo di Reazione e Privazione del Sonno\nNel contesto dello studio sulla privazione del sonno, l’analisi dei dati suggerisce che, a parte una singola eccezione (il soggetto 335), il tempo di reazione medio tende ad aumentare progressivamente con ogni giorno aggiuntivo di privazione del sonno. Questo pattern indica che potrebbe essere utile descrivere le prestazioni di ciascun partecipante attraverso un modello di regressione lineare.\nLa regressione lineare è rappresentata dall’equazione generale:\n\\[\nE(Y) = \\beta_0 + \\beta_1 X,\n\\]\ndove \\(Y\\) è la variabile dipendente (in questo caso, il tempo di reazione), \\(\\beta_0\\) rappresenta l’intercetta (il tempo di reazione medio al giorno zero, prima dell’inizio della privazione del sonno) e \\(\\beta_1\\) è la pendenza (la variazione del tempo di reazione per ogni giorno aggiuntivo di privazione del sonno). Questi parametri (\\(\\beta_0\\) e \\(\\beta_1\\)) sono stimati dai dati.\nQuando si modellano i dati di ciascun partecipante, emergono diverse domande: dobbiamo adattare lo stesso modello di regressione lineare a tutti i partecipanti, o sarebbe più appropriato utilizzare un modello diverso per ogni soggetto? Oppure esiste un approccio intermedio che bilancia questi estremi?\nPer rispondere a queste domande, esploriamo tre approcci differenti, come illustrato da McElreath (2020):\n\nComplete Pooling: Questo approccio implica l’utilizzo di un unico modello di regressione lineare per tutti i partecipanti. Significa che assumiamo la stessa relazione lineare (stessa intercetta e pendenza) per tutti, ignorando le differenze individuali.\nNo Pooling: In questo approccio, ogni partecipante ha un proprio modello di regressione lineare individuale, con intercetta e pendenza uniche. Qui si riconosce che ogni individuo può rispondere diversamente alla privazione del sonno, e quindi il modello è personalizzato per ciascun soggetto.\nPartial Pooling: Questo approccio intermedio cerca di bilanciare gli estremi dei due metodi precedenti. Include alcuni elementi comuni tra i soggetti (ad esempio, una pendenza media) ma permette anche una certa variazione individuale.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#complete-pooling",
    "href": "chapters/raters/01_multilevel.html#complete-pooling",
    "title": "14  Modelli multilivello",
    "section": "14.4 Complete pooling",
    "text": "14.4 Complete pooling\nL’approccio di “complete pooling” in analisi statistica implica l’utilizzo di un modello che calcola un’unica intercetta e una sola pendenza per l’intero dataset. Questo metodo si basa sull’ipotesi che tutti i soggetti nel dataset condividano le stesse caratteristiche di base riguardo alla relazione tra la variabile dipendente e indipendente.\n\n14.4.1 Caratteristiche del Complete Pooling\n\nUnicità delle Stime: Il modello stima un singolo set di parametri (intercetta e pendenza) per tutti i dati, considerando l’intero campione come un’unità omogenea.\nIgnorare le Variazioni Individuali: Questo approccio non tiene conto delle possibili differenze individuali nelle intercette o nelle pendenze tra i diversi soggetti. Ad esempio, ignorara come ciascun soggetto reagisce in modo diverso alla privazione del sonno.\n\n\n\n14.4.2 Limitazioni dell’Approccio di Complete Pooling\nDall’analisi preliminare dei dati, abbiamo notato che l’approccio di complete pooling potrebbe non essere adatto per il nostro studio. La visualizzazione dei dati suggerisce che ogni partecipante potrebbe avere una propria relazione unica tra il tempo di reazione e i giorni di privazione del sonno, indicando la necessità di valori individuali per le intercette e le pendenze.\n\n\n14.4.3 Modello di Regressione Lineare in Complete Pooling\nIl modello generale lineare (GLM) per l’approccio di complete pooling è formulato come segue:\n\\[\nY_{sd} = \\beta_0 + \\beta_1 X_{sd} + e_{sd},\n\\]\ndove:\n\n\\(Y_{sd}\\) rappresenta il tempo di reazione medio del soggetto \\(s\\) nel giorno \\(d\\).\n\\(X_{sd}\\) è il numero di giorni di privazione del sonno (variabile days_deprived), che varia da 0 a 7.\n\\(e_{sd}\\) è il termine di errore, che rappresenta le fluttuazioni casuali non spiegate dal modello.\n\n\n\n14.4.4 Implementazione in R\nPer adattare questo modello in R, si utilizza la funzione lm():\n\ncp_model &lt;- lm(Reaction ~ days_deprived, sleep2)\nsummary(cp_model)\n\n\nCall:\nlm(formula = Reaction ~ days_deprived, data = sleep2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-112.28  -26.73    2.14   27.73  140.45 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     267.97       7.74   34.63  &lt; 2e-16 ***\ndays_deprived    11.44       1.85    6.18  6.3e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 50.9 on 142 degrees of freedom\nMultiple R-squared:  0.212, Adjusted R-squared:  0.207 \nF-statistic: 38.2 on 1 and 142 DF,  p-value: 6.32e-09\n\n\n\n\n14.4.5 Interpretazione del Modello di Regressione e Visualizzazione Grafica\nIl modello di regressione che abbiamo considerato offre una stima del tempo di risposta medio per i soggetti allo studio al Giorno 0 (prima della privazione del sonno) e la variazione media del tempo di risposta per ogni giorno aggiuntivo di privazione. Secondo questo modello, il tempo di risposta medio iniziale è stimato essere di circa 268 millisecondi, con un incremento medio di circa 11 millisecondi per ogni giorno successivo di privazione del sonno.\n\n\n14.4.6 Considerazioni sui Limiti del Modello\nÈ importante notare, tuttavia, che questo modello potrebbe avere delle limitazioni nella sua applicabilità:\n\nAssunzione di Indipendenza: Il modello assume che tutte le osservazioni siano indipendenti. Questa assunzione potrebbe non essere valida nel nostro studio, dato che le osservazioni provengono da misurazioni ripetute sugli stessi soggetti.\nErrori Standard dei Coefficienti: La presunta indipendenza delle osservazioni implica che gli errori standard dei coefficienti di regressione potrebbero non essere completamente affidabili.\n\n\n\n14.4.7 Aggiunta delle Previsioni al Grafico\nPer visualizzare meglio questi risultati, possiamo aggiungere le previsioni del modello al grafico che abbiamo già creato. Utilizziamo la funzione geom_abline() di R per tracciare la linea di regressione stimata direttamente sul grafico esistente:\n\nUtilizzo di geom_abline(): Questa funzione ci permette di aggiungere una linea di regressione al grafico, specificando l’intercetta e la pendenza.\nCoefficienti del Modello: Utilizziamo coef(cp_model) per ottenere i coefficienti di regressione (intercetta e pendenza) dal nostro modello. Questa funzione restituisce un vettore con due elementi corrispondenti all’intercetta e alla pendenza, che possono essere poi utilizzati per definire la linea nel grafico.\n\nIn conclusione, l’aggiunta delle previsioni del modello al grafico fornisce una rappresentazione visiva dell’effetto stimato della privazione del sonno sul tempo di risposta. Tuttavia, è essenziale tenere presente le limitazioni e le assunzioni del modello, in particolare l’indipendenza delle osservazioni, che potrebbe non essere una rappresentazione accurata dei dati nel contesto di misurazioni ripetute.\n\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n    geom_abline(\n        intercept = coef(cp_model)[1],\n        slope = coef(cp_model)[2],\n        color = \"blue\"\n    ) +\n    geom_point() +\n    scale_x_continuous(breaks = 0:7) +\n    facet_wrap(~Subject) +\n    labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")\n\n\n\n\n\n\n\n\nDall’analisi effettuata, emerge che il modello attuale non si adatta in modo ottimale ai dati raccolti. Questa situazione indica la necessità di esplorare un approccio diverso per modellare in modo più accurato le relazioni presenti nei dati.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#approccio-di-no-pooling",
    "href": "chapters/raters/01_multilevel.html#approccio-di-no-pooling",
    "title": "14  Modelli multilivello",
    "section": "14.5 Approccio di No Pooling",
    "text": "14.5 Approccio di No Pooling\nIn alternativa al modello di “complete pooling”, consideriamo l’approccio di “no pooling”. Questo approccio si basa sull’idea di adattare modelli di regressione separati per ogni partecipante, trattando ogni individuo come un’entità distinta.\n\n14.5.1 Caratteristiche del No Pooling\n\nIndipendenza delle Stime: In questo approccio, ogni partecipante ha il proprio set di stime per l’intercetta e la pendenza. Le stime relative a un partecipante non sono influenzate dalle stime degli altri.\nStime Individualizzate: Si stima separatamente una coppia di intercetta/pendenza per ciascuno dei 18 partecipanti, riconoscendo la possibilità di variazioni significative nelle risposte individuali.\n\n\n\n14.5.2 Implementazione del Modello di No Pooling\nEsistono due modi principali per implementare questo approccio:\n\nRegressioni Separate per Ogni Partecipante: Eseguire una serie di regressioni lineari individuali, una per ogni soggetto.\nModello di Regressione Unificato con Effetti Principali e Interazione: Utilizzare un unico modello di regressione che includa sia gli effetti principali sia l’interazione tra le variabili Subject (soggetto) e Day (giorno). Questo metodo permette di includere tutte le stime in un unico modello.\n\nPer il secondo approccio, è necessario considerare le seguenti fasi:\n\nCreazione di Variabili Dummy per il Fattore Subject: Poiché Subject ha 18 livelli, saranno necessarie 17 variabili dummy per rappresentare questi livelli. In R, questo può essere fatto automaticamente definendo Subject come un fattore.\nIncludere Subject come Fattore nel Modello: Aggiungere Subject, definito come un fattore, come predittore nel modello. L’inclusione dell’interazione tra Subject e days_deprived permette variazioni nelle intercette e nelle pendenze tra i soggetti.\n\n\n\n14.5.3 Verifica del Fattore Subject\nPrima di procedere, è importante assicurarsi che Subject sia definito correttamente come un fattore. Questo può essere verificato utilizzando la funzione summary() in R, che fornisce una sintesi delle caratteristiche della variabile, compreso se è trattata come un fattore.\n\nsleep2 |&gt; \n    summary()\n\n    Reaction        Days         Subject   days_deprived \n Min.   :203   Min.   :2.00   308    : 8   Min.   :0.00  \n 1st Qu.:265   1st Qu.:3.75   309    : 8   1st Qu.:1.75  \n Median :303   Median :5.50   310    : 8   Median :3.50  \n Mean   :308   Mean   :5.50   330    : 8   Mean   :3.50  \n 3rd Qu.:348   3rd Qu.:7.25   331    : 8   3rd Qu.:5.25  \n Max.   :466   Max.   :9.00   332    : 8   Max.   :7.00  \n                              (Other):96                 \n\n\nLa funzione pull() viene utilizzata per estrarre una specifica colonna da un data frame. Con le seguenti istruzioni verifichiamo se la colonna Subject è codificata come factor.\n\nsleep2 |&gt;\n    pull(Subject) |&gt;\n    is.factor()\n\nTRUE\n\n\nAdattiamo il modello di regressione ai dati. Si noti che la sintassi seguente può essere semplificata utilizzando Reaction ~ days_deprived * Subject.\n\nnp_model &lt;- lm(Reaction ~ days_deprived + Subject + days_deprived:Subject,\n    data = sleep2\n)\n\nsummary(np_model)\n\n\nCall:\nlm(formula = Reaction ~ days_deprived + Subject + days_deprived:Subject, \n    data = sleep2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-106.52   -8.54    1.14    8.89  128.55 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               288.217     16.477   17.49  &lt; 2e-16 ***\ndays_deprived              21.690      3.939    5.51  2.5e-07 ***\nSubject309                -87.926     23.302   -3.77  0.00026 ***\nSubject310                -62.286     23.302   -2.67  0.00869 ** \nSubject330                -14.953     23.302   -0.64  0.52242    \nSubject331                  9.966     23.302    0.43  0.66974    \nSubject332                 27.816     23.302    1.19  0.23522    \nSubject333                 -2.758     23.302   -0.12  0.90600    \nSubject334                -50.205     23.302   -2.15  0.03342 *  \nSubject335                -25.343     23.302   -1.09  0.27921    \nSubject337                 24.614     23.302    1.06  0.29319    \nSubject349                -59.218     23.302   -2.54  0.01246 *  \nSubject350                -40.202     23.302   -1.73  0.08734 .  \nSubject351                -24.247     23.302   -1.04  0.30042    \nSubject352                 43.065     23.302    1.85  0.06732 .  \nSubject369                -21.504     23.302   -0.92  0.35815    \nSubject370                -53.307     23.302   -2.29  0.02411 *  \nSubject371                -30.490     23.302   -1.31  0.19350    \nSubject372                  2.477     23.302    0.11  0.91554    \ndays_deprived:Subject309  -17.333      5.570   -3.11  0.00238 ** \ndays_deprived:Subject310  -17.792      5.570   -3.19  0.00184 ** \ndays_deprived:Subject330  -13.685      5.570   -2.46  0.01561 *  \ndays_deprived:Subject331  -16.823      5.570   -3.02  0.00315 ** \ndays_deprived:Subject332  -19.295      5.570   -3.46  0.00076 ***\ndays_deprived:Subject333  -10.815      5.570   -1.94  0.05480 .  \ndays_deprived:Subject334   -3.575      5.570   -0.64  0.52242    \ndays_deprived:Subject335  -25.899      5.570   -4.65  9.5e-06 ***\ndays_deprived:Subject337    0.752      5.570    0.13  0.89289    \ndays_deprived:Subject349   -5.264      5.570   -0.95  0.34673    \ndays_deprived:Subject350    1.601      5.570    0.29  0.77438    \ndays_deprived:Subject351  -13.168      5.570   -2.36  0.01987 *  \ndays_deprived:Subject352  -14.402      5.570   -2.59  0.01106 *  \ndays_deprived:Subject369   -7.895      5.570   -1.42  0.15927    \ndays_deprived:Subject370   -1.049      5.570   -0.19  0.85091    \ndays_deprived:Subject371   -9.344      5.570   -1.68  0.09633 .  \ndays_deprived:Subject372  -10.604      5.570   -1.90  0.05961 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.5 on 108 degrees of freedom\nMultiple R-squared:  0.849, Adjusted R-squared:   0.8 \nF-statistic: 17.4 on 35 and 108 DF,  p-value: &lt;2e-16\n\n\nPer chiarire, il soggetto di riferimento è il 308; in R, la modalità predefinita è quella di ordinare i livelli del fattore in ordine alfabetico e di scegliere il primo come soggetto di riferimento. Questo significa che l’intercetta e la pendenza per il soggetto 308 sono rappresentate rispettivamente da (Intercept) e days_deprived, poiché tutte le altre 17 variabili dummy saranno nulle per il soggetto 308.\nTutti i coefficienti di regressione degli altri soggetti sono rappresentati come scostamenti da questo soggetto di riferimento. Se desideriamo calcolare l’intercetta e la pendenza per un dato soggetto, dobbiamo semplicemente sommare gli scostamenti corrispondenti. Pertanto, abbiamo:\nIntercetta per 308: 288.217\nPendenza per 308: 21.69\nIntercetta per 335: (Intercept) + Subject335 = 288.217 + -25.343 = 262.874\nPendenza per 335: days_deprived + days_deprived:Subject335 = 21.69 + -25.899 = -4.209\nE così via.\nNel modello “no pooling”, non viene stimata un’intercetta e una pendenza complessive per l’intera popolazione; in questo caso, (Intercept) e days_deprived sono stime dell’intercetta e della pendenza per il soggetto 308, che è stato scelto (arbitrariamente) come soggetto di riferimento. Per ottenere stime per l’intera popolazione, è possibile procedere con una seconda fase dell’analisi statistica in cui calcoliamo le medie delle intercette e delle pendenze individuali.\n\ncoef(np_model) |&gt; as.data.frame()\n\n\nA data.frame: 36 x 1\n\n\n\ncoef(np_model)\n\n\n\n&lt;dbl&gt;\n\n\n\n\n(Intercept)\n288.217\n\n\ndays_deprived\n21.690\n\n\nSubject309\n-87.926\n\n\nSubject310\n-62.286\n\n\nSubject330\n-14.953\n\n\nSubject331\n9.966\n\n\nSubject332\n27.816\n\n\nSubject333\n-2.758\n\n\nSubject334\n-50.205\n\n\nSubject335\n-25.343\n\n\nSubject337\n24.614\n\n\nSubject349\n-59.218\n\n\nSubject350\n-40.202\n\n\nSubject351\n-24.247\n\n\nSubject352\n43.065\n\n\nSubject369\n-21.504\n\n\nSubject370\n-53.307\n\n\nSubject371\n-30.490\n\n\nSubject372\n2.477\n\n\ndays_deprived:Subject309\n-17.333\n\n\ndays_deprived:Subject310\n-17.792\n\n\ndays_deprived:Subject330\n-13.685\n\n\ndays_deprived:Subject331\n-16.823\n\n\ndays_deprived:Subject332\n-19.295\n\n\ndays_deprived:Subject333\n-10.815\n\n\ndays_deprived:Subject334\n-3.575\n\n\ndays_deprived:Subject335\n-25.899\n\n\ndays_deprived:Subject337\n0.752\n\n\ndays_deprived:Subject349\n-5.264\n\n\ndays_deprived:Subject350\n1.601\n\n\ndays_deprived:Subject351\n-13.168\n\n\ndays_deprived:Subject352\n-14.402\n\n\ndays_deprived:Subject369\n-7.895\n\n\ndays_deprived:Subject370\n-1.049\n\n\ndays_deprived:Subject371\n-9.344\n\n\ndays_deprived:Subject372\n-10.604\n\n\n\n\n\nCalcoliamo le intercette individuali:\n\nall_intercepts &lt;- c(\n    coef(np_model)[\"(Intercept)\"],\n    coef(np_model)[3:19] + coef(np_model)[\"(Intercept)\"]\n)\n\nCalcliamo le pendenze individuali:\n\nall_slopes &lt;- c(\n    coef(np_model)[\"days_deprived\"],\n    coef(np_model)[20:36] + coef(np_model)[\"days_deprived\"]\n)\n\nCreiamo un DataFrame con le colonne Subject, intercept e slope:\n\nids &lt;- sleep2 |&gt;\n    pull(Subject) |&gt;\n    levels() |&gt;\n    factor()\nprint(ids)\n\n [1] 308 309 310 330 331 332 333 334 335 337 349 350 351 352 369 370 371 372\n18 Levels: 308 309 310 330 331 332 333 334 335 337 349 350 351 352 ... 372\n\n\n\n# make a tibble with the data extracted above\nnp_coef &lt;- tibble(\n    Subject = ids,\n    intercept = all_intercepts,\n    slope = all_slopes\n)\n\nprint(np_coef)\n\n# A tibble: 18 x 3\n   Subject intercept slope\n   &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 308          288. 21.7 \n 2 309          200.  4.36\n 3 310          226.  3.90\n 4 330          273.  8.01\n 5 331          298.  4.87\n 6 332          316.  2.40\n 7 333          285. 10.9 \n 8 334          238. 18.1 \n 9 335          263. -4.21\n10 337          313. 22.4 \n11 349          229. 16.4 \n12 350          248. 23.3 \n13 351          264.  8.52\n14 352          331.  7.29\n15 369          267. 13.8 \n16 370          235. 20.6 \n17 371          258. 12.3 \n18 372          291. 11.1 \n\n\nEsaminiamo l’adattamento di questo modello ai dati.\n\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n    geom_abline(\n        data = np_coef,\n        mapping = aes(\n            intercept = intercept,\n            slope = slope\n        ),\n        color = \"blue\"\n    ) +\n    geom_point() +\n    scale_x_continuous(breaks = 0:7) +\n    facet_wrap(~Subject) +\n    labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")\n\n\n\n\n\n\n\n\nQuesta situazione è notevolmente migliorata rispetto al modello di pooling completo. Se desideriamo testare l’ipotesi nulla secondo cui la pendenza della retta di regressione è uguale a zero, possiamo farlo eseguendo un test \\(t\\) di Student sul campione di pendenze individuali.\n\nnp_coef |&gt;\n    pull(slope) |&gt;\n    t.test()\n\n\n    One Sample t-test\n\ndata:  pull(np_coef, slope)\nt = 6, df = 17, p-value = 1e-05\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n  7.54 15.33\nsample estimates:\nmean of x \n     11.4 \n\n\nQuesto test suggerisce che la pendenza media di 11.435 è diversa da zero, t(17) = 6.20 p &lt; .001.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#approccio-di-partial-pooling",
    "href": "chapters/raters/01_multilevel.html#approccio-di-partial-pooling",
    "title": "14  Modelli multilivello",
    "section": "14.6 Approccio di Partial Pooling",
    "text": "14.6 Approccio di Partial Pooling\nNell’ambito dell’analisi dei dati psicologici, i ricercatori si trovano spesso a dover gestire un delicato equilibrio. Da un lato, vi è l’approccio di complete pooling, che tratta tutti i dati come se appartenessero a un unico gruppo omogeneo, e dall’altro, l’approccio di no-pooling, che considera i dati di ciascun soggetto in modo isolato, senza sfruttare le informazioni aggregate. Entrambi gli estremi hanno limitazioni significative: il complete pooling può mascherare le variazioni individuali, mentre il no-pooling può non sfruttare appieno le informazioni disponibili dall’intero set di dati.\nPer superare queste limitazioni, emerge l’approccio di partial pooling. Questo metodo utilizza i modelli lineari a effetti misti, che rappresentano una via di mezzo tra i due estremi menzionati. Il partial pooling permette di trarre vantaggio dalle informazioni provenienti dall’insieme dei partecipanti, migliorando così le stime dei soggetti individuali. Attraverso questo approccio, si ottengono stime che non solo tengono conto delle peculiarità di ciascun individuo ma sono anche informate dalle tendenze generali osservate nel gruppo più ampio.\nL’approccio di partial pooling permette di separare più efficacemente le tendenze generali dagli errori casuali in ciascun partecipante e si dimostra più adatto per generalizzare i risultati a una popolazione più ampia, al di là dei soggetti specifici coinvolti nello studio.\n\n14.6.1 Implementazione dei Modelli a Effetti Misti\n\nTrattare i Soggetti come Fattori Casuali: Nel partial pooling, i soggetti vengono considerati come un fattore casuale anziché fisso. Ciò implica che i livelli del fattore (i soggetti nel nostro caso) sono visti come un campione casuale da una popolazione più ampia.\nModello Lineare a Effetti Misti: Questo tipo di modello statistico consente di includere i fattori casuali nell’analisi. In un modello misto, le stime per ogni soggetto sono “informate” o influenzate dalle informazioni aggregate degli altri soggetti.\nShrinkage o Restringimento: Il fenomeno dello shrinkage indica che le stime per ciascun soggetto vengono regolate o “spostate” verso le stime medie della popolazione, permettendo una valutazione più equilibrata e meno influenzata da variazioni estreme o casuali.\n\n\n\n14.6.2 Articolazione e Applicazione del Modello Multilivello\nIl modello multilivello che analizziamo è strutturato per cogliere le relazioni dinamiche tra variabili a livelli diversi. Esaminiamo i dettagli di ogni livello e il loro significato nel contesto del modello.\n\n\n14.6.3 Livello 1: Modellazione della Relazione Individuale\nIl primo livello del modello esprime la relazione lineare individuale tra la variabile di risposta (tempo di reazione) e i predittori (giorni di privazione del sonno):\n\\[\nY_{sd} = \\beta_{0s} + \\beta_{1s} X_{sd} + e_{sd},\n\\]\ndove \\(Y_{sd}\\) è il tempo di reazione del soggetto \\(s\\) al giorno \\(d\\), \\(\\beta_{0s}\\) e \\(\\beta_{1s}\\) sono i parametri individuali di intercetta e pendenza, e \\(e_{sd}\\) rappresenta l’errore per ogni soggetto e giorno. I parametri \\(\\beta_{0s}\\) e \\(\\beta_{1s}\\) sono considerati derivati, poiché dipendono dalle variabili a Livello 2.\n\n\n14.6.4 Livello 2: Modellazione delle Variazioni Tra Soggetti\nAl secondo livello, definiamo come l’intercetta e la pendenza cambiano tra i soggetti:\n\\[\n\\beta_{0s} = \\gamma_{0} + S_{0s},\n\\]\n\\[\n\\beta_{1s} = \\gamma_{1} + S_{1s}.\n\\]\nQui, \\(\\gamma_0\\) e \\(\\gamma_1\\) sono gli effetti fissi che rappresentano l’intercetta e la pendenza medie della popolazione, mentre \\(S_{0s}\\) e \\(S_{1s}\\) sono gli effetti casuali che permettono variazioni individuali.\n\n\n14.6.5 Componenti di Varianza\nLe componenti di varianza nel modello sono espresse come:\n\\[\n\\langle S_{0s}, S_{1s} \\rangle \\sim N(\\langle 0, 0 \\rangle, \\mathbf{\\Sigma}),\n\\]\n\\[\n\\mathbf{\\Sigma} = \\begin{pmatrix}{\\tau_{00}}^2 & \\rho\\tau_{00}\\tau_{11} \\\\ \\rho\\tau_{00}\\tau_{11} & {\\tau_{11}}^2 \\end{pmatrix},\n\\]\n\\[\ne_{sd} \\sim N(0, \\sigma^2).\n\\]\nLa matrice \\(\\mathbf{\\Sigma}\\) determina la distribuzione degli effetti casuali, con \\({\\tau_{00}}^2\\) e \\({\\tau_{11}}^2\\) che indicano le varianze delle intercette e delle pendenze casuali, mentre \\(\\rho\\) è la correlazione tra questi effetti.\n\n\n14.6.6 Interpretazione Complessiva del Modello\nUnendo le equazioni di Livello 1 e Livello 2, otteniamo una visione completa del modello:\n\\[\nY_{sd} = \\gamma_0 + S_{0s} + (\\gamma_1 + S_{1s}) X_{sd} + e_{sd},\n\\]\ndove\n\n\\(Y_{sd}\\): È il valore osservato della variabile di risposta per il soggetto \\(s\\) al giorno \\(d\\).\n\\(\\gamma_0\\) e \\(\\gamma_1\\): Sono gli effetti fissi. \\(\\gamma_0\\) rappresenta l’intercetta generale, ovvero la media della popolazione per la variabile di risposta quando la variabile predittiva \\(X\\) è zero. \\(\\gamma_1\\) rappresenta la pendenza generale, che indica come la variabile di risposta cambia in media con un’unità di incremento della variabile predittiva \\(X\\).\n\\(S_{0s}\\) e \\(S_{1s}\\): Sono gli effetti casuali. \\(S_{0s}\\) è l’effetto casuale dell’intercetta per il soggetto \\(s\\), mentre \\(S_{1s}\\) è l’effetto casuale della pendenza. Questi termini rappresentano come ciascun soggetto si discosta dalla media della popolazione.\n\\(X_{sd}\\): È la variabile predittiva per il soggetto \\(s\\) al giorno \\(d\\).\n\\(e_{sd}\\): È l’errore casuale associato a ciascuna osservazione.\n\n\n\n14.6.7 Interpretazione degli Effetti Fissi e Casuali\n\nGli effetti fissi (\\(\\gamma_0\\) e \\(\\gamma_1\\)) sono costanti per tutti i soggetti e riflettono le caratteristiche della popolazione generale.\nGli effetti casuali (\\(S_{0s}\\) e \\(S_{1s}\\)) variano da soggetto a soggetto e sono modellati come distribuzioni normali centrate attorno a zero. Questo significa che la distribuzione degli effetti casuali è centrata attorno agli effetti fissi della popolazione.\n\n\n14.6.7.1 Uso di Campioni per Informare sulla Popolazione\nTrattare i soggetti come variabili casuali anziché fisse ci consente di usare i dati del campione per inferire sulla popolazione più ampia. Invece di stimare valori specifici per ogni soggetto, il modello stima la distribuzione da cui questi valori sono estratti, fornendo una visione più generalizzata e applicabile a livello di popolazione.\n\n\n\n14.6.8 Spiegazione della Matrice di Varianza-Covarianza nel Modello Multilivello\nLa matrice di varianza-covarianza nel modello multilivello gioca un ruolo cruciale nel definire come gli effetti casuali variano e sono correlati tra loro all’interno della popolazione studiata. Questa matrice è rappresentata come segue:\n\\[\n\\Sigma =\n\\begin{pmatrix}\n\\tau_{00}^2 & \\rho \\tau_{00} \\tau_{11} \\\\\n\\rho \\tau_{00} \\tau_{11} & \\tau_{11}^2\n\\end{pmatrix},\n\\]\ndove \\(\\Sigma\\) denota la matrice di varianza-covarianza per gli effetti casuali \\(\\langle S_{0s}, S_{1s} \\rangle\\), che sono modellati come normalmente distribuiti con una media di zero e una varianza-covarianza data da \\(\\Sigma\\).\n\n14.6.8.1 Componenti della Matrice\n\nVarianze degli Effetti Casuali:\n\n\\(\\tau_{00}^2\\): Varianza dell’effetto casuale sull’intercetta (\\(S_{0s}\\)). Questo parametro indica quanto i soggetti differiscono nella loro reazione iniziale (tempo di reazione al Giorno 0) prima di qualsiasi privazione del sonno.\n\\(\\tau_{11}^2\\): Varianza dell’effetto casuale sulla pendenza (\\(S_{1s}\\)). Misura la variazione tra i soggetti nella loro reattività agli effetti della privazione del sonno.\n\nCovarianze:\n\n\\(\\rho \\tau_{00} \\tau_{11}\\): Covarianza tra l’intercetta e la pendenza casuali. Questo termine riflette come la reazione iniziale di un soggetto (intercetta) sia correlata con la variazione della sua reattività al sonno (pendenza). Un valore positivo di \\(\\rho\\) indica che soggetti con un tempo di reazione inizialmente più lento tendono a mostrare un aumento più marcato del tempo di reazione con la privazione del sonno, e viceversa.\n\n\n\n\n14.6.8.2 Significato e Importanza di \\(\\Sigma\\)\n\nDistribuzione degli Effetti Casuali: La matrice \\(\\Sigma\\) determina le probabilità di estrarre una coppia specifica di effetti casuali (\\(S_{0s}, S_{1s}\\)) per un soggetto dal pool più ampio della popolazione.\nAnalisi delle Variazioni Individuali: Attraverso \\(\\Sigma\\), possiamo comprendere meglio quanto e in che modo i soggetti variano sia nella loro reazione iniziale sia nella loro risposta alla privazione del sonno.\nInterpretazione dei Risultati: La comprensione di \\(\\Sigma\\) aiuta a interpretare i risultati del modello in termini di varianza e covarianza tra i soggetti, permettendo di trarre conclusioni più precise sulla popolazione studiata.\n\nLa matrice di varianza-covarianza \\(\\Sigma\\) è fondamentale nel modello multilivello perché fornisce un quadro dettagliato della varianza e della covarianza degli effetti casuali, consentendo di cogliere le sottili variazioni e correlazioni tra i soggetti. Questa comprensione arricchisce l’analisi e rende possibili conclusioni più accurate sulla popolazione.\n\n\n\n14.6.9 Tabella delle Variabili\n\n\n\n\n\n\n\n\nVariabile\nTipo\nDescrizione\n\n\n\n\n\\(Y_{sd}\\)\nOsservata\nValore di Reaction per il soggetto \\(s\\) al giorno \\(d\\)\n\n\n\\(X_{sd}\\)\nOsservata\nValore di days_deprived (0-7) per il soggetto \\(s\\) al giorno \\(d\\)\n\n\n\\(\\beta_{0s}\\)\nDerivata\nParametro di intercetta di livello 1 per il soggetto \\(s\\)\n\n\n\\(\\beta_{1s}\\)\nDerivata\nParametro di pendenza di livello 1 per il soggetto \\(s\\)\n\n\n\\(e_{sd}\\)\nDerivata\nErrore per il soggetto \\(s\\) al giorno \\(d\\)\n\n\n\\(\\gamma_0\\)\nFissa\nIntersezione generale (media di \\(\\beta_{0s}\\) nella popolazione)\n\n\n\\(\\gamma_1\\)\nFissa\nPendenza generale (media di \\(\\beta_{1s}\\) nella popolazione)\n\n\n\\(S_{0s}\\)\nCasuale\nEffetto random di intercetta per il soggetto \\(s\\)\n\n\n\\(S_{1s}\\)\nCasuale\nEffetto random di pendenza per il soggetto \\(s\\)\n\n\n\\(\\mathbf{\\Sigma}\\)\nCasuale\nMatrice di varianza-covarianza\n\n\n\\({\\tau_{00}}^2\\)\nCasuale\nVarianza degli effetti random di intercetta\n\n\n\\(\\rho\\)\nCasuale\nCorrelazione tra intercetta e pendenza\n\n\n\\({\\tau_{11}}^2\\)\nCasuale\nVarianza degli effetti random di pendenza\n\n\n\\(\\sigma^2\\)\nCasuale\nVarianza dell’errore\n\n\n\n\n14.6.9.1 Spiegazione delle Categorie di Variabili nel Modello Multilivello\nNella tabella delle variabili del modello multilivello, utilizziamo tre categorie specifiche per classificare le diverse variabili: fisso, casuale e derivato. Queste categorie ci aiutano a comprendere il ruolo e la natura di ciascuna variabile all’interno del modello:\n\n14.6.9.1.1 Variabili Fisse\n\nDefinizione: Le variabili fisse sono quelle che assumiamo costanti attraverso il campione e la popolazione. Sono parametri che rappresentano le caratteristiche generali della popolazione da cui il campione è tratto.\nEsempi: Nella tabella, \\(\\gamma_0\\) (intercetta generale) e \\(\\gamma_1\\) (pendenza generale) sono variabili fisse.\nRuolo nel Modello: Riflettono le tendenze centrali o gli effetti medi nella popolazione oggetto di studio.\n\n\n\n14.6.9.1.2 Variabili Casuali\n\nDefinizione: Le variabili casuali indicano i parametri che possono variare tra i soggetti o altre unità di analisi. Questi parametri sono concepiti come estratti da una distribuzione più ampia.\nEsempi: \\(S_{0s}\\) (intercetta casuale per soggetto) e \\(S_{1s}\\) (pendenza casuale per soggetto) sono esempi di variabili casuali.\nRuolo nel Modello: Consentono di modellare e comprendere la varianza e la covarianza all’interno del campione, riflettendo la variabilità individuale o di gruppo.\n\n\n\n14.6.9.1.3 Variabili Derivate\n\nDefinizione: ‘Derivato’ non è un termine standard nella modellistica statistica, ma lo utilizziamo qui per distinguere le variabili che non sono direttamente stimabili, ma piuttosto calcolate o derivate da altre variabili nel modello.\nEsempi: \\(\\beta_{0s}\\) (intercetta di livello 1 per soggetto) e \\(\\beta_{1s}\\) (pendenza di livello 1 per soggetto) sono variabili derivate, calcolate combinando variabili fisse e casuali.\nRuolo nel Modello: Le variabili derivate rappresentano i parametri specifici per ciascun soggetto o unità di analisi, derivati dalla combinazione delle influenze fisse e casuali.\n\nIn conclusione, il modello misto permette di tenere conto della variazione sia a livello di popolazione sia a livello individuale. Questa capacità di distinguere tra variazioni generali e specifiche dei soggetti è cruciale in molte situazioni, come quando si analizzano effetti di trattamenti o condizioni sperimentali diverse su gruppi di soggetti.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#stimare-i-parametri-del-modello",
    "href": "chapters/raters/01_multilevel.html#stimare-i-parametri-del-modello",
    "title": "14  Modelli multilivello",
    "section": "14.7 Stimare i parametri del modello",
    "text": "14.7 Stimare i parametri del modello\nPer stimare i parametri del modello usando R, utilizzeremo la funzione lmer() del pacchetto lme4 (Bates et al., 2014). Questa funzione permette di specificare sia i fattori fissi (come i giorni di deprivazione del sonno) sia i fattori casuali (come i soggetti), ottenendo un modello che bilancia efficacemente le informazioni individuali con quelle aggregate.\nLa sintassi base di lmer() è la seguente:\n\\[\n\\text{lmer(formula, data, ...)},\n\\]\ndove formula esprime la struttura del modello sottostante in un formato compatto e data è il data frame in cui si trovano le variabili menzionate nella formula.\nIl formato generale della formula del modello per N effetti fissi (fix) e K effetti casuali (ran) è:\n\\[\n\\text{DV ~ fix1 + fix2 + ... + fixN + (ran1 + ran2 + ... + ranK | random\\_factor1)}.\n\\]\nLe interazioni tra i fattori A e B possono essere specificate utilizzando sia A * B (interazione ed effetti principali) che A:B (solo l’interazione).\nUna differenza chiave dalla sintassi standard dei modelli R è la presenza di un termine di effetto casuale, racchiuso tra parentesi, ad esempio (ran1 + ran2 + ... + ranK | random_factor). Ogni espressione tra parentesi rappresenta gli effetti casuali associati a un singolo fattore casuale. È possibile avere più di un termine di effetti casuali in una singola formula (fattori casuali incrociati). I termini relativi agli effetti casuali forniscono istruzioni a lmer() su come costruire le matrici di varianza-covarianza.\nSul lato sinistro della barra | vengono elencati gli effetti che vogliamo fare variare tra i livelli del fattore casuale indicato sul lato destro. Di solito, la variabile sul lato destro è una variabile che identifica i soggetti (ad esempio, subject_id).\nConsideriamo le seguenti possibili formule di modello per i dati sleep2 e le matrici di varianza-covarianza che esse costruiscono.\n\n\n\n\n\n\n\nmodel\nsyntax\n\n\n\n\n1. random intercepts only\nReaction ~ days_deprived + (1 | Subject)\n\n\n2. random intercepts and slopes\nReaction ~ days_deprived + (1 + days_deprived | Subject)\n\n\n3. model 2 alternate syntax\nReaction ~ days_deprived + (days_deprived | Subject)\n\n\n4. random slopes only\nReaction ~ days_deprived + (0 + days_deprived | Subject)\n\n\n5. model 2 + zero-covariances\nReaction ~ days_deprived + (days_deprived || Subject)\n\n\n\nModello 1:\n\\[\n\\mathbf{\\Sigma} =\n\\begin{pmatrix}\n\\tau_{00}^2 & 0 \\\\\n0 & 0\n\\end{pmatrix}\n\\]\nModelli 2 e 3:\n\\[  \n\\mathbf{\\Sigma} =\n\\begin{pmatrix}\n\\tau_{00}^2 & \\rho \\tau_{00} \\tau_{11} \\\\\n\\rho \\tau_{00} \\tau_{11} & \\tau_{11}^2\n\\end{pmatrix}\n\\]\nModello 4:\n\\[  \n\\mathbf{\\Sigma} =\n\\begin{pmatrix}\n0 & 0 \\\\\n0 & \\tau_{11}^2\n\\end{pmatrix}\n\\]\nModello 5:\n\\[  \n\\mathbf{\\Sigma} =\n\\begin{pmatrix}\n\\tau_{00}^2 & 0 \\\\\n0 & \\tau_{11}^2\n\\end{pmatrix}\n\\]\nIl modello più ragionevole per i dati dell’esempio è il Modello 2, quindi useremo quello.\n\npp_mod &lt;- lmer(\n    Reaction ~ 1 + days_deprived + (1 + days_deprived | Subject), \n    data = sleep2\n)\nsummary(pp_mod)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ 1 + days_deprived + (1 + days_deprived | Subject)\n   Data: sleep2\n\nREML criterion at convergence: 1404\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-4.016 -0.354  0.007  0.468  5.073 \n\nRandom effects:\n Groups   Name          Variance Std.Dev. Corr\n Subject  (Intercept)   958.4    30.96        \n          days_deprived  45.8     6.77    0.18\n Residual               651.6    25.53        \nNumber of obs: 144, groups:  Subject, 18\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)     267.97       8.27    32.4\ndays_deprived    11.44       1.85     6.2\n\nCorrelation of Fixed Effects:\n            (Intr)\ndays_deprvd -0.062\n\n\nPrima di discutere come interpretare l’output, iniziamo col rappresentare graficamente i dati rispetto alle previsioni del nostro modello. Possiamo ottenere le previsioni del modello utilizzando la funzione predict() (vedi ?predict.merMod per informazioni sull’uso con modelli a effetti misti).\nPer prima cosa, creaiamo un nuovo dataframe con i valori dei predittori per Subject e days_deprived.\n\nhead(sleep2)\n\n\nA data.frame: 6 x 4\n\n\n\nReaction\nDays\nSubject\ndays_deprived\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;fct&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n251\n2\n308\n0\n\n\n2\n321\n3\n308\n1\n\n\n3\n357\n4\n308\n2\n\n\n4\n415\n5\n308\n3\n\n\n5\n382\n6\n308\n4\n\n\n6\n290\n7\n308\n5\n\n\n\n\n\n\nnewdata &lt;- crossing(\n    Subject = sleep2 |&gt; \n    pull(Subject) |&gt; \n    levels() |&gt; \n    factor(),\n    days_deprived = 0:7\n)\n\nhead(newdata, 17)\n\n\nA tibble: 17 x 2\n\n\nSubject\ndays_deprived\n\n\n&lt;fct&gt;\n&lt;int&gt;\n\n\n\n\n308\n0\n\n\n308\n1\n\n\n308\n2\n\n\n308\n3\n\n\n308\n4\n\n\n308\n5\n\n\n308\n6\n\n\n308\n7\n\n\n309\n0\n\n\n309\n1\n\n\n309\n2\n\n\n309\n3\n\n\n309\n4\n\n\n309\n5\n\n\n309\n6\n\n\n309\n7\n\n\n310\n0\n\n\n\n\n\nIl codice precedente crea un nuovo data frame chiamato newdata utilizzando la funzione crossing() da dplyr.\nSubject = sleep2 |&gt; pull(Subject) |&gt; levels() |&gt; factor(): Questa parte del codice estrae la colonna “Subject” dal data frame “sleep2”, quindi applica una serie di operazioni successive utilizzando l’operatore |&gt; (pipe) per manipolare i dati nella colonna. - sleep2 |&gt; pull(Subject): Inizia estraendo la colonna “Subject” dal data frame “sleep2”. - levels(): Successivamente, applica la funzione “levels()” per ottenere i livelli unici della colonna “Subject”. Questo è utile quando si ha a che fare con variabili categoriche (come un “factor”). - factor(): Infine, trasforma i livelli ottenuti in un “factor”. Questo è importante perché la funzione “crossing()” richiede che le variabili categoriche siano di tipo “factor”.\n\ndays_deprived = 0:7: Questa parte del codice crea una nuova variabile chiamata “days_deprived” che contiene una sequenza da 0 a 7. Questo rappresenta i giorni di privazione.\ncrossing(...): Infine, la funzione “crossing()” viene utilizzata per creare un nuovo data frame chiamato “newdata” combinando tutte le possibili combinazioni di valori tra la variabile “Subject” (con i suoi livelli unici) e la variabile “days_deprived” (con i valori da 0 a 7).\n\nUtilizziamo predict() per trovare le rette di regressione per ciascun soggetto.\n\nnewdata2 &lt;- newdata |&gt;\n    mutate(Reaction = predict(pp_mod, newdata))\n\n\nhead(newdata2)\n\n\nA tibble: 6 x 3\n\n\nSubject\ndays_deprived\nReaction\n\n\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\n308\n0\n292\n\n\n308\n1\n313\n\n\n308\n2\n333\n\n\n308\n3\n353\n\n\n308\n4\n373\n\n\n308\n5\n393\n\n\n\n\n\nOra possiamo creare il grafico con le predizioni del modello.\n\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n    geom_line(\n        data = newdata2,\n        color = \"blue\"\n    ) +\n    geom_point() +\n    scale_x_continuous(breaks = 0:7) +\n    facet_wrap(~Subject) +\n    labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#interpretare-loutput-di-lmer-ed-estrarre-le-stime",
    "href": "chapters/raters/01_multilevel.html#interpretare-loutput-di-lmer-ed-estrarre-le-stime",
    "title": "14  Modelli multilivello",
    "section": "14.8 Interpretare l’output di lmer() ed estrarre le stime",
    "text": "14.8 Interpretare l’output di lmer() ed estrarre le stime\nLa chiamata a lmer() restituisce un oggetto della classe “lmerMod”.\n\n14.8.1 Effetti fissi\nLa sezione dell’output chiamata “Effetti fissi:” è simile a ciò che si vede nell’output per un modello lineare semplice adattato con lm().\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)    267.967      8.266  32.418\ndays_deprived   11.435      1.845   6.197\nL’output precedente indica che il tempo di reazione medio stimato per i partecipanti al Giorno 0 era di circa 268 millisecondi, con ogni giorno di privazione del sonno che aggiungeva mediamente ulteriori 11 millisecondi al tempo di risposta.\nSe dobbiamo ottenere gli effetti fissi dal modello, possiamo estrarli utilizzando la funzione fixef().\n\nfixef(pp_mod) |&gt; \n    print()\n\n  (Intercept) days_deprived \n        268.0          11.4 \n\n\nGli errori standard ci forniscono stime della variabilità di questi parametri dovuta all’errore di campionamento. Possiamo utilizzarli per calcolare i valori \\(t\\) o derivare gli intervalli di confidenza. Per estrarli, utilizziamo vcov(pp_mod), che restituirà una matrice di varianza-covarianza (non quella associata agli effetti casuali), quindi estraiamo la diagonale utilizzando diag() e calcoliamo infine la radice quadrata utilizzando sqrt().\n\nvcov(pp_mod)\n\n2 x 2 Matrix of class \"dpoMatrix\"\n              (Intercept) days_deprived\n(Intercept)         68.33         -0.95\ndays_deprived       -0.95          3.41\n\n\n\nvcov(pp_mod) |&gt; \n    diag() |&gt; \n    sqrt() |&gt; \n    print()\n\n  (Intercept) days_deprived \n         8.27          1.85 \n\n\nSi noti che, nell’output di lmer, i valori \\(t\\) non sono accompagnati dai valori \\(p\\), come avviene di solito nei contesti di modellazione più semplici. Esistono molteplici approcci per ottenere i valori \\(p\\) da modelli a effetti misti, ciascuno con vantaggi e svantaggi (si veda, ad esempio, Luke (2017) per un’analisi delle opzioni disponibili). I valori \\(t\\) non vengono accompagnati dai gradi di libertà, poiché i gradi di libertà in un modello a effetti misti non sono ben definiti. Spesso i ricercatori trattano i valori \\(t\\) come valori \\(z\\) di Wald, ossia come osservazioni provenienti da una distribuzione normale standard. Poiché la distribuzione \\(t\\) si avvicina alla distribuzione normale standard all’aumentare del numero di osservazioni, questa pratica “t-as-z” è legittima se il numero di osservazioni campionarie è sufficientemente grande.\nPer calcolare i valori \\(z\\) di Wald, basta dividere la stima dell’effetto fisso per il suo errore standard:\n\ntvals &lt;- fixef(pp_mod) / sqrt(diag(vcov(pp_mod)))\n\ntvals |&gt; \n    print()\n\n  (Intercept) days_deprived \n         32.4           6.2 \n\n\nI valori-\\(p\\) si ottengono nel modo seguente:\n\nprint(2 * (1 - pnorm(abs(tvals))))\n\n  (Intercept) days_deprived \n     0.00e+00      5.75e-10 \n\n\nQuesto fornisce una forte evidenza contro l’ipotesi nulla \\(H_0: \\gamma_1 = 0\\). Sembra che la privazione del sonno aumenti effettivamente il tempo di risposta.\nÈ possibile ottenere gli intervalli di confidenza per le stime utilizzando la funzione confint() (questa tecnica utilizza il bootstrap parametrico).\n\nconfint(pp_mod) |&gt; \n    print()\n\n                2.5 %  97.5 %\n.sig01         19.098  46.337\n.sig02         -0.405   0.806\n.sig03          4.008  10.249\n.sigma         22.467  29.349\n(Intercept)   251.344 284.590\ndays_deprived   7.725  15.146\n\n\n\n\n14.8.2 Effetti random\nRandom effects:\n Groups   Name          Variance Std.Dev. Corr\n Subject  (Intercept)   958.35   30.957       \n          days_deprived  45.78    6.766   0.18\n Residual               651.60   25.526       \nNumber of obs: 144, groups:  Subject, 18\nLa parte relativa agli effetti casuali dell’output di summary() ci fornisce una tabella con informazioni sulle diverse componenti della varianza: la matrice di varianza-covarianza (o matrici, se ci sono più fattori casuali) e la varianza residua.\nCominciamo con la riga Residual. Questo ci indica che la varianza residua, \\(\\sigma^2\\), è stata stimata a circa 651.6. Il valore nella colonna successiva, 25.526, è la deviazione standard, \\(\\sigma\\), che è la radice quadrata della varianza.\nEstraiamo la deviazione standard dei residui utilizzando la funzione sigma().\n\nsigma(pp_mod) # residual\n\n25.526404577851\n\n\nLe due righe sopra la riga Residual ci forniscono informazioni sulla matrice di varianza-covarianza per il fattore casuale “Subject”.\nRandom effects:\n Groups   Name          Variance Std.Dev. Corr\n Subject  (Intercept)   958.35   30.957       \n          days_deprived  45.78    6.766   0.18\nI valori nella colonna “Variance” ci forniscono la diagonale principale della matrice, mentre i valori nella colonna “Std.Dev.” rappresentano semplicemente le radici quadrate di questi valori. La colonna “Corr” indica la correlazione tra l’intercetta e la pendenza.\nPossiamo estrarre questi valori dall’oggetto adattato pp_mod utilizzando la funzione VarCorr(). Questa funzione restituisce una lista nominata, con un elemento per ciascun fattore casuale. Nel nostro caso, “Subject” è l’unico fattore casuale, quindi la lista avrà lunghezza 1.\n\n# variance-covariance matrix for random factor Subject\nVarCorr(pp_mod)[[\"Subject\"]] |&gt; \n    print() # oppure: VarCorr(pp_mod)[[1]]\n\n              (Intercept) days_deprived\n(Intercept)         958.4          37.2\ndays_deprived        37.2          45.8\nattr(,\"stddev\")\n  (Intercept) days_deprived \n        30.96          6.77 \nattr(,\"correlation\")\n              (Intercept) days_deprived\n(Intercept)         1.000         0.178\ndays_deprived       0.178         1.000\n\n\nLe prime righe rappresentano la matrice di varianza-covarianza. Le varianze sono riportate sulla diagonale principale. correlation indica la correlazione tra la stima della pendenza e la stima dell’intercetta.\nPossiamo estrarre gli effetti casuali stimati (BLUPS) utilizzando la funzione ranef().\n\nranef(pp_mod)[[\"Subject\"]] |&gt; \n    print()\n\n    (Intercept) days_deprived\n308      24.499         8.602\n309     -59.372        -8.128\n310     -39.476        -7.429\n330       1.350        -2.385\n331      18.458        -3.748\n332      30.527        -4.894\n333      13.368         0.289\n334     -18.158         3.844\n335     -16.974       -12.070\n337      44.585        10.176\n349     -26.684         2.195\n350      -5.966         8.176\n351      -5.571        -2.372\n352      46.635        -0.562\n369       0.962         1.739\n370     -18.522         5.632\n371      -7.343         0.273\n372      17.683         0.662\n\n\nPossiamo ottenere i valori stimati dal modello utilizzando fitted() e i residui utilizzando residuals().\n\nmutate(sleep2,\n    fit = fitted(pp_mod),\n    resid = residuals(pp_mod)\n) |&gt;\n    group_by(Subject) %&gt;%\n    slice(c(1, 10)) %&gt;%\n    print(n = +Inf)\n\n# A tibble: 18 x 6\n# Groups:   Subject [18]\n   Reaction  Days Subject days_deprived   fit  resid\n      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;           &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     251.     2 308                 0  292. -41.7 \n 2     203.     2 309                 0  209.  -5.62\n 3     234.     2 310                 0  228.   5.83\n 4     284.     2 330                 0  269.  14.5 \n 5     302.     2 331                 0  286.  15.4 \n 6     273.     2 332                 0  298. -25.5 \n 7     277.     2 333                 0  281.  -4.57\n 8     243.     2 334                 0  250.  -6.44\n 9     254.     2 335                 0  251.   3.50\n10     292.     2 337                 0  313. -20.9 \n11     239.     2 349                 0  241.  -2.36\n12     256.     2 350                 0  262.  -5.80\n13     270.     2 351                 0  262.   7.50\n14     327.     2 352                 0  315.  12.3 \n15     257.     2 369                 0  269. -11.7 \n16     239.     2 370                 0  249. -10.5 \n17     278.     2 371                 0  261.  17.3 \n18     298.     2 372                 0  286.  11.9 \n\n\nInfine, possiamo ottenere previsioni per nuovi dati utilizzando predict(), come abbiamo fatto in precedenza.\n\n## create the table with new predictor values\nndat &lt;- crossing(\n    Subject = sleep2 %&gt;% pull(Subject) %&gt;% levels() %&gt;% factor(),\n    days_deprived = 8:10\n) %&gt;%\n    mutate(Reaction = predict(pp_mod, newdata = .))\n\nndat |&gt; \n    head()\n\n\nA tibble: 6 x 3\n\n\nSubject\ndays_deprived\nReaction\n\n\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\n308\n8\n453\n\n\n308\n9\n473\n\n\n308\n10\n493\n\n\n309\n8\n235\n\n\n309\n9\n238\n\n\n309\n10\n242\n\n\n\n\n\n\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n    geom_line(\n        data = bind_rows(newdata2, ndat),\n        color = \"blue\"\n    ) +\n    geom_point() +\n    scale_x_continuous(breaks = 0:10) +\n    facet_wrap(~Subject) +\n    labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#riflessioni-conclusive",
    "href": "chapters/raters/01_multilevel.html#riflessioni-conclusive",
    "title": "14  Modelli multilivello",
    "section": "14.9 Riflessioni Conclusive",
    "text": "14.9 Riflessioni Conclusive\nQuesto capitolo ha presentato una panoramica sui modelli statistici multilivello, con un focus sull’effetto della deprivazione del sonno sulle prestazioni psicomotorie utilizzando il dataset sleepstudy. In tale contesto, sono stati introdotti e discussi concetti fondamentali come il “complete pooling”, il “no pooling” e il “partial pooling”, esplorandone le implicazioni nella modellazione dei dati con misure ripetute.\nL’analisi ha posto l’accento sull’applicazione pratica dei modelli multilivello, con una particolare attenzione alla distinzione tra variabili fisse e casuali e alla loro rilevanza per la struttura del modello. Un aspetto centrale della trattazione è stata la matrice di varianza-covarianza, essenziale per comprendere le relazioni interne ai modelli multilivello.\nQuesti modelli rivestono un ruolo cruciale nel campo dell’assessment psicologico e della psicometria, poiché permettono di analizzare dati complessi tenendo conto delle variazioni individuali e di gruppo. Offrono strumenti flessibili per esaminare come fattori contestuali e individuali influenzino il comportamento e le prestazioni psicologiche, un elemento chiave per una valutazione psicologica accurata.\nInfine, il capitolo prepara il terreno per il successivo approfondimento sui modelli multilivello nell’ambito del calcolo dell’affidabilità tra giudici, argomento che sarà sviluppato nel capitolo seguente. Inoltre, viene tracciato un collegamento con i modelli di crescita latente, i quali saranno trattati nei capitoli successivi come naturale prosecuzione o alternativa ai modelli multilivello. Questo sottolinea la continuità e la rilevanza di tali approcci nell’ambito della ricerca psicologica e psicometrica.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/01_multilevel.html#session-info",
    "href": "chapters/raters/01_multilevel.html#session-info",
    "title": "14  Modelli multilivello",
    "section": "14.10 Session Info",
    "text": "14.10 Session Info\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] kableExtra_1.4.0  repr_1.1.7        lme4_1.1-35.5     Matrix_1.7-1     \n [5] car_3.1-3         carData_3.0-5     MASS_7.3-61       viridis_0.6.5    \n [9] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n[13] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n[17] lavaan_0.6-19     psych_2.4.6.26    scales_1.3.0      markdown_1.13    \n[21] knitr_1.49        lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n[25] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n[29] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.17.1  jsonlite_1.8.9     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5.1 farver_2.1.2      \n  [7] nloptr_2.1.1       rmarkdown_2.29     vctrs_0.6.5       \n [10] Cairo_1.6-2        minqa_1.2.8        base64enc_0.1-3   \n [13] rstatix_0.7.2      htmltools_0.5.8.1  broom_1.0.7       \n [16] Formula_1.2-5      htmlwidgets_1.6.4  plyr_1.8.9        \n [19] sandwich_3.1-1     emmeans_1.10.5     zoo_1.8-12        \n [22] uuid_1.2-1         igraph_2.1.1       mime_0.12         \n [25] lifecycle_1.0.4    pkgconfig_2.0.3    R6_2.5.1          \n [28] fastmap_1.2.0      shiny_1.9.1        digest_0.6.37     \n [31] OpenMx_2.21.13     fdrtool_1.2.18     colorspace_2.1-1  \n [34] rprojroot_2.0.4    Hmisc_5.2-0        labeling_0.4.3    \n [37] fansi_1.0.6        timechange_0.3.0   abind_1.4-8       \n [40] compiler_4.4.2     withr_3.0.2        glasso_1.11       \n [43] htmlTable_2.4.3    backports_1.5.0    ggsignif_0.6.4    \n [46] corpcor_1.6.10     gtools_3.9.5       tools_4.4.2       \n [49] pbivnorm_0.6.0     foreign_0.8-87     zip_2.3.1         \n [52] httpuv_1.6.15      nnet_7.3-19        glue_1.8.0        \n [55] quadprog_1.5-8     promises_1.3.0     nlme_3.1-166      \n [58] lisrelToR_0.3      grid_4.4.2         pbdZMQ_0.3-13     \n [61] checkmate_2.3.2    cluster_2.1.6      reshape2_1.4.4    \n [64] generics_0.1.3     gtable_0.3.6       tzdb_0.4.0        \n [67] data.table_1.16.2  hms_1.1.3          xml2_1.3.6        \n [70] utf8_1.2.4         sem_3.1-16         pillar_1.9.0      \n [73] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [76] splines_4.4.2      lattice_0.22-6     survival_3.7-0    \n [79] kutils_1.73        tidyselect_1.2.1   miniUI_0.1.1.1    \n [82] pbapply_1.7-2      svglite_2.1.3      stats4_4.4.2      \n [85] xfun_0.49          qgraph_1.9.8       arm_1.14-4        \n [88] stringi_1.8.4      pacman_0.5.1       boot_1.3-31       \n [91] evaluate_1.0.1     codetools_0.2-20   mi_1.1            \n [94] cli_3.6.3          RcppParallel_5.1.9 IRkernel_1.3.2    \n [97] rpart_4.1.23       systemfonts_1.1.0  xtable_1.8-4      \n[100] munsell_0.5.1      Rcpp_1.0.13-1      coda_0.19-4.1     \n[103] png_0.1-8          XML_3.99-0.17      parallel_4.4.2    \n[106] jpeg_0.1-10        mvtnorm_1.3-2      openxlsx_4.2.7.1  \n[109] crayon_1.5.3       rlang_1.1.4        multcomp_1.4-26   \n[112] mnormt_2.1.1      \n\n\n\n\n\n\nBates, D., Mächler, M., Bolker, B., & Walker, S. (2014). Fitting linear mixed-effects models using lme4. arXiv preprint arXiv:1406.5823.\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2nd Edition). CRC Press.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Modelli multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/raters/02_interrater_reliability.html",
    "href": "chapters/raters/02_interrater_reliability.html",
    "title": "15  L’affidabilità tra giudici",
    "section": "",
    "text": "15.1 Introduzione\nL’affidabilità inter-valutatore si misura analizzando la coerenza tra i punteggi o le valutazioni fornite da diversi osservatori. Una forte affidabilità inter-valutatore indica che il test o strumento di valutazione genera risultati consistenti e riproducibili, indipendentemente da chi esegue la valutazione. Questo aspetto è fondamentale per assicurare che le conclusioni tratte dai risultati del test siano solide e affidabili.\nConsideriamo, ad esempio, una diagnosi psicologica: se diversi psicologi valutano i sintomi di un paziente, è essenziale che le loro valutazioni siano coerenti per garantire una diagnosi accurata. Nella valutazione dei disturbi dell’umore, ad esempio, gli psicologi devono arrivare a conclusioni simili basandosi sui sintomi osservabili e sulle risposte del paziente a questionari standardizzati. Un altro esempio riguarda la ricerca sull’efficacia di una nuova terapia per l’ansia, in cui diversi terapeuti valutano i livelli di ansia dei partecipanti prima e dopo il trattamento. In questo caso, l’affidabilità tra le valutazioni dei terapeuti è cruciale per confermare l’efficacia della terapia.\nQuesto capitolo approfondisce l’affidabilità inter-valutatore in due contesti distinti: nel caso di due giudici e nel caso di giudici multipli. Nel primo scenario, esamineremo l’accordo nominale e il Kappa di Cohen; nel secondo, esploreremo l’uso dell’ICC (Coefficienti di Correlazione Intraclasse), dell’ANOVA a una via e dell’ANOVA a due vie. In quest’ultimo contesto, i modelli a effetti misti sono particolarmente utili per gestire disegni con più giudici, permettendo di catturare sia le variazioni tra giudici sia quelle tra partecipanti, e fornendo una valutazione completa dell’affidabilità delle misurazioni.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>L'affidabilità tra giudici</span>"
    ]
  },
  {
    "objectID": "chapters/raters/02_interrater_reliability.html#un-esempio-concreto",
    "href": "chapters/raters/02_interrater_reliability.html#un-esempio-concreto",
    "title": "15  L’affidabilità tra giudici",
    "section": "15.2 Un Esempio Concreto",
    "text": "15.2 Un Esempio Concreto\nIn questo studio, i giudici valutano la “Percentuale di Intelligibilità del Discorso” di persone affette da disartria. Questa misura soggettiva descrive la percentuale di discorso compreso durante una conversazione con un parlante che soffre di disartria, una condizione che influisce sulla chiarezza e sull’intelligibilità della parola. I giudici ascoltano e valutano le registrazioni vocali dei partecipanti affetti da disartria e stimano la percentuale di parole o frasi che sono in grado di comprendere chiaramente. Questa valutazione fornisce una misura importante dell’impatto della disartria sulla comunicazione verbale quotidiana.\n\nNella discussione seguente useremo i dati dello studio The Reliability and Validity of Speech-Language Pathologists’ Estimations of Intelligibility in Dysarthria di Hirsch et al. (2022).\n\n\nslp_dat &lt;- read.csv(\"https://osf.io/download/p9gqk/\")\nslp_vas_wide &lt;- slp_dat |&gt;\n    dplyr::select(slpID, Speaker, slp_VAS) |&gt;\n    pivot_wider(names_from = slpID, values_from = slp_VAS)\nhead(slp_vas_wide)\n\n\nA tibble: 6 x 22\n\n\nSpeaker\nslp10\nslp11\nslp13\nslp14\nslp15\nslp16\nslp17\nslp18\nslp19\n...\nslp21\nslp22\nslp23\nslp25\nslp3\nslp4\nslp5\nslp6\nslp8\nslp9\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n...\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nAF1\n96.3\n100.0\n96.56\n99.30\n100.0\n12.58\n100.00\n88.2\n52.58\n...\n77.4\n73.64\n100.0\n100.0\n37.8\n73.2\n55.01\n100.0\n53.94\n85.1\n\n\nAF9\n12.3\n34.7\n0.86\n3.52\n12.6\n0.00\n6.69\n0.0\n7.42\n...\n13.6\n0.00\n0.0\n2.9\n0.0\n14.5\n1.72\nNA\n6.76\n10.3\n\n\nALSF6\nNA\n30.8\n5.73\n73.94\n73.9\n4.52\n45.13\n41.9\n26.13\n...\n25.8\n8.31\n18.5\n86.8\n0.0\n26.2\n3.72\nNA\n9.86\n37.8\n\n\nALSF7\n94.0\n96.3\n61.32\n64.37\n100.0\n7.74\n69.92\n88.2\n37.42\n...\n59.4\n34.10\n90.6\n53.2\n99.7\n63.1\n56.73\n88.8\n64.93\n53.8\n\n\nALSF9\n29.5\n52.2\n43.55\n46.76\n60.6\n23.87\n25.21\n91.6\n37.10\n...\n57.7\n29.51\n50.1\n64.8\n0.0\n31.1\n40.11\n16.9\n5.92\n50.0\n\n\nALSM1\n96.3\nNA\n29.23\n69.44\n99.3\n82.26\n70.19\n79.7\n78.39\n...\n83.9\n84.24\n92.3\n100.0\n100.0\n18.0\n58.74\n66.5\n51.83\n60.3",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>L'affidabilità tra giudici</span>"
    ]
  },
  {
    "objectID": "chapters/raters/02_interrater_reliability.html#due-giudici",
    "href": "chapters/raters/02_interrater_reliability.html#due-giudici",
    "title": "15  L’affidabilità tra giudici",
    "section": "15.3 Due giudici",
    "text": "15.3 Due giudici\nIn questa sezione, analizziamo come calcolare l’accordo nominale e l’indice di concordanza Kappa di Cohen nel caso di due giudici.\nSelezioniamo due giudici.\n\nslp_2rater &lt;- slp_vas_wide |&gt;\n    dplyr::select(slp14, slp15)\nhead(slp_2rater)\n\n\nA tibble: 6 x 2\n\n\nslp14\nslp15\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n99.30\n100.0\n\n\n3.52\n12.6\n\n\n73.94\n73.9\n\n\n64.37\n100.0\n\n\n46.76\n60.6\n\n\n69.44\n99.3\n\n\n\n\n\n\n15.3.1 Accordo nominale\nL’accordo nominale misura la percentuale di perfetta corrispondenza tra le valutazioni fornite dai due giudici. In altre parole, questo metodo rileva quanto spesso i giudici assegnano esattamente lo stesso punteggio per ciascun caso osservato. Ad esempio, se i giudici forniscono valutazioni identiche per il 40% dei casi, l’accordo nominale sarà pari a 0.4.\nUtilizzando questa definizione, si ottiene:\n\nwith(slp_2rater, mean(slp14 == slp15))\n\n0.1\n\n\n\nPer rendere il criterio di concordanza più flessibile, si può applicare un arrotondamento ai punteggi: dividendo ciascun punteggio per 10 e arrotondando al numero intero più vicino, anche valori leggermente diversi possono essere considerati concordanti. Ad esempio, valutazioni come 75 e 78, dopo essere state divise per 10 e arrotondate, diventano entrambe 8. Questo metodo amplia il criterio di accordo, riconoscendo che piccole discrepanze potrebbero essere irrilevanti nella pratica.\n\nslp_2round &lt;- round(slp_2rater / 10)\nslp_2round |&gt; print()\n\n   slp14 slp15\n1     10    10\n2      0     1\n3      7     7\n4      6    10\n5      5     6\n6      7    10\n7      8     9\n8      8     9\n9      9     8\n10    10    10\n11    10     7\n12    10    10\n13     6     5\n14    10    10\n15     6    10\n16    10    10\n17    10    10\n18     9    10\n19     9    10\n20    10    10\n\n\n\nslp_2round &lt;- lapply(slp_2round, FUN = factor, levels = 0:10)\n# Contingency table\ntable(slp_2round)\n\n     slp15\nslp14 0 1 2 3 4 5 6 7 8 9 10\n   0  0 1 0 0 0 0 0 0 0 0  0\n   1  0 0 0 0 0 0 0 0 0 0  0\n   2  0 0 0 0 0 0 0 0 0 0  0\n   3  0 0 0 0 0 0 0 0 0 0  0\n   4  0 0 0 0 0 0 0 0 0 0  0\n   5  0 0 0 0 0 0 1 0 0 0  0\n   6  0 0 0 0 0 1 0 0 0 0  2\n   7  0 0 0 0 0 0 0 1 0 0  1\n   8  0 0 0 0 0 0 0 0 0 2  0\n   9  0 0 0 0 0 0 0 0 1 0  2\n   10 0 0 0 0 0 0 0 1 0 0  7\n\n\n\nIl codice precedente fa uso di due funzioni principali: lapply() e table(). Esso opera su una lista o su un dataframe chiamato slp_2round, applicando una trasformazione ai dati e successivamente creando una tabella di contingenza.\n\nslp_2round &lt;- lapply(slp_2round, FUN = factor, levels = 0:10)\n\n\nlapply(): questa funzione appartiene alla famiglia delle funzioni “*apply” in R, che sono utilizzate per applicare una funzione a ciascun elemento di una lista o di un vettore. lapply() restituisce sempre una lista come risultato, mantenendo la lunghezza dell’input originale.\nslp_2round: è la lista o il dataframe su cui si sta operando. Il codice modifica questa variabile, sostituendola con il risultato dell’applicazione di lapply().\nFUN = factor: specifica che la funzione da applicare a ciascun elemento di slp_2round è factor(). La funzione factor() è utilizzata per codificare un vettore come un fattore, che è utile in R per rappresentare dati categorici.\nlevels = 0:10: indica i livelli del fattore, che in questo caso sono i numeri da 0 a 10. Questo parametro assicura che i fattori creati abbiano esattamente questi livelli, anche se alcuni di essi non appaiono nei dati. Questo è particolarmente utile per garantire la coerenza dei livelli dei fattori attraverso vari elementi di slp_2round, soprattutto se si intende confrontarli o combinarli in seguito.\n\n\ntable(slp_2round)\n\n\ntable(): questa funzione crea una tabella di contingenza dai dati forniti. Una tabella di contingenza è un tipo di tabella in formato matriciale che conta la frequenza di combinazioni specifiche di valori all’interno di una o più variabili categoriche.\n\nDopo l’applicazione di lapply(), slp_2round diventa una lista di elementi, ciascuno dei quali è un fattore con livelli da 0 a 10. Quando si passa questa lista modificata alla funzione table(), viene generata una tabella di contingenza. Questa tabella riassume le frequenze dei vari livelli dei fattori attraverso gli elementi della lista slp_2round.\n\np0 &lt;- with(slp_2round, mean(slp14 == slp15))\np0\n\n0.4\n\n\n\nQui, l’arrotondamento serve a ridurre la specificità delle valutazioni, permettendo un confronto più più flessibile tra le valutazioni dei due giudici​​. Questo approccio riflette una visione più tollerante dell’accordo, riconoscendo che piccole discrepanze nelle valutazioni potrebbero non essere rilevanti in un contesto pratico.\nNel contesto specifico descritto, l’arrotondamento implica dividere le valutazioni per 10 e poi arrotondarle al numero intero più vicino. Questo processo rende i valori leggermente diversi equivalenti. Ad esempio, se un giudice assegna una valutazione di 75 e l’altro assegna 78, dividendo per 10 otteniamo 7.5 e 7.8; arrotondando, entrambi i valori diventano 8. Così, valutazioni originalmente diverse vengono arrotondate allo stesso numero intero, permettendo un accordo più ampio tra i giudici. Questo metodo riconosce l’accordo anche in presenza di piccole variazioni nelle valutazioni, riflettendo una visione più flessibile dell’accordo tra i giudici.\n\n\n15.3.2 Kappa di Cohen\nL’indice di concordanza Kappa di Cohen è una misura più robusta dell’accordo tra i giudici, poiché tiene conto della concordanza casuale. A differenza dell’accordo nominale, che considera solo la frequenza delle concordanze perfette, Kappa di Cohen corregge per l’accordo che potrebbe verificarsi per puro caso.\nKappa è definito dalla formula:\n\\[\n\\kappa = \\frac{p_o - p_c}{1 - p_c},\n\\tag{15.1}\\]\ndove:\n\n\\(p_o\\) è la proporzione di accordo osservato, ossia la frazione di volte in cui i giudici concordano effettivamente nelle loro valutazioni,\n\\(p_c\\) è la proporzione di accordo attesa per caso, calcolata assumendo che le valutazioni siano indipendenti.\n\nIl valore \\(p_o\\) si calcola nel modo seguente:\n\\[\np_o = \\frac{{\\text{Numero di accordi osservati}}}{{\\text{Numero totale di confronti}}}.\n\\]\nIn altre parole, \\(p_o\\) è il rapporto tra il numero di volte in cui gli osservatori sono concordi (hanno dato la stessa valutazione) e il numero totale di confronti effettuati.\nIl valore \\(p_c\\) rappresenta l’indice di concordanza attesa per caso:\n\\[\np_c = \\frac{{\\sum (\\text{Riga i-esima del totale}) \\times (\\text{Colonna i-esima del totale})}}{{\\text{Numero totale di confronti}}^2}.\n\\]\nIn altre parole, \\(p_c\\) è il rapporto atteso di concordanza tra gli osservatori nel caso in cui le loro valutazioni siano indipendenti.\nUn valore di Kappa vicino a 1 indica un’alta concordanza corretta per l’accordo casuale, mentre un valore vicino a 0 suggerisce che l’accordo non è superiore a quello casuale. Un valore negativo indica una concordanza peggiore di quella casuale.\nAd esempio, calcolando il Kappa per due giudici che valutano lo stesso campione, possiamo ottenere una misura di concordanza che tiene conto della probabilità di accordo fortuito.\n\nslp_2tab &lt;- table(slp_2round)\npc &lt;- sum(colSums(slp_2tab) * rowSums(slp_2tab)) / sum(slp_2tab)^2\n(kappa &lt;- (p0 - pc) / (1 - pc))\n\n0.166666666666667\n\n\n\nTuttavia, Kappa può risultare basso quando i giudizi si concentrano su una o poche categorie, creando una distribuzione non uniforme, come nell’esempio fornito.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>L'affidabilità tra giudici</span>"
    ]
  },
  {
    "objectID": "chapters/raters/02_interrater_reliability.html#giudici-multipli",
    "href": "chapters/raters/02_interrater_reliability.html#giudici-multipli",
    "title": "15  L’affidabilità tra giudici",
    "section": "15.4 Giudici multipli",
    "text": "15.4 Giudici multipli\n\n15.4.1 Coefficiente \\(\\alpha\\)\nIl coefficiente \\(\\alpha\\) di Cronbach è comunemente utilizzato per misurare l’affidabilità interna di un test, cioè la coerenza con cui gli item all’interno del test misurano un costrutto comune. Tuttavia, può essere utilizzato anche per valutare l’affidabilità tra giudici in uno studio in cui più valutatori assegnano punteggi o giudizi a una serie di item.\nNel contesto dell’affidabilità tra giudici, il coefficiente \\(\\alpha\\) di Cronbach misura quanto consistentemente i diversi giudici valutano gli stessi item. Un valore di \\(\\alpha\\) vicino a 1 indica un alto livello di consistenza (o affidabilità) tra i giudici, suggerendo che stanno valutando gli item in modo simile. Un valore più basso indica una minore coerenza nelle valutazioni tra i giudici.\nPer calcolare il coefficiente \\(\\alpha\\) di Cronbach in questo contesto, si considerano i punteggi assegnati da ciascun giudice a ciascun item come se fossero item individuali di un test. Quindi, si applica la formula standard del coefficiente \\(\\alpha\\) per valutare la coerenza interna di questi “item” (in questo caso, le valutazioni dei giudici).\n\npsych::alpha(slp_vas_wide[-1])\n\n\nReliability analysis   \nCall: psych::alpha(x = slp_vas_wide[-1])\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean sd median_r\n      0.98      0.98       1      0.69  46 0.0076   62 25      0.7\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.96  0.98  0.99\nDuhachek  0.96  0.98  0.99\n\n Reliability if an item is dropped:\n      raw_alpha std.alpha G6(smc) average_r S/N var.r med.r\nslp10      0.97      0.98    0.99      0.68  43 0.014  0.70\nslp11      0.97      0.98    0.99      0.68  43 0.014  0.70\nslp13      0.97      0.98    0.99      0.69  44 0.015  0.70\nslp14      0.97      0.98    0.99      0.69  44 0.015  0.70\nslp15      0.97      0.98    0.99      0.69  44 0.014  0.70\nslp16      0.98      0.98    0.99      0.70  46 0.014  0.71\nslp17      0.97      0.98    0.99      0.68  43 0.015  0.69\nslp18      0.97      0.98    0.99      0.69  45 0.013  0.70\nslp19      0.97      0.98    0.99      0.69  45 0.014  0.71\nslp2       0.97      0.98    1.00      0.69  45 0.014  0.71\nslp20      0.97      0.98    0.99      0.68  43 0.015  0.69\nslp21      0.97      0.98    0.99      0.68  43 0.014  0.69\nslp22      0.97      0.98    0.99      0.69  44 0.015  0.70\nslp23      0.97      0.98    0.99      0.68  43 0.015  0.69\nslp25      0.97      0.98    0.99      0.69  45 0.014  0.71\nslp3       0.98      0.98    0.99      0.70  46 0.014  0.71\nslp4       0.97      0.98    0.99      0.69  45 0.014  0.71\nslp5       0.97      0.98    0.99      0.69  45 0.015  0.70\nslp6       0.98      0.98    0.99      0.70  47 0.010  0.71\nslp8       0.97      0.98    0.99      0.69  44 0.015  0.70\nslp9       0.97      0.98    1.00      0.68  43 0.014  0.69\n\n Item statistics \n       n raw.r std.r r.cor r.drop mean sd\nslp10 18  0.89  0.89  0.90   0.86   81 28\nslp11 18  0.89  0.90  0.90   0.90   69 33\nslp13 19  0.86  0.86  0.85   0.86   54 33\nslp14 20  0.82  0.84  0.84   0.81   80 25\nslp15 20  0.81  0.83  0.83   0.80   85 22\nslp16 20  0.77  0.76  0.76   0.73   50 42\nslp17 20  0.90  0.91  0.91   0.90   63 34\nslp18 20  0.82  0.80  0.77   0.79   70 30\nslp19 19  0.83  0.82  0.82   0.81   56 25\nslp2  20  0.83  0.82  0.82   0.80   64 32\nslp20 20  0.90  0.90  0.90   0.89   49 25\nslp21 19  0.91  0.91  0.90   0.91   59 25\nslp22 20  0.86  0.86  0.86   0.84   50 27\nslp23 20  0.89  0.89  0.89   0.86   71 32\nslp25 20  0.79  0.79  0.79   0.77   79 28\nslp3  19  0.76  0.75  0.76   0.74   52 40\nslp4  20  0.79  0.79  0.78   0.77   47 27\nslp5  19  0.82  0.82  0.82   0.81   43 23\nslp6  13  0.80  0.71  0.71   0.54   76 23\nslp8  20  0.86  0.87  0.87   0.86   53 31\nslp9  19  0.89  0.90  0.90   0.90   58 23\n\n\nNell’esempio in discussione, il valore del coefficiente \\(\\alpha\\) di Cronbach è 0.98. Questo indica un’altissima affidabilità interna, suggerendo che i giudici coinvolti nello studio hanno fornito delle valutazioni molto consistenti tra loro. Un valore così elevato di \\(\\alpha\\) è raro e suggerisce che ci sia un’alta coerenza nelle valutazioni tra i giudici. In termini pratici, questo significa che si può avere un’elevata fiducia nel fatto che le valutazioni fornite dai diversi giudici siano intercambiabili e riflettano in modo affidabile la “Percentuale di Intelligibilità del Discorso” che viene valutata.\n\n\n15.4.2 Coefficiente di correlazione intraclasse\nIl Coefficiente di Correlazione Intraclasse (ICC) quantifica il grado di somiglianza tra le unità all’interno dello stesso gruppo. A differenza della maggior parte delle altre misure di correlazione, l’ICC viene impiegato con dati organizzati in gruppi anziché con coppie di osservazioni. Questo lo rende particolarmente idoneo per valutare la concordanza tra giudici che stanno valutando lo stesso insieme di individui o item.\nL’ICC si basa sul framework del modello a effetti misti. Questi modelli statistici consentono di analizzare le variazioni dei punteggi sia all’interno dei gruppi (ovvero le differenze tra le osservazioni all’interno dello stesso gruppo) che tra i gruppi (ovvero le differenze tra i gruppi stessi). Pertanto, l’ICC tiene conto di due fonti di varianza: la varianza all’interno dei gruppi (varianza delle valutazioni dei giudici all’interno dello stesso gruppo) e la varianza tra i gruppi (varianza delle valutazioni dei giudici tra gruppi diversi). Inoltre, viene considerato l’errore casuale presente nelle valutazioni.\nIn sintesi, l’ICC valuta la consistenza delle misurazioni quando queste vengono effettuate da diversi osservatori su un insieme di soggetti o item. Un valore elevato di ICC segnala una forte concordanza tra le valutazioni degli osservatori, indicando che le misurazioni sono affidabili e riproducibili. Al contrario, un ICC basso riflette una discrepanza significativa tra gli osservatori, suggerendo che le valutazioni sono meno consistenti.\n\n15.4.2.1 Calcolo dell’ICC\nIl calcolo dell’ICC si basa su un modello ad effetti misti, in cui i giudici (o osservatori) sono considerati come variabili indipendenti e i punteggi assegnati costituiscono la variabile dipendente. Questo modello permette di scomporre e quantificare le diverse fonti di variazione nei dati, tra cui:\n\nLa variazione attribuibile ai soggetti (o item) valutati, che rappresenta la varianza principale di interesse.\nLa variazione imputabile ai giudici (differenze tra valutatori).\nLa variazione dovuta all’interazione tra soggetti (o item) e giudici.\nLa variazione derivante dall’errore casuale.\n\nL’ICC si ottiene dividendo la varianza attribuibile ai soggetti valutati per la somma di questa varianza con la varianza dovuta all’errore. Questo rapporto rappresenta la quota della varianza totale dei punteggi che può essere attribuita alle differenze tra i soggetti stessi, fornendo un indicatore dell’affidabilità delle misurazioni. Un ICC elevato indica che la maggior parte della varianza dei punteggi è dovuta alle differenze reali tra i soggetti, suggerendo che le valutazioni sono affidabili.\nTuttavia, cosa viene considerato come “errore” dipende da alcune considerazioni chiave:\n\nTipo di disegno: crossed o nested\n\nIn un disegno crossed, tutti i giudici valutano ogni soggetto o item, quindi l’errore riflette le differenze tra giudici che valutano gli stessi soggetti.\nIn un disegno nested, diversi gruppi di giudici valutano gruppi diversi di soggetti o item; in questo caso, l’errore riflette le differenze tra i gruppi di giudici.\n\nGiudici come fixed o random\n\nSe i giudici sono considerati fixed (fissi), si presume che i giudici inclusi siano gli unici rilevanti e che l’interesse sia nella loro specifica concordanza.\nSe i giudici sono random (casuali), si assume che siano un campione rappresentativo di una popolazione più ampia di giudici, e l’ICC misura la concordanza generalizzabile a tutta questa popolazione.\n\nConsistenza vs. accordo assoluto\n\nConsistenza: misura il grado di concordanza nell’ordinamento dei punteggi assegnati dai giudici, ignorando le differenze nei valori numerici assoluti. È utile quando interessa solo l’ordine relativo dei punteggi.\nAccordo assoluto: valuta la concordanza nei valori effettivi dei punteggi assegnati dai giudici. È rilevante quando i giudici devono assegnare esattamente lo stesso punteggio.\n\n\nQueste considerazioni sono essenziali per interpretare correttamente l’ICC o altri indici di affidabilità tra giudici. La scelta del modello statistico e la definizione dell’errore dipendono da queste variabili e da come si desidera interpretare la concordanza. Una comprensione approfondita di questi aspetti è necessaria per una valutazione accurata della coerenza e dell’affidabilità delle misurazioni effettuate da giudici o osservatori multipli.\n\n\n\n15.4.3 ANOVA ad una via per disegni nidificati\nConsideriamo il caso in cui i dati rappresentano da due valutazioni assegnate a ciascuna persona da giudici diversi.\n\nslp_vas_nested &lt;- slp_dat |&gt;\n    mutate(SpeakerID = as.numeric(as.factor(Speaker))) |&gt;\n    # Select only 10 speakers\n    dplyr::filter(SpeakerID &lt;= 10) |&gt;\n    group_by(Speaker) |&gt;\n    # Filter specific raters\n    dplyr::filter(row_number() %in% (SpeakerID[1] * 2 - (1:0)))\n\nhead(slp_vas_nested)\n\n\nA grouped_df: 6 x 7\n\n\nslpID\nSpeaker\nslp_VAS\nslp_VAS.Rel\nslp_EST\nslp_EST.Rel\nSpeakerID\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nslp10\nAF1\n96.28\nNA\n90\nNA\n1\n\n\nslp11\nAF1\n100.00\nNA\n80\nNA\n1\n\n\nslp13\nAF9\n0.86\nNA\n10\nNA\n2\n\n\nslp14\nAF9\n3.52\nNA\n10\nNA\n2\n\n\nslp15\nALSF6\n73.87\nNA\n10\nNA\n3\n\n\nslp16\nALSF6\n4.52\n9.03\n5\nNA\n3\n\n\n\n\n\nVerifichiamo che ogni giudice abbia fornito due giudizi per soggetto:\n\nslp_vas_nested %&gt;%\n  group_by(Speaker) %&gt;%\n  summarise(Count = n())\n\n\nA tibble: 10 x 2\n\n\nSpeaker\nCount\n\n\n&lt;chr&gt;\n&lt;int&gt;\n\n\n\n\nAF1\n2\n\n\nAF9\n2\n\n\nALSF6\n2\n\n\nALSF7\n2\n\n\nALSF9\n2\n\n\nALSM1\n2\n\n\nALSM4\n2\n\n\nAM1\n2\n\n\nAM3\n2\n\n\nAM5\n2\n\n\n\n\n\nCi sono 20 giudici:\n\nlength(slp_vas_nested$Speaker)\n\n20\n\n\nIn questo studio, analizziamo dati costituiti da due valutazioni fornite a ciascun individuo da giudici diversi. Il nostro obiettivo è identificare e separare due principali fonti di variazione: le differenze tra i giudici e le variazioni casuali all’interno delle valutazioni di ciascun giudice.\nLa sfida risiede nel distinguere queste fonti di variazione, essenziale per determinare la proporzione di variabilità nei punteggi attribuibile a reali differenze tra i giudici, rispetto a quella derivante da errori casuali. Per affrontare questo problema, applichiamo un’analisi della varianza a effetti casuali (Random-Effects ANOVA), utilizzando il modello lineare misto implementato attraverso la funzione lmer() di R. Questa metodologia ci permette di stimare separatamente la varianza dovuta alle differenze tra i giudici e quella associata all’errore casuale.\nImplementiamo il modello lmer(slp_VAS ~ 1 + (1 | Speaker), data = slp_vas_nested), dove il termine Speaker rappresenta i giudici. Attraverso questo modello, trattiamo le intercette associate a ciascun giudice come effetti casuali, consentendoci di catturare le specifiche variazioni tra i giudici al di là delle fluttuazioni casuali.\n\nm1 &lt;- lmer(slp_VAS ~ 1 + (1 | Speaker), data = slp_vas_nested)\nsummary(m1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: slp_VAS ~ 1 + (1 | Speaker)\n   Data: slp_vas_nested\n\nREML criterion at convergence: 180\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.5005 -0.7761 -0.0595  0.8758  1.3559 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Speaker  (Intercept) 309      17.6    \n Residual             817      28.6    \nNumber of obs: 19, groups:  Speaker, 10\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)    53.61       8.63    6.21\n\n\nDall’analisi emergono due componenti principali di varianza: - La varianza attribuibile a differenze intrinseche tra i soggetti valutati (slpID), che riflette la variabilità naturale tra gli individui. - La varianza attribuibile ai giudici (Speaker), che misura l’estensione del bias sistematico, ossia la tendenza di alcuni giudici a fornire valutazioni sistematicamente più alte o più basse rispetto ad altri.\nIn aggiunta, abbiamo la varianza residua, che include l’errore di misurazione e altre fonti di variabilità non spiegate dal modello.\n\nvc_m1 &lt;- as.data.frame(VarCorr(m1))\nvc_m1\n\n\nA data.frame: 2 x 5\n\n\ngrp\nvar1\nvar2\nvcov\nsdcor\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nSpeaker\n(Intercept)\nNA\n309\n17.6\n\n\nResidual\nNA\nNA\n817\n28.6\n\n\n\n\n\nPer valutare l’affidabilità delle valutazioni, calcoliamo l’Intraclass Correlation Coefficient (ICC) utilizzando i dati estratti dal modello. Questo coefficiente quantifica la proporzione della varianza totale attribuibile alle differenze tra i soggetti valutati, offrendoci un indice dell’affidabilità delle valutazioni in contesti dove sono coinvolti giudizi o misurazioni ripetute.\nUn ICC prossimo a 1 indica che la maggior parte della variabilità nei dati è dovuta a differenze reali tra i soggetti valutati, mentre un valore più basso suggerisce un’influenza significativa di variazioni casuali o di bias dei giudici sulla variabilità totale osservata.\nCalcoliamo l’ICC per una singola valutazione:\n\\[\nICC = \\frac{\\sigma^2_{\\text{giudici}}} {\\sigma^2_{\\text{giudici}} + \\sigma^2_E}.\n\\]\nNella formula, l’ICC misura la proporzione della varianza totale del punteggio slp_VAS che è attribuibile alla variazione tra i gruppi dei giudici (Speaker) rispetto alla variazione residua o errore.\n\nvc_m1$vcov[1] / (vc_m1$vcov[1] + vc_m1$vcov[2])\n\n0.274407605476794\n\n\nUtilizziamo la funzione icc() del pacchetto performance:\n\nperformance::icc(m1)\n\n\nA icc: 1 x 3\n\n\nICC_adjusted\nICC_conditional\nICC_unadjusted\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0.274\n0.274\n0.274\n\n\n\n\n\nUtilizzando il modello specificato m1 &lt;- lmer(slp_VAS ~ 1 + (1 | Speaker), data = slp_vas_nested), con l’ICC calcolato tramite la formula vc_m1$vcov[1] / (vc_m1$vcov[1] + vc_m1$vcov[2]), è possibile fare alcune osservazioni sulle considerazioni precedenti:\n\nCrossed vs. Nested:\nIl disegno sia “nested”, il che implica che diversi gruppi di giudici valutano gruppi diversi di soggetti o item. In un disegno nested, non tutti i giudici valutano tutti i soggetti, quindi l’errore riflette le differenze tra i gruppi di giudici assegnati a specifici soggetti o item. Questo approccio è adatto in situazioni dove non si può o non si vuole che tutti i giudici valutino ogni soggetto.\nFixed vs. Random:\nNella formulazione (1 | Speaker), l’effetto per “Speaker” (che identifica i giudici) è specificato come random. Ciò significa che stiamo trattando i giudici come un campione casuale di una popolazione più ampia, piuttosto che come un insieme fisso di valutatori. Di conseguenza, l’ICC calcolato qui è interpretato come una misura di affidabilità che potrebbe essere generalizzata a una popolazione di giudici simili, non limitata solo ai giudici specifici coinvolti nel campione.\nConsistenza vs. Accordo Assoluto:\nIl modello calcolato è basato su un’intercetta casuale (1 | Speaker), il che indica che l’ICC stima la consistenza tra i giudici, piuttosto che l’accordo assoluto. Questo significa che l’ICC si concentra sul grado in cui i giudici mantengono lo stesso ordine o classificazione tra i soggetti, senza richiedere che assegnino esattamente gli stessi valori numerici.\n\n\n15.4.3.1 ICC per la valutazione media\nCalcoliamo ora l’ICC per la valutazione media:\n\\[\nICC = \\frac{\\sigma^2_{\\text{giudici}}} {\\sigma^2_{\\text{giudici}} + \\sigma^2_E / k},\n\\]\ndove \\(k\\) rappresenta il numero di giudizi assegnati a ciascun soggetto da ciascun giudice.\nLa formula calcola l’ICC basandosi sulla varianza tra i gruppi (in questo caso, la varianza attribuibile ai giudici, \\(\\sigma^2_{\\text{giudici}}\\)) e la varianza entro i gruppi (l’errore casuale, \\(\\sigma^2_E\\)), aggiustando per il numero di giudizi (\\(k\\)) dati per ogni soggetto. L’inserimento di \\(k\\) nel denominatore serve a normalizzare l’effetto dell’errore casuale in base al numero di giudizi, assumendo che più giudizi per soggetto possano ridurre l’impatto dell’errore casuale sulla stima dell’affidabilità.\nQuindi, se l’ICC è vicino a 1, ciò indica che la maggior parte della varianza nei dati è dovuta a differenze reali tra i soggetti valutati, piuttosto che a variazioni casuali o a differenze tra i giudici. In questo modo, l’ICC fornisce un indice utile per valutare l’affidabilità delle valutazioni in studi in cui sono coinvolti giudizi o misurazioni ripetute.\nNel caso presente, ogni giudice fornisce due valutazioni per ogni soggetto:\n\nvc_m1$vcov[1] / (vc_m1$vcov[1] + vc_m1$vcov[2] / 2)\n\n0.430643389599249\n\n\nQuesta seconda formula calcola l’affidabilità per la media delle valutazioni di più giudici. È pertinente quando si ha a che fare con più valutazioni per ciascun soggetto e si vuole sapere l’affidabilità della media di queste valutazioni. Questa formula differisce dalla prima perché considera la riduzione dell’errore di misurazione che si verifica quando si calcola la media di più valutazioni.\n\n\n\n15.4.4 ANOVA a due vie e l’Impatto del Bias dei Giudici\nL’ANOVA a due vie è uno strumento statistico cruciale per esaminare gli effetti di più fattori, inclusi i giudici, sulla variabile dipendente in contesti di valutazione. Per interpretare accuratamente i risultati, è fondamentale distinguere tra il bias sistematico introdotto dai giudici e la variabilità intrinseca nelle valutazioni.\n\nBias dei Giudici: Questo si verifica quando i giudici hanno una tendenza costante a fornire valutazioni più alte o più basse rispetto agli altri, influenzando potenzialmente l’equità delle valutazioni.\nVariabilità Intrinseca: Si riferisce alle differenze naturali nelle valutazioni che emergono dalle percezioni individuali o dalle interpretazioni soggettive dei giudici.\n\nIn termini di Contesto di Valutazione: - Nelle analisi di Consistenza, dove l’attenzione è sull’ordine relativo delle valutazioni, il bias dei giudici non è considerato problematico poiché non altera questo ordine. - Nelle analisi di Accordo, che si concentrano sulla concordanza dei valori assoluti delle misurazioni, il bias dei giudici è trattato come una fonte di errore significativa.\nL’applicazione dell’ANOVA a due vie consente di distinguere l’effetto del bias dei giudici rispetto ad altre fonti di varianza, offrendo così un’importante metodologia per valutare l’affidabilità delle valutazioni e l’impatto del bias in diversi contesti.\nUtilizzando lmer(slp_VAS ~ 1 + (1 | Speaker) + (1 | slpID), data = slp_dat), analizziamo i dati considerando due principali fonti di variazione: - Effetti Casuali dei Giudici ((1 | Speaker)): Valutiamo il bias dei giudici come fonte di variazione, distinguendo tra analisi di consistenza e di accordo a seconda di come questo bias influisce sui risultati. - Variabilità Intrinseca tra i Soggetti ((1 | slpID)): Esploriamo le differenze naturali tra i soggetti valutati, fondamentali per comprendere la diversità delle risposte individuali.\nQuesto approccio ci permette di facilitare la distinzione tra l’effetto del bias dei giudici e altre fonti di varianza, fornendo una metodologia preziosa per valutare l’affidabilità delle valutazioni e l’impatto del bias in diversi contesti.\nEseguiamo l’analisi con lmer():\n\nm2 &lt;- lmer(slp_VAS ~ 1 + (1 | Speaker) + (1 | slpID), data = slp_dat)\n\nQuesta formula considera due principali fonti di variazione:\n\nEffetti Casuali dei Giudici ((1 | Speaker)): Valutiamo il bias dei giudici come una fonte di variazione, differenziando tra le analisi di consistenza e di accordo a seconda dell’impatto di questo bias sui risultati.\nVariabilità Intrinseca tra i Soggetti ((1 | slpID)): Esploriamo le differenze naturali tra i soggetti valutati, cruciali per comprendere la diversità delle risposte individuali.\n\n\nsummary(m2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: slp_VAS ~ 1 + (1 | Speaker) + (1 | slpID)\n   Data: slp_dat\n\nREML criterion at convergence: 3544\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-3.328 -0.581  0.086  0.661  2.575 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n slpID    (Intercept) 133      11.5    \n Speaker  (Intercept) 586      24.2    \n Residual             291      17.1    \nNumber of obs: 403, groups:  slpID, 21; Speaker, 20\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)    61.88       6.03    10.3\n\n\nEstraiamo le fonti di varianza:\n\nvc_m2 &lt;- as.data.frame(VarCorr(m2))\nvc_m2\n\n\nA data.frame: 3 x 5\n\n\ngrp\nvar1\nvar2\nvcov\nsdcor\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nslpID\n(Intercept)\nNA\n133\n11.5\n\n\nSpeaker\n(Intercept)\nNA\n586\n24.2\n\n\nResidual\nNA\nNA\n291\n17.1\n\n\n\n\n\n\n\n15.4.5 Componenti di Varianza\n\nVariazione dovuta a slpID (Intercept): Indica la variabilità attribuibile alle differenze tra i soggetti.\nVariazione dovuta a Speaker (Intercept): Riflette la variabilità nelle valutazioni dovuta al bias dei giudici.\nResidui (Residual): Comprende l’errore di misurazione e altre fonti di variabilità non spiegate.\n\n\n\n15.4.6 Calcolo dell’ICC\nPer calcolare l’ICC, dobbiamo distinguere tra le diverse versioni dell’ICC basate sulle definizioni generali. Tuttavia, il calcolo specifico dell’ICC può variare a seconda della struttura del modello e dell’interpretazione desiderata. Nella versione più semplice e comune, si considera la formula:\n\\[\n\\text{ICC} = \\frac{\\sigma^2_{\\text{tra gruppo}}}{\\sigma^2_{\\text{tra gruppo}} + \\sigma^2_{\\text{errore}}},\n\\]\ndove \\(\\sigma^2_{\\text{tra gruppo}}\\) è la varianza attribuita agli effetti casuali tra gruppi, e \\(\\sigma^2_{\\text{errore}}\\) è la varianza residua.\nNel modello precedente, ci sono due componenti di varianza tra gruppi (slpID e Speaker), quindi è possibile considerare la somma di queste due come la varianza totale tra gruppo. Così, l’ICC adjusted può essere calcolato come:\n\\[\n\\text{ICC}_{\\text{adjusted}} = \\frac{\\sigma^2_{slpID} + \\sigma^2_{Speaker}}{\\sigma^2_{slpID} + \\sigma^2_{Speaker} + \\sigma^2_{Residual}}.\n\\]\nSostituendo i valori ottenuti:\n\\[\n\\text{ICC}_{\\text{adjusted}} = \\frac{132.8257 + 585.6568}{132.8257 + 585.6568 + 291.2401}.\n\\]\nQuesto calcolo assume che tutte le componenti di varianza contribuiscano al calcolo dell’ICC in modo equivalente e che l’ICC adjusted consideri la somma delle varianze tra gruppi (in questo caso, slpID e Speaker) rispetto alla varianza totale (inclusa la varianza residua).\nCalcoliamo ora questo valore usando R per ottenere il risultato esatto.\n\n(vc_m2$vcov[1] + vc_m2$vcov[2]) / (vc_m2$vcov[1] + vc_m2$vcov[2] + vc_m2$vcov[3])\n\n0.711564270978534\n\n\nQuesto valore riproduce quello trovato da performance::icc():\n\nperformance::icc(m2)\n\n\nA icc: 1 x 3\n\n\nICC_adjusted\nICC_conditional\nICC_unadjusted\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0.712\n0.712\n0.712\n\n\n\n\n\nL’Intraclass Correlation Coefficient (ICC) di 0.7115643 offre una misura dell’affidabilità o della coerenza delle valutazioni all’interno dei gruppi definiti nel tuo modello (in questo caso, slpID e Speaker).\n\nValore dell’ICC: Il valore di 0.71 indica un livello relativamente alto di coerenza o omogeneità tra le misurazioni all’interno dei gruppi rispetto alla variabilità totale. In altre parole, una proporzione sostanziale della varianza totale nei dati è attribuibile alle differenze tra i gruppi piuttosto che alle variazioni casuali all’interno dei gruppi o agli errori di misurazione.\nAffidabilità delle Misure: Un ICC più vicino a 1 suggerisce che le misure sono molto affidabili, poiché indica che la maggior parte della varianza nei dati può essere attribuita alle differenze sistematiche tra i gruppi piuttosto che al rumore casuale o agli errori. Nel tuo caso, un ICC di circa 0.71 può essere interpretato come indicativo di un’alta affidabilità, suggerendo che le differenze tra i gruppi (ad esempio, tra diversi Speaker o differenti slpID) sono significative e consistenti.\nImplicazioni per la Ricerca: Per la ricerca, un ICC alto come questo implica che il disegno dello studio e la misurazione utilizzata sono adeguatamente sensibili per distinguere tra gli individui o le unità all’interno dei gruppi definiti. Questo è particolarmente rilevante quando si studiano gli effetti di interventi o trattamenti specifici su gruppi distinti o si valuta la consistenza delle risposte tra i membri di un gruppo.\nContesto e Benchmark: L’interpretazione dell’ICC dipende dal contesto specifico e dal campo di studio. Alcuni campi potrebbero considerare un ICC di 0.71 come eccellente, mentre altri potrebbero considerarlo solo moderatamente buono. È importante confrontare questo valore con benchmark o standard specifici del campo di interesse.\nStruttura del Modello e Dati: L’interpretazione dovrebbe anche tenere conto della struttura specifica del modello e della natura dei dati. Diversi tipi di ICC possono essere calcolati a seconda degli obiettivi dello studio e della configurazione del modello misto, quindi è cruciale assicurarsi che l’ICC calcolato sia il più appropriato per il tuo specifico contesto di ricerca.\n\nIn sintesi, un ICC di 0.7115643 indica un’alta affidabilità nelle misure all’interno dei gruppi definiti nel tuo modello, suggerendo che le variazioni osservate sono significativamente influenzate dalle differenze tra i gruppi piuttosto che dalla varianza casuale o dall’errore di misurazione.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>L'affidabilità tra giudici</span>"
    ]
  },
  {
    "objectID": "chapters/raters/02_interrater_reliability.html#riflessioni-conclusive",
    "href": "chapters/raters/02_interrater_reliability.html#riflessioni-conclusive",
    "title": "15  L’affidabilità tra giudici",
    "section": "15.5 Riflessioni Conclusive",
    "text": "15.5 Riflessioni Conclusive\nL’Intraclass Correlation Coefficient (ICC) può essere interpretato come “la proporzione di varianza spiegata dalla struttura di raggruppamento nella popolazione”. Questa struttura di raggruppamento implica che le misurazioni siano organizzate in gruppi (ad esempio, i punteggi di un test in una scuola possono essere raggruppati per classe, se ci sono più classi e a ciascuna è stato somministrato lo stesso test), e l’ICC indica quanto fortemente le misurazioni nello stesso gruppo si assomiglino. Questo indice varia da 0, se il raggruppamento non trasmette alcuna informazione, a 1, se tutte le osservazioni in un gruppo sono identiche (Gelman e Hill, 2007, p. 258). In altre parole, l’ICC - a volte concettualizzato come ripetibilità della misurazione - “può anche essere interpretato come la correlazione attesa tra due unità estratte casualmente che si trovano nello stesso gruppo” (Hox 2010), sebbene questa definizione potrebbe non applicarsi a modelli misti con strutture di effetti casuali più complesse. L’ICC può aiutare a determinare se un modello misto è necessario: un ICC pari a zero (o molto vicino a zero) significa che le osservazioni all’interno dei cluster non sono più simili tra loro rispetto a osservazioni di cluster diversi, e quindi considerarlo come un fattore casuale potrebbe non essere necessario.\n\nAlto ICC significa che i dati sono altamente raggruppati, il che significa che i risultati dipendono fortemente dai gruppi a cui appartengono le osservazioni.\nBasso ICC significa che i dati non sono altamente (o per niente) raggruppati, il che significa che i risultati non dipendono molto (o per niente) dai gruppi a cui appartengono le osservazioni.\n\nDifferenza con R^2\nIl coefficiente di determinazione R^2 quantifica la proporzione di varianza spiegata da un modello statistico, ma la sua definizione in modelli misti è complessa (da qui l’esistenza di diversi metodi per calcolare un’approssimazione). L’ICC è associato a R^2 poiché entrambi sono rapporti di componenti di varianza. Più precisamente, R^2 rappresenta la proporzione della varianza spiegata (del modello completo), mentre l’ICC è la proporzione della varianza spiegata attribuibile agli effetti casuali.\nCalcolo\nL’ICC viene calcolato dividendo la varianza degli effetti casuali, σ^2_i, per la varianza totale, cioè la somma della varianza degli effetti casuali e della varianza residua, σ^2_ε.\nICC aggiustato e non aggiustato\nLa funzione icc() calcola un ICC aggiustato e uno non aggiustato, che tengono conto di tutte le fonti di incertezza (cioè di tutti gli effetti casuali). Mentre l’ICC aggiustato si riferisce solo agli effetti casuali, l’ICC non aggiustato considera anche le varianze degli effetti fissi, aggiungendo più precisamente la varianza degli effetti fissi al denominatore della formula per calcolare l’ICC (vedi Nakagawa et al. 2017). Tipicamente, l’ICC aggiustato è di interesse quando l’analisi degli effetti casuali è di interesse. icc() restituisce un valore ICC anche per strutture di effetti casuali più complesse, come modelli con pendenze casuali o design nidificato (più di due livelli) ed è applicabile per modelli con distribuzioni diverse dalla gaussiana.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>L'affidabilità tra giudici</span>"
    ]
  },
  {
    "objectID": "chapters/raters/02_interrater_reliability.html#session-info",
    "href": "chapters/raters/02_interrater_reliability.html#session-info",
    "title": "15  L’affidabilità tra giudici",
    "section": "15.6 Session Info",
    "text": "15.6 Session Info\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lme4_1.1-35.5     Matrix_1.7-1      MASS_7.3-61       viridis_0.6.5    \n [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n[13] lavaan_0.6-19     psych_2.4.6.26    scales_1.3.0      markdown_1.13    \n[17] knitr_1.49        lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n[21] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n[25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.17.1  jsonlite_1.8.9     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5.1 farver_2.1.2      \n  [7] nloptr_2.1.1       rmarkdown_2.29     vctrs_0.6.5       \n [10] minqa_1.2.8        base64enc_0.1-3    rstatix_0.7.2     \n [13] htmltools_0.5.8.1  broom_1.0.7        Formula_1.2-5     \n [16] htmlwidgets_1.6.4  plyr_1.8.9         sandwich_3.1-1    \n [19] emmeans_1.10.5     zoo_1.8-12         uuid_1.2-1        \n [22] igraph_2.1.1       mime_0.12          lifecycle_1.0.4   \n [25] pkgconfig_2.0.3    R6_2.5.1           fastmap_1.2.0     \n [28] shiny_1.9.1        digest_0.6.37      OpenMx_2.21.13    \n [31] fdrtool_1.2.18     colorspace_2.1-1   rprojroot_2.0.4   \n [34] Hmisc_5.2-0        fansi_1.0.6        timechange_0.3.0  \n [37] abind_1.4-8        compiler_4.4.2     withr_3.0.2       \n [40] glasso_1.11        htmlTable_2.4.3    backports_1.5.0   \n [43] carData_3.0-5      performance_0.12.4 ggsignif_0.6.4    \n [46] corpcor_1.6.10     gtools_3.9.5       tools_4.4.2       \n [49] pbivnorm_0.6.0     foreign_0.8-87     zip_2.3.1         \n [52] httpuv_1.6.15      nnet_7.3-19        glue_1.8.0        \n [55] quadprog_1.5-8     promises_1.3.0     nlme_3.1-166      \n [58] lisrelToR_0.3      grid_4.4.2         pbdZMQ_0.3-13     \n [61] checkmate_2.3.2    cluster_2.1.6      reshape2_1.4.4    \n [64] generics_0.1.3     gtable_0.3.6       tzdb_0.4.0        \n [67] data.table_1.16.2  hms_1.1.3          car_3.1-3         \n [70] utf8_1.2.4         sem_3.1-16         pillar_1.9.0      \n [73] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [76] splines_4.4.2      lattice_0.22-6     survival_3.7-0    \n [79] kutils_1.73        tidyselect_1.2.1   miniUI_0.1.1.1    \n [82] pbapply_1.7-2      stats4_4.4.2       xfun_0.49         \n [85] qgraph_1.9.8       arm_1.14-4         stringi_1.8.4     \n [88] pacman_0.5.1       boot_1.3-31        evaluate_1.0.1    \n [91] codetools_0.2-20   mi_1.1             cli_3.6.3         \n [94] RcppParallel_5.1.9 IRkernel_1.3.2     rpart_4.1.23      \n [97] xtable_1.8-4       repr_1.1.7         munsell_0.5.1     \n[100] Rcpp_1.0.13-1      coda_0.19-4.1      png_0.1-8         \n[103] XML_3.99-0.17      parallel_4.4.2     jpeg_0.1-10       \n[106] mvtnorm_1.3-2      insight_0.20.5     openxlsx_4.2.7.1  \n[109] crayon_1.5.3       rlang_1.1.4        multcomp_1.4-26   \n[112] mnormt_2.1.1      \n\n\n\n\n\n\nHirsch, M. E., Thompson, A., Kim, Y., & Lansford, K. L. (2022). The Reliability and Validity of Speech-Language Pathologists’ Estimations of Intelligibility in Dysarthria. Brain Sciences, 12(8), 1011.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>L'affidabilità tra giudici</span>"
    ]
  },
  {
    "objectID": "chapters/raters/E1_irr.html",
    "href": "chapters/raters/E1_irr.html",
    "title": "16  ✏️ Esercizi",
    "section": "",
    "text": "16.1 Dati categoriali\nIniziamo con dati categoriali e utilizziamo un sottoinsieme dei dati forniti dal data frame diagnoses del pacchetto irr.\ndata(diagnoses)\nglimpse(diagnoses)\n\nRows: 30\nColumns: 6\n$ rater1 &lt;fct&gt; 4. Neurosis, 2. Personality Disorder, 2. Personality Disorder, ~\n$ rater2 &lt;fct&gt; 4. Neurosis, 2. Personality Disorder, 3. Schizophrenia, 5. Othe~\n$ rater3 &lt;fct&gt; 4. Neurosis, 2. Personality Disorder, 3. Schizophrenia, 5. Othe~\n$ rater4 &lt;fct&gt; 4. Neurosis, 5. Other, 3. Schizophrenia, 5. Other, 4. Neurosis,~\n$ rater5 &lt;fct&gt; 4. Neurosis, 5. Other, 3. Schizophrenia, 5. Other, 4. Neurosis,~\n$ rater6 &lt;fct&gt; 4. Neurosis, 5. Other, 5. Other, 5. Other, 4. Neurosis, 3. Schi~\ndat &lt;- diagnoses[, 1:3]\nhead(dat)\n\n\nA data.frame: 6 x 3\n\n\n\nrater1\nrater2\nrater3\n\n\n\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;fct&gt;\n\n\n\n\n1\n4. Neurosis\n4. Neurosis\n4. Neurosis\n\n\n2\n2. Personality Disorder\n2. Personality Disorder\n2. Personality Disorder\n\n\n3\n2. Personality Disorder\n3. Schizophrenia\n3. Schizophrenia\n\n\n4\n5. Other\n5. Other\n5. Other\n\n\n5\n2. Personality Disorder\n2. Personality Disorder\n2. Personality Disorder\n\n\n6\n1. Depression\n1. Depression\n3. Schizophrenia\nIniziamo considerando il caso di due giudici. Per questi dati calcoleremo il Kappa di Cohen.\nkappa2(dat[, c(1, 2)], \"unweighted\")\n\n Cohen's Kappa for 2 Raters (Weights: unweighted)\n\n Subjects = 30 \n   Raters = 2 \n    Kappa = 0.651 \n\n        z = 7 \n  p-value = 2.63e-12\nIl Kappa di Cohen valuta la concordanza tra due valutatori su una scala categoriale. Può essere usato quando i valutatori assegnano categorie, non punteggi numerici, agli oggetti di interesse. Un valore di Kappa maggiore di 0 indica una maggiore concordanza tra i valutatori rispetto a quella che ci si aspetterebbe per caso, mentre un valore di 0 indica che la concordanza non è maggiore di quella casuale.\nNel caso presente, l’output indica che:\nL’output fornisce anche un test statistico per valutare se la concordanza osservata è significativamente maggiore di quella che ci si aspetterebbe per caso:\nIn sintesi, l’output indica una concordanza sostanziale tra i due valutatori sulla scala categorica utilizzata, e questa concordanza è statisticamente significativa. Questo suggerisce che i giudizi dei due valutatori sono affidabili e consistenti tra loro oltre quello che ci si aspetterebbe semplicemente per caso.\nSe ci sono più di due giudici, usiamo il Kappa di Fleiss.\nkappam.fleiss(dat)\n\n Fleiss' Kappa for m Raters\n\n Subjects = 30 \n   Raters = 3 \n    Kappa = 0.534 \n\n        z = 9.89 \n  p-value = 0\nÈ anche possibile utilizzare il Kappa esatto di Conger (1980).\nkappam.fleiss(dat, exact = TRUE)\n\n Fleiss' Kappa for m Raters (exact value)\n\n Subjects = 30 \n   Raters = 3 \n    Kappa = 0.55\nNel caso di 3 giudici, otteniamo un valore di Kappa di Fleiss pari a 0.53, il che indica un’accordo moderato tra gli osservatori. Questo significa che c’è una certa concordanza tra di loro, ma non c’è né una forte né una perfetta corrispondenza. In generale, un Kappa di Fleiss compreso tra 0.41 e 0.60 può essere considerato come un’accordo moderato.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/raters/E1_irr.html#dati-categoriali",
    "href": "chapters/raters/E1_irr.html#dati-categoriali",
    "title": "16  ✏️ Esercizi",
    "section": "",
    "text": "Subjects = 30: Ci sono 30 oggetti o soggetti che sono stati valutati dai giudici.\nRaters = 2: Due valutatori hanno partecipato alla valutazione.\nKappa = 0.651: Il valore di Kappa, 0.651, suggerisce una concordanza sostanziale tra i due valutatori. Secondo le linee guida generalmente accettate per l’interpretazione del coefficiente di Kappa, un valore tra 0.61 e 0.80 indica una concordanza sostanziale.\n\n\n\nz = 7: Il valore z del test statistico è 7. Questo indica la distanza della statistica di test (Kappa osservato) dal valore di Kappa atteso sotto l’ipotesi nulla (nessuna concordanza oltre il caso), misurata in unità di deviazione standard. Un valore elevato indica una forte evidenza contro l’ipotesi nulla.\np-value = 2.63e-12: Il p-value è estremamente basso, molto inferiore a qualsiasi soglia di significatività comune (ad esempio, 0.05 o 0.01). Questo suggerisce che la probabilità di osservare un valore di Kappa come quello ottenuto (o più estremo) se in realtà non ci fosse concordanza tra i valutatori (oltre quella casuale) è estremamente bassa. In altre parole, c’è una forte evidenza statistica che la concordanza tra i due valutatori è significativamente maggiore di quella che ci si aspetterebbe per caso.",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/raters/E1_irr.html#dati-ordinali",
    "href": "chapters/raters/E1_irr.html#dati-ordinali",
    "title": "16  ✏️ Esercizi",
    "section": "16.2 Dati ordinali",
    "text": "16.2 Dati ordinali\nSe i dati sono ordinali, è necessario utilizzare un Kappa ponderato. Ad esempio, se i valori possibili sono basso, medio e alto, allora se un caso viene valutato come medio da un codificatore e alto dall’altro, essi sarebbero in un accordo maggiore rispetto a una situazione in cui le valutazioni fossero basso e alto.\nPer chiarire ulteriormente, il concetto di Kappa ponderato si basa sull’idea che non tutte le discrepanze tra i codificatori siano ugualmente gravi. Nell’esempio dato, la differenza tra le valutazioni medio e alto è considerata meno significativa rispetto alla differenza tra basso e alto, perché le categorie sono vicine l’una all’altra nell’ordine. Il Kappa ponderato introduce quindi dei pesi per riflettere questa differenza di gravità nelle discrepanze, valutando le discrepanze minori (come tra medio e alto) meno severamente delle discrepanze maggiori (come tra basso e alto). Questo approccio è particolarmente utile in contesti dove l’ordine e la distanza tra le categorie sono informativi e importanti per l’analisi.\nUsiamo i dati forniti dal data frame anxiety del pacchetto irr.\n\ndata(anxiety)\n\ndfa &lt;- anxiety[, c(1, 2)]\ndfa\n\n\nA data.frame: 20 x 2\n\n\n\nrater1\nrater2\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n3\n3\n\n\n2\n3\n6\n\n\n3\n3\n4\n\n\n4\n4\n6\n\n\n5\n5\n2\n\n\n6\n5\n4\n\n\n7\n2\n2\n\n\n8\n3\n4\n\n\n9\n5\n3\n\n\n10\n2\n3\n\n\n11\n2\n2\n\n\n12\n6\n3\n\n\n13\n1\n3\n\n\n14\n5\n3\n\n\n15\n2\n2\n\n\n16\n2\n2\n\n\n17\n1\n1\n\n\n18\n2\n3\n\n\n19\n4\n3\n\n\n20\n3\n4\n\n\n\n\n\nPossiamo calcolare il Kappa ponderato sui giudizi di due valutatori. È possibile usare pesi lineari o quadrati delle differenze.\n\nkappa2(dfa, \"squared\")\n\n Cohen's Kappa for 2 Raters (Weights: squared)\n\n Subjects = 20 \n   Raters = 2 \n    Kappa = 0.297 \n\n        z = 1.34 \n  p-value = 0.18 \n\n\nQuesto valore di Kappa suggerisce che c’è un livello di accordo “equo” tra i due valutatori, considerando che i pesi delle discrepanze tra le valutazioni sono calcolati al quadrato. La scala convenzionale per interpretare Kappa è la seguente: valori ≤ 0 indicano nessun accordo, 0.01–0.20 leggero, 0.21–0.40 equo, 0.41–0.60 moderato, 0.61–0.80 sostanziale, e 0.81–1.00 quasi perfetto.\nIl valore di z è una misura della distanza statistica del valore di Kappa dal valore nullo (nessun accordo oltre il caso), espresso in termini di deviazioni standard. Un p-value di 0.18 indica che non c’è una significatività statistica per rifiutare l’ipotesi nulla di nessun accordo oltre il caso, al livello di significatività convenzionale di 0.05.\n\nkappa2(dfa, \"equal\")\n\n Cohen's Kappa for 2 Raters (Weights: equal)\n\n Subjects = 20 \n   Raters = 2 \n    Kappa = 0.189 \n\n        z = 1.42 \n  p-value = 0.157 \n\n\nCon pesi uguali per le discrepanze tra le valutazioni, il valore di Kappa scende a 0.189, indicando sempre un livello di accordo “leggero” verso “equo” tra i valutatori. Questo suggerisce che l’accordo non è molto forte e che le valutazioni differiscono più di quanto non facciano con i pesi al quadrato.\nCon un valore di z leggermente più alto rispetto al primo caso e un p-value di 0.157, anche qui non c’è evidenza statistica sufficiente per rifiutare l’ipotesi nulla. Il risultato implica che l’accordo osservato potrebbe ancora essere dovuto al caso, anche se il p-value è leggermente più basso qui, suggerendo una tendenza (anche se non significativa) verso un accordo maggiore rispetto al caso.\nIn conclusione, entrambi gli output indicano un livello di accordo che va da leggero a equo tra i due valutatori, con nessuna delle due misure che raggiunge la significatività statistica. Ciò suggerisce che, mentre c’è qualche grado di accordo oltre la coincidenza casuale, non è forte. La scelta dei pesi influisce leggermente sui risultati, con i pesi al quadrato che mostrano un livello di accordo leggermente superiore rispetto ai pesi uguali. Questo potrebbe riflettere la natura delle discrepanze nelle valutazioni: l’utilizzo di pesi al quadrato penalizza di più le grandi discrepanze rispetto ai pesi uguali.\n\nkappa2(dfa, \"unweighted\")\n\n Cohen's Kappa for 2 Raters (Weights: unweighted)\n\n Subjects = 20 \n   Raters = 2 \n    Kappa = 0.119 \n\n        z = 1.16 \n  p-value = 0.245 \n\n\n\nKappa = 0.119: Questo valore rappresenta il livello più basso di accordo tra i tre casi analizzati, indicando un accordo molto debole tra i valutatori. La mancanza di pesi implica che tutti i disaccordi sono stati trattati uniformemente, indipendentemente dalla loro gravità o distanza.\nz = 1.16, p-value = 0.245: Questo risultato conferma ulteriormente che l’accordo tra i valutatori non è statisticamente significativo, con un p-value ancora più alto rispetto ai casi precedenti, suggerendo una forte possibilità che qualsiasi accordo osservato sia casuale.\n\nI diversi valori di Kappa nei tre scenari riflettono l’effetto della ponderazione (o mancanza di essa) sulla valutazione dell’accordo. L’accordo sembra diminuire man mano che si passa da pesi quadrati a pesi uguali e infine a nessuna ponderazione, indicando che la severità dei disaccordi ha un impatto notevole sull’accordo percepito.\nNessuno dei tre scenari ha mostrato un accordo statisticamente significativo tra i valutatori, come indicato dai p-value superiori a 0.05. Ciò suggerisce che, indipendentemente dal metodo di ponderazione utilizzato, l’accordo osservato tra i valutatori potrebbe non essere distintamente migliore di quello che ci si aspetterebbe per caso.\nLa scelta del metodo di ponderazione può influenzare fortemente la stima dell’accordo tra i valutatori. In contesti dove la gravità dei disaccordi è importante, i pesi possono fornire una misura più accurata dell’accordo effettivo. Tuttavia, la mancanza di significatività statistica in tutti e tre i casi solleva questioni sulla coerenza delle valutazioni o sulla possibile necessità di formazione aggiuntiva per i valutatori per migliorare l’affidabilità delle loro valutazioni.\nI dati usati in precedenza erano numerici, ma il Kappa ponderato può anche essere calcolato nel caso di dati categoriali. In R i dati categoriali sono codificati in termini di fattori. Si noti che i livelli dei fattori devono essere nell’ordine corretto, altrimenti i risultati saranno errati.\n\ndfa2 &lt;- dfa\ndfa2$rater1 &lt;- factor(dfa2$rater1, levels = 1:6, labels = LETTERS[1:6])\ndfa2$rater2 &lt;- factor(dfa2$rater2, levels = 1:6, labels = LETTERS[1:6])\ndfa2 |&gt; head()\n\n\nA data.frame: 6 x 2\n\n\n\nrater1\nrater2\n\n\n\n&lt;fct&gt;\n&lt;fct&gt;\n\n\n\n\n1\nC\nC\n\n\n2\nC\nF\n\n\n3\nC\nD\n\n\n4\nD\nF\n\n\n5\nE\nB\n\n\n6\nE\nD\n\n\n\n\n\n\nlevels(dfa2$rater1) |&gt; print()\n\n[1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\"\n\n\n\nlevels(dfa2$rater2) |&gt; print()\n\n[1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\"\n\n\nI risultati sono uguali a quelli ottenuti con i dati numerici.\n\nkappa2(dfa2, \"squared\")\n\n Cohen's Kappa for 2 Raters (Weights: squared)\n\n Subjects = 20 \n   Raters = 2 \n    Kappa = 0.297 \n\n        z = 1.34 \n  p-value = 0.18 \n\n\n\nkappa2(dfa2, \"equal\")\n\n Cohen's Kappa for 2 Raters (Weights: equal)\n\n Subjects = 20 \n   Raters = 2 \n    Kappa = 0.189 \n\n        z = 1.42 \n  p-value = 0.157",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/raters/E1_irr.html#dati-continui",
    "href": "chapters/raters/E1_irr.html#dati-continui",
    "title": "16  ✏️ Esercizi",
    "section": "16.3 Dati continui",
    "text": "16.3 Dati continui\nQuando ci si confronta con variabili continue, è necessario calcolare il Coefficiente di Correlazione Intraclasse (ICC) per valutare l’accordo tra le misurazioni. La selezione dell’ICC appropriato richiede l’attenzione su diversi aspetti (Shrout e Fleiss, 1979). Questi includono:\n\nSelezione del Modello: È cruciale determinare se trattare esclusivamente i soggetti come effetti casuali, seguendo il modello “oneway” (impostazione predefinita), oppure se sia soggetti che valutatori sono stati scelti casualmente da un pool più ampio, adottando il modello “twoway”. Tale decisione dipende dalla struttura del disegno di studio e dall’obiettivo dell’analisi.\nInteresse per le Differenze: Nel caso in cui si desideri esaminare le differenze nei punteggi medi tra i valutatori, è preferibile calcolare l’“accordo” anziché la “coerenza” (quest’ultima è l’opzione predefinita). Questa scelta è cruciale quando le differenze sistematiche tra i valutatori sono rilevanti per l’analisi.\nUnità di Analisi: È necessario decidere se l’unità di analisi dovrebbe essere la media di più punteggi o se mantenere l’analisi su valori singoli (impostazione predefinita, unità=“singola”). La trasformazione dell’unità di analisi in “media” può essere appropriata quando l’interesse è concentrato sulla stima aggregata delle misurazioni, mentre l’opzione “singola” è generalmente preferita per esaminare la variabilità tra misurazioni individuali.\n\nLa scelta tra queste opzioni dovrebbe essere guidata dagli obiettivi specifici della ricerca, dalla natura dei dati e dal contesto in cui le misurazioni sono state raccolte. Tali decisioni influenzano notevolmente l’interpretazione del Coefficiente di Correlazione Intraclasse e, di conseguenza, le conclusioni che possono essere tratte riguardo all’accordo o alla variabilità delle misurazioni all’interno del campione studiato.\nPer illustrare questi concetti, useremo il set di dati anxiety dal pacchetto “irr”.\n\ndata(anxiety)\nanxiety |&gt; head()\n\n\nA data.frame: 6 x 3\n\n\n\nrater1\nrater2\nrater3\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n3\n3\n2\n\n\n2\n3\n6\n1\n\n\n3\n3\n4\n4\n\n\n4\n4\n6\n4\n\n\n5\n5\n2\n3\n\n\n6\n5\n4\n2\n\n\n\n\n\n\nicc(anxiety, model = \"twoway\", type = \"agreement\")\n\n Single Score Intraclass Correlation\n\n   Model: twoway \n   Type : agreement \n\n   Subjects = 20 \n     Raters = 3 \n   ICC(A,1) = 0.198\n\n F-Test, H0: r0 = 0 ; H1: r0 &gt; 0 \n F(19,39.7) = 1.83 , p = 0.0543 \n\n 95%-Confidence Interval for ICC Population Values:\n  -0.039 &lt; ICC &lt; 0.494\n\n\nPossiamo replicare questo risultato usando un modello ad effetti misti. Per fare questo dobbiamo prima trasformare i dati in formato long.\n\nanxiety_long &lt;- anxiety %&gt;%\n    mutate(subject = row_number()) %&gt;%\n    pivot_longer(\n        cols = starts_with(\"rater\"),\n        names_to = \"rater\",\n        values_to = \"score\",\n        names_prefix = \"rater\"\n    )\nglimpse(anxiety_long)\n\nRows: 60\nColumns: 3\n$ subject &lt;int&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7,~\n$ rater   &lt;chr&gt; \"1\", \"2\", \"3\", \"1\", \"2\", \"3\", \"1\", \"2\", \"3\", \"1\", \"2\", \"3\", \"1~\n$ score   &lt;int&gt; 3, 3, 2, 3, 6, 1, 3, 4, 4, 4, 6, 4, 5, 2, 3, 5, 4, 2, 2, 2, 1,~\n\n\nPossiamo ora adattare il modello misto ai dati.\n\nmodel &lt;- lmer(score ~ 1 + (1 | subject) + (1 | rater), data = anxiety_long)\n\nIl procedimento utilizzato da lmer per modellare i dati e calcolare componenti di varianza, che poi possono essere utilizzati per stimare l’Intraclass Correlation Coefficient (ICC), si basa sull’analisi degli effetti misti. Ecco una spiegazione dettagliata del processo:\n\nDefinizione del Modello: lmer(score ~ 1 + (1 | subject) + (1 | rater), data = anxiety_long) specifica un modello lineare misto, dove score è la variabile dipendente. La notazione (1 | subject) indica che si vuole considerare un effetto casuale per ogni soggetto (subject) che corrisponde unicamente all’intercetta. Analogamente, (1 | rater) indica un effetto casuale, corrispondente all’intercetta, per ogni valutatore (rater), permettendo che ogni valutatore possa avere una sua propria tendenza generale nella valutazione. Il termine 1 rappresenta l’intercetta fissa comune a tutti i dati.\nEffetti Fissi e Casuali: In questo modello, l’unico effetto fisso è l’intercetta globale (il termine 1 nel modello), che rappresenta la media generale dei punteggi di ansia. Gli effetti casuali sono rappresentati dalle varianze associate a ciascun soggetto e valutatore, indicando che si riconosce la presenza di variazioni individuali nei punteggi di ansia attribuibili a questi due fattori.\nInterpretazione delle Componenti di Varianza: Le componenti di varianza estratte dal modello rappresentano la quantità di variazione nei punteggi di ansia attribuibili a variazioni tra soggetti (var_subject), a variazioni tra valutatori (var_rater), e alla variazione residua non spiegata dal modello (var_residual).\n\nDopo aver stimato le componenti di varianza, l’ICC può essere calcolato come una proporzione della varianza tra soggetti e tra giudici rispetto alla varianza totale.\nEsaminiamo le componenti di varianza:\n\nvc &lt;- as.data.frame(VarCorr(model))\nvc\n\n\nA data.frame: 3 x 5\n\n\ngrp\nvar1\nvar2\nvcov\nsdcor\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nsubject\n(Intercept)\nNA\n0.3991227\n0.6317616\n\n\nrater\n(Intercept)\nNA\n0.1684205\n0.4103906\n\n\nResidual\nNA\nNA\n1.4482458\n1.2034308\n\n\n\n\n\nCalcolo dell’ICC:\n\n(vc$vcov[1] + vc$vcov[2]) / (vc$vcov[1] + vc$vcov[2] + vc$vcov[3])\n\n0.281548906954477\n\n\nQuesto risultato riproduce quello trovato da icc():\n\nperformance::icc(model)\n\n\nA icc: 1 x 3\n\n\nICC_adjusted\nICC_conditional\nICC_unadjusted\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0.2815489\n0.2815489\n0.2815489",
    "crumbs": [
      "Giudici",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/validity/01_validity.html",
    "href": "chapters/validity/01_validity.html",
    "title": "17  La validità del test",
    "section": "",
    "text": "17.1 Introduzione\nOltre all’affidabilità, la validità rappresenta la seconda caratteristica essenziale che uno strumento psicometrico deve avere. La validità è una proprietà psicometrica fondamentale dei test psicologici. La definizione degli Standards for educational and psychological testing (American Educational Research Association et al., 2014) è la seguente:\nIn altre parole, la validità riguarda sia il significato dei punteggi del test che il modo in cui li utilizziamo. Pertanto, la validità è giustamente “la considerazione più fondamentale nello sviluppo e nella valutazione dei test”, come indicato negli Standards (p. 11).\nIl concetto di validità, un tempo circoscritto alla triade contenuto-criterio-costrutto, si è evoluto in un quadro concettuale più ampio e dinamico. Gli Standards affermano:\nDi conseguenza, la maggior parte delle concezioni moderne di validità enfatizzano un’integrazione di tutte le forme di evidenza utili a chiarire il significato(i) che possono essere attribuiti ai punteggi del test. Spetta all’utente del test valutare le prove disponibili per giudicare in che misura la sua interpretazione o utilizzo previsto sia appropriato.\nNel campo della psicometria, esiste un consenso sul fatto che i concetti tradizionali di validità, legati direttamente a un test, siano stati superati. Oggi si riconosce che la validità non riguarda il test in sé, ma l’adeguatezza e l’accuratezza delle interpretazioni dei punteggi ottenuti. In altre parole, non è corretto parlare di “validità di un test”. La validità si riferisce alle interpretazioni che vengono fatte dei punteggi del test.\nPertanto, non è corretto chiedere: “Il Wechsler Intelligence Scale for Children—Quinta Edizione (WISC-V) è un test valido?”. La domanda più appropriata sarebbe: “È valida l’interpretazione delle prestazioni sul WISC-V come misura dell’intelligenza?”. La validità dipende sempre dal contesto dell’interpretazione: cosa significa ottenere un certo punteggio su questo test? La validità si applica all’interpretazione di questo risultato, non al test stesso.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>La validità del test</span>"
    ]
  },
  {
    "objectID": "chapters/validity/01_validity.html#introduzione",
    "href": "chapters/validity/01_validity.html#introduzione",
    "title": "17  La validità del test",
    "section": "",
    "text": "Validity refers to the degree to which evidence and theory support the interpretations of test scores for proposed uses of tests.\n\n\n\n\nValidity is a unitary concept. It is the degree to which all the accumulated evidence supports the intended interpretation of test scores for the proposed use. Like the 1999 Standards, this edition refers to types of validity evidence, rather than distinct types of validity. To emphasize this distinction, the treatment that follows does not follow historical nomenclature (i.e., the use of the terms content validity or predictive validity). (2014, p. 14)",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>La validità del test</span>"
    ]
  },
  {
    "objectID": "chapters/validity/01_validity.html#minacce-alla-validità",
    "href": "chapters/validity/01_validity.html#minacce-alla-validità",
    "title": "17  La validità del test",
    "section": "17.2 Minacce alla Validità",
    "text": "17.2 Minacce alla Validità\nLa validità di un test può essere compromessa quando non riesce a misurare in modo completo o accurato il costrutto di interesse, oppure quando il test valuta aspetti non rilevanti per quel costrutto. Anche test con alta affidabilità possono essere vulnerabili a queste problematiche, portando a interpretazioni distorte dei risultati. In questa sezione, esamineremo i principali tipi di validità e le relative evidenze, discutendo come integrare diverse fonti di prova per costruire un argomento solido a supporto della validità di un test.\n\n17.2.1 Sotto-Rappresentazione del Costrutto\nLa sotto-rappresentazione del costrutto si verifica quando il test non riesce a misurare aspetti cruciali del costrutto target. Ad esempio, un test di matematica per la terza elementare che valuta solo la divisione non rappresenta adeguatamente tutte le competenze matematiche previste per quel livello scolastico. Per affrontare questa lacuna, è necessario ampliare il contenuto del test per includere tutte le abilità matematiche rilevanti nel curriculum della terza elementare.\n\n\n17.2.2 Varianza Estranea al Costrutto\nLa varianza estranea al costrutto si verifica quando il test misura, involontariamente, caratteristiche o competenze non pertinenti al costrutto che dovrebbe valutare. Un esempio è un test di matematica che richiede un elevato livello di comprensione del testo, finendo per misurare anche la capacità di lettura, oltre alle competenze matematiche. Per ridurre questa varianza estranea, è essenziale che il test sia progettato con istruzioni semplici e che il livello di lettura richiesto sia adeguato alla popolazione di riferimento.\n\n\n17.2.3 Altri Fattori Che Influenzano la Validità\nOltre alle caratteristiche intrinseche del test, ci sono fattori esterni che possono influenzare la validità delle interpretazioni dei risultati. Questi includono:\n\nCaratteristiche dell’Esaminando:\n\nFattori personali, come ansia, bassa motivazione o distrazioni, possono influenzare le prestazioni e ridurre la validità delle interpretazioni dei punteggi.\n\nProcedure di Amministrazione e Valutazione:\n\nQualsiasi deviazione dalle procedure standard di somministrazione può compromettere la validità. Anche gli adattamenti per esigenze speciali devono essere gestiti con cura per garantire che le interpretazioni dei risultati rimangano valide.\n\nIstruzione e Coaching:\n\nIstruzioni o coaching specifici prima del test possono alterare la validità, soprattutto se gli esaminandi vengono addestrati a rispondere a particolari tipologie di domande, distorcendo così l’interpretazione delle loro competenze reali.\n\n\nInfine, la validità delle interpretazioni basate su punteggi norm-referenced (cioè confronti rispetto a un gruppo di riferimento) dipende dall’adeguatezza e rappresentatività del campione di riferimento utilizzato per il confronto.\nIn sintesi, le minacce alla validità richiedono un’attenta valutazione. Solo affrontando questi fattori sarà possibile garantire che le interpretazioni dei risultati del test siano appropriate. Il processo di validazione deve quindi considerare sia il contenuto e la struttura del test sia le influenze esterne che possono distorcere le conclusioni tratte dai punteggi.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>La validità del test</span>"
    ]
  },
  {
    "objectID": "chapters/validity/01_validity.html#tipologie-di-validità",
    "href": "chapters/validity/01_validity.html#tipologie-di-validità",
    "title": "17  La validità del test",
    "section": "17.3 Tipologie di Validità",
    "text": "17.3 Tipologie di Validità\n\n17.3.1 Contestualizzazione Storica ed Evoluzione Terminologica\nGli standard per la creazione e l’uso dei test educativi e psicologici, come quelli presentati negli Standards del 2014 (American Educational Research Association et al., 2014), hanno subito un’evoluzione significativa nel corso degli anni. Inizialmente, la validità era suddivisa in tre categorie principali, come proposto da Messick (1989): validità di contenuto, validità criteriale e validità di costrutto.\n\nValidità di Contenuto: Misura la pertinenza e la rappresentatività del contenuto del test rispetto al dominio del costrutto. Si basa su giudizi esperti che valutano quanto bene il contenuto del test rifletta il costrutto che si intende misurare.\nValidità Criteriale: Si concentra sulla relazione tra i punteggi del test e variabili esterne legate al costrutto misurato, utilizzando analisi di correlazione o di regressione.\nValidità di Costrutto: Integra varie prove che dimostrano il significato e l’interpretazione dei punteggi del test, per assicurarsi che misuri effettivamente il costrutto in questione.\n\nCon il tempo, queste categorie distinte sono state sostituite da un approccio unitario alla validità, che considera le diverse tipologie come modalità complementari per raccogliere prove a supporto delle interpretazioni dei punteggi di un test. Gli Standards del 1985 (APA et al., 1985) hanno introdotto la terminologia di “tipi di prove di validità”, sostituendo la precedente classificazione.\nGli Standards del 2014 (American Educational Research Association et al., 2014) hanno ulteriormente sviluppato questa visione olistica, chiarendo che la validità è definita come il grado in cui tutte le evidenze disponibili supportano l’interpretazione prevista dei punteggi del test per uno specifico scopo. Le prove di validità sono ora suddivise in cinque categorie principali:\n\nProve Basate sul Contenuto del Test: Esaminano la corrispondenza tra il contenuto del test e il costrutto che si intende misurare.\nProve Basate sui Processi di Risposta: Analizzano i processi cognitivi e comportamentali utilizzati dagli esaminandi per rispondere agli item del test.\nProve Basate sulla Struttura Interna: Valutano la coerenza tra gli elementi del test e le dimensioni teoriche che rappresentano il costrutto.\nProve Basate sulle Relazioni con Altre Variabili: Esaminano la correlazione tra i punteggi del test e variabili esterne rilevanti.\nProve Basate sulle Conseguenze del Test: Valutano le implicazioni, previste e non, derivanti dall’uso del test.\n\n\n\n17.3.2 Ottenere Evidenze per la Validità\nLa definizione di validità come “il grado in cui le evidenze e la teoria supportano le interpretazioni dei punteggi del test per gli utilizzi proposti” implica che non è possibile ottenere prove per tutte le possibili interpretazioni o utilizzi di un test. Pertanto, il primo passo nel processo di validazione è la specificazione delle interpretazioni e degli utilizzi previsti dei punteggi. Di seguito vengono esplorati in dettaglio i cinque tipi di prove di validità delineati negli Standards del 2014 (American Educational Research Association et al., 2014).\n1. Prove Basate sul Contenuto del Test\nL’evidenza basata sul contenuto riguarda la misura in cui il contenuto di un test rappresenta adeguatamente il dominio che intende misurare. Tuttavia, nei test pratici, non è sempre possibile coprire interamente il dominio del costrutto. Ciò può portare a due problemi: sotto-rappresentazione del costrutto o varianza estranea, entrambi minacce alla validità. Per mitigare queste minacce, i test devono essere attentamente progettati per garantire che il loro contenuto rifletta il dominio in modo equilibrato.\nQuesta forma di validità è spesso valutata da esperti che giudicano la corrispondenza tra il contenuto del test e il costrutto. Durante la revisione, si esaminano due aspetti principali: la rilevanza degli item (ogni item rappresenta adeguatamente il costrutto) e la copertura del contenuto (se l’insieme degli item copre in modo completo il costrutto).\nLa validità di faccia, che riguarda l’apparente plausibilità di un test agli occhi degli esaminandi o dei non esperti, non è considerata una misura tecnica della validità ma può influenzare la cooperazione degli esaminandi.\n2. Prove Basate sui Processi di Risposta\nQuesto tipo di evidenza valuta se i processi cognitivi che gli esaminandi utilizzano per rispondere al test riflettono il costrutto che il test intende misurare. Per esempio, in un test di ragionamento matematico, è essenziale che i partecipanti applichino strategie di risoluzione dei problemi piuttosto che semplici procedure meccaniche.\nI processi di risposta possono essere valutati tramite interviste, analisi dei tempi di risposta, o monitoraggio dei movimenti oculari. Anche i criteri utilizzati dai valutatori nel punteggio possono essere inclusi in questo tipo di evidenza. L’obiettivo è garantire che il processo di risoluzione degli item sia coerente con il costrutto che si desidera misurare.\n3. Prove Basate sulla Struttura Interna\nL’evidenza basata sulla struttura interna si concentra sulle relazioni tra gli elementi del test e la loro coerenza con le dimensioni teoriche del costrutto. L’analisi fattoriale è uno strumento chiave per esaminare se la struttura interna del test riflette le dimensioni ipotizzate. Questo tipo di evidenza è particolarmente importante per i test multidimensionali, come quelli di personalità, in cui ci si aspetta che diverse dimensioni siano rappresentate coerentemente dagli item.\n4. Prove Basate sulle Relazioni con Altre Variabili\nLe prove basate sulle relazioni con altre variabili rappresentano una parte cruciale nella raccolta di evidenze di validità. Questa forma di validità si concentra sulla relazione tra i punteggi di un test e altre misure esterne rilevanti, contribuendo a dimostrare che il test misura effettivamente il costrutto previsto. Le relazioni con altre variabili possono includere correlazioni con test simili, con criteri di riferimento specifici, o con variabili teoricamente collegate. Questo tipo di validità si suddivide principalmente in due categorie: validità convergente e validità discriminante.\nValidità convergente si riferisce al grado in cui i punteggi del test in questione correlano con i punteggi di altri test che misurano lo stesso costrutto o costrutti strettamente correlati. Se il test è valido, ci si aspetta che le sue misurazioni siano coerenti con quelle ottenute attraverso altri strumenti simili. Ad esempio, se stiamo validando un nuovo test di intelligenza, i suoi punteggi dovrebbero avere una forte correlazione con quelli di altri test noti per misurare l’intelligenza. Questo tipo di prova fornisce evidenza di validità poiché dimostra che il test misura effettivamente il costrutto che dichiara di misurare.\nInvece, la validità discriminante (o divergenza) riguarda la capacità di un test di non correlarsi con misure di costrutti che dovrebbero essere distinti. Per esempio, in un test di intelligenza, ci si aspetta che i punteggi non correlino con misure di abilità fisiche, poiché queste ultime non sono teoricamente collegate al costrutto dell’intelligenza. La mancanza di correlazione con costrutti irrilevanti è una prova che il test è focalizzato e non influenzato da fattori estranei.\nUna delle relazioni più rilevanti da considerare è quella tra il test e una variabile criterio, un concetto che si collega alla validità criteriale. Un test criteriale viene valutato confrontando i suoi punteggi con una misura esterna o criterio di riferimento, come il rendimento accademico, la performance lavorativa o altri esiti comportamentali che si desidera predire. Esistono due tipi principali di validità criteriale:\n\nValidità predittiva: Si riferisce alla capacità del test di predire un criterio futuro. Ad esempio, un test utilizzato per le assunzioni dovrebbe essere in grado di predire le future performance lavorative. In questo tipo di studio, si amministra il test, si attendono i risultati del criterio (come il successo lavorativo) e si analizza la correlazione tra i due.\nValidità concorrente: Si verifica quando il test e il criterio vengono misurati contemporaneamente, per valutare se i punteggi del test siano coerenti con una misura esistente di un costrutto simile. Ad esempio, un test di abilità verbali somministrato contemporaneamente a un altro noto per misurare la stessa abilità dovrebbe mostrare una correlazione significativa.\n\nIn entrambi i casi, la correlazione tra il test e la variabile criterio, nota come coefficiente di validità, fornisce una misura quantitativa dell’efficacia del test nel predire o correlarsi con il criterio. Valori elevati indicano una forte relazione, suggerendo che il test è valido per l’uso a cui è destinato.\nUn’altra fonte di evidenza di validità basata sulle relazioni con altre variabili riguarda il confronto tra gruppi. La teoria psicometrica può prevedere che diversi gruppi di persone, definiti da criteri esterni, mostrino differenze nei punteggi del test. Ad esempio, ci si aspetta che un test di intelligenza mostri differenze significative nei punteggi tra gruppi di persone con abilità intellettive tipiche e persone con disabilità intellettive. Se il test è valido, i risultati dovrebbero riflettere queste differenze teoriche, fornendo ulteriore evidenza di validità.\nQuando un test viene utilizzato per classificare o diagnosticare gli individui, sono fondamentali i concetti di sensibilità e specificità. La sensibilità di un test misura la sua capacità di identificare correttamente i casi positivi (ovvero, le persone che possiedono il costrutto o la condizione che il test mira a misurare). La specificità, d’altra parte, si riferisce alla capacità del test di escludere correttamente i casi negativi (ovvero, le persone che non possiedono il costrutto o la condizione). Un test valido dovrebbe bilanciare sensibilità e specificità per garantire accuratezza nel rilevare le condizioni target e minimizzare i falsi positivi e negativi.\nLa generalizzazione della validità si riferisce alla misura in cui le relazioni osservate tra test e criteri possono essere estese a nuove popolazioni, contesti o condizioni senza ulteriori studi. Studi di meta-analisi hanno dimostrato che molte delle differenze apparenti nei coefficienti di validità tra studi possono essere dovute a artefatti statistici o metodologici. Questo suggerisce che la validità di un test, una volta stabilita in un contesto, può spesso essere generalizzata in altri contesti simili. Tuttavia, è essenziale condurre ulteriori studi per confermare la validità quando si applica un test in un nuovo contesto o con una popolazione diversa.\nIn conclusione, le prove basate sulle relazioni con altre variabili rappresentano una componente cruciale nella valutazione della validità di un test psicometrico. Forniscono una visione chiara di come il test si comporta rispetto a misure esterne rilevanti, predice risultati importanti e differenzia tra gruppi distinti. Attraverso l’analisi di validità convergente, discriminante, criteriale e predittiva, così come tramite il confronto di gruppi e l’uso di sensibilità e specificità, queste evidenze contribuiscono a dimostrare che il test è effettivamente in grado di misurare il costrutto target in modo accurato e utile per le decisioni pratiche.\n5. Prove Basate sulle Conseguenze del Test\nLe conseguenze di un test, sia previste che impreviste, rappresentano una dimensione critica della validità. Gli effetti di un test sull’individuo o sulla società, come l’impatto di un test di ammissione scolastica o un test psicologico utilizzato in contesti forensi, devono essere attentamente considerati. Anche le conseguenze indesiderate, come l’uso improprio del test o la creazione di disuguaglianze, devono essere prese in esame.\nIn conclusione, l’evoluzione del concetto di validità riflette una comprensione sempre più approfondita della complessità delle misurazioni psicometriche. Oggi, la validità è considerata un concetto olistico, che si basa su prove raccolte da diverse fonti per supportare le interpretazioni e gli utilizzi dei punteggi del test. I professionisti devono integrare queste diverse forme di evidenza per costruire un argomento convincente a favore della validità delle interpretazioni dei punteggi, garantendo al contempo che i test siano utilizzati in modo appropriato, equo e responsabile. Hai ragione, il contenuto sulla “Evidenza di Validità Basata sulle Relazioni con Altre Variabili” appartiene a una sezione separata e non dovrebbe essere discusso in una trattazione sulle conseguenze del testing. Di seguito ti propongo una versione migliorata, più allineata con i contenuti degli Standards nella loro versione più recente, e senza includere sezioni inappropriate.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>La validità del test</span>"
    ]
  },
  {
    "objectID": "chapters/validity/01_validity.html#conseguenze-intenzionali-e-non-intenzionali",
    "href": "chapters/validity/01_validity.html#conseguenze-intenzionali-e-non-intenzionali",
    "title": "17  La validità del test",
    "section": "17.4 Conseguenze Intenzionali e Non Intenzionali",
    "text": "17.4 Conseguenze Intenzionali e Non Intenzionali\nGli Standards sottolineano l’importanza di considerare le conseguenze derivanti dall’uso dei test, distinguendo tra conseguenze intenzionali e conseguenze non intenzionali. Queste conseguenze rappresentano un elemento critico nella valutazione della validità, in quanto l’uso di un test può avere effetti previsti e positivi, ma anche effetti imprevisti o negativi che possono compromettere l’integrità e l’etica del processo di misurazione.\n\n17.4.1 Conseguenze Intenzionali\nLe conseguenze intenzionali del testing riguardano gli obiettivi dichiarati per cui un test viene utilizzato. Ad esempio, un test di selezione per un ruolo lavorativo ha lo scopo di identificare i candidati più qualificati per quella posizione. Analogamente, un test di rendimento scolastico mira a valutare le competenze degli studenti rispetto a uno standard educativo.\nLa validità consequenziale si riferisce alla misura in cui un test produce i risultati desiderati e contribuisce agli scopi per i quali è stato progettato. Per esempio, un test impiegato per l’assunzione dovrebbe portare a decisioni di reclutamento migliori, con conseguenti benefici come una riduzione del turnover e dei costi di formazione. Gli Standards evidenziano la necessità di monitorare tali benefici per assicurarsi che gli scopi del test siano effettivamente raggiunti e giustificati.\n\n\n17.4.2 Conseguenze Non Intenzionali\nLe conseguenze non intenzionali si riferiscono a quegli effetti che emergono dall’uso di un test ma non erano previsti o desiderati. Questi possono includere discriminazioni non giustificate, disparità nell’accesso alle risorse o l’induzione di ansia nei partecipanti. Le conseguenze non intenzionali possono minacciare la validità del test, in particolare quando generano effetti distorti o ingiusti che violano i principi di equità.\nUn esempio potrebbe essere un test che, pur essendo valido per la selezione di lavoratori, penalizza sistematicamente candidati di gruppi etnici o socioeconomici svantaggiati. Questo potrebbe essere un segnale di una distorsione non intenzionale nel test o nei suoi criteri di valutazione.\n\n\n17.4.3 Valutazione delle Conseguenze\nSecondo gli Standards, è essenziale esaminare le conseguenze sia previste che non previste, nonché le implicazioni etiche dell’uso del test. La valutazione delle conseguenze dovrebbe essere parte integrante del processo di validazione, assicurando che il test non causi danni e che sia utilizzato in modo responsabile. Questa valutazione è cruciale per garantire che le misurazioni fornite dal test siano significative, eticamente corrette e utili per gli scopi previsti.\n\n17.4.3.1 Conseguenze Sociali e Politiche\nGli Standards fanno una distinzione tra le conseguenze legate alla validità e quelle associate a questioni sociali o politiche. Anche se è importante considerare l’impatto sociale dell’uso dei test, includere questioni di valore all’interno della validità stessa può complicare il quadro. Gli Standards invitano a considerare le conseguenze sociali e politiche come parte delle responsabilità etiche legate all’uso dei test, ma distinguono queste questioni dall’evidenza strettamente legata alla validità.\n\n\n17.4.3.2 Considerazione delle Alternative all’Uso del Test\nÈ importante valutare anche gli effetti di non utilizzare un test o di adottare un approccio alternativo. L’abbandono dei test standardizzati potrebbe portare a decisioni basate su criteri meno oggettivi, come valutazioni soggettive o pregiudizi personali. Gli Standards riconoscono che, pur con i loro limiti, i test strutturati possono offrire un grado di equità superiore rispetto a metodi di valutazione meno formali, che potrebbero essere maggiormente soggetti a distorsioni culturali, etniche o di genere.\n\n\n\n17.4.4 Validità Consequenziale e Responsabilità Etica\nGli Standards sottolineano l’importanza della validità consequenziale, ovvero l’analisi delle conseguenze che derivano dall’uso di un test. Questa responsabilità non si esaurisce nel garantire che il test misuri accuratamente il costrutto di interesse, ma include anche la considerazione di come i risultati vengono utilizzati e delle conseguenze che ne derivano.\nGli sviluppatori e gli utilizzatori di test hanno quindi la responsabilità di monitorare costantemente gli effetti dell’uso del test, garantendo che le conseguenze indesiderate vengano ridotte al minimo. Questo richiede un continuo processo di revisione e aggiornamento, nonché un’attenzione particolare all’equità e alla giustizia nell’uso dei test.\nIn sintesi, l’analisi delle conseguenze intenzionali e non intenzionali è un elemento fondamentale nella valutazione della validità di un test psicometrico. Le conseguenze del test, sia positive che negative, devono essere costantemente monitorate e analizzate per garantire che l’uso del test sia eticamente corretto e coerente con i suoi scopi dichiarati. Questo tipo di validità consequenziale è essenziale per l’uso responsabile dei test e contribuisce a mantenere un equilibrio tra l’efficacia del test e il rispetto dei diritti degli esaminandi.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>La validità del test</span>"
    ]
  },
  {
    "objectID": "chapters/validity/01_validity.html#integrazione-delle-prove-di-validità",
    "href": "chapters/validity/01_validity.html#integrazione-delle-prove-di-validità",
    "title": "17  La validità del test",
    "section": "17.5 Integrazione delle Prove di Validità",
    "text": "17.5 Integrazione delle Prove di Validità\nGli Standards descrivono la validità come un processo continuo di costruzione di un argomento coerente e supportato da evidenze a favore dell’interpretazione e dell’uso dei punteggi di un test. L’integrazione delle diverse prove di validità è cruciale per garantire che le interpretazioni dei risultati siano appropriate e sostenute da evidenze robuste.\n\n17.5.1 Come si realizza l’integrazione delle prove di validità?\nL’integrazione delle prove di validità comporta la combinazione di diverse fonti di evidenza per costruire un argomento completo e coerente che giustifichi l’uso del test per uno scopo specifico. Questo processo si realizza raccogliendo diverse linee di prova, che possono includere prove basate sul contenuto, sulla struttura interna, sui processi di risposta, sulle relazioni con altre variabili e sulle conseguenze del test.\nL’integrazione non avviene in modo meccanico, ma richiede una riflessione critica su come ogni prova contribuisca all’argomento complessivo. Ogni tipo di prova fornisce un’informazione parziale, e il loro insieme contribuisce a creare un quadro completo della validità del test. Ad esempio, se un test mostra coerenza interna ma non è in grado di predire accuratamente i criteri per cui è stato progettato, la validità potrebbe essere compromessa. Viceversa, l’integrazione di evidenze positive da più fonti rafforza la giustificazione per l’uso del test.\n\n\n17.5.2 A cosa serve l’integrazione delle prove di validità?\nL’integrazione delle prove di validità serve a supportare l’argomento secondo cui i punteggi del test sono appropriati per l’interpretazione e l’uso previsto. Questo approccio consente di ottenere una visione olistica della validità del test e di garantire che le diverse dimensioni della validità siano state considerate in modo approfondito. L’obiettivo finale è dimostrare che il test è non solo tecnicamente affidabile, ma anche giustificato eticamente e utilizzabile per prendere decisioni informate.\nLa validazione non è mai un processo singolare o statico. Un argomento di validità ben costruito considera come le diverse evidenze interagiscono per confermare o smentire l’uso del test in contesti specifici. Gli Standards sottolineano che la validità non è una proprietà del test in sé, ma riguarda le interpretazioni e gli utilizzi dei punteggi. Pertanto, ogni volta che il test viene applicato in un nuovo contesto o con un obiettivo diverso, è necessario rivalutare la validità delle interpretazioni, raccogliendo nuove prove se necessario.\n\n\n17.5.3 Il Ruolo della Continuità e della Ricerca\nL’integrazione delle prove di validità non si conclude con lo sviluppo iniziale del test. Al contrario, è un processo continuo che si evolve con il tempo, man mano che vengono condotte nuove ricerche o che cambiano i contesti d’uso del test. Gli Standards evidenziano che, oltre alle prove fornite dai creatori del test, la ricerca indipendente svolge un ruolo fondamentale nel mantenere e aggiornare l’argomento di validità. Studi successivi all’adozione del test, condotti da ricercatori indipendenti, possono rafforzare, modificare o persino contraddire le evidenze iniziali, contribuendo così a una comprensione più completa della validità.\nIn sintesi, l’integrazione delle prove di validità è un processo critico per costruire un argomento solido a sostegno dell’uso e dell’interpretazione dei punteggi di un test. Essa comporta la raccolta e la sintesi di diverse linee di evidenza, ciascuna delle quali contribuisce a illuminare un aspetto particolare della validità. L’obiettivo finale è garantire che i test siano non solo tecnicamente adeguati, ma anche utili e giustificabili per gli scopi previsti, e che continuino a esserlo nel tempo grazie alla continua ricerca e revisione.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>La validità del test</span>"
    ]
  },
  {
    "objectID": "chapters/validity/01_validity.html#considerazioni-conclusive",
    "href": "chapters/validity/01_validity.html#considerazioni-conclusive",
    "title": "17  La validità del test",
    "section": "17.6 Considerazioni conclusive",
    "text": "17.6 Considerazioni conclusive\nNel campo della psicometria, la validità rappresenta un concetto dinamico, sfaccettato e complesso, che richiede un’integrazione critica di molteplici forme di evidenza. Questo capitolo ha esplorato le diverse dimensioni della validità, mettendo in luce l’importanza di un approccio olistico per garantire che l’interpretazione dei punteggi dei test sia appropriata e significativa. L’analisi della validità va ben oltre la semplice coerenza tra il contenuto del test e il costrutto target; essa include un esame rigoroso della struttura interna del test, dei processi cognitivi e comportamentali attivati nei rispondenti e delle conseguenze – sia attese che inattese – del suo utilizzo.\nGli Standards sottolineano come la validità non sia un attributo fisso del test stesso, ma una proprietà delle interpretazioni e degli utilizzi dei punteggi del test. Questo implica che il processo di validazione deve essere continuo, assimilando nuove ricerche e aggiornamenti man mano che emergono nuove evidenze. La validità si costruisce attraverso l’integrazione di diverse linee di prova – dalle relazioni con altre variabili, alla struttura interna, ai processi di risposta, fino alle conseguenze del test – ognuna delle quali contribuisce a consolidare l’argomento di validità.\nL’integrazione delle prove non si esaurisce con lo sviluppo iniziale del test, ma continua nel tempo, con una costante attenzione critica che deve accompagnare ogni nuovo contesto d’uso. Gli psicologi, pertanto, hanno la responsabilità di valutare e rivalutare l’uso dei test nel loro specifico contesto professionale, garantendo che le decisioni prese siano informate, etiche e giustificate da prove solide.\nIn conclusione, la validazione di un test psicometrico deve essere intesa come un processo evolutivo e dinamico. Non si tratta di un’analisi statica, ma di una valutazione continua della capacità del test di produrre interpretazioni affidabili e pertinenti nei diversi contesti applicativi. Questo richiede un costante impegno da parte dei professionisti nel garantire che i test siano non solo strumenti tecnicamente validi, ma anche adeguati e responsabili dal punto di vista etico e pratico. La validità, dunque, è il risultato di un’interazione tra prove empiriche, teoria e pratica, che richiede una continua revisione e miglioramento per mantenere l’efficacia e l’integrità del test nel tempo.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>La validità del test</span>"
    ]
  },
  {
    "objectID": "chapters/validity/01_validity.html#esercizi",
    "href": "chapters/validity/01_validity.html#esercizi",
    "title": "17  La validità del test",
    "section": "17.7 Esercizi",
    "text": "17.7 Esercizi\nPresentazione in classe dei lavori di Randall et al. (2023) e Randall (2021).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmerican Educational Research Association, American Psychological Association, & National Council on Measurement in Education. (2014). Standards for Educational and Psychological Testing. American Educational Research Association.\n\n\nArias, A. (2024). A Short Tutorial on Validation in Educational and Psychological Assessment. Teaching Quantitative Methods Vignettes, 20(3).\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.\n\n\nRandall, J. (2021). «Color-neutral» is not a thing: Redefining construct definition and representation through a justice-oriented critical antiracist lens. Educational Measurement: Issues and Practice, 40(4), 82–90.\n\n\nRandall, J., Slomp, D., Poe, M., & Oliveri, E. (2023). Disrupting white supremacy in assessment: Toward a justice-oriented, antiracist validity framework. In Twin Pandemics (pp. 78–86). Routledge.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>La validità del test</span>"
    ]
  },
  {
    "objectID": "chapters/validity/02_other_variables.html",
    "href": "chapters/validity/02_other_variables.html",
    "title": "18  Relazioni test-criterio",
    "section": "",
    "text": "18.1 Analisi della Relazione Test-Criterio mediante la Regressione Logistica\nIn questo capitolo, esploriamo approfonditamente la relazione tra i punteggi dei test e vari criteri esterni, focalizzandoci su come i test possono predire o differenziare fenomeni specifici. Questo tipo di analisi è cruciale per valutare la validità dei test in contesti pratici e teorici.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Relazioni test-criterio</span>"
    ]
  },
  {
    "objectID": "chapters/validity/02_other_variables.html#analisi-della-relazione-test-criterio-mediante-la-regressione-logistica",
    "href": "chapters/validity/02_other_variables.html#analisi-della-relazione-test-criterio-mediante-la-regressione-logistica",
    "title": "18  Relazioni test-criterio",
    "section": "",
    "text": "18.1.1 Categorie di Evidenze basate su Relazioni con Altre Variabili\nIn psicometria, vengono utilizzate diverse categorie di prove per valutare le relazioni dei punteggi dei test con altre variabili:\n\nRelazioni Test-Criterio:\n\nQueste si concentrano sull’uso dei punteggi dei test per prevedere il rendimento o lo stato attuale in vari ambiti, come l’adattamento accademico o lavorativo.\n\nDifferenze tra Gruppi:\n\nAnalizziamo se i test evidenziano differenze significative nei punteggi tra gruppi definiti da criteri specifici, come la presenza o assenza di una diagnosi clinica.\n\nProve di Convergenza e Discriminazione:\n\nEsploriamo se i punteggi di un test sono correlati a quelli di altri test che misurano costrutti simili (validità convergente) e se sono meno correlati a test che misurano costrutti diversi (validità discriminante).\n\n\nQuando si affronta la questione delle relazioni test-criterio, un aspetto cruciale è la selezione di un criterio appropriato e la valutazione quantitativa della relazione tra il test e il criterio. In situazioni dove il criterio è di natura categorica, come il superamento o il fallimento di un test, la regressione logistica diventa uno strumento essenziale.\nLa regressione logistica è un metodo statistico usato per analizzare la relazione tra una variabile dipendente binaria e una o più variabili indipendenti. Questa tecnica stima la probabilità che un’osservazione appartenga a una delle categorie della variabile dipendente, basandosi sui valori delle variabili indipendenti.\nSupponiamo che una variabile risposta \\(Y_i\\) (per l’osservazione \\(i\\)-esima, con \\(i = 1, \\dots, n\\)) assuma due modalità, definite come successo e insuccesso. Ogni osservazione può essere associata a un vettore di variabili esplicative (\\(x_1, \\dots, x_p\\)), ma per semplicità, considereremo il caso di una singola variabile indipendente.\nNella regressione logistica, il logaritmo delle probabilità delle due categorie è modellato come una funzione lineare delle variabili indipendenti. Questo ci permette di esaminare come i cambiamenti nelle variabili indipendenti influenzino la probabilità di successo o insuccesso.\nNel contesto dei test psicometrici, la regressione logistica può essere usata per determinare il grado in cui i punteggi di un test sono predittivi di un risultato categorico. Ad esempio, possiamo valutare la probabilità che gli studenti con determinati punteggi ad un test di ammissione universitario riescano o meno nel loro primo anno accademico.\nQuesto metodo fornisce un quadro più chiaro della validità di un test nel predire risultati specifici, migliorando così l’affidabilità e l’applicabilità dei test in vari contesti. Attraverso l’uso della regressione logistica, possiamo quindi formulare interpretazioni più precise e informate dei risultati dei test, contribuendo a una migliore comprensione della validità di uno strumento psicometrico.\nNello specifico, possiamo dire che l’obiettivo della regressione logistica è studiare la relazione tra la probabilità di risposta\n\\[\nPr(Y=1 | X=x_i) \\equiv Pr(Y_i) \\equiv \\pi_i\n\\]\ne la variabile esplicativa \\(x\\).\nLa componente sistematica del modello è espressa come funzione lineare del predittore\n\\[\n\\eta_i = logit(\\pi_i) = \\alpha + \\beta x_i.\n\\]\nLa componente aleatoria del modello suppone l’esistenza di \\(k\\) osservazioni indipendenti \\(y_1, y_2, \\dots, y_k\\), ciascuna delle quali viene trattata come la realizzazione di una variabile casuale \\(Y_i\\). Nel caso di dati ragruppati, si assume che \\(Y_i\\) abbia una distribuzione binomiale\n\\[\nY_i \\sim Bin(n_i, \\pi_i),\n\\]\ncon parametri \\(n_i\\) e \\(\\pi_i\\). Per dati individuali (uno per ciascun valore \\(x_i\\)), \\(n_i=1, \\forall i\\).\nLa funzione link del modello trasforma il predittore lineare nel valore atteso della variabile risposta condizionato alla variabile \\(X\\):\n\\[\n\\begin{equation}\n\\pi_i = g(\\eta_i) = \\frac{e^{\\alpha + \\beta x_i}}{1+e^{\\alpha + \\beta x_i}}.\n\\end{equation}\n\\] (eq-reg-logistic-prob)\nIn sintesi, la regressione logistica è una tecnica statistica che stima la probabilità che l’evento \\(Y = 1\\) si verifichi per ciascun valore della variabile indipendente \\(X\\).\n\n\n18.1.2 Un Esempio Pratico\nPer fare un esempio pratico, consideriamo i dati dello studio Pitfalls When Using Area Under the Curve to Evaluate Item Content for Early Screening Tests for Autism di Lucas, Brewer e Young (2022).\nIl campione raccolto da Nah et al. (2018) include 270 bambini di età compresa tra 12 e 36 mesi (M = 25.4, SD = 7.0). Sulla base di una diagnosi clinica eseguita secondo il DSM-5, 106 bambini sono stati diagnosticati con ASD (Autism Spectrum Disorder, Disturbo dello Spettro Autistico), 86 erano in sviluppo non tipico (non-TD) e 78 erano considerati in sviluppo tipico (TD). Qui considereremo solo i gruppi ASD e non-TD.\nIl test di interesse è l’Autism Detection in Early Childhood (ADEC). Si tratta di una checklist comportamentale composta da 16 item, appositamente sviluppata per rilevare comportamenti pre-verbali che possono essere predittivi dell’autismo nei bambini di età inferiore ai tre anni (Young, 2007).\nLeggiamo i dati.\n\ntmp_path &lt;- tempfile(fileext = \"xlsx\") # temporary file\ndownload.file(\"https://osf.io/download/tsm7x/\", destfile = tmp_path)\ndat1 &lt;- readxl::read_xlsx(tmp_path, na = \"NA\")\ndat1$asd &lt;- recode(\n    dat1$`Diagnosis(1=Non-typically developing; 2=ASD; 3=Neurotypical)`,\n    `1` = \"Non-TD\",\n    `2` = \"ASD\",\n    `3` = \"TD\"\n)\n# Filter out Neurotypical\ndat1_sub &lt;- filter(dat1, asd != \"TD\")\nglimpse(dat1_sub)\n\nRows: 192\nColumns: 21\n$ ID                                                             &lt;dbl&gt; 2, 3, 4~\n$ `Sex (1=Male; 2=Female)`                                       &lt;dbl&gt; 1, 1, 2~\n$ `Age (in Months)`                                              &lt;dbl&gt; 12, 12,~\n$ ADEC_I01                                                       &lt;dbl&gt; 0, 2, 2~\n$ ADEC_I02                                                       &lt;dbl&gt; 0, 1, 2~\n$ ADEC_I03                                                       &lt;dbl&gt; 0, 0, 0~\n$ ADEC_I04                                                       &lt;dbl&gt; 0, 1, 2~\n$ ADEC_I05                                                       &lt;dbl&gt; 0, 1, 0~\n$ ADEC_I06                                                       &lt;dbl&gt; 0, 1, 2~\n$ ADEC_I07                                                       &lt;dbl&gt; 2, 2, 2~\n$ ADEC_I08                                                       &lt;dbl&gt; 0, 1, 1~\n$ ADEC_I09                                                       &lt;dbl&gt; 0, 0, 1~\n$ ADEC_I10                                                       &lt;dbl&gt; 0, 1, 2~\n$ ADEC_I11                                                       &lt;dbl&gt; 0, 1, 1~\n$ ADEC_I12                                                       &lt;dbl&gt; 1, 0, 0~\n$ ADEC_I13                                                       &lt;dbl&gt; 1, 1, 1~\n$ ADEC_I14                                                       &lt;dbl&gt; 1, 0, 1~\n$ ADEC_I15                                                       &lt;dbl&gt; 1, 2, 1~\n$ ADEC_I16                                                       &lt;dbl&gt; 0, 1, 1~\n$ `Diagnosis(1=Non-typically developing; 2=ASD; 3=Neurotypical)` &lt;dbl&gt; 1, 1, 1~\n$ asd                                                            &lt;chr&gt; \"Non-TD~\n\n\nIniziamo trovando il punteggio totale ADEC. Si noti che la somma sarà NA se un item è NA.\n\ndat1_sub$ADEC &lt;- rowSums(select(dat1_sub, ADEC_I01:ADEC_I16)) # NA if any item is NA\n# Or treat missing as 0\ndat1_sub$ADEC_rm_na &lt;- rowSums(select(dat1_sub, ADEC_I01:ADEC_I16), na.rm = TRUE)\n\nUtilizzeremo il modello di regressione logistica per esaminare la relazione tra il punteggio totale del test ADEC e la probabilità che il bambino appartenga al gruppo diagnostico ASD.\nEsaminiamo la distribuzione del punteggio totale di ADEC in base alla diagnosi:\n\nggplot(dat1_sub, aes(x = ADEC)) +\n    geom_histogram() +\n    facet_wrap(~asd)\n\n\n\n\n\n\n\n\nRicodifichiamo asd in modo tale che assuma valore 1 per i bambini con ASD e 0 altrimenti.\n\ndat1_sub$y &lt;- ifelse(dat1_sub$asd == \"ASD\", 1, 0)\ndat1_sub$y\n\n\n000000010000001100000000000000001100000000010011011011101100100111110101111000111001111000110111110101101111001111000010111010111111011101011111111111011001111111111101110100011111111111001000\n\n\nEseguiamo l’analisi di regressione logistica usando la funzione ´glm()´.\n\nfm &lt;- glm(y ~ ADEC, family = binomial(link=\"logit\"), data = dat1_sub)\n\nEsaminiamo i risultati.\n\nsummary(fm)\n\n\nCall:\nglm(formula = y ~ ADEC, family = binomial(link = \"logit\"), data = dat1_sub)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.76480    0.58071  -6.483 8.99e-11 ***\nADEC         0.35447    0.05201   6.816 9.37e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 247.73  on 179  degrees of freedom\nResidual deviance: 131.12  on 178  degrees of freedom\n  (12 osservazioni eliminate a causa di valori mancanti)\nAIC: 135.12\n\nNumber of Fisher Scoring iterations: 6\n\n\nCreiamo un grafico che mostri la relazione tra la probabilità che Y = 1 (cioè la diagnosi di ASD per il bambino), calcolata utilizzando i coefficienti del modello di regressione logistica, e il punteggio ottenuto nella scala ADEC.\n\n# Filter out rows with missing values in ADEC column\ndat1_sub &lt;- na.omit(dat1_sub)\n# Predict the probabilities of y == 1 for the filtered data\npredictions &lt;- predict(fm, type = \"response\")\n# Create a data frame with ADEC and the predicted probabilities\nplot_data &lt;- data.frame(ADEC = dat1_sub$ADEC, Prob_Y_1 = predictions)\n\n# Plot the probability of y == 1 as a function of ADEC\nlibrary(ggplot2)\nggplot(plot_data, aes(x = ADEC, y = Prob_Y_1)) +\n    geom_line() +\n    geom_point() +\n    xlab(\"ADEC\") +\n    ylab(\"Probability of y == 1\") +\n    ggtitle(\"Probability of y == 1 vs. ADEC\")\n\n\n\n\n\n\n\n\nIl grafico presenta una curva sigmoidale che illustra come, per punteggi bassi nella scala ADEC, la probabilità di una diagnosi di autismo sia bassa, mentre per punteggi alti nella scala ADEC, la probabilità di una diagnosi di autismo sia alta.\n\n\n18.1.3 Accuratezza della classificazione\nUna volta compreso come il modello di regressione logistica associa a ciascun valore di \\(x\\) la probabilità dell’evento \\(y = 1\\), esaminiamo ora come sia possibile utilizzare questo modello per valutare l’accuratezza di una classificazione binaria. Nel nostro caso, la classificazione riguarda ciascuna osservazione nelle categorie ASD e Non-TD. In questo contesto, il modello di regressione logistica stima la probabilità di appartenenza a una delle due categorie basandosi sui valori di ADEC (eq. {eq}eq-reg-logistic-prob).\nPer effettuare la classificazione, dobbiamo stabilire un punto di taglio (cut-off) che separi le due categorie. Questo punto di taglio definisce il valore della variabile dipendente al di sopra del quale l’osservazione sarà assegnata alla categoria positiva (y = 1, ovvero ASD) e al di sotto del quale sarà assegnata alla categoria negativa (y = 0). La scelta del punto di taglio è critica poiché influisce sull’accuratezza delle previsioni del modello.\nI coefficienti di regressione stimati dalla regressione logistica sono i parametri del modello che descrivono la relazione tra la variabile indipendente ADEC e la probabilità di appartenenza alla categoria positiva (y = 1). Questi coefficienti ci consentono di calcolare la probabilità stimata di y = 1 per ciascuna osservazione, utilizzando l’eq. {eq}eq-reg-logistic-prob. Una volta calcolata la probabilità stimata di y = 1 per ogni osservazione, possiamo utilizzare un determinato punto di taglio (ad esempio, 0.5) per classificare le osservazioni nelle due categorie y = 0 e y = 1. Se la probabilità stimata è maggiore o uguale al punto di taglio, l’osservazione verrà classificata come y = 1 (positiva); altrimenti, verrà classificata come y = 0 (negativa).\nTuttavia, la scelta del punto di taglio non è banale e ha un impatto diretto sulla sensibilità (la proporzione dei casi positivi effettivi che il test identifica correttamente come positivi) e specificità (la proporzione dei casi negativi effettivi che il test identifica correttamente come negativi) del modello. Un punto di taglio più alto potrebbe aumentare la specificità, ma ridurre la sensibilità, mentre un punto di taglio più basso avrebbe l’effetto opposto.\nPer ottenere una valutazione più completa delle prestazioni del modello, consideriamo tutti i possibili punti di taglio e visualizziamo la relazione tra sensibilità e specificità tramite la curva ROC (Receiver Operating Characteristic). Questa curva è un grafico che mostra come variano sensibilità e specificità al variare del punto di taglio. Un elemento chiave della curva ROC è l’Area Under the Curve (AUC), che rappresenta una misura aggregata dell’accuratezza complessiva del modello. L’AUC calcola l’area sottesa alla curva ROC e riflette la capacità del modello di discriminare tra le due categorie di risultato. L’AUC, dunque, fornisce una misura sintetica dell’accuratezza del modello su tutta la gamma di possibili punti di taglio. Un valore di AUC pari a 1 indica una perfetta capacità di discriminazione, mentre un valore pari a 0.5 indica una capacità di discriminazione casuale.\nPer mostrare come trovare l’AUC, iniziamo calcolando l’accuratezza della classificazione mediante un valore di cut-off pari a 0.5, ad esempio.\n\n# Predict the probabilities of y == 1 for the filtered data\nprob_pred &lt;- predict(fm, type = \"response\")\n\n# Use cut-off 0.5 for classification\nclassification &lt;- ifelse(prob_pred &gt;= 0.5, 1, 0)\n\n# Classification accuracy\nmean(classification == dat1_sub$y)\n\n0.855555555555556\n\n\nCon il cut-off di 0.5, l’accuratezza è 0.855, ovvero l’86% dei casi è stato correttamente classificato dal test. Vogliamo però trovare l’accuratezza della classificazione per tutti i possibili valori di cut-off. Costruiamo dunque la curva ROC in R.\n\ncompute_sens &lt;- function(\n    cut, x = dat1_sub$ADEC,\n    crit = dat1_sub$asd == \"ASD\") {\n    tp &lt;- sum(x &gt;= cut & crit, na.rm = TRUE)\n    fn &lt;- sum(x &lt; cut & crit, na.rm = TRUE)\n    tp / (tp + fn)\n}\ncompute_spec &lt;- function(\n    cut, x = dat1_sub$ADEC,\n    crit = dat1_sub$asd == \"ASD\") {\n    tn &lt;- sum(x &lt; cut & !crit, na.rm = TRUE)\n    fp &lt;- sum(x &gt;= cut & !crit, na.rm = TRUE)\n    tn / (tn + fp)\n}\nsensitivity &lt;- lapply(32:0, compute_sens) |&gt;\n    unlist()\nspecificity &lt;- lapply(32:0, compute_spec) |&gt;\n    unlist()\ndata.frame(sensitivity, fpr = 1 - specificity) |&gt;\n    ggplot(aes(x = fpr, y = sensitivity)) +\n    geom_point() +\n    geom_line()\n\n\n\n\n\n\n\n\nUna volta costruita la curva ROC possiamo calcolare l’area sottesa alla curva ROC.\n\n# AUC\ndfpr &lt;- c(diff(1 - specificity), 0)\ndsens &lt;- c(diff(sensitivity), 0)\nsum(sensitivity * dfpr) + sum(dsens * dfpr) / 2\n\n0.920626013218606\n\n\nSe consideriamo tutti i possibili valori di cut-off, l’accuratezza risulta uguale a 0.92. Questo valore indica un’ottima capacità del modello di regressione logistica nel discriminare con precisione i bambini con diagnosi di autismo da quelli senza.\nQuesti risultati suggeriscono che la scala ADEC è uno strumento utile e valido per identificare precocemente i bambini a rischio di sviluppare un disturbo dello spettro autistico. La sua elevata accuratezza predittiva consente di fornire una valutazione tempestiva e accurata per l’intervento precoce e il supporto necessario ai bambini e alle loro famiglie. In conclusione, l’analisi di regressione logistica e il calcolo dell’AUC hanno fornito evidenze solide sulla validità predittiva della scala ADEC nella diagnosi precoce dell’autismo.\n\n18.1.3.1 Pacchetto ROCit\nSi noti che si possono ottenere gli stessi risultati trovati sopra usando le funzioni del pacchetto ROCit.\n\nroc_adec &lt;- rocit(dat1_sub$ADEC, class = dat1_sub$asd == \"ASD\")\n\n\nplot(roc_adec)\n\n\n\n\n\n\n\n\n\nsummary(roc_adec)\n\n                          \n Method used: empirical   \n Number of positive(s): 99\n Number of negative(s): 81\n Area under curve: 0.9206 \n\n\n\nciAUC(roc_adec)\n\n                                                          \n   estimated AUC : 0.920626013218606                      \n   AUC estimation method : empirical                      \n                                                          \n   CI of AUC                                              \n   confidence level = 95%                                 \n   lower = 0.880257283328194     upper = 0.960994743109017",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Relazioni test-criterio</span>"
    ]
  },
  {
    "objectID": "chapters/validity/02_other_variables.html#considerazioni-conclusive",
    "href": "chapters/validity/02_other_variables.html#considerazioni-conclusive",
    "title": "18  Relazioni test-criterio",
    "section": "18.2 Considerazioni conclusive",
    "text": "18.2 Considerazioni conclusive\nIn conclusione, il capitolo ha dimostrato l’utilizzo dell’analisi di regressione logistica e del calcolo dell’Area Under the Curve (AUC) come strumenti importanti per valutare la validità di criterio di un test, specialmente quando il criterio riguarda l’appartenenza a un gruppo diagnostico specifico.\nL’analisi di regressione logistica e il calcolo dell’AUC forniscono una valutazione accurata della capacità del test di classificare correttamente i partecipanti in base al criterio diagnostico desiderato. Questa valutazione accurata offre una solida base per l’applicazione pratica del test, consentendo decisioni informate e mirate nell’identificazione di individui con particolari caratteristiche o condizioni. Inoltre, l’utilizzo di questi strumenti statistici rappresenta un passo importante verso la validazione dei test, garantendo la qualità delle diagnosi e facilitando l’implementazione di interventi mirati ed efficaci. La corretta valutazione della validità di criterio è essenziale per ottenere risultati affidabili e utili nella pratica clinica e nella ricerca.",
    "crumbs": [
      "Validità",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Relazioni test-criterio</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html",
    "href": "chapters/gtheory/01_gtheory.html",
    "title": "19  Teoria della generalizzabilità",
    "section": "",
    "text": "19.1 Introduzione\nNei precedenti capitoli è stato illustrato come la Teoria Classica dei Test (CTT) identifichi l’errore di misurazione come una fonte di varianza non spiegata e definisca l’affidabilità come la proporzione di varianza vera rispetto alla varianza totale, che include anche l’errore di misurazione. La teoria della generalizzabilità estende questo concetto nella CTT, consentendo di distinguere tra diverse fonti di errore di misurazione in casi di disegni complessi, come errori associati alle persone, alle occasioni e agli item.\nQuesto capitolo si concentra su un’applicazione specifica della teoria della generalizzabilità, che è quella di stimare l’affidabilità delle misure all’interno di un disegno longitudinale. Nel corso di questo tutorial, esploreremo come affrontare questa sfida utilizzando il framework della teoria della generalizzabilità. Nei capitoli successivi esamineremo un approccio alternativo per risolvere lo stesso problema, che è il Latent Growth Modeling.",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html#molteplici-fonti-di-errore-di-misurazione",
    "href": "chapters/gtheory/01_gtheory.html#molteplici-fonti-di-errore-di-misurazione",
    "title": "19  Teoria della generalizzabilità",
    "section": "19.2 Molteplici fonti di errore di misurazione",
    "text": "19.2 Molteplici fonti di errore di misurazione\nLa Teoria della Generalizzabilità, nota anche come “Generalizability Theory,” è una teoria statistica che fornisce un quadro per studiare l’affidabilità e la validità delle misurazioni in diversi contesti. In questa teoria, i fattori che possono contribuire all’errore nelle misurazioni vengono chiamati “facets” (es. valutatori, compiti, occasioni). Ogni fattore può essere considerato fisso o casuale.\nLa terminologia utilizzata è la seguente:\n\nLe “condizioni” (condition) rappresentano i livelli dei vari fattori.\nL’oggetto di misurazione (Object of Measurement), che di solito sono le persone, non è considerato un fattore ma è sempre considerato casuale.\nL’“Universo delle Operazioni Ammissibili” (Universe of Admissible Operations, UAO) è un ampio insieme di condizioni alle quali si vogliono generalizzare i risultati osservati.\nLo “Score dell’Universo” (Universe Score) rappresenta il punteggio medio di una persona considerando tutte le possibili combinazioni di condizioni nell’UAO.\nLo studio “G” (G Study) mira a ottenere informazioni accurate sulla grandezza dei fattori di errore.\nLo studio “D” (D Study) riguarda la progettazione di uno scenario di misurazione con il livello desiderato di affidabilità utilizzando il minor numero possibile di condizioni.\n\nLa Teoria della Generalizzabilità è particolarmente utile quando si desidera valutare l’affidabilità e la validità delle misurazioni in situazioni complesse, in cui diversi fattori possono influenzare l’errore di misurazione.",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html#differenze-tra-la-teoria-g-e-la-ctt",
    "href": "chapters/gtheory/01_gtheory.html#differenze-tra-la-teoria-g-e-la-ctt",
    "title": "19  Teoria della generalizzabilità",
    "section": "19.3 Differenze tra la teoria G e la CTT",
    "text": "19.3 Differenze tra la teoria G e la CTT\nLa Teoria G e la Teoria Classica dei Test (CTT) sono due approcci distinti per valutare l’affidabilità di un test psicometrico. La Teoria G fornisce una valutazione più completa delle fonti di errore di misurazione, consentendo di stimare simultaneamente molteplici fonti di errore in un’unica analisi. I coefficienti di affidabilità nella Teoria G tengono conto di tutte le fonti misurate di errore, fornendo una stima più accurata dell’affidabilità complessiva del test.\nD’altra parte, la Teoria CTT permette di stimare solo una fonte di errore di misurazione alla volta. I coefficienti di affidabilità nella Teoria CTT si concentrano su una sola fonte di errore e non forniscono una visione completa dell’affidabilità del test. Inoltre, la Teoria G offre un metodo per determinare il numero di livelli di ciascuna fonte di errore necessari per ottenere livelli di affidabilità accettabili. Ciò consente di ottimizzare il design del test e garantire che il test sia affidabile in diverse situazioni e condizioni. Infine, la Teoria G fornisce coefficienti di affidabilità sia per decisioni riferite a norme (come classificazioni rispetto a una distribuzione di riferimento) che per decisioni riferite a criteri specifici (come confronti con standard prestabiliti). D’altro canto, i coefficienti di affidabilità più comunemente utilizzati nella Teoria CTT sono più adatti per test riferiti a norme, mentre per test riferiti a criteri possono essere meno accurati.\nIn sintesi, la Teoria G offre una valutazione più completa e accurata dell’affidabilità di un test, considerando diverse fonti di errore e fornendo informazioni utili per ottimizzare il design del test. La Teoria CTT, sebbene più semplice, è limitata nella sua capacità di catturare la complessità delle fonti di errore di misurazione. Pertanto, la Teoria G è spesso preferita quando si tratta di valutare l’affidabilità di test psicometrici complessi e con molteplici fonti di variabilità.",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html#fattori-incrociati-o-nidificati",
    "href": "chapters/gtheory/01_gtheory.html#fattori-incrociati-o-nidificati",
    "title": "19  Teoria della generalizzabilità",
    "section": "19.4 Fattori incrociati o nidificati",
    "text": "19.4 Fattori incrociati o nidificati\nUn concetto fondamentale della teoria della generalizzabilità è la gerarchia dei dati, che si riferisce alla struttura del disegno di ricerca. Esistono due tipi principali di disegni: annidati (nested) e incrociati (crossed). Nei disegni annidati, i livelli di un fattore sono contenuti all’interno dei livelli di un altro; nei disegni incrociati, ogni livello di un fattore si combina con tutti i livelli di un altro fattore.\nConsideriamo ad esempio uno studio in cui diversi psicologi valutano l’intelligenza di studenti in diverse scuole. Se ogni psicologo valuta gli studenti di tutte le scuole, allora i fattori “psicologo” e “scuola” sono incrociati. Ciò significa che tutte le combinazioni di psicologi e scuole sono rappresentate nel campione.\nD’altra parte, supponiamo che vi siano gruppi distinti di psicologi assegnati a diverse scuole. Ad esempio, un gruppo di psicologi potrebbe valutare gli studenti di una scuola, mentre un altro gruppo di psicologi potrebbe valutare gli studenti di un’altra scuola. In questo caso, i valutatori sono nidificati all’interno delle scuole. Ciò significa che ogni scuola ha un gruppo specifico di psicologi associati a essa per le valutazioni.\nLa distinzione tra fattori incrociati e nidificati è importante perché influisce sulla generalizzabilità delle conclusioni dello studio. Nel caso incrociato, le conclusioni possono essere generalizzate a tutte le combinazioni di valutatori e scuole presenti nel campione. Nel caso nidificato, le conclusioni possono essere generalizzate solo alle scuole specifiche associate ai rispettivi gruppi di valutatori.",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html#fattori-casuali-o-fissi",
    "href": "chapters/gtheory/01_gtheory.html#fattori-casuali-o-fissi",
    "title": "19  Teoria della generalizzabilità",
    "section": "19.5 Fattori casuali o fissi",
    "text": "19.5 Fattori casuali o fissi\nUn secondo concetto fondamentale della teoria della generalizzabilità riguarda la distinzione tra fattori casuali e fattori fissi in un disegno di ricerca. Un fattore è detto casuale quando le sue condizioni specifiche nello studio sono viste come un campione di un universo più ampio di condizioni, e si presume che queste siano equivalenti e interscambiabili con qualsiasi altra condizione nello stesso universo (Universo delle Operazioni Ammissibili, UAO). Ciò consente di generalizzare i risultati dello studio a tutte le condizioni all’interno dell’UAO. Ad esempio, i valutatori in uno studio sono considerati un fattore casuale se si assume che le valutazioni di uno siano sostituibili con quelle di un altro.\nContrariamente, un fattore fisso si concentra su condizioni specifiche e predeterminate, che costituiscono il completo insieme di interesse per il ricercatore. Questo approccio è utilizzato quando lo scopo è analizzare l’effetto di queste condizioni particolari senza cercare di generalizzare i risultati oltre a queste. Ad esempio, consideriamo uno studio sull’effetto di diverse terapie sulla riduzione dell’ansia in pazienti affetti da disturbi d’ansia, dove si esaminano specificamente tre tipi di terapia: Terapia A, Terapia B e Terapia C. Se ogni paziente partecipa a una ed una sola di queste terapie, selezionata in modo casuale, il fattore “Tipo di Terapia” è considerato fisso perché l’intenzione è di valutare l’effetto specifico di queste terapie selezionate, non di altre potenziali terapie non incluse nello studio.",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html#la-teoria-g",
    "href": "chapters/gtheory/01_gtheory.html#la-teoria-g",
    "title": "19  Teoria della generalizzabilità",
    "section": "19.6 La teoria G",
    "text": "19.6 La teoria G\nIn questo capitolo, esamineremo uno specifico uso della Teoria della Generalizzabilità in uno studio longitudinale. Specificamente, ci porremo il problema di stimare l’affidabilità delle misure dei partecipanti nel tempo.\nSupponiamo di condurre uno studio in cui vogliamo valutare le performance di diversi studenti nel corso del tempo. Raccogliamo i punteggi di cinque studenti (A, B, C, D, E) su un compito misurato in tre diverse occasioni temporali (T1, T2, T3). Il nostro obiettivo è comprendere come variano le performance tra gli studenti e nel corso del tempo. Il disegno è persona-per-tempo (5 persone x 3 occasioni di misurazione).\nL’equazione di decomposizione della varianza degli score osservati in questo disegno è la seguente:\n\\[\n\\sigma^2(X_{pt}) = \\sigma^2_p + \\sigma^2_t + \\sigma^2_{pt} + \\sigma^2_{pt,e},\n\\]\ndove:\n\n\\(\\sigma^2_p\\): rappresenta l’effetto principale delle persone, ovvero quanto variano le performance tra gli studenti.\n\\(\\sigma^2_t\\): indica l’effetto principale del tempo, ovvero quanto variano le performance degli studenti nel corso del tempo.\n\\(\\sigma^2_{pt}\\): rappresenta l’interazione persona-per-tempo, ovvero quanto variano le performance degli studenti nel corso del tempo.\n\\(\\sigma^2_{pt,e}\\): è la varianza residua o non misurata, che include l’errore casuale e altre fonti di varianza non considerate nel disegno.\n\nIn un secondo esempio, consideriamo un disegno a tre fattori. Supponiamo di condurre uno studio per valutare le performance di diversi studenti (A, B, C, D, E) su diversi compiti (item) nel corso del tempo (T1, T2, T3). Vogliamo comprendere come le performance degli studenti possono variare tra gli item, nel tempo e se ci sono interazioni tra questi fattori. Il disegno è persona-per-item-per-tempo (5 persone x 3 item x 3 occasioni di misura).\nL’equazione di decomposizione della varianza degli score osservati in questo disegno sarà la seguente:\n\\[\n\\sigma^2(X_{pit}) = \\sigma^2_p + \\sigma^2_i + \\sigma^2_t + \\sigma^2_{pi} + \\sigma^2_{pt} + \\sigma^2_{it} + \\sigma^2_{pit,e},\n\\]\ndove:\n\n\\(\\sigma^2_p\\): rappresenta l’effetto principale delle persone, ovvero quanto variano le performance tra gli studenti.\n\\(\\sigma^2_i\\): indica l’effetto principale degli item, ovvero quanto variano i punteggi dei compiti tra i diversi item.\n\\(\\sigma^2_t\\): rappresenta l’effetto principale del tempo, ovvero quanto variano le performance degli studenti nel corso del tempo.\n\\(\\sigma^2_{pi}\\): indica l’interazione persona-per-item, ovvero quanto variano le performance degli studenti tra i diversi compiti.\n\\(\\sigma^2_{pt}\\): rappresenta l’interazione persona-per-tempo, ovvero quanto variano le performance degli studenti nel corso del tempo.\n\\(\\sigma^2_{it}\\): indica l’interazione item-per-tempo, ovvero quanto variano i punteggi dei compiti nel corso del tempo.\n\\(\\sigma^2_{pit,e}\\): è la varianza residua o non misurata, che include l’errore casuale e altre fonti di varianza non considerate nel disegno.\n\nQuesti modelli ci permettono di analizzare come ciascuna fonte di variabilità influenzi i punteggi osservati.",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html#affidabilità",
    "href": "chapters/gtheory/01_gtheory.html#affidabilità",
    "title": "19  Teoria della generalizzabilità",
    "section": "19.7 Affidabilità",
    "text": "19.7 Affidabilità\nIl Cambiamento di Affidabilità, nota come Cambiamento di Affidabilità (RC, da Reliability Change), valuta in che misura le variazioni nei punteggi di soggetti, valutati ripetutamente nel tempo, riflettano cambiamenti reali piuttosto che essere causate da errori di misurazione. Questo indice è fondamentale in studi longitudinali, dove si osserva lo stesso gruppo di individui in più occasioni, per stabilire se i cambiamenti nei punteggi sono sistematici e affidabili, legati al tempo e alle caratteristiche uniche dei partecipanti. Un alto valore di RC indica che le misurazioni sono coerenti nel tempo, suggerendo che qualsiasi variazione rilevata rappresenti un vero cambiamento nel comportamento o nelle risposte dei soggetti. Pertanto, l’RC aiuta a distinguere tra cambiamenti autentici e fluttuazioni dovute a inesattezze nella raccolta dei dati.\nAd esempio, nel caso di un disegno a tre fattori (perone \\(\\times\\) item \\(\\times\\) tempo), la formula di RC è la seguente:\n\\[\nR_c = \\frac{\\sigma^2_{TP}}{\\sigma^2_{TP} + \\frac{1}{k}(\\sigma^2_{TPI} + \\sigma^2_{v})},\n\\]\ndove:\n\n\\(R_c\\) rappresenta la misura di affidabilità focale (Reliability Change).\n\\(\\sigma^2_{TP}\\) è la varianza tra il tempo e le persone (time by person variance).\n\\(\\sigma^2_{TPI}\\) è la varianza dell’interazione tra il tempo, le persone e gli item.\n\\(\\sigma^2_{v}\\) è la varianza dell’errore (error variance).\n\\(k\\) è il numero di item utilizzati.\n\nIl numeratore contiene solo una componente, ovvero la varianza tra il tempo e le persone, \\(\\sigma^2_{TP}\\). Il denominatore invece contiene la stessa componente di varianza, sommata alla componente di varianza dell’errore, divisa per il numero di item \\(k\\). Si noti che la componente di varianza dell’errore è la somma \\(\\sigma^2_{TPI} + \\sigma^2_{v}\\).\n\n19.7.1 Un esempio concreto\nApplichiamo questi concetti ai dati di Bolger e Laurenceau (2013) riguardanti uno studio che ha coinvolto 50 persone valutate per 10 giorni su 4 item. Gli item sono “interessato,” “determinato,” “entusiasta” e “ispirato,” rispettivamente. Le valutazioni per ciascun item sono state fatte su una scala da 1 (per niente) a 5 (estremamente).\nNel contesto della Teoria della Generalizzabilità, possiamo esaminare la variabilità dei punteggi dei partecipanti e suddividerla nelle diverse fonti di errore, che in questo caso includono la variazione dovuta ai diversi partecipanti (fonte di errore “Valutatori”), a diversi giorni (fonte di errore “Giorni”) e agli item specifici utilizzati (fonte di errore “Item”). Possiamo valutare quanto della variazione totale nei punteggi è attribuibile a ciascuna di queste fonti di errore.\nPer ottenere misure più precise e generalizzabili degli stati emotivi delle persone, possiamo calcolare gli “Score dell’Universo” per ciascun individuo. Gli score dell’universo rappresentano la media dei loro punteggi su tutti gli item, in tutti i giorni e su tutti i partecipanti. Questi score ci forniranno una stima più accurata del livello medio di “interessato,” “determinato,” “entusiasta” e “ispirato” per ciascun partecipante, considerando tutte le fonti di errore specificate.\nInfine, utilizzando la Teoria della Generalizzabilità, possiamo progettare uno studio ottimale (“D Study”) per massimizzare l’affidabilità delle misurazioni con il minor numero possibile di partecipanti, giorni e item. In questo modo, otterremo misurazioni più affidabili senza la necessità di sottoporre i partecipanti a un numero eccessivo di giorni di valutazione o di utilizzare troppi item.\nIn questo esempio, ci poniamo la seguente domanda: “Le variazioni all’interno dei soggetti possono essere misurate in modo affidabile?” Per rispondere a questa domanda, dobbiamo specificare le dimensioni di generalizzabilità. In questo caso, le dimensioni sono i momenti nel tempo, le persone e gli item.\nPer condurre questa analisi, useremo un modello a effetti misti per ciascuna delle dimensioni specificate.\nIniziamo leggendo i dati di Bolger e Laurenceau (2013).\n\nfilepath &lt;- \"https://quantdev.ssri.psu.edu/sites/qdev/files/psychometrics.csv\"\nd &lt;- read.csv(filepath)\nhead(d)\n\n\nA data.frame: 6 x 4\n\n\n\nperson\ntime\nitem\ny\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n301\n1\n1\n2\n\n\n2\n301\n1\n2\n2\n\n\n3\n301\n1\n3\n3\n\n\n4\n301\n1\n4\n4\n\n\n5\n301\n2\n1\n2\n\n\n6\n301\n2\n2\n3\n\n\n\n\n\nRicodifichiamo la variabile item in modo che sia categorica utilizzando la funzione factor().\n\nd$item &lt;- factor(d$item)\n\nIl pacchetto lme4 contiene la funzione lmer(), che permette di adattare un modello lineare a effetti misti a un specifico set di dati. Utilizzando il summary() del nostro modello, possiamo osservare gli effetti di ciascuna dimensione di generalizzabilità. Questo modello è specificato solo con l’intercetta (ANOVA con effetti casuali) allo scopo di comprendere le fonti di variabilità tra le diverse dimensioni.\n\nmodel1 &lt;- lmer(\n    y ~ 1 +\n        (1 | person) +\n        (1 | time) +\n        (1 | item) +\n        (1 | person:time) +\n        (1 | person:item) +\n        (1 | time:item),\n    data = d\n)\nsummary(model1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: \ny ~ 1 + (1 | person) + (1 | time) + (1 | item) + (1 | person:time) +  \n    (1 | person:item) + (1 | time:item)\n   Data: d\n\nREML criterion at convergence: 4046.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2721 -0.5311 -0.0105  0.5044  3.8614 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n person:time (Intercept) 0.255272 0.50524 \n person:item (Intercept) 0.190111 0.43602 \n person      (Intercept) 0.361774 0.60148 \n time:item   (Intercept) 0.004983 0.07059 \n time        (Intercept) 0.000000 0.00000 \n item        (Intercept) 0.048596 0.22044 \n Residual                0.299542 0.54730 \nNumber of obs: 1802, groups:  \nperson:time, 455; person:item, 200; person, 50; time:item, 40; time, 10; item, 4\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   2.4340     0.1456   16.71\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nUtilizzando la funzione VarCorr(), possiamo estrarre e salvare ciascun valore di varianza dalla tabella di riepilogo dei risultati.\n\n(personTime &lt;- VarCorr(model1)[[1]][1, 1]) # person:time\n\n0.25527190865661\n\n\n\n(personItem &lt;- VarCorr(model1)[[2]][1, 1]) # person:item\n\n0.190110610750237\n\n\n\n(person &lt;- VarCorr(model1)[[3]][1, 1]) # person\n\n0.361774346872358\n\n\n\n(timeItem &lt;- VarCorr(model1)[[4]][1, 1]) #time:item \n\n0.00498254640955633\n\n\n\n(time &lt;- VarCorr(model1)[[5]][1, 1]) #time \n\n0\n\n\n\n(item &lt;- VarCorr(model1)[[6]][1, 1]) # item\n\n0.0485955132048978\n\n\n\n(residual &lt;- sigma(model1)^2) # residual\n\n0.299541866895693\n\n\nTornando alla nostra domanda iniziale: Esistono differenze affidabili all’interno di una persona nel corso del tempo?\nUtilizzeremo la seguente formula per calcolare il coefficiente di affidabilità:\n\\[\nR_c = \\frac{\\sigma^2_{\\text{persona:tempo}}}{\\sigma^2_{\\text{persona:tempo}} + \\frac{\\sigma^2_{\\text{persona:tempo:item}} + \\sigma^2_{\\text{errore}}}{k}},\n\\]\ndove \\(k\\) si riferisce al numero di elementi. Nel nostro caso, \\(k\\)=4.\nNon possiamo distinguere il termine \\(\\sigma^2_{\\text{persona:tempo:item}}\\) da \\(\\sigma^2_{\\text{errore}}\\), quindi useremo il termine di errore residuo.\n\nk &lt;- 4\n(Rc &lt;- personTime / (personTime + residual / k))\n\n0.773182511408047\n\n\nQuesto coefficiente rappresenta il grado di adeguatezza e sistematicità delle misurazioni ripetute. Utilizzando le stesse regole interpretative del coefficiente alfa di Cronbach, possiamo determinare che quattro elementi possono catturare il cambiamento all’interno di una persona in modo affidabile, con \\(R_c = 0.77\\).\nUn altro coefficiente di interesse è \\(R_{1F}\\), che viene calcolato come:\n\\[\nR_{1F} = \\frac{\\sigma^2_{\\text{persona}} + \\left(\\frac{\\sigma^2_{\\text{persona:item}}}{k}\\right)}{\\sigma^2_{\\text{persona}} + \\left(\\frac{\\sigma^2_{\\text{persona:item}}}{k}\\right) + \\left(\\frac{\\sigma^2_{\\text{errore}}}{k}\\right)}\n\\]\ndove \\(k\\) si riferisce al numero di item. Nel nostro caso, \\(k\\)=4.\nPer i dati presenti, abbiamo:\n\nk &lt;- 4\n(R1f &lt;- (person + (personItem / k)) / (person + (personItem / k) + (residual / k)))\n\n0.845337866139592\n\n\nIl coefficiente di affidabilità \\(R_{1F}\\) rappresenta la stima attesa dell’affidabilità tra le persone per un giorno fisso, una sorta di media degli alfa di Cronbach specifici per ogni giorno in diverse occasioni. Utilizzando le stesse regole interpretative del coefficiente alfa di Cronbach, possiamo concludere che quattro item possono catturare in modo affidabile le differenze tra le persone in qualsiasi giorno specifico, con \\(R_{1F}\\) = 0.85.",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html#conclusioni",
    "href": "chapters/gtheory/01_gtheory.html#conclusioni",
    "title": "19  Teoria della generalizzabilità",
    "section": "19.8 Conclusioni",
    "text": "19.8 Conclusioni\nLa Teoria della Generalizzabilità (G theory) fornisce un quadro completo per stimare l’impatto di molteplici fonti di errore di misurazione simultaneamente. La G theory si basa su un modello ANOVA in cui i fattori sono chiamati “facet” (fattori) e i loro livelli sono noti come “conditions” (condizioni). Ad esempio, gli studenti potrebbero essere valutati su un insieme di compiti da un gruppo di valutatori in diverse occasioni. Compiti, valutatori e occasioni potrebbero tutti contribuire all’errore di misurazione e verrebbero considerati “facet” nel disegno della G theory. La varianza dovuta a questi “facet” e alle loro interazioni potrebbe essere stimata, e le loro relative contribuzioni alla varianza dell’errore di misurazione valutate.\nIl concetto di generalizzabilità o affidabilità nella G theory è analogo al concetto di affidabilità nella CTT. Nella G theory, l’interesse è rivolto al grado in cui i punteggi osservati ottenuti in un determinato insieme di condizioni possono essere generalizzati alla media del punteggio che potrebbe essere ottenuto in un insieme di condizioni più ampiamente definite, noto come “UAO” (Universal Attribute Object). L’UAO è definito dal ricercatore e include tutte le condizioni che produrrebbero punteggi accettabili. Il grado in cui i punteggi si generalizzano dalle condizioni osservate all’UAO è definito come affidabilità. Livelli elevati di affidabilità indicano che i punteggi ottenuti nelle condizioni osservate si generalizzeranno ai punteggi universali delle persone. Il punteggio universale è analogo al punteggio vero nella CTT e può essere concepito come il punteggio medio che una persona otterrebbe se sottoposta a ripetuti test in tutte le possibili combinazioni di condizioni presenti nell’UAO.",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/gtheory/01_gtheory.html#session-info",
    "href": "chapters/gtheory/01_gtheory.html#session-info",
    "title": "19  Teoria della generalizzabilità",
    "section": "19.9 Session Info",
    "text": "19.9 Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] lme4_1.1-35.5     Matrix_1.7-0      nortest_1.0-4    \n [4] MASS_7.3-61       ggokabeito_0.1.0  viridis_0.6.5    \n [7] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1   \n[10] gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1 \n[13] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19    \n[16] psych_2.4.6.26    scales_1.3.0      markdown_1.13    \n[19] knitr_1.48        lubridate_1.9.3   forcats_1.0.0    \n[22] stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[25] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n[28] ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.16.0  jsonlite_1.8.9     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5.1 farver_2.1.2      \n  [7] nloptr_2.1.1       rmarkdown_2.28     vctrs_0.6.5       \n [10] minqa_1.2.8        base64enc_0.1-3    rstatix_0.7.2     \n [13] htmltools_0.5.8.1  broom_1.0.7        Formula_1.2-5     \n [16] htmlwidgets_1.6.4  plyr_1.8.9         sandwich_3.1-1    \n [19] emmeans_1.10.4     zoo_1.8-12         uuid_1.2-1        \n [22] igraph_2.0.3       mime_0.12          lifecycle_1.0.4   \n [25] pkgconfig_2.0.3    R6_2.5.1           fastmap_1.2.0     \n [28] shiny_1.9.1        digest_0.6.37      OpenMx_2.21.12    \n [31] fdrtool_1.2.18     colorspace_2.1-1   rprojroot_2.0.4   \n [34] Hmisc_5.1-3        fansi_1.0.6        timechange_0.3.0  \n [37] abind_1.4-8        compiler_4.4.1     withr_3.0.1       \n [40] glasso_1.11        htmlTable_2.4.3    backports_1.5.0   \n [43] carData_3.0-5      ggsignif_0.6.4     corpcor_1.6.10    \n [46] gtools_3.9.5       tools_4.4.1        pbivnorm_0.6.0    \n [49] foreign_0.8-87     zip_2.3.1          httpuv_1.6.15     \n [52] nnet_7.3-19        glue_1.8.0         quadprog_1.5-8    \n [55] promises_1.3.0     nlme_3.1-166       lisrelToR_0.3     \n [58] grid_4.4.1         pbdZMQ_0.3-13      checkmate_2.3.2   \n [61] cluster_2.1.6      reshape2_1.4.4     generics_0.1.3    \n [64] gtable_0.3.5       tzdb_0.4.0         data.table_1.16.0 \n [67] hms_1.1.3          car_3.1-3          utf8_1.2.4        \n [70] sem_3.1-16         pillar_1.9.0       IRdisplay_1.1     \n [73] rockchalk_1.8.157  later_1.3.2        splines_4.4.1     \n [76] lattice_0.22-6     survival_3.7-0     kutils_1.73       \n [79] tidyselect_1.2.1   miniUI_0.1.1.1     pbapply_1.7-2     \n [82] stats4_4.4.1       xfun_0.48          qgraph_1.9.8      \n [85] arm_1.14-4         stringi_1.8.4      pacman_0.5.1      \n [88] boot_1.3-31        evaluate_1.0.0     codetools_0.2-20  \n [91] mi_1.1             cli_3.6.3          RcppParallel_5.1.9\n [94] IRkernel_1.3.2     rpart_4.1.23       xtable_1.8-4      \n [97] repr_1.1.7         munsell_0.5.1      Rcpp_1.0.13       \n[100] coda_0.19-4.1      png_0.1-8          XML_3.99-0.17     \n[103] parallel_4.4.1     jpeg_0.1-10        mvtnorm_1.3-1     \n[106] openxlsx_4.2.7.1   crayon_1.5.3       rlang_1.1.4       \n[109] multcomp_1.4-26    mnormt_2.1.1",
    "crumbs": [
      "Generalizzabilità",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Teoria della generalizzabilità</span>"
    ]
  },
  {
    "objectID": "chapters/items/01_item_development.html",
    "href": "chapters/items/01_item_development.html",
    "title": "20  Lo sviluppo degli item",
    "section": "",
    "text": "20.1 Introduzione\nI test psicologici sono composti da item, quindi la bontà degli item determina la bontà del test. A prima vista, lo sviluppo di buoni item può sembrare un’impresa semplice e diretta, ma in realtà, la bontà degli item è determinata dalla attenta considerazione di diversi importanti fattori combinata con una valutazione quantitativa tramite specifiche procedure psicometriche. In questo capitolo, forniamo una discussione pratica su come sviluppare buoni item. Ciò include la discussione dei diversi formati di item disponibili agli autori di test e alcune linee guida di base per lo sviluppo degli item. Discutiamo lo sviluppo di item per test di massima prestazione e test di risposta tipica. Ricorderete che i test di massima prestazione sono progettati per determinare i limiti superiori delle abilità o conoscenze delle persone, mentre i test di risposta tipica valutano le loro caratteristiche quotidiane o abitudinarie. In un contesto occupazionale, un datore di lavoro potrebbe utilizzare un test di risposta tipica per determinare se un dipendente sta completando le attività quotidiane richieste per il lavoro e un test di massima prestazione per determinare se il dipendente ha la conoscenza o l’abilità necessaria per una promozione a un lavoro di livello superiore e più complesso. I test di massima prestazione e di risposta tipica hanno ruoli importanti nella valutazione psicologica, quindi consideriamo gli item utilizzati in entrambi i casi. Iniziamo questo capitolo con una breve panoramica dei formati di item più popolari prima di procedere con una discussione delle linee guida per lo sviluppo degli item.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Lo sviluppo degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/01_item_development.html#classificazione-degli-item-nei-test-psicometrici",
    "href": "chapters/items/01_item_development.html#classificazione-degli-item-nei-test-psicometrici",
    "title": "20  Lo sviluppo degli item",
    "section": "20.2 Classificazione degli Item nei Test Psicometrici",
    "text": "20.2 Classificazione degli Item nei Test Psicometrici\nNel panorama della psicometria, la classificazione degli item di test è fondamentale per determinare la loro validità e affidabilità. Tradizionalmente, gli item si distinguono in due categorie principali: oggettivi e soggettivi. Questa bipartizione, seppur utile, non esaurisce l’ampiezza e la complessità della materia. Pertanto, è essenziale esplorare in modo più approfondito i criteri di classificazione e le loro implicazioni.\n\n20.2.1 Item Oggettivi e Soggettivi: Un Continuum di Valutazione\nLa distinzione primaria tra item oggettivi e soggettivi si basa sul metodo di valutazione. Gli item oggettivi si caratterizzano per la presenza di un consenso ampio tra gli esperti circa la correttezza delle risposte, come nel caso degli item a scelta multipla, vero/falso e di abbinamento. In questi formati, la correttezza della risposta è inequivocabile e non presta il fianco a interpretazioni soggettive.\nAl contrario, gli item soggettivi implicano una maggiore discrezionalità nella valutazione. Esempi tipici sono gli item a tema o le risposte in un esame orale, dove l’apporto soggettivo del valutatore gioca un ruolo cruciale. Questa categoria di item richiede una valutazione più articolata e può portare a divergenze tra i valutatori.\n\n\n20.2.2 Classificazione Alternativa: Risposta Selezionata vs Risposta Costruita\nUna classificazione più moderna e funzionale distingue gli item in base alla natura della risposta richiesta: risposta selezionata e risposta costruita. In questo schema, gli item a risposta selezionata includono quelli a scelta multipla, vero/falso e di abbinamento, dove la risposta è già fornita e il candidato deve selezionarla. Questi item permettono una valutazione rapida, oggettiva e affidabile, rendendoli particolarmente adatti per test di ampio respiro.\nGli item a risposta costruita, invece, richiedono al candidato di generare una risposta, come nei casi di risposte brevi, saggi o valutazioni delle prestazioni. Questi item sono più idonei per valutare abilità cognitive di ordine superiore e competenze specifiche, ma sono più soggetti a valutazioni soggettive e richiedono un tempo maggiore sia per la risposta sia per la valutazione.\n\n\n20.2.3 Punti di Forza e Limitazioni\nOgni categoria di item presenta specifici punti di forza e limitazioni. Gli item a risposta selezionata sono efficienti, affidabili e permettono di includere un maggior numero di domande nel test, ma possono essere complessi da formulare e potrebbero non essere adatti per valutare tutte le tipologie di competenze. Inoltre, sono soggetti al rischio di risposte casuali o indovinate.\nGli item a risposta costruita, d’altra parte, sono più adatti per valutare competenze complesse e abilità di ordine superiore, ma richiedono più tempo per la risposta e la valutazione, e possono essere influenzati da fattori estranei non correlati al costrutto da misurare.\nNella scelta del formato di valutazione, il fattore determinante dovrebbe essere l’adeguatezza del formato nel misurare il costrutto di interesse in modo diretto e puro. La scelta dipenderà dagli obiettivi specifici del test e dalla natura del costrutto da valutare. In generale, si raccomanda di preferire gli item a risposta selezionata per la loro capacità di campionare ampiamente il dominio del contenuto e per le loro caratteristiche di valutazione più oggettive e affidabili. Tuttavia, gli item a risposta costruita sono indispensabili per valutare certe competenze e abilità cognitive di ordine superiore.\nIn conclusione, una comprensione approfondita delle diverse tipologie di item e delle loro specificità è fondamentale per lo sviluppo di strumenti psicometrici efficaci e affidabili, in grado di fornire valutazioni precise e pertinenti ai costrutti psicologici in esame.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Lo sviluppo degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/01_item_development.html#linee-guida-generali-per-la-redazione-di-item-di-test",
    "href": "chapters/items/01_item_development.html#linee-guida-generali-per-la-redazione-di-item-di-test",
    "title": "20  Lo sviluppo degli item",
    "section": "20.3 Linee Guida Generali per la Redazione di Item di Test",
    "text": "20.3 Linee Guida Generali per la Redazione di Item di Test\nVengono qui presentate alcune linee guida per lo sviluppo di vari tipi di item di test. Queste indicazioni devono però essere applicate in modo flessibile. L’obiettivo principale nella creazione di item di test è sviluppare domande che misurino in modo preciso il costrutto specificato, contribuendo alla validità psicometrica del test. I criteri usati per lo sviluppo degli item devono comunque sempre in primo luogo cercare di raggiungere quello che è l’obiettivo primario per cui il test viene costruito.\n\nFornire Istruzioni Chiare: È comune per i redattori di test inesperti assumere che i candidati sappiano come rispondere a diversi formati di item. Includere sempre istruzioni dettagliate che specificano chiaramente come il candidato debba rispondere a ciascun formato di item. Assumere che i candidati non abbiano mai visto un test simile e fornire istruzioni dettagliate per garantire che sappiano cosa ci si aspetta da loro. Tuttavia, istruzioni troppo lunghe e dettagliate possono diminuire la chiarezza.\nPresentare il Problema in Modo Chiaro: Mantenere la redazione degli item il più semplice possibile. A meno che non si stia valutando la capacità di lettura, puntare a un livello di lettura basso. Evitare termini scientifici o tecnici non necessari, così come costruzioni di frasi complesse o ambigue.\nSviluppare Item Valutabili in Modo Decisivo: Assicurarsi che gli item abbiano risposte chiare su cui quasi tutti gli esperti sarebbero d’accordo. Nel caso di saggi e valutazioni delle prestazioni, considerare se gli esperti concorderebbero sulla qualità della prestazione nel compito.\nEvitare Indizi Involontari: Fare attenzione a non includere indizi involontari che potrebbero guidare il candidato verso la risposta corretta.\nSistemare gli Item in Modo Sistematico: Organizzare gli item in modo che favoriscano la prestazione ottimale dei candidati. Se il test contiene più formati di item, raggrupparli in sezioni in base al tipo di item. Disporre gli item secondo il loro livello di difficoltà, iniziando da quelli più facili.\nMantenere gli Item su Una Pagina: Assicurarsi che ciascun item a risposta selezionata sia contenuto in una pagina, per evitare confusione e errori.\nPersonalizzare gli Item per la Popolazione di Riferimento: Considerare attentamente il tipo di clienti con cui il test sarà utilizzato e personalizzare gli item di conseguenza.\nMinimizzare l’Impatto di Fattori Irrilevanti: Cercare e minimizzare i fattori cognitivi, motori e altri che sono necessari per rispondere correttamente agli item, ma irrilevanti per il costrutto misurato.\nEvitare la Parafrasi del Materiale di Studio: Quando si preparano test di rendimento, evitare di usare la stessa formulazione presente nei libri di testo o altri materiali di studio.\nEvitare Linguaggio Pregiudizievole o Offensivo: Rivedere attentamente gli item per lingua potenzialmente pregiudizievole o offensiva.\nUsare un Formato di Stampa Chiaro e Leggibile: Utilizzare una dimensione e un interlinea del carattere chiari e appropriati per i candidati.\nDeterminare il Numero di Item da Includere: Considerare fattori come il tempo disponibile, l’età dei candidati, i tipi di item, l’ampiezza del materiale o degli argomenti valutati e il tipo di test.\n\nQueste linee guida sono fondamentali per lo sviluppo di item di test che siano non solo tecnicamente validi, ma anche accessibili e giusti per i candidati. L’applicazione flessibile e consapevole di queste indicazioni contribuirà significativamente all’efficacia e all’affidabilità degli strumenti di valutazione psicometrica.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Lo sviluppo degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/01_item_development.html#test-di-massima-prestazione",
    "href": "chapters/items/01_item_development.html#test-di-massima-prestazione",
    "title": "20  Lo sviluppo degli item",
    "section": "20.4 Test di Massima Prestazione",
    "text": "20.4 Test di Massima Prestazione\nIn questa sezione, ci concentreremo sullo sviluppo degli item per i test di massima prestazione, in particolare quelli progettati per valutare obiettivi educativi o di apprendimento. Sebbene tali linee guida siano pensate principalmente per i test di rendimento, molte di esse sono applicabili anche ai test di attitudine che utilizzano questi tipi di item. Inizieremo esaminando gli item a risposta selezionata, per poi passare agli item a risposta costruita. In sezioni successive, forniremo suggerimenti per sviluppare linee guida per i test di prestazione tipica.\n\n20.4.1 Item a Scelta Multipla\nGli item a scelta multipla sono tra i più popolari nel formato a risposta selezionata. Sono molto diffusi perché applicabili in diverse aree tematiche e capaci di valutare obiettivi semplici e complessi. Generalmente, assumono la forma di una domanda o di un’affermazione incompleta, con un insieme di possibili risposte, una delle quali è corretta. La parte dell’item che presenta la domanda o l’affermazione incompleta è chiamata “stem” o “radice”. Le possibili risposte sono denominate “alternative”. L’alternativa corretta è detta “risposta”, mentre le alternative errate sono note come “distrattori”.\n\n20.4.1.1 Suggerimenti per Sviluppare Item a Scelta Multipla\n\nUsare un Formato Chiaro: Non esiste un formato universalmente accettato, ma alcune raccomandazioni sul layout possono migliorare la chiarezza.\n\nNumerare lo stem dell’item per un’identificazione facile.\nIndentare le alternative e identificarle con lettere.\nNon capitalizzare l’inizio delle alternative, a meno che non inizino con un nome proprio.\nDisporre le alternative in un elenco verticale per facilitarne la lettura rapida.\n\nFornire Tutte le Informazioni Necessarie nello Stem dell’Item: Il problema o la domanda deve essere completamente sviluppato nello stem dell’item. Leggere lo stem dell’item senza esaminare le alternative per assicurarsi che sia sufficiente per comprendere la domanda.\nFornire da Tre a Cinque Alternative: L’uso di più alternative riduce la possibilità di indovinare la risposta corretta. Quattro è il numero più comune di alternative, ma cinque è raccomandato per ridurre ulteriormente il tasso di successo casuale.\nMantenere le Alternative Brevi e Ordinate: Le alternative devono essere il più brevi possibile e disposte in un ordine logico.\nEvitare Affermazioni Negative nello Stem dell’Item: Limitare l’uso di termini come “eccetto”, “meno”, “mai” o “non”. In casi eccezionali, evidenziare i termini negativi con maiuscole, sottolineatura o grassetto.\nAssicurare una Sola Risposta Corretta o la Migliore Risposta: Rivedere attentamente le alternative per garantire una sola risposta corretta o la migliore.\nCoerenza Grammaticale tra Stem dell’Item e Alternative: Tutte le alternative devono essere grammaticalmente corrette rispetto allo stem dell’item.\nRendere Tutti i Distrattori Plausibili: I distrattori devono sembrare ragionevoli e basarsi su errori comuni.\nPosizionare Casualmente la Risposta Corretta: Distribuire equamente la risposta corretta tra le posizioni delle alternative per evitare schemi prevedibili.\nLimitare l’Uso di ‘Nessuna delle Precedenti’ e Evitare ‘Tutte le Precedenti’: Utilizzare “Nessuna delle sopra” con parsimonia e evitare completamente “Tutte le sopra”.\nLimitare l’Uso di ‘Sempre’ e ‘Mai’ nelle Alternative: Evitare generalmente l’uso di termini assoluti come “sempre” e “mai”.\n\nGli item a scelta multipla sono un formato efficace per la valutazione, grazie alla loro versatilità, valutazione oggettiva e affidabile, e capacità di coprire ampiamente il dominio del contenuto. Tuttavia, la loro redazione non è semplice e non sono adatti per misurare tutti gli obiettivi di apprendimento.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Lo sviluppo degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/01_item_development.html#formato-di-risposta-verofalso",
    "href": "chapters/items/01_item_development.html#formato-di-risposta-verofalso",
    "title": "20  Lo sviluppo degli item",
    "section": "20.5 Formato di Risposta Vero/Falso",
    "text": "20.5 Formato di Risposta Vero/Falso\nIl formato di risposta vero/falso rappresenta una delle tipologie più popolari di item a risposta selezionata, seconda solo alla scelta multipla. Utilizzeremo il termine “vero/falso” per riferirci a una classe più ampia di item che possono includere formati binari, come accordo/disaccordo, corretto/errato, sì/no, fatto/opinione. Poiché il formato più comune è vero/falso, useremo questo termine in senso generico per indicare tutti gli item a due opzioni. Di seguito, forniremo linee guida per lo sviluppo di item vero/falso.\n\n20.5.1 Linee Guida per Sviluppare Item Vero/Falso\n\nEvitare Più di un’idea per Affermazione: Ogni item vero/falso dovrebbe affrontare una sola idea centrale. Evitare determinanti specifici e qualificatori che possano fungere da indizi per la risposta. Determinanti come “mai”, “sempre”, “nessuno” e “tutti” si trovano più frequentemente in affermazioni false e possono guidare i candidati non informati verso la risposta corretta. Al contrario, affermazioni moderate come “di solito”, “a volte” e “frequentemente” tendono a essere più veritiere e possono servire come indizi. Sebbene sia difficile evitare completamente i qualificatori, si consiglia di bilanciarli tra affermazioni vere e false per ridurne il valore come indizi.\nAvere Affermazioni Vere e False di Lunghezza Simile: Spesso gli autori tendono a formulare affermazioni vere più lunghe di quelle false. Per evitare che la lunghezza diventi un indizio involontario, è necessario assicurarsi che non ci sia una differenza evidente tra la lunghezza delle affermazioni vere e quelle false.\nIncludere un Numero Approssimativamente Uguale di Affermazioni Vere e False: Alcuni candidati tendono a scegliere “Vero” quando non sono sicuri della risposta, e altri “Falso”. Per prevenire un incremento artificiale dei punteggi dovuto a questi schemi di risposta, è consigliato includere un numero approssimativamente uguale di item veri e falsi. Alcuni autori suggerivano che nel formato vero/falso, il 60% degli item dovesse essere vero. Tuttavia, ciò è utile solo in circostanze limitate e non si applica ai test di prestazione tipica, essendo superato dal problema degli schemi di risposta e delle strategie di indovinamento. È preferibile un equilibrio.\n\nGli item vero/falso sono popolari nei test di massima prestazione. Sebbene possano essere valutati in modo oggettivo e affidabile e permettano ai candidati di rispondere a molti item in breve tempo, presentano varie debolezze. Ad esempio, sono spesso limitati alla valutazione di obiettivi di apprendimento piuttosto semplici e sono vulnerabili all’indovinamento. Prima di utilizzare gli item vero/falso, è raccomandato valutare i loro punti di forza e debolezze per assicurarsi che siano il formato più appropriato per valutare gli obiettivi specifici di apprendimento. Una checklist per lo sviluppo di item vero/falso può fornire un riferimento utile, applicabile anche ai formati sì/no spesso usati con individui più giovani.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Lo sviluppo degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/01_item_development.html#formato-di-risposta",
    "href": "chapters/items/01_item_development.html#formato-di-risposta",
    "title": "20  Lo sviluppo degli item",
    "section": "20.6 Formato di Risposta",
    "text": "20.6 Formato di Risposta\n\n20.6.1 Item di Abbinamento\nGli item di abbinamento (Matching Items) consistono in due colonne di parole o frasi: una colonna contiene i termini da abbinare (solitamente a sinistra, denominati “premesse”), e l’altra contiene le opzioni di risposta (a destra, chiamate “risposte”). Le premesse sono numerate, mentre le risposte sono identificate con lettere. Di seguito, alcune linee guida per lo sviluppo di questi item:\n\nLimitare l’Uso di Materiali Omogenei: È fondamentale che le liste siano il più omogenee possibile, basate su un tema comune. Evitare di includere materiali eterogenei.\nSpecificare nella Direzione le Basi dell’Abbinamento: Indicare chiaramente nelle istruzioni la base logica per l’abbinamento delle premesse con le risposte.\nIncludere più Risposte che Premesse: Ciò riduce la possibilità che i candidati non informati indovinino correttamente tramite eliminazione.\nIndicare che le Risposte Possono Essere Utilizzate Più Volte o Non Utilizzate: Questo riduce l’impatto dell’indovinamento.\nMantenere le Liste Brevi: Liste più brevi sono più gestibili sia per chi redige il test sia per chi lo svolge, evitando fattori confondenti come la memoria a breve termine.\nAssicurare che le Risposte Siano Brevi e Ordinate Logicamente: Ciò facilita la scansione efficiente delle opzioni da parte dei candidati.\n\nGli item di abbinamento possono essere valutati in modo oggettivo e affidabile, e sono relativamente facili da sviluppare. Tuttavia, hanno uno scopo limitato e possono promuovere la memorizzazione meccanica.\n\n\n20.6.2 Saggi\nUn item di saggio pone al candidato una domanda o un problema da rispondere in un formato scritto aperto. Essendo item a risposta costruita, richiedono una risposta elaborata dal candidato, non la selezione tra alternative. Di seguito alcune linee guida:\n\nSpecificare Chiaramente il Compito di Valutazione: È cruciale che il compito richiesto dall’item di saggio sia chiaramente definito, specificando la forma e l’ambito della risposta attesa.\nUtilizzare più Item a Risposta Ristretta Rispetto a Quelli a Risposta Estesa: Gli item a risposta ristretta sono più facili da valutare in modo affidabile e consentono una migliore campionatura del dominio di contenuto.\nSviluppare e Utilizzare una Rubrica di Valutazione: Una rubrica di valutazione fornisce indicazioni chiare per la valutazione di una risposta costruita, essenziale per una valutazione affidabile.\nLimitare l’Uso degli Item di Saggio a Obiettivi Non Misurabili con Item a Risposta Selezionata: Gli item di saggio hanno limitazioni, inclusa la difficoltà di valutazione affidabile e una minore campionatura del dominio di contenuto.\n\nIn generale, gli item di saggio sono adatti per misurare obiettivi complessi e sono relativamente facili da scrivere, ma presentano difficoltà nella valutazione affidabile e nella limitata campionatura del dominio di contenuto. Si raccomanda di limitare l’uso degli item di saggio alla misurazione di obiettivi che non sono facilmente valutabili tramite item a risposta selezionata.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Lo sviluppo degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/01_item_development.html#formato-di-risposta-breve",
    "href": "chapters/items/01_item_development.html#formato-di-risposta-breve",
    "title": "20  Lo sviluppo degli item",
    "section": "20.7 Formato di Risposta Breve",
    "text": "20.7 Formato di Risposta Breve\nGli item a risposta breve richiedono al candidato di fornire una parola, frase, numero o simbolo in risposta a una domanda diretta. Possono anche essere formulati come frasi incomplete, in un formato talvolta definito come “completamento”. Rispetto agli item di saggio, gli item a risposta breve pongono limiti più stretti sulla natura e lunghezza della risposta. Praticamente, un item a risposta breve è simile a un item di saggio a risposta ristretta, ma con ulteriori restrizioni. Di seguito alcune indicazioni specifiche per la redazione di item a risposta breve:\n\nStrutturare l’Item per una Risposta il più Breve Possibile: Gli item a risposta breve dovrebbero richiedere risposte concise, semplificando così la valutazione e rendendola più affidabile.\nGarantire una Sola Risposta Corretta: È importante che ci sia una sola risposta corretta per ogni item, evitando interpretazioni multiple.\nPreferire il Formato di Domanda Diretta alla Frase Incompleta: Generalmente, il formato di domanda diretta è meno confuso per i candidati. Utilizzare il formato di frase incompleta solo se questo comporta una maggiore brevità senza perdere in chiarezza.\nNel Formato di Frase Incompleta, Utilizzare un Solo Spazio Vuoto: Limitare ciascuna frase incompleta a uno spazio vuoto, preferibilmente vicino alla fine della frase, per maggior chiarezza.\nFornire Spazi Adeguati per le Risposte: Assicurarsi che ogni spazio vuoto fornisca spazio sufficiente per la risposta del candidato, evitando che la lunghezza dello spazio possa fornire indizi sulla risposta.\nPer Domande Quantitative, Indicare il Grado di Precisione Richiesto: Specificare, ad esempio, se la risposta deve essere espressa in pollici, o se le frazioni devono essere ridotte ai minimi termini.\nCreare una Rubrica di Valutazione e Applicarla in Modo Coerente: Come per gli item di saggio, è importante creare e utilizzare in modo coerente una rubrica di valutazione.\n\nGli item a risposta breve, simili agli item di saggio, richiedono una risposta scritta dal candidato ma con limiti più ristretti nella formulazione della risposta. Sono adatti per misurare determinati obiettivi di apprendimento (ad esempio, calcoli matematici) e sono relativamente facili da scrivere. Tuttavia, come gli item di saggio, presentano sfide nella valutazione affidabile e dovrebbero essere usati in modo oculato. Una checklist può fornire una guida utile per lo sviluppo di questi item.\n\n20.7.1 Test di Risposta Tipica\nDopo aver esaminato vari formati di item utilizzati nei test di massima prestazione, ci concentreremo sugli item comunemente usati nei test di risposta tipica, come le scale di personalità e di atteggiamento. Descriveremo diversi formati di item comuni a questi test e forniremo alcune linee guida generali per lo sviluppo di item. La valutazione di sentimenti, pensieri, dialoghi interni e altri comportamenti occulti è meglio realizzata tramite autovalutazione, che sarà il focus della nostra discussione. Tuttavia, come nei test di massima prestazione, esistono numerosi formati di item disponibili per le misure di autovalutazione.\n\n20.7.1.1 Linee Guida per la Redazione di Item in Test di Risposta Tipica\n\nConcentrarsi su Pensieri, Sentimenti e Comportamenti, non su Fatti: Nei test di risposta tipica, l’obiettivo è valutare le esperienze del candidato: i suoi pensieri, sentimenti e comportamenti tipici. Di conseguenza, si dovrebbero evitare affermazioni basate su informazioni fattuali che possono essere valutate come “corrette” o “errate”.\nLimitare le Affermazioni a un Singolo Pensiero, Sentimento o Comportamento: Ogni affermazione dovrebbe concentrarsi su un solo pensiero, sentimento, comportamento o atteggiamento.\nEvitare Affermazioni Universali: Per aumentare la varianza e migliorare l’affidabilità, si dovrebbero scrivere item che misurano le differenze individuali. Se tutti o quasi tutti rispondono a un item nello stesso modo, questo non contribuisce alla misurazione dei costrutti identificati.\nIncludere Item Formulati sia in Modo “Positivo/Favorevole” che “Negativo/Sfavorevole”: Come regola generale, usare una combinazione di item formulati in modo “positivo” e “negativo”. Ciò può incoraggiare i candidati a evitare uno stile di risposta in cui semplicemente segnano la stessa opzione di risposta su tutti gli item. Questo è più applicabile agli item Vero/Falso e alle scale Likert, e meno alle scale di valutazione dove si cerca di valutare la frequenza di pensieri, sentimenti e comportamenti problematici.\nUtilizzare un Numero Appropriato di Opzioni: Per le scale di valutazione, 4 o 5 opzioni di risposta sembrano ottimali per sviluppare affidabilità senza allungare eccessivamente il tempo richiesto per completare le valutazioni. Le scale di valutazione con più di 4 o 5 opzioni raramente migliorano l’affidabilità o la validità dell’interpretazione dei punteggi del test e richiedono più tempo ai candidati per essere completate. Per gli item Likert, il numero massimo di opzioni sembra essere di sette gradini, con un piccolo aumento dell’affidabilità oltre tale numero.\nValutare i Benefici dell’Uso di un Numero Pari o Dispari di Opzioni: Negli item Likert, generalmente si raccomanda l’uso di un numero dispari di scelte con l’opzione centrale come “Neutrale” o “Indeciso”. Questo non è universalmente accettato poiché alcuni autori sostengono l’uso di un numero pari di scelte senza opzione neutra, basandosi sul fatto che alcuni rispondenti tendono a utilizzare eccessivamente la scelta neutra se disponibile. Ciò può risultare in una ridotta varianza e affidabilità. L’eliminazione dell’opzione neutra potrebbe frustrare alcuni rispondenti che potrebbero non completare gli item quando non hanno un’opinione forte. I dati mancanti possono essere un problema significativo in questi casi. La nostra raccomandazione è di usare un numero dispari di opzioni con un’opzione neutra. Con le scale di valutazione della frequenza, questo è meno importante poiché non è necessaria un’opzione “neutrale”.\nEtichettare Chiaramente Ciascuna delle Opzioni nelle Scale di Valutazione e negli Item Likert: Per esempio, fornire etichette per ciascuna delle opzioni di risposta per risolvere eventuali incertezze.\nMinimizzare l’Uso di Determinanti Specifici: L’uso di determinanti specifici come “mai”, “sempre”, “nessuno” e “tutti” dovrebbe essere usato con cautela in quanto possono complicare il processo di risposta.\nCon i Bambini Piccoli, Considerare l’Uso di un Formato di Intervista: Per i bambini piccoli, prendere in considerazione l’utilizzo di un formato di intervista in cui gli item vengono letti al bambino. Questo può aiutare a ridurre la varianza irrilevante al costrutto introdotta dall’eliminazione dell’impatto delle abilità di lettura.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Lo sviluppo degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/01_item_development.html#sommario",
    "href": "chapters/items/01_item_development.html#sommario",
    "title": "20  Lo sviluppo degli item",
    "section": "20.8 Sommario",
    "text": "20.8 Sommario\nAll’inizio di questo capitolo, viene fatta una distinzione principale tra gli item di test in base a se sono a risposta selezionata o a risposta costruita. Successivamente, vengono considerate le loro applicazioni nei test di massima prestazione e nei test di risposta tipica. Per i test di massima prestazione, gli item a risposta selezionata includono formati a scelta multipla, vero/falso e di abbinamento, mentre gli item a risposta costruita comprendono gli item a risposta breve e i saggi. Ogni tipo di item presenta punti di forza e debolezze che sono riassunti di seguito.\nItem a Scelta Multipla: Molto popolari nei test di massima prestazione, presentano numerosi punti di forza come versatilità, valutazione oggettiva e affidabile, e campionatura efficiente del dominio di contenuto. La loro limitazione principale è che non sono efficaci per misurare tutti gli obiettivi e non sono facili da sviluppare.\nItem Vero/Falso: Possono essere valutati in modo oggettivo e affidabile e permettono di rispondere a molti item in breve tempo. Tuttavia, hanno molte debolezze, come la limitazione a obiettivi di apprendimento semplici e una forte vulnerabilità all’indovinamento.\nItem di Abbinamento: Anche questi possono essere valutati in modo oggettivo e affidabile, completati in maniera efficiente e sono relativamente facili da sviluppare. Le loro principali limitazioni includono uno scopo limitato e la possibilità di promuovere la memorizzazione meccanica.\nSaggi: Presentano una domanda o un problema a cui il candidato risponde in formato scritto. I saggi danno una notevole libertà nella formulazione delle risposte, ma sono difficili da valutare in modo affidabile e offrono una campionatura limitata del contenuto. Sono tuttavia adatti per misurare molti obiettivi complessi.\nItem a Risposta Breve: Simili ai saggi, richiedono una risposta scritta, ma con limiti più stretti. Sono adatti per misurare specifici obiettivi di apprendimento come i calcoli matematici e sono relativamente facili da scrivere.\nGli item per i test di risposta tipica si concentrano sull’autovalutazione di sentimenti, pensieri, dialoghi interni e altri comportamenti occulti. Alcuni esempi:\nItem Vero/Falso e Altri Item Dicotomici: Comuni nei test di risposta tipica, questi item si focalizzano sulle esperienze attuali del candidato.\nScale di Valutazione: Possono essere progettate sia per misure di autovalutazione sia per la valutazione di altri individui. A differenza degli item vero/falso che offrono solo due scelte, le scale di valutazione hanno tipicamente da quattro a cinque opzioni e denotano la frequenza.\nGli item Likert, simili alle scale di valutazione, si concentrano sul grado di accordo piuttosto che sulla frequenza. Sono diventati il formato più popolare per la valutazione delle attitudini. In passato, le scale cumulative come quelle di Guttman e Thurstone erano popolari, ma le scale Likert si sono rivelate più facili da sviluppare e con proprietà psicometriche equivalenti o superiori.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Lo sviluppo degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/02_item_analysis.html",
    "href": "chapters/items/02_item_analysis.html",
    "title": "21  Analisi degli item",
    "section": "",
    "text": "21.1 Introduzione\nNel processo di sviluppo di un test, esistono numerose procedure utili per valutare la qualità e le caratteristiche di misurazione degli item del test. Tuttavia, non tutte queste procedure sono adatte per tutti i tipi di test, e non tutte forniscono lo stesso livello di informazioni sulla qualità di un determinato item. Le caratteristiche della teoria classica dei test, come la difficoltà dell’item, la discriminazione dell’item e le opzioni di risposta sbagliate, sono utili, così come le caratteristiche associate alle analisi qualitative e alle tecniche della teoria della risposta agli item.\nLa sfida per tutti i sviluppatori di test è quella di valutare i risultati di queste procedure alla luce dell’uso previsto del test e prendere decisioni nella selezione degli item che supportino e massimizzino l’efficacia complessiva del test nel misurare ciò che si propone di misurare. In altre parole, è importante garantire che gli item selezionati siano coerenti con l’obiettivo del test e possano fornire una misurazione accurata di ciò che si intende misurare.\nCome evidenziato nel capitolo precedente, la bontà di un test è determinata dalla qualità dei suoi item. Fortunatamente, esistono diverse procedure quantitative di analisi degli item che sono utili per valutare la qualità e le caratteristiche di misurazione degli item individuali che compongono i test. Queste procedure sono comunemente denominate statistiche o procedure di analisi degli item. A differenza delle analisi di affidabilità e validità che valutano le caratteristiche di misurazione di un test nel suo insieme, le procedure di analisi degli item esaminano gli item individualmente, non l’intero test. Le statistiche di analisi degli item sono utili per aiutare gli sviluppatori di test a decidere quali item mantenere, quali modificare e quali eliminare.\nLa affidabilità dei punteggi di un test e la validità dell’interpretazione dei punteggi del test dipendono dalla qualità degli item presenti nel test. Migliorando la qualità degli item individuali, si migliorerà la qualità complessiva del test. Quando si discute di affidabilità, si è notato che uno dei modi più semplici per aumentare l’affidabilità dei punteggi del test è aumentare il numero di item che contribuiscono a tali punteggi. Questa affermazione è generalmente vera ed è basata sull’assunzione che allungando un test si aggiungano item della stessa qualità degli item esistenti. Se si utilizza l’analisi degli item per eliminare gli item di scarsa qualità e migliorare gli altri, è effettivamente possibile ottenere un test più breve rispetto alla versione originale, ma che produce punteggi più affidabili e risultati con interpretazioni più valide.\nInizieremo la nostra discussione descrivendo le principali procedure quantitative di analisi degli item, tra cui la Difficoltà dell’Item, la Discriminazione dell’Item e l’Analisi delle Opzioni di Risposta. Tuttavia, è importante notare che diversi tipi di item e diversi tipi di test richiedono diverse procedure di analisi degli item. Gli item che vengono valutati in modo dicotomico (cioè giusto o sbagliato) sono gestiti diversamente rispetto agli item valutati su una scala continua (ad esempio, un saggio che può ricevere punteggi da 0 a 10). I test progettati per massimizzare la variabilità dei punteggi (ad esempio, i test con riferimento alla norma) sono gestiti diversamente rispetto ai test di padronanza (cioè valutati come superato o non superato). Mentre discutiamo delle diverse procedure di analisi degli item, specificheremo quali procedure sono più appropriate per quali tipi di item e test.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Analisi degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/02_item_analysis.html#indice-di-difficoltà-dellitem",
    "href": "chapters/items/02_item_analysis.html#indice-di-difficoltà-dellitem",
    "title": "21  Analisi degli item",
    "section": "21.2 Indice di Difficoltà dell’Item",
    "text": "21.2 Indice di Difficoltà dell’Item\nL’Indice di Difficoltà dell’Item, indicato frequentemente con la sigla “p”, rappresenta un parametro fondamentale nella valutazione degli item nei test di massima performance o competenza. Esso è definito come la frazione o la percentuale di candidati che elicitano una risposta corretta all’item in questione. Matematicamente, è espresso dalla formula:\n\\[ p = \\frac{\\text{Numero di Candidati con Risposta Corretta}}{\\text{Numero Totale di Candidati}} \\]\nPer esemplificare, in un contesto di 30 studenti, qualora 20 studenti forniscano una risposta esatta, l’indice si calcola come:\n\\[ p = \\frac{20}{30} = 0.66 \\]\nQuesto indice varia nell’intervallo [0.0, 1.0], dove valori prossimi a 1.0 indicano un’alta facilità dell’item, e viceversa valori prossimi a 0.0 denotano un’alta difficoltà. Un indice pari a 1.0 o 0.0 non contribuisce significativamente alla discriminazione tra i candidati, benché talvolta possano essere impiegati per scopi motivazionali all’inizio di un test.\n\n21.2.1 Efficienza Temporale e Livello di Difficoltà Ottimale\nLa selezione degli item in base al loro livello di difficoltà deve tenere conto anche dell’efficienza temporale. Spesso, item estremamente facili o difficili non aggiungono valore significativo alla misura del test e possono risultare in una gestione subottimale del tempo disponibile.\nIdealmente, un indice di difficoltà medio di 0.50, dove la metà dei candidati risponde correttamente e l’altra metà no, massimizza la variabilità e l’affidabilità del test. Tuttavia, questa uniformità non è sempre desiderabile o fattibile, a causa delle interrelazioni tra gli item e delle specifiche esigenze di misurazione.\n\n\n21.2.2 Influenza del Guessing e Tipologie di Test\nIl livello di difficoltà ottimale varia a seconda della tipologia del test e della possibilità di indovinare le risposte. Nei test con item a risposta costruita, dove l’indovinare è meno rilevante, un indice medio di 0.50 è generalmente preferibile. Nei test a risposta selezionata, come quelli a scelta multipla, si considera un valore medio di “p” più elevato per bilanciare l’effetto dell’indovinamento.\n\n\n21.2.3 Contesti Specifici di Valutazione\nIn test con riferimento ai criteri o test di padronanza, la valutazione della difficoltà segue logiche differenti. Per esempio, in test di padronanza, è comune che la maggior parte degli item abbia un indice “p” elevato, per riflettere l’aspettativa che la maggioranza dei candidati superi il test. In contesti di selezione o per test destinati a individuare candidati altamente performanti, si potrebbero preferire item con un indice di difficoltà significativamente diverso.\n\n\n21.2.4 Variazioni in Funzione del Campione\nÈ cruciale notare che l’indice di difficoltà è intrinsecamente legato alle caratteristiche del campione considerato. Ad esempio, lo stesso item può presentare indici di difficoltà differenti se somministrato a gruppi con livelli di competenza diversi.\n\n\n21.2.5 Statistica della Percentuale di Approvazione\nPer i test di risposta tipica, si utilizza un indice analogo all’indice di difficoltà, noto come statistica della percentuale di approvazione. Questa statistica indica la percentuale di candidati che rispondono in un determinato modo a un item, e varia a seconda del campione e del contesto.\n\n\n21.2.6 Applicazioni nell’Analisi e Sviluppo di Test\nL’analisi dell’indice di difficoltà e di altre statistiche relative agli item è cruciale per gli sviluppatori di test nella selezione, modifica, o eliminazione degli item durante lo sviluppo o la revisione dei test. Tale analisi è complementare ad altre procedure di analisi degli item, come l’indice di discriminazione dell’item, che sarà discusso successivamente.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Analisi degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/02_item_analysis.html#discriminazione-dellitem",
    "href": "chapters/items/02_item_analysis.html#discriminazione-dellitem",
    "title": "21  Analisi degli item",
    "section": "21.3 Discriminazione dell’item",
    "text": "21.3 Discriminazione dell’item\nL’item discrimination, in italiano “discriminazione dell’item,” si riferisce a quanto bene un item può discriminare o differenziare tra i partecipanti al test che differiscono sulla costrutto misurato dal test. Ad esempio, se un test è progettato per misurare l’abilità di lettura, la discriminazione dell’item riflette la capacità di un item di distinguere tra individui con buone capacità di lettura e quelli con scarse capacità di lettura. A differenza del livello di difficoltà dell’item, per il quale esiste un accordo su come calcolare la statistica, nel corso degli anni sono stati sviluppati oltre 50 diversi indici di discriminazione dell’item (Anastasi & Urbina, 1997). Fortunatamente, la maggior parte di questi indici produce risultati simili (Engelhart, 1965; Oosterhof, 1976). Ci concentreremo sulla discussione di due dei più popolari indici di discriminazione dell’item: l’indice di discriminazione e le correlazioni item-totali.\n\n21.3.1 Indice di Discriminazione\nUn metodo popolare per calcolare un indice di discriminazione dell’item si basa sulla differenza nelle prestazioni tra due gruppi. Sebbene ci siano modi diversi per selezionare i due gruppi, vengono tipicamente definiti in termini di prestazioni totali al test. Un approccio comune è selezionare il 27% migliore e il 27% peggiore dei partecipanti in termini di prestazioni complessive al test, escludendo il 46% centrale (Kelley, 1939). Alcuni esperti di valutazione hanno suggerito di utilizzare il 25% migliore e il 25% peggiore, alcuni il 33% migliore e il 33% peggiore, e alcuni la metà superiore e inferiore. In pratica, tutti questi sono probabilmente accettabili, ma la nostra raccomandazione è di utilizzare il tradizionale 27% superiore e inferiore. La difficoltà dell’item è calcolata separatamente per ciascun gruppo, e questi sono denominati pT e pB (“T” per il gruppo superiore, “B” per il gruppo inferiore). La differenza tra pT e pB è l’indice di discriminazione, designato come D. Viene calcolato con la seguente formula (es. Johnson, 1951):\n\\[\nD = p_T - p_B\n\\]\ndove: - D = Indice di Discriminazione - pT = Proporzione dei partecipanti nel Gruppo Superiore che Risponde Correttamente all’Item - pB = Proporzione dei partecipanti nel Gruppo Inferiore che Risponde Correttamente all’Item\nPer illustrare la logica di questo indice, consideriamo un test di rendimento progettato per misurare il rendimento accademico in un’area specifica. Se l’item sta discriminando tra partecipanti che conoscono il materiale e quelli che non lo conoscono, allora i partecipanti più informati (cioè, quelli nel “gruppo superiore”) dovrebbero rispondere correttamente all’item più spesso dei partecipanti meno informati (cioè, quelli nel “gruppo inferiore”). Ad esempio, se pT = 0.80 (indicando che l’80% dei partecipanti nel gruppo superiore ha risposto correttamente all’item) e pB = 0.30 (indicando che il 30% dei partecipanti nel gruppo inferiore ha risposto correttamente all’item), allora:\n\\[\nD = 0.80 - 0.30 = 0.50.\n\\]\nHopkins (1998) ha fornito linee guida per valutare gli item in termini dei loro valori di D. Secondo queste linee guida, i valori di D superiori a 0.40 sono considerati eccellenti, tra 0.30 e 0.39 sono buoni, tra 0.11 e 0.29 sono accettabili e tra 0.00 e 0.10 sono scadenti. Gli item con valori di D negativi probabilmente sono stati formulati in modo errato o presentano altri problemi gravi. Altri esperti di valutazione hanno fornito linee guida diverse, alcune più rigorose e altre più indulgenti. Come regola generale, si suggerisce che gli item con valori di D superiori a 0.30 siano accettabili (quanto più alti, tanto meglio), mentre gli item con valori di D inferiori a 0.30 dovrebbero essere attentamente valutati, e eventualmente rivisti o eliminati.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Analisi degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/02_item_analysis.html#test-di-padronanza",
    "href": "chapters/items/02_item_analysis.html#test-di-padronanza",
    "title": "21  Analisi degli item",
    "section": "21.4 Test di Padronanza",
    "text": "21.4 Test di Padronanza\nNei test di padronanza, gli item tendono ad avere indici di difficoltà più elevati rispetto a quelli di test normativi, implicando che gli item sono generalmente più facili. Questa caratteristica deriva dall’assunzione che la maggior parte dei partecipanti otterrà risultati positivi nei test di padronanza. Di conseguenza, è frequente che gli item in questi test abbiano una proporzione elevata di risposte corrette (valori medi di p), talvolta raggiungendo il 90%.\nQuesta tendenza richiede un adattamento nell’interpretazione degli indici di difficoltà degli item. La necessità di adattamento si estende anche all’interpretazione degli indici di discriminazione. Nei test di padronanza, dove è comune che sia partecipanti con punteggi alti sia quelli con punteggi bassi otteniano valori elevati di p, gli indici tradizionali di discriminazione potrebbero non riflettere accuratamente le capacità di misurazione di un item.\nPer affrontare questa sfida, sono stati proposti diversi metodi per calcolare la discriminazione degli item nei test di padronanza. Aiken (2000), ad esempio, suggerisce un metodo che considera la difficoltà degli item basandosi sui partecipanti che hanno raggiunto (o non raggiunto) il punteggio di padronanza. La formula proposta è:\n\\[\nD = p_{mastery} - p_{non-mastery}\n\\]\ndove \\(p_{mastery}\\) rappresenta la proporzione di partecipanti che hanno raggiunto la padronanza e hanno risposto correttamente all’item, mentre \\(p_{non-mastery}\\) indica la proporzione di partecipanti che non hanno raggiunto la padronanza e hanno risposto correttamente.\nUn altro metodo per valutare la discriminazione degli item è l’uso della correlazione item-totale. Questo approccio correla le prestazioni su un singolo item con il punteggio totale del test, utilizzando di solito la correlazione punto-biserial. Un’alta correlazione item-totale indica che l’item misura lo stesso costrutto dell’intero test e discrimina efficacemente tra individui con alta e bassa competenza nel costrutto misurato. È preferibile calcolare questa correlazione escludendo l’item in esame dal punteggio totale del test, per evitare di “contaminare” o gonfiare la correlazione. Attualmente, la correlazione item-totale è il metodo più utilizzato per esaminare la discriminazione degli item nei test.\nQuesti approcci offrono strumenti importanti per comprendere e migliorare la qualità dei test di padronanza, garantendo che siano sia accessibili sia capaci di distinguere accuratamente tra diversi livelli di competenza.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Analisi degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/02_item_analysis.html#discriminazione-degli-item-nei-test-con-risposte-tipiche",
    "href": "chapters/items/02_item_analysis.html#discriminazione-degli-item-nei-test-con-risposte-tipiche",
    "title": "21  Analisi degli item",
    "section": "21.5 Discriminazione degli Item nei Test con Risposte Tipiche",
    "text": "21.5 Discriminazione degli Item nei Test con Risposte Tipiche\nL’analisi degli item nei test con risposte tipiche riguarda l’analisi di item quelli finalizzati alla misurazione di tendenze comportamentali. Un esempio pertinente è un item di un test progettato per valutare la propensione alla ricerca di sensazioni, basato su affermazioni a cui si risponde con “Vero” o “Falso”. In questo contesto, le risposte “Vero” (valutate con “1”) indicano una tendenza verso comportamenti di ricerca di sensazioni, mentre le risposte “Falso” (valutate con “0”) denotano una propensione ad evitarli. Pertanto, punteggi elevati in tali test suggeriscono un gusto per comportamenti ad alta sensazione, mentre punteggi bassi indicano una tendenza all’evitamento.\nLa correlazione tra gli item e il punteggio totale del test può essere usata per l’identificazione degli item più efficaci nel discriminare tra individui con diverse propensioni alla ricerca di sensazioni. Gli item con elevata correlazione risultano particolarmente utili per distinguere soggetti con alta o bassa propensione a tali comportamenti.\nL’interpretazione degli indici di difficoltà e discriminazione diventa però più complessa nei cosiddetti “test di velocità”. Questi test sono caratterizzati da item generalmente facili, ma con limiti di tempo stringenti che impediscono ai candidati di completarli tutti. La prestazione nei test di velocità è quindi influenzata principalmente dalla rapidità di risposta, contrariamente ai test di potenza, dove il tempo non è un fattore limitante e la difficoltà degli item varia significativamente.\nNei test di velocità, la difficoltà e la capacità discriminativa degli item tendono a riflettere la loro posizione all’interno del test piuttosto che la loro intrinseca difficoltà o capacità di discriminare tra candidati. Gli item situati verso la fine del test tendono ad essere completati da un numero minore di candidati a causa dei limiti temporali, non perché siano effettivamente più difficili. Allo stesso modo, l’indice di discriminazione degli item posti verso la fine potrebbe essere esagerato, in quanto solo i candidati più capaci riescono a raggiungerli e completarli.\nNonostante siano state proposte varie metodologie per controbilanciare questi fattori, ogni approccio presenta delle limitazioni e non ha ancora guadagnato un’ampia accettazione nella comunità scientifica. Pertanto, è essenziale essere consapevoli di queste complessità e tenerle in considerazione nell’interpretazione delle analisi degli item nei test di velocità.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Analisi degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/02_item_analysis.html#analisi-dei-distrattori",
    "href": "chapters/items/02_item_analysis.html#analisi-dei-distrattori",
    "title": "21  Analisi degli item",
    "section": "21.6 Analisi dei Distrattori",
    "text": "21.6 Analisi dei Distrattori\nÈ cruciale valutare l’efficacia dei distrattori nell’analisi quantitativa dei test a scelta multipla. I distrattori sono le opzioni errate fornite nelle domande, progettate per deviare l’attenzione degli esaminandi meno preparati. Questa analisi esamina la frequenza con cui esaminandi con punteggi alti e bassi scelgono ciascun distrattore. Un aspetto fondamentale è analizzare ogni distrattore ponendo due questioni centrali.\nPrima, il distrattore è effettivamente un’opzione che confonde alcuni esaminandi? Se nessuno sceglie il distrattore, questo indica che non sta adempiendo alla sua funzione. Un distrattore adeguato dovrebbe essere selezionato da alcuni esaminandi. Al contrario, se un distrattore è palesemente errato e nessuno lo sceglie, risulta inefficace e necessita di revisione o sostituzione.\nSeconda, quanto bene il distrattore discrimina tra i diversi gruppi di esaminandi? I distrattori efficaci tendono ad attrarre più candidati con punteggi bassi rispetto a quelli con punteggi alti. Analizzando la risposta corretta, ci aspettiamo che più esaminandi con punteggi alti la scelgano rispetto a quelli con punteggi bassi, indicando una discriminazione positiva. Per i distrattori, l’effetto dovrebbe essere l’opposto: più esaminandi con punteggi bassi dovrebbero scegliere i distrattori rispetto a quelli con punteggi alti, dimostrando così una discriminazione negativa.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Analisi degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/02_item_analysis.html#le-curve-caratteristiche-degli-item-nella-irt",
    "href": "chapters/items/02_item_analysis.html#le-curve-caratteristiche-degli-item-nella-irt",
    "title": "21  Analisi degli item",
    "section": "21.7 Le Curve Caratteristiche Degli Item nella IRT",
    "text": "21.7 Le Curve Caratteristiche Degli Item nella IRT\nUn argomento importante nella discussione dell’analisi degli item riguarda la nozione di curva caratteristica dell’item, così com’è stata formultata dalla IRT. Questo argomento verrà trattato in maniera approfondita in un capitolo successivo.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Analisi degli item</span>"
    ]
  },
  {
    "objectID": "chapters/items/02_item_analysis.html#considerazioni-conclusive",
    "href": "chapters/items/02_item_analysis.html#considerazioni-conclusive",
    "title": "21  Analisi degli item",
    "section": "21.8 Considerazioni Conclusive",
    "text": "21.8 Considerazioni Conclusive\nQuesto capitolo fornisce una panoramica approfondita delle procedure di analisi degli item, strumenti fondamentali per gli sviluppatori di test nel decidere quali item mantenere, modificare o eliminare. Si esplorano diverse procedure, tra cui:\n\nLivello di Difficoltà dell’Item: Definito come la percentuale di esaminandi che rispondono correttamente a un item, l’indice di difficoltà (p) varia da 0.0 a 1.0. Gli item più facili presentano valori decimali maggiori, mentre quelli difficili valori minori. Il livello ottimale di difficoltà, per massimizzare la variabilità tra gli esaminandi, è 0.50. Tuttavia, a seconda della situazione, possono essere preferiti valori diversi, generalmente tra 0.20 e 0.80.\nDiscriminazione dell’Item: Questo concetto si riferisce alla capacità di un item di distinguere tra esaminandi che variano rispetto al costrutto del test. Discutiamo l’indice di discriminazione dell’item (D), considerando accettabili valori di D pari o superiori a 0.30, mentre quelli inferiori a 0.30 potrebbero richiedere revisione o eliminazione. Esploriamo anche la correlazione item-totalità come metodo alternativo per esaminare la discriminazione.\nAnalisi dei Distrattori: Questa procedura valuta l’efficacia dei distrattori nelle domande a scelta multipla, ponendo due domande principali: un distrattore funzionale dovrebbe attirare alcuni esaminandi e, per la discriminazione, dovrebbe attrarre più esaminandi nel gruppo con punteggi bassi rispetto a quelli con punteggi alti.\nProcedure Qualitative: Oltre alle procedure quantitative, si suggerisce l’utilizzo di metodi qualitativi per migliorare i test. Tra questi, la revisione accurata del test, la valutazione da parte di colleghi fidati e il feedback degli esaminandi sulla chiarezza e problemi degli item.\nCurve Caratteristiche dell’Item e Teoria della Risposta all’Item (IRT): Le curve caratteristiche dell’item (ICC) rappresentano graficamente la relazione tra l’abilità e la probabilità di risposta corretta. L’IRT, componente centrale delle ICC, assume che le risposte agli item siano determinate da tratti latenti e ha influenzato significativamente lo sviluppo di test moderni, inclusi i test adattivi computerizzati.\n\nL’utilizzo di queste procedure durante lo sviluppo del test migliora l’affidabilità dei punteggi e la validità delle loro interpretazioni, essendo entrambe dipendenti dalla qualità degli item. La rimozione degli item scarsi e il miglioramento degli altri possono anche portare a un test più corto ed efficiente, con punteggi più affidabili e interpretazioni più valide.",
    "crumbs": [
      "Items",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Analisi degli item</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html",
    "href": "chapters/path_analysis/01_path_analysis.html",
    "title": "22  Analisi dei percorsi",
    "section": "",
    "text": "22.1 Introduzione\nIl fulcro dell’analisi del percorso è la decomposizione e la quantificazione delle relazioni tra le variabili, permettendo agli analisti di distinguere tra effetti diretti, indiretti e totali. Gli effetti diretti corrispondono all’influenza immediata che una variabile esercita su un’altra, mentre gli effetti indiretti rappresentano l’impatto mediato attraverso una o più variabili intermedie. L’effetto totale è la somma degli effetti diretti e indiretti.\nSewall Wright, un genetista che operava presso il Dipartimento dell’Agricoltura degli Stati Uniti, fu il precursore nello sviluppo dei diagrammi di percorso per descrivere i modelli di equazioni strutturali già negli anni ’20 del secolo scorso. Questa sua innovazione ha permesso di ottenere una rappresentazione visiva delle connessioni tra variabili, aprendo la strada all’analisi del percorso.\nCon il trascorrere del tempo, questa metodologia è stata adottata con successo come uno strumento efficace per discriminare gli effetti diretti da quelli indiretti nelle relazioni tra variabili. Inoltre, essa si è dimostrata di grande utilità nel valutare la solidità e la validità delle relazioni causali ipotizzate all’interno dei modelli di equazioni strutturali.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#path-diagram",
    "href": "chapters/path_analysis/01_path_analysis.html#path-diagram",
    "title": "22  Analisi dei percorsi",
    "section": "22.2 Path diagram",
    "text": "22.2 Path diagram\nIl diagramma del percorso, noto anche come “path diagram,” costituisce uno strumento per la rappresentazione grafica delle relazioni tra variabili all’interno di un modello. All’interno di questo diagramma, le variabili latenti o non osservate sono rappresentate mediante cerchi o ellissi, mentre le variabili osservate sono rappresentate da quadrati o rettangoli.\nAll’interno del path diagram, è possibile individuare due categorie di variabili: quelle che subiscono influenze da parte di altre variabili nel sistema e quelle che svolgono il ruolo di generatori di effetti. Nello specifico, le variabili esogene costituiscono elementi esterni al sistema in esame, operando in qualità di variabili indipendenti che generano effetti causalmente. Al contrario, le variabili endogene possono agire sia come risultati di altre variabili che come cause per ulteriori variabili, oppure possono essere strettamente variabili dipendenti. Le origini causali delle variabili endogene trovano collocazione all’interno del path diagram, mentre quelle delle variabili esogene si trovano esternamente al diagramma. Tale distinzione presenta affinità con la distinzione tra variabili indipendenti e dipendenti all’interno dei modelli lineari.\nUn diagramma di percorso è costituito dai seguenti simboli grafici.\n\nVariabili osservate (indicatori) rappresentate con quadrati o rettangoli.\nProxy per variabili latenti, come fattori comuni con indicatori multipli, rappresentate con cerchi o ellissi.\n\nIl diagramma di percorso mette in evidenza le interazioni tra le variabili d’interesse, sottolineando i legami causali o associativi che le connettono. Le frecce unidirezionali (ad esempio,\\(\\rightarrow\\)) illustrano relazioni causali: una variabile subisce influenza da un’altra variabile collegata attraverso una freccia. Invece, le frecce curve bidirezionali denotano relazioni associative, senza implicare una causalità diretta tra le variabili, ovvero covarianze (nella soluzione non standardizzata) o correlazioni (nella soluzione standardizzata).\nL’assenza di una freccia tra due variabili nel diagramma suggerisce l’assenza di correlazione tra di esse. Nel caso della Figura 22.1, si illustrano le relazioni tra nove variabili osservate e tre variabili latenti mediante il path diagram.\n\n\n\n\n\n\nFigura 22.1: Diagramma di percorso per un modello a tre fattori comuni.\n\n\n\nUna freccia curva bidirezionale che si collega a una singola variabile rappresenta la varianza residua della variabile, ovvero la quota di varianza non spiegata dalle relazioni causali illustrate nel diagramma di percorso. Un triangolo contenente il numero 1 simboleggia la media di una variabile (qui non presente).\n\n22.2.1 Parametri nei Modelli di Equazioni Strutturali\nI parametri nei modelli di equazioni strutturali possono essere categorizzati come segue, quando le medie non sono oggetto di analisi:\n\nVarianze e Covarianze delle Variabili Esogene:\n\nQuesti parametri rappresentano la variabilità intrinseca delle variabili esogene (quelle non influenzate da altre nel modello) e le relazioni reciproche tra di esse.\n\nEffetti Diretti sulle Variabili Endogene da Altre Variabili:\n\nQuesti parametri descrivono come le variabili endogene sono influenzate direttamente da altre variabili nel modello.\n\n\nIn termini di specificazione, un parametro nel modello può essere classificato come libero, fisso o vincolato:\n\nParametro Libero:\n\nQuesto tipo di parametro è stimato dal software statistico utilizzando i dati a disposizione.\n\nParametro Fisso:\n\nUn parametro fisso è definito per essere uguale a una costante specificata a priori. In questo caso, il software accetta il valore costante come stima, indipendentemente dai dati. Ad esempio, l’ipotesi che la variabile X non abbia effetti diretti su Y corrisponde alla specifica che il coefficiente per il percorso da X a Y sia fissato a zero.\n\nParametro Vincolato:\n\nIn questo caso, il parametro segue certe restrizioni imposte nell’analisi, che possono essere basate su teorie o ipotesi precedenti. Ad esempio, l’analista può assumere che due paraemetri siano uguali.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#gradi-di-libertà-nei-modelli-parametrici",
    "href": "chapters/path_analysis/01_path_analysis.html#gradi-di-libertà-nei-modelli-parametrici",
    "title": "22  Analisi dei percorsi",
    "section": "22.3 Gradi di Libertà nei Modelli Parametrici",
    "text": "22.3 Gradi di Libertà nei Modelli Parametrici\nIn statistica, la complessità di un modello parametrico, in termini di parametri da stimare, è limitata dalla quantità di informazioni statistiche fornite dai dati. Questo non equivale alla dimensione del campione (\\(N\\)), ma si riferisce al numero di varianze e covarianze uniche che si possono derivare dalla matrice di covarianza campionaria in forma triangolare inferiore.\nLa quantità di informazioni statistiche disponibili in un modello si calcola come segue:\n\nSe \\(v\\) è il numero di variabili osservate, la quantità di informazioni è data dalla formula:\n\n\\[\\frac{v(v + 1)}{2},\\]\nquando le medie delle variabili non sono incluse nell’analisi.\nAd esempio, se \\(v = 5\\) (cioè ci sono 5 variabili osservate nel modello), la quantità di informazioni statistiche sarà:\n\\[\\frac{5 \\times 6}{2} = 15.\\]\nQuesto valore (15) rappresenta il numero totale di varianze (5) e covarianze uniche (10) che si trovano sotto la diagonale principale nella matrice di covarianza campionaria. In questo caso, il massimo numero di parametri stimabili è 15. Un modello più semplice potrebbe stimare un numero inferiore di parametri, ma non più di 15. La quantità di informazioni statistiche non dipende dalla dimensione del campione: anche se ci fossero 100 o 1000 casi, con 5 variabili misurate, la quantità di informazioni resterebbe 15. Solo l’aggiunta di nuove variabili osservate può incrementare questo numero.\nLa differenza tra la quantità di informazioni statistiche (\\(p\\)) e il numero di parametri liberi (\\(q\\)) determina i gradi di libertà del modello (\\(df_M\\)), che si calcolano come:\n\\[df_M = p - q.\\]\nPerché un modello sia identificabile, è necessario che i gradi di libertà siano almeno pari a zero (\\(df_M \\geq 0\\)). Se il numero di parametri da stimare supera la quantità di informazioni disponibili (\\(df_M &lt; 0\\)), il modello non è identificabile e non sarà possibile stimare i parametri in modo univoco, poiché esisterebbero infinite soluzioni possibili. Tentare di stimare un modello con gradi di libertà negativi genera solitamente errori nei software di modellazione. In questi casi, il modello deve essere ridefinito riducendo il numero di parametri liberi, ad esempio, imponendo vincoli o fissando alcuni parametri a valori specifici.\nUn modello con zero gradi di libertà (\\(df_M = 0\\)) si adatta perfettamente ai dati di un campione, così come a qualsiasi altro campione delle stesse variabili. Tuttavia, i modelli con gradi di libertà positivi (\\(df_M &gt; 0\\)) generalmente non offrono un adattamento perfetto, lasciando spazio a discrepanze tra i dati osservati e le stime del modello. Raykov e Marcoulides (2006) hanno descritto i gradi di libertà come dimensioni lungo le quali un modello può potenzialmente essere rifiutato. Pertanto, i modelli che risultano validi con maggiori gradi di libertà hanno superato un rischio maggiore di rifiuto. Questo rafforza il principio di parsimonia: a parità di adattamento ai dati, è preferibile il modello più semplice, purché sia teoricamente plausibile.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#varianza-residua-nelle-variabili-endogene",
    "href": "chapters/path_analysis/01_path_analysis.html#varianza-residua-nelle-variabili-endogene",
    "title": "22  Analisi dei percorsi",
    "section": "22.4 Varianza Residua nelle Variabili Endogene",
    "text": "22.4 Varianza Residua nelle Variabili Endogene\nLa Figura 22.2 mostra la relazione tra due variabili osservabili. L’effetto totale presunto di X su Y è illustrato tramite un percorso diretto, rappresentando l’effetto causale lineare di X su Y. La varianza di X, una variabile esogena, è un parametro libero e viene rappresentata nella figura con il simbolo RAM che indica una varianza (indicata da una freccia curva bidirezionale). Al contrario, la varianza di Y, una variabile endogena, non è libera di variare; invece, è associata a una variabile latente D, il termine di disturbo o errore, che rappresenta la variazione in Y non spiegata da X.\n\n\n\n\n\n\nFigura 22.2: Diagramma per una rappresentazione contratta nel modello completo di azione reticolare McArdle-McDonald (RAM) con simbolismo grafico (a) rispetto a una versione più compatta (b). (Figura adattata da Kline (2023))\n\n\n\nIl numero (1) vicino al percorso nella Figura 22.2 (a) è una costante di scala che assegna una metrica al termine di disturbo. Questa specificazione è essenziale perché la varianza del termine di disturbo è latente e le variabili latenti richiedono che un fattore di scala sia fissato per la loro stima. Questa costante di scala è nota anche come il vincolo di identificazione del carico unitario (unit loading identification constraint, ULI). Il valore “1” comunica al software di suddividere la varianza totale (osservata) di Y in due componenti distinte (ortogonali): la varianza spiegata da X e la varianza non spiegata (o varianza del disturbo, \\(var_D\\)).\nLa rappresentazione nella Figura 22.2 (b) fornisce le stesse informazioni in modo più sintetico. Alternativamente, nella Figura 22.2 (a) si potrebbe rappresentare la varianza residua di Y con una freccia curva bidirezionale, al posto di utilizzare il termine di disturbo D identificato dal vincolo di identificazione del carico unitario. Il valore numerico associato a questa freccia curva bidirezionale sarebbe lo stesso di quello che si ottiene con la rappresentazione della variabile latente di disturbo:\\(1 \\times var_D \\times 1\\).\nUn’altra rappresentazione equivalente assegnerebbe il valore 1 a \\(var_D\\) e attribuirebbe alla freccia causale da D a Y il valore \\(\\sqrt{var_D}\\). Il risultato finale sarebbe identico, in quanto anche in questo caso la varianza residua di Y sarebbe rappresentata come\\(\\sqrt{var_D} \\times 1 \\times \\sqrt{var_D}\\).\nProseguendo la discussione sulla varianza del disturbo, possiamo identificare quattro fonti principali che contribuiscono a questa varianza:\n\nVariazione Sistematica da Cause Non Misure: Questa varianza origina da fattori non misurati che influenzano sistematicamente l’esito della variabile di interesse. Si tratta di influenze esterne o variabili nascoste che hanno un impatto significativo ma non sono incluse nel modello.\nVariazione Casuale Intrinseca: Questo tipo di varianza è una caratteristica fondamentale di quasi tutti i sistemi o variabili individuali. Rappresenta la variabilità naturale che esiste indipendentemente dalle misure o dagli effetti che si tenta di analizzare.\nErrore di Misurazione Casuale: Questa varianza è legata agli errori che si verificano durante il processo di misurazione. Includono gli errori casuali che possono essere stimati attraverso analisi di affidabilità, come l’accuratezza e la precisione degli strumenti di misurazione utilizzati.\nMancata Specificazione della Corretta Forma Funzionale dell’Effetto Causale: Questa varianza emerge quando la forma funzionale dell’effetto causale nel modello non corrisponde alla vera natura della relazione. Un esempio comune è modellare una relazione come lineare quando in realtà è non lineare, portando a una rappresentazione imprecisa del fenomeno sotto indagine.\n\nNel pannello (a) della Figura 22.2, il percorso da D a Y rappresenta l’effetto diretto di tutte queste cause omesse, oltre agli errori, sulla variabile endogena Y. In sostanza, questo percorso simboleggia l’insieme di tutte le influenze non incluse nel modello che possono impattare su Y. È importante notare che, mentre queste fonti di varianza del disturbo possono essere teoricamente distinte, nella pratica possono sovrapporsi e interagire tra loro.\nProseguendo il discorso sulla rappresentazione della varianza residua nelle variabili endogene, è importante notare che diversi software SEM trattano in modi differenti i termini di errore nei modelli di equazioni strutturali. Ad esempio, nella sintassi del software lavaan, il comando:\nY ~ X\ndirettamente istruisce il software a regredire la variabile Y su X e a gestire automaticamente il termine di disturbo, come rappresentato nel pannello (a) della Figura 22.2. Questo comando, oltre a definire l’effetto di X su Y, stabilisce anche che le varianze di X e il termine di disturbo di Y sono parametri liberi da stimare.\nDa queste considerazioni emergono due requisiti fondamentali per l’identificazione di un modello a percorsi:\n\nI gradi di libertà del modello (\\(df_M\\)) devono essere maggiori o uguali a zero.\nOgni variabile latente, inclusi i termini di errore, deve avere una scala definita (una metrica assegnata).\n\nIl conteggio dei parametri liberi è una componente cruciale nel calcolo dei \\(df_M\\). L’inclusione esplicita delle costanti di scala nei diagrammi serve come promemoria per i ricercatori sulla necessità di assegnare una scala alle variabili latenti.\nIl pannello (b) della Figura 22.2 mostra una versione più sintetica del modello, utilizzando un simbolismo grafico che omette i simboli per i parametri di varianza (per X, D), la costante di scala (1) e la rappresentazione grafica del disturbo come variabile latente. Questo diagramma fornisce una visione meno dettagliata del modello, evidenziando solamente le relazioni di base, ovvero X che causa Y, e Y influenzata da un termine di disturbo.\n\n22.4.1 Considerazioni sugli Errori di Misurazione nei Modelli a Percorsi\nRiprendendo la discussione sulla Figura 22.2, possiamo delineare le seguenti ipotesi fondamentali:\n\nAffidabilità della Variabile Esogena X: Si assume che i punteggi sulla variabile esogena X siano privi di errore, ovvero perfettamente affidabili, con un coefficiente di affidabilità (\\(r_{XX}\\)) di 1.0.\nCorrettezza della Direzione Causale: La relazione causale da X a Y è assunta come correttamente specificata e caratterizzata da una stretta linearità.\nIndipendenza delle Cause Non Misurate di Y da X: Si presume che le cause non misurate (latenti) di Y non siano correlate con X, escludendo quindi l’esistenza di cause comuni non misurate che influenzano simultaneamente entrambe le variabili – ricordiamo la discusione precedente sull’errore di specificazione.\n\nIn ambito di modellazione dei percorsi, l’assunzione che le variabili esogene siano prive di errori di misurazione riflette un presupposto simile a quello adottato nelle regressioni multiple standard, dove i predittori sono considerati esenti da errori di misurazione. Questa assunzione è necessaria poiché le variabili esogene nei modelli a percorsi non includono termini di errore, rendendo impossibile incorporare l’errore casuale in tali modelli. Al contrario, nelle variabili endogene di tali modelli, la presenza di termini di errore permette di tenere conto dell’errore di misurazione.\nNel caso di una regressione bivariata, un errore di misurazione presente solo nella variabile dipendente Y influisce sul modello aumentando l’errore standard della stima di regressione, riducendo il valore di \\(R^2\\) e diminuendo il valore assoluto del coefficiente di regressione standardizzato, a causa dell’incremento dell’errore di misurazione in Y. Invece, l’errore di misurazione presente solo nella variabile predittiva X (ma non in Y) tende a introdurre un bias negativo nei coefficienti di regressione – cioè una sistematica sottostima dei veri valori dei coefficienti di regressione.\nQuando entrambe le variabili X e Y presentano errori di misurazione, la dinamica risultante è più complessa da prevedere. Se gli errori di misurazione in X e Y sono indipendenti, il risultato più comune è un bias negativo (ossia una sottostima dei coefficienti di regressione della popolazione). Tuttavia, se gli errori di misurazione sono comuni tra X e Y, la regressione potrebbe sovrastimare i coefficienti della popolazione, portando a un bias positivo. È essenziale riconoscere che l’errore di misurazione non causa sempre un bias negativo. Di conseguenza, la presenza di errori di misurazione non modellati nelle variabili esogene può significativamente distorcere i risultati, specialmente in presenza di forti correlazioni tra multiple variabili esogene. Per ridurre questi rischi, si raccomanda di valutare l’affidabilità dei punteggi associati alle variabili esogene. Questa pratica metodologica, che consiste nel verificare la precisione e la consistenza delle misure delle variabili predittive, aiuta a identificare e quantificare eventuali errori di misurazione. Un’accurata stima dell’affidabilità contribuisce a garantire l’integrità e la validità dei risultati dei modelli a percorsi, mitigando l’impatto che gli errori di misurazione possono avere sull’analisi.\n\n\n22.4.2 Direzionalità Causale e Forma Funzionale della Relazione X-Y\nL’assunzione che la relazione tra le variabili X e Y sia lineare, come presentato nella Figura 22.2, può essere esaminata attraverso l’analisi dei dati. Se si osserva che la relazione è significativamente curvilinea, si può adeguare l’analisi per attenuare il presupposto di linearità. Ciò può essere realizzato attraverso metodi come la regressione polinomiale o la regressione non parametrica, che permettono di modellare relazioni più complesse rispetto a un semplice modello lineare.\nTuttavia, la direzionalità dell’effetto causale rappresenta una sfida differente e non è direttamente testabile attraverso metodi statistici standard. Nell’ambito dei modelli SEM, le direzioni degli effetti causali sono generalmente ipotizzate piuttosto che empiricamente verificate. Questo perché è possibile costruire modelli SEM equivalenti che utilizzano le stesse variabili e hanno lo stesso numero di gradi di libertà (\\(df_M\\)), ma con direzioni inverse di alcuni effetti causali. Inoltre, entrambi i modelli, nonostante le differenze nelle direzionalità causali, mostreranno lo stesso grado di adattamento ai dati osservati.\nUn’ulteriore ragione per cui la direzionalità causale è tipicamente assunta piuttosto che testata in SEM risiede nella natura degli studi SEM stessi. La maggior parte degli studi SEM si basa su disegni trasversali, dove tutte le variabili sono misurate contemporaneamente, senza una chiara precedenza temporale. In questi contesti, l’unica base per definire la direzionalità causale è l’argomentazione teorica del ricercatore, che deve giustificare perché si presume che X influenzi Y e non viceversa, o perché non si considera una relazione di feedback o causazione reciproca tra le due variabili.\nDi conseguenza, la metodologia SEM non è intrinsecamente una tecnica per la scoperta di relazioni causali. Se un modello è corretto, SEM può essere utilizzato per stimare le direzioni, le dimensioni e la precisione degli effetti causali. Tuttavia, questo non è il modo in cui i ricercatori tipicamente impiegano le analisi SEM. Piuttosto, un modello causale viene ipotizzato e poi adattato ai dati basandosi sulle assunzioni delineate. Se queste assunzioni risultano essere errate, anche i risultati dell’analisi saranno invalidi. Questo enfatizza il punto sollevato da Pearl (2000), che sostiene che\n\nle ipotesi causali sono un prerequisito essenziale per validare qualsiasi conclusione causale (p. 136).\n\nQuesto implica la necessità di una solida base teorica e concettuale nella formulazione di modelli causali nella modellazione SEM.\n\n\n22.4.3 Confondimento nei Modelli Parametrici\nNella teoria dei modelli statistici, l’endogenità si riferisce a una situazione in cui una variabile all’interno di un modello è correlata con i termini di errore. Questo può creare problemi nella stima dei parametri del modello e può portare a conclusioni errate riguardo le relazioni causali tra le variabili.\nNel contesto del diagramma di una catena contratta della Figura 22.3, l’endogenità è visualizzata come una covarianza tra la variabile causale misurata X e il disturbo (termine di errore) di Y, indicata con un simbolo specifico. Questo simbolo mostra che c’è una relazione non spiegata tra la causa X e il disturbo associato a Y, suggerendo che X potrebbe non essere una variabile completamente indipendente, come idealmente dovrebbe essere in un modello causale chiaro.\nIl modello nella Figura 22.3 (a) non è identificabile per due ragioni principali:\n\nGradi di libertà negativi (dfM = -1): Questo indica che ci sono più parametri da stimare nel modello rispetto al numero di informazioni (osservazioni) disponibili. In sostanza, il modello sta cercando di “apprendere” troppo da troppo pochi dati, il che lo rende statisticamente non identificabile.\nPercorso di confondimento non chiuso tra X e D: Il percorso di confondimento (o back-door) tra X e D indica che c’è una relazione non controllata o non misurata tra la variabile indipendente X e il disturbo D di Y. Poiché D è trattato come una variabile latente (cioè, una variabile non osservata direttamente), questo percorso non può essere chiuso o controllato nel modello. Ciò significa che non possiamo essere sicuri se la relazione osservata tra X e Y è effettivamente causata da X o se è influenzata da altri fattori non considerati nel modello.\n\nIn sintesi, l’endogenità in questo contesto si riferisce al problema di avere una variabile indipendente (X) che non è veramente indipendente a causa della sua relazione non spiegata con il termine di errore associato alla variabile dipendente (Y), compromettendo così la chiarezza delle relazioni causali nel modello.\n\n\n\n\n\n\nFigura 22.3: Endogenità in una catena contratta (a). Identificazione del modello controllando un proxy (P) di una causa comune non misurata (b) e attraverso metodi di variabile strumentale (Z), che affrontano anche l’errore di misurazione nella variabile X (c). Tutti i diagrammi sono mostrati in simbolismo compatto. (Figura tratta da Kline (2023))\n\n\n\nL’endogenità nei modelli parametrici può essere indotta dalle seguenti condizioni:\n\nUna causa comune non misurata di X e Y (cioè, un confonditore).\nErrore di misurazione casuale in X (cioè, \\(r_{XX} &lt; 1.0\\)).\nCausalità reciproca, o X e Y si causano a vicenda (cioè, sono entrambe variabili endogene) in un ciclo di feedback.\nErrori autoregressivi, dove X è una versione ritardata di Y e gli errori persistono tra le due variabili.\nAutoregressione spaziale, che si verifica quando i punteggi di ciascun caso sono influenzati da quelli di casi vicini o adiacenti spazialmente.\n\nNel contesto dei modelli statistici, è possibile affrontare il problema dei confonditori non misurati in due modi principali: attraverso la selezione di covariate appropriate o utilizzando i metodi delle variabili strumentali. Per illustrare, la Figura 22.3 (b) propone l’uso di un proxy (P) che funge da sostituto per un confonditore non misurato che influisce su entrambe le variabili X e Y. In questo contesto, la variabile X è considerata endogena, il che significa che è influenzata dal proxy P (che a sua volta influisce anche su Y), indicando una possibile relazione di causa-effetto tra P e X.\nPer chiarire, consideriamo il seguente esempio. Immaginiamo di essere interessati a studiare l’effetto dello stress sulle prestazioni accademiche degli studenti universitari. In questo esempio, “stress” è la variabile X e “prestazioni accademiche” è la variabile Y. Tuttavia, c’è un potenziale confonditore che potrebbe influenzare sia lo stress sia le prestazioni accademiche, ma che non è stato misurato o non può essere facilmente misurato. Questo confonditore potrebbe essere, ad esempio, il “benessere psicologico generale” degli studenti.\nIn questo caso, un proxy (P) per il benessere psicologico generale potrebbe essere “l’attività fisica regolare”, che è più facilmente misurabile. La ricerca ha mostrato che l’attività fisica regolare può influenzare sia il benessere psicologico generale sia lo stress, rendendola un buon proxy per il nostro confonditore non misurato.\nNel modello, l’attività fisica (il nostro proxy P) presumibilmente influisce sia sulla variabile causale (lo stress) sia sulla variabile di esito (le prestazioni accademiche). Analizzando i dati con questo modello, possiamo cercare di isolare meglio l’effetto dello stress sulle prestazioni accademiche, controllando per l’effetto del benessere psicologico generale tramite il proxy dell’attività fisica. In questo modo, possiamo ottenere una stima più accurata dell’effetto diretto dello stress sulle prestazioni accademiche, riducendo la distorsione potenzialmente causata dal confonditore non misurato.\nI metodi delle variabili strumentali, come mostrato nella Figura 22.3 (c), sono utilizzati per affrontare sia i confonditori non misurati sia gli errori di misurazione nella variabile esogena X. Questo viene fatto sostituendo X con una variabile strumentale XZ in un modello di regressione a due stadi (2SLS). In questo approccio, qualsiasi errore di misurazione casuale in X viene trasferito alla variabile strumentale XZ, seguendo le ipotesi standard dei metodi delle variabili strumentali. È importante notare che, nel pannello (c), la variabile X è considerata endogena, sebbene non tutti i ricercatori scelgano di includere variabili strumentali nei loro diagrammi di modelli statistici. Questo approccio consente di isolare meglio l’effetto di X su Y, controllando per le influenze esterne non misurate e gli errori di misurazione.\nPer chiarire ulteriormente questi concetti, esamineremo separatamente il modello autoregressivo e l’autoregressione spaziale.\n\n22.4.3.1 Modello Autoregressivo\nUn modello autoregressivo è un tipo di modello statistico utilizzato per analizzare dati sequenziali o temporali. In un modello autoregressivo, si prevedono i valori futuri di una variabile basandosi sui suoi valori passati. Questo è particolarmente utile in studi longitudinali o in serie temporali dove si misura la stessa variabile in diversi punti nel tempo.\nNell’esempio della Figura 22.3 (a), immaginiamo che X e Y siano le stesse variabili misurate in due momenti diversi. Ad esempio, X potrebbe essere il livello di ansia di uno studente misurato all’inizio dell’anno scolastico, mentre Y potrebbe essere il livello di ansia dello stesso studente misurato alla fine dell’anno scolastico. In questo caso, stiamo cercando di prevedere i punteggi futuri di ansia (Y) basandoci sui punteggi passati (X).\nUn aspetto importante da considerare è che gli errori nelle misure ripetute (le variazioni nei punteggi che non sono spiegati dal modello) possono essere correlati. Ad esempio, se le misurazioni sono fatte in intervalli temporali ravvicinati, le circostanze o gli stati interni che hanno influenzato la prima misurazione potrebbero ancora essere presenti durante la seconda misurazione.\n\n\n22.4.3.2 Autoregressione Spaziale\nL’autoregressione spaziale, invece, si riferisce a un modello che considera le correlazioni spaziali tra dati. Questo tipo di analisi è particolarmente rilevante quando si studiano fenomeni geografici o ambientali. Ad esempio, la diffusione di una malattia in diverse località geografiche potrebbe non essere indipendente: le aree vicine geograficamente potrebbero mostrare pattern simili di diffusione della malattia a causa della loro vicinanza.\nIn quest’ultimo caso, non stiamo più parlando di misure ripetute nel tempo sulla stessa unità, ma piuttosto di misure effettuate in diverse unità in un contesto spaziale. Le variabili misurate in diverse località fisiche possono influenzarsi a vicenda, e un modello autoregressivo spaziale cerca di catturare queste interdipendenze.\n\n\n\n22.4.4 Modelli con Cause Correlate o Effetti Indiretti\nIl modello parametrico mostrato nella Figura 22.4 (a) suggerisce che la variabile Y sia influenzata da due variabili esogene correlate, X e W. Questo significa che X e W sono due fattori esterni che hanno un impatto su Y e tra loro esiste una relazione di covarianza, ovvero tendono a variare insieme in un certo modo. Tuttavia, il diagramma non spiega il motivo della relazione tra X e W, lasciando la loro interdipendenza non esaminata in termini causali.\n\n\n\n\n\n\nFigura 22.4: Modelli con cause correlate (a) e sia effetti diretti che indiretti (b). Tutti i diagrammi sono mostrati in simbolismo compatto. (Figura tratta da Kline (2023))\n\n\n\nNell’analizzare questi dati con un software, si prenderanno in considerazione gli effetti sia di X che di W, tenendo conto della loro covarianza campionaria. Ciò significa che quando si stimano gli impatti di X e W su Y, si aggiusta per il fatto che X e W sono correlate tra loro. Alcuni software, come lavaan, presuppongono automaticamente che tutte le cause esogene misurate che influenzano lo stesso risultato (in questo caso Y) siano correlate. Utilizzando il comando\nY ~ X + W\nin lavaan, si definisce il modello rappresentato nella Figura 22.4 (a), permettendo al software di stimare gli effetti di X e W tenendo conto della loro covarianza osservata. Questo comando specifica inoltre che le varianze di X, W e il disturbo associato a Y sono tutti considerati parametri liberi da stimare.\nSe, invece, si ipotizza che le variabili esogene X e W siano indipendenti, ovvero che non ci sia una covarianza tra di loro, si può usare un comando aggiuntivo in lavaan\nX ~~ 0*W\nper impostare la covarianza tra X e W a zero. Questo comando mantiene le varianze di X e W come parametri liberi, ma specifica che non c’è una relazione di covarianza diretta tra queste due variabili. In questo modo, il modello considererà X e W come influenze separate e indipendenti su Y.\nNel modello presentato nella Figura 22.4 (a), è importante notare come vengano trattate le interazioni tra le variabili causali X e W. In questo specifico caso, si presume che non ci sia alcuna interazione tra X e W; in altre parole, l’effetto di X sulla variabile di esito Y si assume essere costante a prescindere dai diversi livelli di W, e viceversa. Questa assunzione implica che l’effetto di X su Y è indipendente da W, e l’effetto di W su Y è indipendente da X.\nIn termini di modellazione, questo significa che stiamo considerando una causalità incondizionata, dove l’effetto di una causa su un esito è costante e non influenzato da altre variabili nel modello. Il modello non prevede, quindi, che l’effetto di X su Y cambi in funzione dei diversi livelli di W. Questo è in contrasto con l’ipotesi di causalità condizionale, dove gli effetti di una variabile su un’altra possono variare in base al livello di una terza variabile. In un modello di causalità condizionale, ad esempio, si potrebbe ipotizzare che l’effetto di X su Y vari a seconda dei diversi livelli di W.\nIn sintesi, la Figura 22.4 (a) delinea un modello dove le relazioni causali tra X, W e Y sono considerate fisse e non influenzate da potenziali interazioni tra X e W. Questo tipo di modellazione fornisce una visione semplificata delle relazioni causali, che potrebbe essere appropriata in determinate circostanze, ma non tiene conto di possibili dinamiche più complesse tra le variabili.\nÈ fondamentale riconoscere che le ripercussioni degli errori di misurazione in modelli che includono cause correlate sono notevolmente intricate e imprevedibili. Questa complessità deriva principalmente dalla natura del bias che può emergere a seguito di errori di misurazione. In particolare, il bias introdotto da questi errori può manifestarsi in modi diversi, assumendo una forma sia negativa che positiva. Tale variazione dipende da diversi fattori, tra cui se l’errore di misurazione è distribuito in maniera uniforme tra molteplici variabili predittive o se è presente sia nelle variabili predittive che nella variabile di esito. Un altro elemento influente è la natura delle covarianze campionarie tra tutte le variabili coinvolte nel modello.\nData questa complessità, la capacità di modellare esplicitamente gli errori di misurazione all’interno dei modelli SEM rappresenta un vantaggio significativo. Questo approccio permette una maggiore precisione nell’analisi, consentendo di tenere conto delle varie modalità in cui gli errori di misurazione possono influenzare i risultati. La modellazione esplicita degli errori di misurazione in SEM offre quindi la possibilità di ottenere stime più accurate e affidabili, mitigando il rischio di trarre conclusioni errate a causa di bias non riconosciuti o non gestiti adeguatamente.\nNella Figura 22.4 (b), il modello mostra come la variabile X abbia sia effetti diretti che indiretti sulla variabile di esito Y. L’effetto indiretto segue il percorso X → M → Y, dove M funge da variabile intermedia o mediatrice. Questo significa che M è il canale attraverso il quale gli effetti di X sono trasmessi a Y. In questo modello, M è una variabile endogena, nel senso che è influenzata da X (indicato dal percorso X → M), e allo stesso tempo agisce come una variabile causale nei confronti di Y (come indicato da M → Y).\nLa variabile M assume un doppio ruolo in termini di affidabilità e precisione della misurazione. Da una parte, essendo un esito di X, M è soggetta a disturbi, che includono potenziali errori di misurazione. Dall’altra, nel suo ruolo di causa per Y insieme a X, si presume nelle analisi di regressione che sia X che M siano prive di errore di misurazione. Questa assunzione non presenta problemi se l’affidabilità delle misure su M è elevata, ossia se i punteggi di M sono accurati e consistenti.\nIn aggiunta, il modello descritto nella Figura 22.4 (b) include tre ipotesi importanti:\n\nX ha un effetto diretto su Y, oltre al suo effetto indiretto tramite M.\nNon ci sono interazioni negli effetti lineari di X e M su Y, il che significa che l’effetto di X su Y è lo stesso a prescindere dai livelli di M, e viceversa.\nIl modello non omette confonditori potenzialmente importanti tra le coppie di variabili X, M e Y. In altre parole, non ci sono fattori esterni non considerati nel modello che potrebbero influenzare le relazioni tra queste tre variabili.\n\nIn sintesi, la Figura 22.4 (b) presenta un modello in cui X influisce su Y sia direttamente che indirettamente attraverso M, e queste relazioni sono considerate prive di interazioni complesse o di confonditori non rilevati.\nLa gestione degli errori di misurazione e dei confonditori non considerati in modelli che includono effetti causali indiretti rappresenta una sfida notevole, poiché i loro effetti sulle stime possono essere complessi e non sempre prevedibili. Per esemplificare, consideriamo il modello della Figura 22.4 (b) dove si analizza l’effetto indiretto di X su Y attraverso la variabile intermedia M.\nSe assumiamo che non ci siano errori di misurazione nella variabile causale X, qualsiasi errore di misurazione presente nella variabile intermedia M può introdurre un bias negativo nelle stime dell’effetto indiretto di X su Y. Ciò significa che l’effetto indiretto potrebbe essere sottostimato a causa dell’errore in M. D’altro canto, se non si tiene conto dei confonditori tra M e Y, cioè se ci sono variabili o fattori non considerati che influenzano sia M che Y, ciò può portare a un bias positivo, sovrastimando l’effetto indiretto.\nQuando entrambe queste situazioni – errori di misurazione in M e confonditori tra M e Y – si verificano contemporaneamente, le conseguenze sulle stime dell’effetto indiretto possono variare ampiamente. Potrebbe verificarsi una sovrastima, una sottostima o, in rari casi, nessun bias significativo. Studi di simulazione hanno rivelato che tentare di correggere solo una fonte di bias (come l’errore di misurazione in M) in presenza di entrambi i tipi di bias può addirittura aggravare il problema, portando a stime più distorte rispetto a quelle che non tengono conto di alcun bias.\nIn sintesi, la valutazione accurata dell’effetto indiretto in un modello che comprende una variabile intermedia richiede un’attenta considerazione sia degli errori di misurazione che dei confonditori potenziali, poiché la loro interazione può influenzare in modi complessi e talvolta inaspettati la validità delle stime.\n\n\n22.4.5 Modelli Ricorsivi, Non Ricorsivi e Parzialmente Ricorsivi\nTutti i modelli di percorso parametrici più complessi possono essere “assemblati” a partire dai modelli elementari mostrati nelle figure precedenti. Ci sono due tipi fondamentali di modelli: ricorsivi e non ricorsivi. I modelli ricorsivi hanno due caratteristiche essenziali: tutti gli effetti causali sono unidirezionali e i loro disturbi sono indipendenti. La Figura 22.5 (a) è un esempio di un modello ricorsivo (Tutti i modelli considerati finora sono ricorsivi.)\nI modelli non ricorsivi, invece, hanno cicli causali (feedback) in cui ≥ 2 variabili endogene sono specificate come cause ed effetti l’una dell’altra, direttamente o indirettamente. Nella loro forma non parametrica, corrispondono a grafi ciclici diretti. La Figura 22.5 (b) è un esempio di un modello parametrico non ricorsivo con causazione reciproca rappresentata come\n\\(Y1 \\overset{\\rightarrow}{\\underset{\\leftarrow}{}} Y2,\\)\nindicando che le variabili Y1 e Y2 hanno effetti simultanei l’una sull’altra.\n\n\n\n\n\n\nFigura 22.5: Esempi di modelli ricorsivi, non ricorsivi e parzialmente ricorsivi con due diversi schemi di correlazione degli errori. Tutti i diagrammi sono mostrati in simbolismo compatto. (Figura tratta da Kline (2023))\n\n\n\nI modelli che includono cicli causali possono presentare, o meno, covarianze tra i loro termini di disturbo. La presenza di errori correlati in questi modelli implica l’esistenza di ipotesi su cause comuni non misurate che influenzano le variabili in questione.\nAd esempio, nel modello rappresentato nella Figura 22.5 (b), le variabili Y1 e Y2 sono definite come cause reciproche, ovvero ognuna influisce sull’altra. In aggiunta a ciò, se nel modello è specificata una covarianza tra i termini di disturbo \\(D_1\\) e \\(D_2\\), ciò suggerisce che Y1 e Y2 condividono almeno una causa comune non misurata. In altre parole, l’ipotesi è che esistano fattori non osservati che influenzano entrambe le variabili, \\(Y1\\) e \\(Y2\\), e questa influenza comune si manifesta attraverso la covarianza tra i loro termini di disturbo.\nEsiste anche un altro tipo di modello di percorso, quello che ha effetti unidirezionali e covarianze dei disturbi. I modelli parzialmente ricorsivi con un pattern di correlazioni dei disturbi senza “archi” possono essere trattati nell’analisi proprio come i modelli ricorsivi. Un pattern senza “archi” significa che gli errori correlati sono limitati a coppie di variabili endogene senza effetti diretti tra di loro, come Y1 e Y2 nella Figura 22.5 (c).\nI modelli parzialmente ricorsivi che presentano un pattern di correlazioni dei disturbi caratterizzato dalla presenza di “archi” richiedono un trattamento analitico simile a quello dei modelli non ricorsivi. Un pattern con “archi” si verifica quando esiste una covarianza tra i termini di disturbo di due variabili endogene che sono collegate da un effetto diretto. Ad esempio, nella Figura 22.5 (d), le variabili Y1 e Y2 sono collegate da un effetto diretto e presentano una covarianza tra i loro disturbi \\(D_1\\) e \\(D_2\\).\nLa presenza di un effetto diretto insieme a disturbi correlati tra due variabili crea un percorso di confondimento nel modello. Un percorso di confondimento è una via attraverso la quale può fluire l’influenza causale indiretta, potenzialmente distorcendo l’interpretazione dei rapporti causali diretti. In questi casi, la selezione di covariate (variabili aggiuntive che potrebbero spiegare parte della relazione osservata) non è sufficiente per “chiudere” o eliminare questo percorso di confondimento. Pertanto, questi modelli richiedono un’attenzione particolare nell’analisi per garantire che le stime degli effetti causali siano accurate e non siano influenzate in modo improprio da questi percorsi di confondimento.\nI modelli ricorsivi e quelli parzialmente ricorsivi che non includono cicli causali possono essere efficacemente rappresentati tramite grafi aciclici diretti (DAG). Questo tipo di rappresentazione grafica implica che è possibile applicare tutte le regole di identificazione grafica esposte in questo capitolo. Nei DAG, le relazioni causali sono rappresentate come flussi unidirezionali senza cicli, rendendo più chiaro e diretto l’analisi delle relazioni tra le variabili.\nD’altra parte, i modelli non ricorsivi che includono cicli causali, come quello illustrato nella Figura 22.5 (b), sono rappresentati da grafi ciclici diretti. In questi grafi, le variabili possono influenzarsi a vicenda in un ciclo continuo, creando una struttura più complessa. A causa di questa complessità, le regole di identificazione grafica per i grafi ciclici diretti non sono sviluppate quanto quelle per i DAG. Questo significa che analizzare e interpretare i modelli non ricorsivi con cicli causali è più complesso e richiede l’uso di approcci analitici più avanzati o specifici per gestire correttamente le relazioni cicliche tra le variabili.\nPossiamo stabilire una regola generale per i modelli di percorso parametrici: i modelli ricorsivi o parzialmente ricorsivi, che presentano schemi di covarianze dei disturbi privi di “archi” e che soddisfano due condizioni specifiche, sono considerati identificati. Queste condizioni sono: (1) i gradi di libertà del modello (dfM) devono essere maggiori o uguali a zero e (2) ogni variabile non misurata, inclusi i termini di errore, deve essere associata a una scala metrica.\nInoltre, i modelli di equazioni strutturali che sono identificati e hanno un numero di osservazioni uguale al numero dei parametri liberi (dfM = 0) sono classificati come “appena identificati”. Al contrario, i modelli con più osservazioni rispetto ai parametri liberi (dfM &gt; 0) sono considerati “sovraidentificati”.\nUn modello di equazioni strutturali può risultare sotto-identificato in due modi distinti: (1) se dfM è inferiore a zero, oppure (2) se, pur avendo un dfM maggiore o uguale a zero, alcuni parametri liberi rimangono sotto-identificati perché non vi sono sufficienti informazioni per la loro stima, anche se altri parametri all’interno dello stesso modello sono identificati. Nel secondo scenario, l’intero modello è considerato non identificato, anche se dfM è maggiore o uguale a zero. In generale, un modello si considera sotto-identificato quando non è possibile stimare in modo univoco tutti i suoi parametri liberi.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#analisi-dei-percorsi-e-regressione-bivariata",
    "href": "chapters/path_analysis/01_path_analysis.html#analisi-dei-percorsi-e-regressione-bivariata",
    "title": "22  Analisi dei percorsi",
    "section": "22.5 Analisi dei percorsi e regressione bivariata",
    "text": "22.5 Analisi dei percorsi e regressione bivariata\nCominciamo esaminando l’analisi dei percorsi partendo dall’esempio più semplice, ovvero il modello di regressione lineare. Il modello di regressione bivariata si esprime tramite l’equazione seguente:\n\\[y_1 = b_0 + b_1 x_1 + \\epsilon_1,\\]\ndove \\(y\\) rappresenta la variabile dipendente, \\(b_0\\) rappresenta l’intercetta, \\(b_1\\) rappresenta la pendenza della retta di regressione, \\(x\\) è la variabile indipendente e \\(\\epsilon\\) è il termine di errore.\nNell’ambito della descrizione delle relazioni tra variabili manifeste e latenti, si adotta spesso la notazione LISREL. In questa notazione, il modello presentato in precedenza può essere espresso come segue:\n\\[y_1 = \\alpha + \\gamma x_1 + \\zeta_1,\\]\ndove:\n\n\\(x_1\\): variabile esogena singola,\n\\(y_1\\): variabile endogena singola,\n\\(\\alpha\\): intercetta di \\(y_1\\),\n\\(\\gamma_1\\): coefficiente di regressione,\n\\(\\zeta_1\\): termine di errore di \\(y_1\\),\n\\(\\phi\\): varianza o covarianza della variabile esogena,\n\\(\\psi\\): varianza o covarianza residuale della variabile endogena.\n\nIl diagramma di percorso per il modello di regressione bivariata è illustrato nella Figura 22.6.\n\n\n\n\n\n\nFigura 22.6: Diagramma di percorso per il modello di regressione bivariato.\n\n\n\nFacciamo un esempio numerico. Simuliamo tre variabili: x1, x2, y.\n\nset.seed(42)\nn &lt;- 100\nx1 &lt;- rnorm(n, 90, 20)\nx2 &lt;- x1 + rnorm(n, 0, 30)\ny &lt;- 25 + 0.5 * x1 + 1.0 * x2 + rnorm(n, 0, 30)\n\ndat &lt;- data.frame(\n    y, x1, x2\n)\n\ncor(dat) |&gt;\n    round(2)\n\n\nA matrix: 3 x 3 of type dbl\n\n\n\ny\nx1\nx2\n\n\n\n\ny\n1.00\n0.55\n0.80\n\n\nx1\n0.55\n1.00\n0.62\n\n\nx2\n0.80\n0.62\n1.00\n\n\n\n\n\nConsideriamo la relazione tra x1 (variabile endogena) e y (variabile endogena). In R possiamo adattare ai dati un modello di regressione mediante la funzione lm.\n\nm1a &lt;- lm(y ~ x1, data = dat)\nsummary(m1a)\n\n\nCall:\nlm(formula = y ~ x1, data = dat)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-82.46 -29.54  -3.44  29.20 122.23 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   37.597     18.984    1.98     0.05 .  \nx1             1.329      0.204    6.51  3.3e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 42.3 on 98 degrees of freedom\nMultiple R-squared:  0.302, Adjusted R-squared:  0.295 \nF-statistic: 42.4 on 1 and 98 DF,  p-value: 3.25e-09\n\n\nUsiamo ora lavaan per adattare lo stesso modello ai dati.\n\nm1b &lt;- \"\n    y ~ 1 + x1\n\"\nfit1b &lt;- sem(m1b, data = dat)\nparameterEstimates(fit1b) \n\n\nA lavaan.data.frame: 5 x 9\n\n\nlhs\nop\nrhs\nest\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\ny\n~1\n\n37.60\n18.794\n2.00\n4.54e-02\n0.763\n74.43\n\n\ny\n~\nx1\n1.33\n0.202\n6.57\n4.90e-11\n0.933\n1.72\n\n\ny\n~~\ny\n1754.10\n248.067\n7.07\n1.54e-12\n1267.897\n2240.30\n\n\nx1\n~~\nx1\n429.43\n0.000\nNA\nNA\n429.432\n429.43\n\n\nx1\n~1\n\n90.65\n0.000\nNA\nNA\n90.650\n90.65\n\n\n\n\n\nL’intercetta di y ~1 (37.597) e il coefficiente di regressione di y ~ x1 (1.329) corrispondono all’output di lm() con piccoli errori di arrotondamento. L’intercetta per x1 ~1 (90.650) e la sua varianza x1 ~~ x1 (429.432) descrivono una media ed una varianza esogena e corrispondono alla media e alla varianza univariate:\n\nmean(dat$x1)\n\n90.6502963122602\n\n\n\nvar(dat$x1) * (length(dat$x1) - 1) / length(dat$x1)\n\n429.432031274383\n\n\nLa varianza residua di y, y ~~ y corrisponde alla quota della varianza osservata della variabile y che non è spiegata dalla relazione lineare su x1:\n\nvar(dat$y) * 99 / 100 - (1.3286 * 429.432 * 1.3286)\n\n1754.15693691975\n\n\nLa funzione semPaths consente di creare un diagramma di percorso a partire dall’oggetto creato da sem.\n\nsemPlot::semPaths(\n    fit1b,\n    layout = \"tree\", sizeMan = 7, sizeInt = 5, style = \"ram\", \n    residuals = TRUE, intAtSide = FALSE, edge.label.cex = 1.15,\n    whatLabels = \"est\", nCharNodes = 0, normalize = FALSE\n)",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#analisi-dei-percorsi-e-regressione-multipla",
    "href": "chapters/path_analysis/01_path_analysis.html#analisi-dei-percorsi-e-regressione-multipla",
    "title": "22  Analisi dei percorsi",
    "section": "22.6 Analisi dei percorsi e regressione multipla",
    "text": "22.6 Analisi dei percorsi e regressione multipla\nLa regressione semplice è limitata a una sola variabile esogena. Nella pratica, un ricercatore può essere interessato a studiare come un gruppo di variabili esogene possano predire una variabile di esito. Supponiamo di avere ancora una sola variabile di esito endogena ma due predittori esogeni; questo caso è noto come regressione multipla:\n\\[\ny_1 = \\alpha_1 + \\gamma_1 x_1 + \\gamma_2 x_2 + \\zeta_1.\n\\]\nIl diagramma di percorso mostra la relazione tra tutte le variabili, comprendendo anche i fattori di disturbo, e fornisce dunque la rappresentazione grafica dell’equazione precedente.\n\n\n\n\n\n\nFigura 22.7: Diagramma di percorso per il modello di regressione multipla.\n\n\n\nI coefficienti di percorso associati alle frecce orientate esprimono la portata del nesso causale e corrispondono ai pesi beta (ovvero ai coefficienti parziali di regressione standardizzati). Le frecce non orientate esprimono la portata della pura associazione tra variabili e dunque corrispondono alle correlazioni/covarianze.\nIn un diagramma di percorso, il numero di equazioni corrisponde al numero di variabili endogene del modello. Nel caso specifico, poiché vi è una sola variabile endogena (ovvero \\(y\\)), esiste un’unica equazione che descrive le relazioni causalitiche interne al path diagram. All’interno di ciascuna equazione, inoltre, il numero di termini corrisponde al numero di frecce orientate che puntano verso la variabile endogena. Nell’esempio sopra citato, pertanto, la sola equazione del modello contiene tre termini, ciascuno associato ad una freccia orientata.\nUsando lm otteniamo la seguente stima dei coefficienti:\n\nm2a &lt;- lm(y ~ 1 + x1 + x2, data = dat)\nfit2a &lt;- summary(m2a) \n\nGli stessi risultati si ottengono con lavaan.\n\nm2b &lt;- \"\n    y ~ 1 + x1 + x2\n    x1 ~~ x1\n    x2 ~~ x2\n    x1 ~~ x2\n\"\n\n\nfit2b &lt;- sem(m2b, data = dat)\n\n\nparameterEstimates(fit2b)\n\n\nA lavaan.data.frame: 9 x 9\n\n\nlhs\nop\nrhs\nest\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\ny\n~1\n\n44.454\n13.457\n3.30\n9.56e-04\n18.078\n70.830\n\n\ny\n~\nx1\n0.199\n0.185\n1.08\n2.82e-01\n-0.164\n0.562\n\n\ny\n~\nx2\n1.085\n0.111\n9.78\n0.00e+00\n0.868\n1.303\n\n\nx1\n~~\nx1\n429.432\n60.731\n7.07\n1.54e-12\n310.402\n548.462\n\n\nx2\n~~\nx2\n1192.840\n168.693\n7.07\n1.54e-12\n862.208\n1523.472\n\n\nx1\n~~\nx2\n446.927\n84.379\n5.30\n1.18e-07\n281.546\n612.307\n\n\ny\n~~\ny\n896.963\n126.850\n7.07\n1.54e-12\n648.342\n1145.584\n\n\nx1\n~1\n\n90.650\n2.072\n43.74\n0.00e+00\n86.589\n94.712\n\n\nx2\n~1\n\n88.026\n3.454\n25.49\n0.00e+00\n81.257\n94.795\n\n\n\n\n\nEsaminiamo il diagramma di percorso.\n\nsemPlot::semPaths(\n    fit2b,\n    layout = \"tree\", sizeMan = 7, sizeInt = 5, style = \"ram\",\n    residuals = TRUE, intAtSide = FALSE, edge.label.cex = 1.15,\n    whatLabels = \"est\", nCharNodes = 0, normalize = FALSE\n)",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#effetti-diretti-e-indiretti",
    "href": "chapters/path_analysis/01_path_analysis.html#effetti-diretti-e-indiretti",
    "title": "22  Analisi dei percorsi",
    "section": "22.7 Effetti diretti e indiretti",
    "text": "22.7 Effetti diretti e indiretti\nL’analisi del percorso offre un metodo essenziale per distinguere tra diverse tipologie di effetti che influenzano le variabili in esame: l’effetto diretto, l’effetto indiretto e l’effetto totale. Gli effetti diretti rappresentano l’influenza che una variabile esercita su un’altra senza mediazione di altre variabili intermedie. Gli effetti indiretti, invece, operano attraverso l’intermediazione di almeno una variabile aggiuntiva nel processo. L’effetto totale è la somma cumulativa degli effetti diretti e indiretti.\nNella Figura 22.8, la variabile \\(y_1\\) esercita un effetto diretto sulla variabile \\(y_2\\). Allo stesso tempo, \\(y_1\\) produce un effetto indiretto sulla variabile \\(y_3\\), poiché non esiste una connessione causale diretta tra \\(y_1\\) e \\(y_3\\). Nel contesto rappresentato, la variabile \\(y_1\\) agisce come variabile esogena, mentre le variabili \\(y_2\\) e \\(y_3\\) fungono da variabili endogene.\n\n\n\n\n\n\nFigura 22.8: Diagramma di percorso per un modello a catena.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#le-regole-di-wright",
    "href": "chapters/path_analysis/01_path_analysis.html#le-regole-di-wright",
    "title": "22  Analisi dei percorsi",
    "section": "22.8 Le regole di Wright",
    "text": "22.8 Le regole di Wright\nL’obiettivo primario dell’analisi del percorso consiste nella decomposizione della correlazione (o della covarianza) in base alla somma dei vari percorsi (diretti e indiretti) che collegano due variabili mediante coefficienti noti come “path coefficients.” Utilizzando il diagramma del percorso, Sewall Wright (1921, 1934) formulò le regole che, tramite le “tracing rules,” stabiliscono il collegamento tra le correlazioni (o covarianze) delle variabili e i parametri del modello. Le tracing rules si possono esprimere nei seguenti termini:\n\nÈ possibile procedere in avanti lungo una freccia e poi a ritroso, seguendo la direzione della freccia, ma non è permesso muoversi in avanti e poi tornare indietro.\nUn percorso composto non deve attraversare più di una volta la stessa variabile, cioè non possono esserci cicli.\nUn percorso non può contenere più di una linea curva.\n\nIl termine “percorso” fa riferimento al tracciato che connette due variabili e si compone di sequenze di frecce unidirezionali e curve non direzionali. A ciascun percorso valido (cioè conforme alle regole di Wright) viene assegnato un valore numerico che rappresenta il prodotto dei coefficienti presenti lungo il percorso stesso. I coefficienti di percorso possono essere coefficienti parziali di regressione standardizzati se il legame è unidirezionale, oppure coefficienti di correlazione se il legame è bidirezionale.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#scomposizione-delle-correlazionicovarianze",
    "href": "chapters/path_analysis/01_path_analysis.html#scomposizione-delle-correlazionicovarianze",
    "title": "22  Analisi dei percorsi",
    "section": "22.9 Scomposizione delle correlazioni/covarianze",
    "text": "22.9 Scomposizione delle correlazioni/covarianze\nIl principio fondamentale è stato formulato da Sewall Wright (1934) nel seguente modo:\n\nOgni correlazione tra variabili in una rete di relazioni sequenziali può essere analizzata nei contributi provenienti da tutti i percorsi (diretti o attraverso fattori comuni) con i quali le due variabili sono connesse. Ogni contributo ha un valore pari al prodotto dei coefficienti relativi ai percorsi elementari. Se sono presenti correlazioni residue (rappresentate da frecce bidirezionali), uno (ma mai più di uno) dei coefficienti moltiplicati per ottenere il contributo del percorso di connessione può essere un coefficiente di correlazione. Gli altri sono tutti coefficienti di percorso.\n\nDa questo principio possiamo derivare la regola di scomposizione della correlazione: la correlazione o covarianza tra due variabili può essere scomposta in un numero di termini uguale al numero di percorsi che le collegano. Ogni termine è ottenuto dal prodotto dei coefficienti associati alle variabili lungo il percorso. In altre parole, è possibile decomporre la correlazione o la covarianza tra due variabili in tanti contributi quanti sono i percorsi possibili che collegano le due variabili.\n\n22.9.1 Scomposizione della varianza\nLa decomposizione della varianza di una variabile endogena può essere affrontata attraverso una suddivisione in due componenti: una componente spiegata, attribuibile alle variabili che esercitano un’influenza causale su di essa, e una componente non spiegata. La componente spiegata della varianza deriva dall’aggregazione degli effetti delle diverse variabili che sono connessi alla variabile endogena, rispettando le regole di tracciamento definite da Wright. Il numero di addendi corrisponde al numero di percorsi che collegano la variabile endogena a se stessa. In tal modo, la varianza spiegata rappresenta la parte della varianza totale della variabile endogena che può essere attribuita alle influenze delle variabili correlate attraverso i percorsi definibili all’interno del modello.\n\n\n22.9.2 Relazioni tra variabili endogene e esogene\nComplessivamente, i concetti di varianza, covarianza e correlazione informano direttamente il calcolo dei coefficienti di percorso in un path diagram secondo le seguenti “8 regole dei coefficienti di percorso”.\n\nRegola 1: Le relazioni non specificate tra le variabili esogene sono semplicemente le loro correlazioni bivariate.\nRegola 2: Quando due variabili sono collegate da un singolo percorso, il coefficiente di quel percorso è il coefficiente di regressione.\nRegola 3: La forza di un percorso composto (che include più collegamenti) è il prodotto dei coefficienti individuali.\nRegola 4: Quando le variabili sono collegate da più di un percorso, ciascun percorso è il coefficiente di regressione “parziale”.\nRegola 5: Gli errori sulle variabili endogene si riferiscono alle correlazioni o varianze non spiegate che derivano dalle variabili non misurate.\nRegola 6: Le correlazioni non analizzate (residui) tra due variabili endogene sono le loro correlazioni parziali.\nRegola 7: L’effetto totale che una variabile ha su un’altra è la somma dei suoi effetti diretti e indiretti.\nRegola 8: L’effetto totale (compresi i percorsi non diretti) è equivalente alla correlazione totale.\n\nEsempio. Consideriamo nuovamente il modello di regressione multipla con due variabili esogene e una sola variabile endogena che è stato presentato sopra.\nLa la covarianza tra y e x1\n\ncov(dat$y, dat$x1) * 99 / 100\n\n570.56471345413\n\n\npuò essere ricavata usando le regole di Wright nel modo seguente:\n\n0.199 * 429.43 + 1.085 * 446.93\n\n570.37562\n\n\nLa quota di varianza non spiegata della variabile endogena è:\n\n(var(dat$y) * 99 / 100) - (\n    0.199^2 * 429.43 + (1.085)^2 * 1192.84 + 2 * (0.199 * 1.085 * 446.93)\n)\n\n897.936130308472",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#oltre-la-regressione-multipla",
    "href": "chapters/path_analysis/01_path_analysis.html#oltre-la-regressione-multipla",
    "title": "22  Analisi dei percorsi",
    "section": "22.10 Oltre la regressione multipla",
    "text": "22.10 Oltre la regressione multipla\nApprofondiamo l’utilizzo dell’analisi dei percorsi (path analysis) per studiare un modello di mediazione che supera i limiti della classica regressione multipla. L’analisi di mediazione è un metodo statistico ampiamente impiegato dai psicologi per esaminare le relazioni complesse tra le variabili di uno studio. Questa metodologia può essere applicata in studi osservazionali per affinare la comprensione della relazione tra variabili predittive e variabili di esito, introducendo mediatori. L’integrazione di mediatori permette di esplorare e chiarire i meccanismi sottostanti che influenzano la relazione tra le variabili principali, fornendo una visione più dettagliata e complessa degli effetti e delle interazioni in gioco.\nPrenderemo in esame un modello teorico specifico, ispirato alla Self Determination Theory (SDT) di Deci e Ryan (2000), una delle teorie più influenti nel campo della motivazione. In questo contesto, utilizzeremo la path analysis per valutare come la SDT possa aiutarci a comprendere i fattori psicologici e comportamentali che influenzano i sintomi bulimici in un gruppo di giovani donne adulte. Più specificamente, esamineremo come l’appagamento (soddisfazione) e l’esaurimento (frustrazione) delle risorse psicologiche essenziali, o bisogni psicologici (ad esempio, per l’autonomia, la competenza e la relazionalità), possono prevedere in modo differenziale i sintomi bulimici nelle donne attraverso due mediatori chiave, l’approvazione degli ideali culturali sulla magrezza e inflessibilità delle opinioni sul proprio corpo. Secondo la SDT, i bisogni psicologici influenzano la capacità di un individuo di autoregolarsi e far fronte alle richieste della vita quotidiana e possono rendere gli individui vulnerabili al malessere psicologico se i bisogni psicologici vengono frustrati (Vansteenkiste & Ryan, 2013). La frustrazione dei bisogni può essere psicologicamente più depauperante della mancanza di soddisfazione dei bisogni.\nGli individui i cui bisogni vengono frustrati possono impegnarsi in attività malsane e comportamenti compensatori al fine di riconquistare una soddisfazione dei bisogni a breve termine. La frustrazione dei bisogni rende gli individui più vulnerabili agli ideali culturali, in quanto le risorse personali per rifiutare questi ideali sono esaurite (Pelletier & Dion, 2007).\nIl modello che verrà testato propone che le donne i cui bisogni psicologici sono frustrati avalleranno ideali sociali più problematici sulla magrezza rispetto alle donne i cui bisogni psicologici sono soddisfatti. La frustrazione dei bisogni sarà anche predittiva dell’inflessibilità degli schemi corporei, poiché è stato dimostrato che la frustrazione dei bisogni porta a disturbi dell’immagine corporea e a comportamenti alimentari patologici (Boone, Vansteenkiste, Soenens, Van der Kaap-Deeder e Verstuyf, 2014). Il modello propone inoltre che una maggiore approvazione degli ideali culturali sulla sarà predittiva di una maggiore inflessibilità sugli schemi corporei che, di per sé, è predittiva dei sintomi bulimici.\nIl campione include 192 partecipanti, in maggioranza donne, di età media 21.2 anni (SD = 6.89). Sono stati somministrati i seguenti strumenti:\n\nBody Image-Acceptance and Action Questionnaire (Sandoz, Wilson, Merwin, & Kellum, 2013), per misurare l’inflessibilità relativa alla propria immagine corporea,\nEndorsement of Society’s Beliefs Related to Thinness and Obesity (Boyer, 1991), per valutare l’internalizzazione degli ideali di magrezza,\nBasic Psychological Needs Satisfaction and Frustration Scale (Chen et al., 2015), per misurare la soddisfazione e la frustrazione dei bisogni,\nEating Disorders Inventory-2 – Bulimic Symptomology Subscale (Garner, 1991), per misurare i sintomi bulimici.\n\nI dati sono i seguenti.\n\nupper &lt;- '\n  1 0.44 -0.41 0.55 0.63\n  1 -0.37 0.45 0.44\n  1 -0.71 -0.39\n  1 0.47\n  1\n  '\n\n\n# BFLX – Body Inflexibility,\n# END – Endorsement of Societal Beliefs about Thinness and Obesity,\n# MNS – Mean Need Satisfaction,\n# MNF – Mean Need Frustration,\n# BULS – Bulimic Symptoms\ndat_cov &lt;- lavaan::getCov(\n    upper,\n    lower = FALSE,\n    names = c(\"BFLX\", \"END\", \"MNS\", \"MNF\", \"BULS\")\n)\ndat_cov\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nBFLX\nEND\nMNS\nMNF\nBULS\n\n\n\n\nBFLX\n1.00\n0.44\n-0.41\n0.55\n0.63\n\n\nEND\n0.44\n1.00\n-0.37\n0.45\n0.44\n\n\nMNS\n-0.41\n-0.37\n1.00\n-0.71\n-0.39\n\n\nMNF\n0.55\n0.45\n-0.71\n1.00\n0.47\n\n\nBULS\n0.63\n0.44\n-0.39\n0.47\n1.00\n\n\n\n\n\n\n22.10.1 Modello di mediazione\n\ndata &lt;- c(\n    0, \"a\", 0,\n    0, 0, 0,\n    \"b\", \"c_prime\", 0\n)\nM &lt;- matrix(nrow = 3, ncol = 3, byrow = TRUE, data = data)\nplot &lt;- plotmat(M,\n    pos = c(1, 2),\n    name = c(\"X\", \"M\", \"Y\"),\n    box.type = \"rect\", box.size = 0.12, box.prop = 0.5, curve = 0\n)\n\n\n\n\n\n\n\n\nNel modello di mediazione di base abbiamo tre variabili chiave: la variabile indipendente \\(X\\), la variabile dipendente \\(Y\\) e il mediatore \\(M\\). La freccia da \\(X\\) a \\(M\\) (etichettata con ‘a’) rappresenta l’effetto di \\(X\\) su \\(M\\). La freccia da \\(M\\) a \\(Y\\) (etichettata con ‘b’) rappresenta l’effetto di \\(M\\) su \\(Y\\), mentre la freccia tratteggiata da \\(X\\) a \\(Y\\) (etichettata con “c’”) rappresenta l’effetto diretto di \\(X\\) su \\(Y\\), escludendo la mediazione di \\(M\\).\nPossiamo formalizzare il modello statistico di mediazione in equazioni di regressione lineare come segue:\n\\[M = a_0 + a \\times X + e_M\\]\n\\[Y = b_0 + b \\times M + c' \\times X + e_Y\\]\nNella prima equazione lineare, \\(M\\) è regredito su \\(X\\) con un’intercetta\\(a_0\\), pendenza\\(a\\)e termine di errore\\(e_M\\). Questa equazione rappresenta il percorso \\(X \\rightarrow M\\)con\\(a\\)come effetto di\\(X\\)su\\(M\\). Nella seconda equazione, \\(Y\\) è regredito su \\(M\\) e \\(X\\) con un’intercetta \\(b_0\\), pendenze \\(b\\) e \\(c'\\) e termine di errore \\(e_Y\\). Questa equazione rappresenta due percorsi: \\(M \\rightarrow Y\\) e \\(X \\rightarrow Y\\), con \\(b\\) come effetto di \\(M\\) su \\(Y\\) e \\(c'\\) come effetto diretto di \\(X\\) su \\(Y\\).\nNel contesto della modellazione di equazioni strutturali, il metodo più comune per calcolare gli effetti diretti, indiretti e totali si basa sul prodotto dei coefficienti, come riassunto nelle seguenti formule:\n\nEffetto diretto = \\(c'\\)\nEffetto indiretto = \\(b \\times a\\)\nEffetto totale = \\(c' + b \\times a\\)\n\n\n\n22.10.2 Stima degli Effetti Diretti, Indiretti e Totali\nLe tecniche di modellazione di equazioni strutturali permettono di stimare tutti i parametri (a, b, c’) simultaneamente, date le specifiche del modello per i dati. Di conseguenza, possiamo utilizzare le seguenti formule per stimare gli effetti diretti, indiretti e totali dai dati:\n\nEffetto diretto = \\(\\hat{c}'\\)\nEffetto indiretto = \\(\\hat{b} \\times \\hat{a}\\)\nEffetto totale = \\(\\hat{c}' + \\hat{b} \\times \\hat{a}\\)\n\ndove \\(\\hat{a}\\) è una stima di \\(a\\), e così via. Gli errori standard stimati di questi effetti possono poi essere calcolati utilizzando metodi asintotici standard, supportato dalla maggior parte dei software SEM.\n\n\n22.10.3 Analisi con lavaan\nIl caso presente prende in considerazione BFLX come variabile endogena, MNF come variabile esogena e END come variabile mediatrice. Utilizzando Mplus, Barbeau, Boileau, Sarr e Smith (2019) hanno identificato i seguenti coefficienti di percorso: \\(a = 0.37\\), \\(b = 0.29\\) e \\(c = 0.34\\).\nProcediamo con l’analisi utilizzando il pacchetto lavaan in R. Cominciamo definendo il modello di mediazione.\n\nmod &lt;- \"\n  # direct effect\n  BFLX ~ c*MNF\n  # mediator\n  BFLX ~ b*END\n  END ~ a*MNF\n\n  # indirect effect (a*b)\n  ab := a*b\n  # total effect\n  total := c + (a*b)\n\"\n\nAdattiamo il modello ai dati.\n\nfit &lt;- sem(\n    mod,\n    sample.cov = dat_cov,\n    sample.nobs = 192\n)\n\nEsaminiamo i risultati:\n\nsummary(fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         5\n\n  Number of observations                           192\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nModel Test Baseline Model:\n\n  Test statistic                               125.849\n  Degrees of freedom                                 3\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.000\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)               -480.945\n  Loglikelihood unrestricted model (H1)       -480.945\n                                                      \n  Akaike (AIC)                                 971.890\n  Bayesian (BIC)                               988.178\n  Sample-size adjusted Bayesian (SABIC)        972.339\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: RMSEA &lt;= 0.050                       NA\n  P-value H_0: RMSEA &gt;= 0.080                       NA\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n  BFLX ~                                                       \n    MNF        (c)    0.441    0.065    6.769    0.000    0.441\n    END        (b)    0.241    0.065    3.702    0.000    0.241\n  END ~                                                        \n    MNF        (a)    0.450    0.064    6.982    0.000    0.450\n  Std.all\n         \n    0.441\n    0.241\n         \n    0.450\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n   .BFLX              0.648    0.066    9.798    0.000    0.648\n   .END               0.793    0.081    9.798    0.000    0.793\n  Std.all\n    0.651\n    0.797\n\nR-Square:\n                   Estimate\n    BFLX              0.349\n    END               0.203\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n    ab                0.109    0.033    3.271    0.001    0.109\n    total             0.550    0.060    9.125    0.000    0.550\n  Std.all\n    0.109\n    0.550\n\n\n\nGeneriamo un diagramma di percorso.\n\nsemPlot::semPaths(\n    fit,\n    layout = \"tree\", sizeMan = 7, sizeInt = 5, style = \"ram\",\n    residuals = TRUE, intAtSide = FALSE, edge.label.cex = 1.15,\n    whatLabels = \"est\", nCharNodes = 0, normalize = FALSE\n)\n\n\n\n\n\n\n\n\nI coefficienti di percorso sono simili, ma non identici, a quelli trovati con Mplus.\nL’effetto diretto di MNF (Need Frustration) su BFLX (Body Inflexibility) è uguale a 0.44. L’effetto totale è \\(0.44 + 0.45*0.24 = 0.55\\). L’effetto di mediazione è uguale a \\(0.45*0.24 = 0.109\\). L’outout di lavaan fornisce anche gli errori standard e il test che tali effetti siano uguali a zero.\nLe correlazioni tra le variabili sono esprimibili nei termini dei coefficienti di percorso. Per esempio la correlazionetra BFLX e MNF è\n\n.44 + .45 * .24\n\n0.548\n\n\nLa correlazione tra BFLX e END è\n\n.24 + .44 * .45\n\n0.438\n\n\nL’output di lavaan fornisce anche la porzione di varianza che viene spiegata dalle variabili esogene per le due variabili endogene nel modello.\nPer esempio, la varianza spiegata di END è\n\n0.45^2\n\n0.2025\n\n\ncome riportato dall’output di lavaan.\nContinuiamo con l’analisi di questi dati e esaminiamo ora un modello di path analisi più complesso (Fig. 4 di Barbeau et al., 2019). Usando la sintassi di lavaan, il modello diventa\n\n# BFLX – Body Inflexibility,\n# END – Endorsement of Societal Beliefs about Thinness and Obesity,\n# MNS – Mean Need Satisfaction,\n# MNF – Mean Need Frustration,\n# BULS – Bulimic Symptoms\nmod &lt;- \"\n  BULS ~ MNF + BFLX\n  BFLX ~ END + MNF\n  END ~ MNS + MNF\n\"\n\nAdattiamo il modello ai dati.\n\nfit2 &lt;- sem(\n    mod,\n    sample.cov = dat_cov,\n    sample.nobs = 192\n)\n\nEsaminiamo la soluzione ottenuta.\n\nsummary(fit2, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n\n  Number of observations                           192\n\nModel Test User Model:\n                                                      \n  Test statistic                                 8.229\n  Degrees of freedom                                 3\n  P-value (Chi-square)                           0.042\n\nModel Test Baseline Model:\n\n  Test statistic                               239.501\n  Degrees of freedom                                 9\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.977\n  Tucker-Lewis Index (TLI)                       0.932\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)               -700.169\n  Loglikelihood unrestricted model (H1)       -696.054\n                                                      \n  Akaike (AIC)                                1418.338\n  Bayesian (BIC)                              1447.655\n  Sample-size adjusted Bayesian (SABIC)       1419.146\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.095\n  90 Percent confidence interval - lower         0.017\n  90 Percent confidence interval - upper         0.176\n  P-value H_0: RMSEA &lt;= 0.050                    0.130\n  P-value H_0: RMSEA &gt;= 0.080                    0.696\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.035\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n  BULS ~                                                       \n    MNF               0.177    0.066    2.688    0.007    0.177\n    BFLX              0.533    0.066    8.085    0.000    0.533\n  BFLX ~                                                       \n    END               0.241    0.065    3.702    0.000    0.241\n    MNF               0.441    0.065    6.769    0.000    0.441\n  END ~                                                        \n    MNS              -0.102    0.091   -1.116    0.264   -0.102\n    MNF               0.378    0.091    4.140    0.000    0.378\n  Std.all\n         \n    0.177\n    0.533\n         \n    0.241\n    0.441\n         \n   -0.102\n    0.378\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n   .BULS              0.578    0.059    9.798    0.000    0.578\n   .BFLX              0.648    0.066    9.798    0.000    0.648\n   .END               0.788    0.080    9.798    0.000    0.788\n  Std.all\n    0.581\n    0.651\n    0.792\n\nR-Square:\n                   Estimate\n    BULS              0.419\n    BFLX              0.349\n    END               0.208\n\n\n\nGeneriamo il diagramma di percorso.\n\nsemPlot::semPaths(\n    fit2,\n    layout = \"tree\", sizeMan = 7, sizeInt = 5, style = \"ram\",\n    residuals = TRUE, intAtSide = FALSE, edge.label.cex = 1.15,\n    whatLabels = \"est\", nCharNodes = 0, normalize = FALSE\n)\n\n\n\n\n\n\n\n\nAnche in questo caso i coefficienti di percorso sono simili, ma non identici, a quelli riportati da Barbeau et al. (2019). Gli autori riportano una varianza spiegata di END pari a 0.209; con lavaan si ottiene 0.208. Per BFLX gli autori riportano 0.292; lavaan ottiene 0.349. Per BULS gli autori riportano 0.478; con lavaan si ottiene 0.419.\nCalcoliamo, ad esempio, la correlazione tra MNF e BULS prevista dal modello, combinando gli effetti diretti e indiretti. Questo processo consiste nel sommare gli effetti diretti tra queste due variabili con quelli indiretti mediati da altre variabili nel modello.\n\n-.71 * -.10 * .24 * .53 +\n.38 *.24 * .53 +\n.44 * .53 +\n.18\n\n0.4705672\n\n\nIl valore trovato corrisponde bene al valore osservato nel campione, che è pari a 0.47.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#modellare-le-medie",
    "href": "chapters/path_analysis/01_path_analysis.html#modellare-le-medie",
    "title": "22  Analisi dei percorsi",
    "section": "22.11 Modellare le Medie",
    "text": "22.11 Modellare le Medie\nIn un modello di equazioni strutturali (SEM), modellare le medie è un processo che si concentra sulla stima e sull’analisi delle medie delle variabili all’interno del modello. Questo processo è fondamentale per comprendere il comportamento medio delle variabili in studio e per integrare queste informazioni nella struttura complessiva del modello SEM.\n\nRaccolta dei Dati: Per iniziare, è necessario avere a disposizione i dati grezzi o una matrice di covarianza che riassuma le relazioni tra le variabili, oltre alle medie di tutte le variabili. Questi dati servono come input fondamentale per il modello SEM.\nSpecifica del Modello: Nel definire il modello SEM, è cruciale specificare che il modello deve considerare sia la struttura di covarianza (le relazioni tra le variabili) sia la struttura di media (le medie delle variabili). Questo assicura che il modello analizzi sia le relazioni tra le variabili sia i loro valori medi.\nInclusione delle Interfacce e delle Medie: Nel modello, le variabili endogene (quelle che sono influenzate da altre nel modello) richiedono l’inclusione delle intercette, mentre per le variabili esogene (quelle che non sono influenzate da altre nel modello) si considerano le loro medie.\nSpecificare la Costante “1”: Per modellare le medie, si include la costante “1” nell’equazione per tutte le variabili misurate, sia esogene sia endogene. Questo passaggio è fondamentale per dire al software di SEM di considerare le medie nelle sue analisi.\nUso di Software SEM (es. lavaan): In software come lavaan, si può utilizzare una sintassi specifica (ad es. meanstructure = true) per automatizzare l’inclusione delle medie. Questo comando dice al software di aggiungere la costante “1” alle equazioni, facilitando il processo di modellazione delle medie.\nCalcolo del Numero di Osservazioni e Parametri Liberi: Quando si modella sia la struttura di covarianza sia quella di media, è importante calcolare correttamente il numero di osservazioni e parametri liberi nel modello. Si utilizza una formula specifica per determinare questi numeri, considerando il numero di variabili osservate.\n\n\n22.11.1 Interpretazione delle Medie Previste\nIn un modello SEM, la struttura di covarianza genera “covarianze previste”, che possono essere confrontate con le covarianze effettivamente osservate nei dati. Analogamente, la struttura di media del modello produce “medie previste” (o “medie adattate”), che si possono confrontare con le medie osservate delle variabili.\nIl “residuo di media” è fondamentalmente la differenza tra la media osservata e la media prevista per una specifica variabile. Quando i gradi di libertà (df) sono zero per la struttura di media (come può accadere in alcune analisi), tutti i residui di media risultano essere zero. Questo significa che le medie previste dal modello corrispondono esattamente alle medie osservate. Tuttavia, se i gradi di libertà non sono zero, potrebbero esserci delle discrepanze tra le medie previste e quelle osservate.\nPer calcolare la media prevista di una variabile target nel modello, si seguono questi passaggi:\n\nSi inizia con il coefficiente del percorso diretto dalla costante “1” al target. Questo coefficiente rappresenta l’intercetto nella regressione della variabile target sulle altre variabili. In pratica, è un valore che indica la media prevista della variabile target quando tutte le altre variabili indipendenti nel modello sono a zero.\nSi aggiunge a questo il contributo di ogni “variabile genitore” (ovvero, una variabile che influisce sul target). Per ogni genitore, si moltiplica la media prevista di quel genitore per il coefficiente del percorso che collega il genitore alla variabile target. Questa operazione si ripete per tutti i genitori e i risultati si sommano insieme.\n\nIl risultato finale di questo processo è la media prevista per la variabile target nel modello SEM. Questa media prevista è un elemento cruciale per valutare l’aderenza del modello ai dati osservati, confrontando le medie previste con quelle effettivamente osservate.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#commenti-e-considerazioni-finali",
    "href": "chapters/path_analysis/01_path_analysis.html#commenti-e-considerazioni-finali",
    "title": "22  Analisi dei percorsi",
    "section": "22.12 Commenti e considerazioni finali",
    "text": "22.12 Commenti e considerazioni finali\nIl diagramma di un modello parametrico funge anche da mezzo di comunicazione, in quanto un diagramma completo rappresenta essenzialmente un insieme di istruzioni visive su come specificare il modello nella sintassi per computer. Ogni parametro del modello, sia libero sia fisso (ad esempio, le costanti di scala), è rappresentato nei diagrammi basati sul simbolismo grafico RAM di McArdle-McDonald, il quale può anche aiutare i ricercatori che stanno imparando l’analisi SEM a comprendere meglio l’analisi.\nIn pratica, questo significa che guardando il diagramma di un modello parametrico, un ricercatore può ottenere una guida visiva chiara su come impostare e strutturare il modello all’interno di un software di analisi SEM. Ogni elemento del diagramma, come le frecce e i nodi, e ogni annotazione, come le etichette dei parametri o le costanti, fornisce informazioni specifiche su come ciascuna parte del modello dovrebbe essere codificata nella sintassi del software. Questo approccio visuale facilita la comprensione dei complessi rapporti tra le variabili e dei diversi componenti del modello, rendendo più accessibile l’apprendimento e l’applicazione dell’analisi SEM.\nUtilizzando l’analisi dei percorsi per decomporre la correlazione o la covarianza, disponiamo di un metodo efficace per delineare le associazioni tra le variabili e mappare le loro potenziali connessioni causali. Questo strumento si rivela particolarmente utile per descrivere in modo chiaro e strutturato le relazioni tra diverse variabili, facilitando l’interpretazione dei loro legami e interazioni all’interno del modello considerato.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/01_path_analysis.html#session-info",
    "href": "chapters/path_analysis/01_path_analysis.html#session-info",
    "title": "22  Analisi dei percorsi",
    "section": "22.13 Session Info",
    "text": "22.13 Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] diagram_1.6.5     shape_1.4.6.1     rsvg_2.6.1       \n [4] DiagrammeRsvg_0.1 lavaanPlot_0.8.1  lavaanExtra_0.2.1\n [7] nortest_1.0-4     MASS_7.3-61       ggokabeito_0.1.0 \n[10] viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n[13] ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0  \n[16] bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n[19] lavaan_0.6-19     psych_2.4.6.26    scales_1.3.0     \n[22] markdown_1.13     knitr_1.48        lubridate_1.9.3  \n[25] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n[28] purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n[31] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n[34] here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] RColorBrewer_1.1-3 rstudioapi_0.16.0  jsonlite_1.8.9    \n  [4] magrittr_2.0.3     TH.data_1.1-2      estimability_1.5.1\n  [7] farver_2.1.2       nloptr_2.1.1       rmarkdown_2.28    \n [10] vctrs_0.6.5        Cairo_1.6-2        minqa_1.2.8       \n [13] base64enc_0.1-3    rstatix_0.7.2      htmltools_0.5.8.1 \n [16] curl_5.2.3         broom_1.0.7        Formula_1.2-5     \n [19] htmlwidgets_1.6.4  plyr_1.8.9         sandwich_3.1-1    \n [22] emmeans_1.10.4     zoo_1.8-12         uuid_1.2-1        \n [25] igraph_2.0.3       mime_0.12          lifecycle_1.0.4   \n [28] pkgconfig_2.0.3    Matrix_1.7-0       R6_2.5.1          \n [31] fastmap_1.2.0      shiny_1.9.1        digest_0.6.37     \n [34] OpenMx_2.21.12     fdrtool_1.2.18     colorspace_2.1-1  \n [37] rprojroot_2.0.4    Hmisc_5.1-3        fansi_1.0.6       \n [40] timechange_0.3.0   abind_1.4-8        compiler_4.4.1    \n [43] withr_3.0.1        glasso_1.11        htmlTable_2.4.3   \n [46] backports_1.5.0    carData_3.0-5      ggsignif_0.6.4    \n [49] corpcor_1.6.10     gtools_3.9.5       tools_4.4.1       \n [52] pbivnorm_0.6.0     foreign_0.8-87     zip_2.3.1         \n [55] httpuv_1.6.15      nnet_7.3-19        glue_1.8.0        \n [58] quadprog_1.5-8     DiagrammeR_1.0.11  promises_1.3.0    \n [61] nlme_3.1-166       lisrelToR_0.3      grid_4.4.1        \n [64] pbdZMQ_0.3-13      checkmate_2.3.2    cluster_2.1.6     \n [67] reshape2_1.4.4     generics_0.1.3     gtable_0.3.5      \n [70] tzdb_0.4.0         data.table_1.16.0  hms_1.1.3         \n [73] car_3.1-3          utf8_1.2.4         sem_3.1-16        \n [76] pillar_1.9.0       IRdisplay_1.1      rockchalk_1.8.157 \n [79] later_1.3.2        splines_4.4.1      lattice_0.22-6    \n [82] survival_3.7-0     kutils_1.73        tidyselect_1.2.1  \n [85] miniUI_0.1.1.1     pbapply_1.7-2      V8_5.0.1          \n [88] stats4_4.4.1       xfun_0.48          qgraph_1.9.8      \n [91] arm_1.14-4         visNetwork_2.1.2   stringi_1.8.4     \n [94] pacman_0.5.1       boot_1.3-31        evaluate_1.0.0    \n [97] codetools_0.2-20   mi_1.1             cli_3.6.3         \n[100] RcppParallel_5.1.9 IRkernel_1.3.2     rpart_4.1.23      \n[103] xtable_1.8-4       repr_1.1.7         munsell_0.5.1     \n[106] Rcpp_1.0.13        coda_0.19-4.1      png_0.1-8         \n[109] XML_3.99-0.17      parallel_4.4.1     jpeg_0.1-10       \n[112] lme4_1.1-35.5      mvtnorm_1.3-1      openxlsx_4.2.7.1  \n[115] crayon_1.5.3       rlang_1.1.4        multcomp_1.4-26   \n[118] mnormt_2.1.1      \n\n\n\n\n\n\nKline, R. B. (2023). Principles and practice of structural equation modeling. Guilford publications.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analisi dei percorsi</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/02_clement_2022.html",
    "href": "chapters/path_analysis/02_clement_2022.html",
    "title": "23  Riparazione affettiva post-infedeltà",
    "section": "",
    "text": "23.1 Introduzione\nLo scopo di questo capitolo è quello di discutere il tutorial di Clement & Bradley-Garcia (2022) in cui la path anaysis viene impiegata per tre analisi statistiche: il modello di moderazione, il modello di mediazione semplice, e il modello di mediazione moderata.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Riparazione affettiva post-infedeltà</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/02_clement_2022.html#modello-di-mediazione-semplice",
    "href": "chapters/path_analysis/02_clement_2022.html#modello-di-mediazione-semplice",
    "title": "23  Riparazione affettiva post-infedeltà",
    "section": "23.2 Modello di Mediazione Semplice",
    "text": "23.2 Modello di Mediazione Semplice\nEsplorare se esiste un’associazione tra variabili implica l’uso di analisi correlazionali, come la correlazione bivariata. In questo tipo di analisi, non ci si preoccupa di capire se una variabile indipendente influenza una variabile dipendente, come nel caso della previsione. Tuttavia, quando è importante capire quale variabile influenza l’altra (ovvero la previsione), si utilizza spesso l’analisi di regressione lineare. Per eseguire un’analisi di regressione lineare, il ricercatore deve scegliere una variabile esplicativa (la variabile indipendente, IV o X) e una o più variabili di risposta (DV o Y). È importante notare che eseguire semplicemente un’analisi di regressione non fornisce prove conclusive sull’influenza di un’IV su una DV. Nel caso di un disegno di ricerca correlazionale, la scelta dell’IV e della DV per un’analisi di regressione si basa su un quadro teorico.\nNel suo tutorial, Clement & Bradley-Garcia (2022) propone il seguente esempio. Tra le coppie che hanno vissuto una trasgressione nella loro relazione, incoraggiare la ricostruzione della fiducia nel trasgressore può aiutare il partner ferito a perdonarlo (Hargrave & Hammer, 2016). In questo scenario, l’IV è la fiducia nel trasgressore, e la DV è il perdono. Tuttavia, esplorare come la fiducia nel trasgressore può aiutare il partner ferito a perdonarlo potrebbe fornire un’idea più approfondita delle aree su cui concentrarsi nella terapia di coppia, che potrebbe incoraggiare la fiducia e quindi il perdono. Eseguire un’analisi di un modello di mediazione semplice può far luce su tali aree di intervento.\nIn un modello di mediazione semplice, l’IV ha un effetto su una variabile aggiuntiva o terza, chiamata variabile mediatrice (M), che a sua volta ha un effetto sulla DV quando l’IV è mantenuta costante. In altre parole, l’IV è associata alla DV attraverso M. Questa relazione è l’effetto indiretto, che costituisce la relazione di mediazione.\n\n\n\n\n\n\nFigura 23.1: Pannello A: Mediazione semplice; Pannello B: Moderazione; e Pannello C: Modelli di Mediazione Moderata. Nel Pannello A, le linee tratteggiate indicano il percorso indiretto (ab), mentre la linea continua indica il percorso diretto (c’). (Figura tratta da Clement & Bradley-Garcia (2022))\n\n\n\nAd esempio, la ricerca dimostra che la fiducia in un partner romantico è associata alla compassione per quel partner (Salazar, 2015), e la compassione è anche uno dei più forti predittori di perdono (Davis, 2017). Pertanto, si potrebbe ipotizzare che la fiducia sia associata al perdono attraverso la compassione per il partner romantico. Nello specifico, si potrebbe testare un modello di mediazione in cui la fiducia nel partner romantico influenza la compassione per il partner (a), e la compassione per il partner influenza il perdono di quel partner (b), il che costituirebbe l’effetto indiretto della fiducia nel partner romantico sul perdono attraverso la compassione per il partner romantico (ab). Se l’analisi di questo modello suggerisce che l’effetto indiretto è robusto, si potrebbe concludere statisticamente che la compassione media l’associazione tra fiducia nel partner romantico e perdono. Se l’effetto indiretto è trascurabile, allora non c’è mediazione, e non si può concludere che la compassione giochi un ruolo nell’associazione tra fiducia nel partner romantico e perdono.\nOltre all’effetto indiretto, un modello di mediazione esamina anche l’effetto diretto dell’IV sulla DV. Continuando con lo scenario sopra, questo comporterebbe l’esame se la fiducia nel partner romantico sia associata al perdono di quel partner quando la compassione è mantenuta costante. L’effetto diretto (c′) e l’effetto indiretto (ab) combinati costituiscono l’effetto totale (c) del modello di mediazione, come mostrato nella Figura 1A. Nello specifico, l’effetto indiretto è il prodotto del percorso a e del percorso b (a × b), mentre l’effetto totale è la somma dell’effetto indiretto e dell’effetto diretto (c = c′ + a × b).\nIn letteratura, si parla di “mediazione completa” quando c’è un effetto indiretto significativo ed effetto diretto non significativo; si parla di “mediazione parziale” quando gli effetti indiretto e diretto sono entrambi significativi (Meule, 2019).\nPertanto, analizzare un modello di mediazione semplice potrebbe aiutare a comprendere come la compassione per un partner romantico sia associata alla fiducia in quel partner e al perdono. Ugualmente importante è esaminare quando la fiducia in un partner romantico è correlata alla compassione per il partner, il che comporta l’esame di un modello di moderazione.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Riparazione affettiva post-infedeltà</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/02_clement_2022.html#modello-di-moderazione",
    "href": "chapters/path_analysis/02_clement_2022.html#modello-di-moderazione",
    "title": "23  Riparazione affettiva post-infedeltà",
    "section": "23.3 Modello di Moderazione",
    "text": "23.3 Modello di Moderazione\nNel caso di un modello di moderazione siamo interessati a determinare se una terza variabile, o una variabile moderatrice (W), influenzi la forza o la direzione dell’associazione tra la variabile indipendente (IV) e la variabile dipendente (DV). In particolare:\n\nIdentificare un moderatore di un effetto aiuta a stabilire le condizioni limite di quell’effetto o le circostanze, gli stimoli o il tipo di persone per cui l’effetto è maggiore o minore, presente o assente, positivo o negativo, e così via” (Hayes, 2018, p. 220).\n\nDal punto di vista statistico, testare un modello di moderazione è simile a testare l’interazione tra fattori in un’analisi della varianza (ANOVA; Frazier, Tix, & Barron, 2004). Un effetto di interazione è presente quando l’effetto dell’IV sulla DV è condizionato da W (Hayes, 2018). In altre parole, siamo interessati a capire se l’associazione tra IV e DV varia a diversi livelli di W.\nLa Figura 23.1 B mostra un diagramma di un modello di moderazione. Ritornando all’esempio di fiducia, compassione e perdono, i ricercatori potrebbero essere interessati a capire in quali circostanze la fiducia influenza la compassione. Nella ricerca sui comportamenti sociali sani, l’umiltà è stata identificata come un elemento importante nella coltivazione della compassione (Worthington & Allison, 2018). Infatti, studi hanno dimostrato che considerare il proprio partner romantico come umile è associato al vederlo anche come compassionevole (es. McDonald, Olson, Goddard, & Marshall, 2018). Inoltre, la ricerca ha mostrato un’associazione tra fiducia e umiltà (es. Wang, Edwards, & Hill, 2017). Si potrebbe ipotizzare un modello di moderazione in cui l’associazione tra fiducia e compassione per il partner romantico è condizionata dal considerare il partner come umile.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Riparazione affettiva post-infedeltà</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/02_clement_2022.html#modello-di-mediazione-moderata",
    "href": "chapters/path_analysis/02_clement_2022.html#modello-di-mediazione-moderata",
    "title": "23  Riparazione affettiva post-infedeltà",
    "section": "23.4 Modello di Mediazione Moderata",
    "text": "23.4 Modello di Mediazione Moderata\nIl concetto di mediazione moderata, introdotto da James e Brett (1984), riguarda l’esame di come la variabile moderatrice (W) influenzi l’entità di un effetto indiretto (Preacher, Rucker, & Hayes, 2007). Più precisamente, si parla di mediazione moderata quando una relazione di mediazione dipende dal livello di un moderatore (Preacher et al., 2007).\nLa Figura 23.1 C rappresenta un modello di mediazione moderata. Riprendendo l’esempio della fiducia, umiltà, compassione e perdono, potremmo ipotizzare un modello di mediazione moderata in cui la fiducia nel partner romantico (X) è associata al perdono (Y) attraverso la compassione per quel partner (M), e questa associazione è rafforzata dalla percezione del partner come umile (W).",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Riparazione affettiva post-infedeltà</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/02_clement_2022.html#esempio-di-mediazione-moderata",
    "href": "chapters/path_analysis/02_clement_2022.html#esempio-di-mediazione-moderata",
    "title": "23  Riparazione affettiva post-infedeltà",
    "section": "23.5 Esempio di Mediazione Moderata",
    "text": "23.5 Esempio di Mediazione Moderata\nNel suo tutorial, Clement & Bradley-Garcia (2022) utilizza i dati di uno studio che ha esaminato l’associazione tra fiducia di coppia, compassione verso il partner che ha ferito, percezione di quel partner come umile, e perdono in persone che hanno vissuto un “attaccamento ferito” nella loro relazione romantica. Le ferite da attaccamento sono definite come una violazione percepita della fiducia o un abbandono che si verifica in un momento critico in cui c’era bisogno del sostegno e della cura del partner romantico (Johnson, Makinen, & Millikin, 2001). In questo studio, la fiducia di coppia si riferisce al grado di onestà e buona volontà che il partner ferito percepisce nel partner che ha commesso l’offesa (Larzelere & Huston, 1980). Un aspetto cruciale per la risoluzione delle ferite da attaccamento e il ripristino della fiducia in una relazione romantica è il perdono, che nello studio è stato definito come la presenza di alti livelli di motivazioni benevolenti (es. buona volontà) e bassi livelli di motivazioni di evitamento o vendetta verso il partner che ha commesso l’offesa.\nPartecipanti\nIl campione utilizzato per questo tutorial era composto da 138 persone che hanno riferito di aver vissuto una ferita da attaccamento nella loro attuale relazione romantica e di aver perdonato il proprio partner.\nI partecipanti hanno completato i questionari auto-compilati online.\nI partecipanti hanno descritto brevemente la ferita da attaccamento subita nella relazione e indicato se avevano perdonato o meno il partner romantico per tale ferita (sì/no).\nLa fiducia di coppia è stata misurata utilizzando la Dyadic Trust Scale (DTS; Larzelere & Huston, 1980), composta da 8 item valutati su una scala da 1 (fortemente d’accordo) a 7 (fortemente in disaccordo).\nLa compassione verso il partner è stata misurata con la Compassion Scale (CS; Pommier, Neff, & Tóth-Király, 2020), modificata per riflettere la compassione verso il partner che ha commesso l’offesa. La scala includeva 16 item valutati da 1 (quasi mai) a 5 (quasi sempre).\nLa percezione del partner come umile è stata misurata con la Relational Humility Scale (RHS; Davis et al., 2011), modificata per riflettere i sentimenti attuali del partner ferito verso il partner che ha commesso l’offesa. Gli item sono stati valutati su una scala da 1 (fortemente in disaccordo) a 5 (fortemente d’accordo).\nIl perdono del partner che ha commesso l’offesa è stato misurato con il Transgressions-related Interpersonal Motivations Inventory (TRIM; McCullough et al., 1998; McCullough, Fincham, & Tsang, 2003). Il TRIM misura: - Motivazione ad evitare il partner. - Motivazione a cercare vendetta. - Motivazione a dimostrare benevolenza verso il partner.\nPer questo studio, il perdono è stato definito come bassi livelli di motivazione ad evitare o vendicarsi del partner, e alti livelli di motivazione benevolente verso di lui/lei. L’esempio discusso nel tutorial include solo la sottoscala della benevolenza del TRIM-18.\n\n# Load the data from SPSS file\ndata &lt;- read_sav(here::here(\"data\", \"clement_2022.sav\"))\n\n\nglimpse(data)\n\nRows: 138\nColumns: 4\n$ CS_TOT   &lt;dbl&gt; 58, 68, 66, 70, 70, 44, 66, 76, 67, 43, 66, 68, 68,~\n$ RHSTOT   &lt;dbl&gt; 59, 53, 52, 35, 55, 36, 53, 54, 49, 45, 27, 52, 34,~\n$ TRIM_Ben &lt;dbl&gt; 19, 22, 26, 24, 25, 12, 21, 29, 30, 6, 25, 17, 29, ~\n$ DTST     &lt;dbl&gt; 54, 51, 48, 34, 48, 32, 52, 52, 49, 47, 36, 34, 47,~\n\n\n\n# Statistiche descrittive\ndescribe(data)\n\n\nA psych: 4 x 13\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nCS_TOT\n1\n138\n64.18841\n8.530367\n66.0\n65.04464\n8.1543\n35\n76\n41\n-0.8842272\n0.5045965\n0.7261531\n\n\nRHSTOT\n2\n138\n42.92029\n10.191639\n44.5\n43.33929\n11.1195\n17\n60\n43\n-0.3316798\n-0.5684271\n0.8675700\n\n\nTRIM_Ben\n3\n138\n21.85507\n5.679456\n22.0\n22.34821\n5.9304\n6\n30\n24\n-0.7369178\n0.1218593\n0.4834674\n\n\nDTST\n4\n138\n38.59420\n10.249898\n39.0\n38.96429\n11.8608\n11\n56\n45\n-0.2566815\n-0.6359079\n0.8725292\n\n\n\n\n\n\n# Identificazione degli outliers per ogni variabile\noutliers_results &lt;- check_outliers(data)\n\n# Visualizzazione dei risultati\nprint(outliers_results)\n\nOK: No outliers detected.\n- Based on the following method and threshold: mahalanobis (20).\n- For variables: CS_TOT, RHSTOT, TRIM_Ben, DTST\n\n\n\n\n# Visualizzare un boxplot con outliers\nboxplot(data, main = \"Boxplot delle variabili\", col = \"lightblue\", las = 2)",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Riparazione affettiva post-infedeltà</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/02_clement_2022.html#indipendenza",
    "href": "chapters/path_analysis/02_clement_2022.html#indipendenza",
    "title": "23  Riparazione affettiva post-infedeltà",
    "section": "23.6 Indipendenza",
    "text": "23.6 Indipendenza\nPer soddisfare l’assunzione di indipendenza nella regressione, i residui del modello devono essere indipendenti. Questo significa che l’errore nella stima di un partecipante non deve influenzare l’errore nella stima di un altro partecipante. Nel nostro esempio, ciò vuol dire che l’errore nella stima del punteggio di benevolenza di un partecipante non dovrebbe influenzare l’errore nella stima del punteggio di un altro partecipante. Per verificare questa assunzione, utilizzeremo la statistica di Durbin-Watson, che testa la presenza di autocorrelazione nei termini di errore (Uyanto, 2020).\nLa statistica di Durbin-Watson varia tra 0 e 4, e valori compresi tra 1.5 e 2.5 indicano che l’assunzione di indipendenza è rispettata (Glen, 2022).\nIn R, possiamo utilizzare la funzione durbinWatsonTest() dal pacchetto car per testare l’assunzione di indipendenza dei residui in un modello di regressione.\nPrima di testare l’indipendenza dei residui, dobbiamo definire un modello di regressione. Per esempio, possiamo creare un modello dove TRIM_Ben è la variabile dipendente e DTST, CS_TOT e RHSTOT sono le variabili indipendenti:\n\n# Creazione del modello di regressione\nmodel &lt;- lm(TRIM_Ben ~ DTST + CS_TOT + RHSTOT, data = data)\n\nDopo aver creato il modello, possiamo testare l’indipendenza dei residui utilizzando la funzione durbinWatsonTest():\n\n# Esecuzione del test di Durbin-Watson per verificare l'indipendenza dei residui\ndw_test &lt;- durbinWatsonTest(model)\n\n# Visualizzazione del risultato\nprint(dw_test)\n\n lag Autocorrelation D-W Statistic p-value\n   1     -0.03195449      2.035422    0.83\n Alternative hypothesis: rho != 0\n\n\nIl valore del test di Durbin-Watson varierà tra 0 e 4, e un valore compreso tra 1.5 e 2.5 indica che l’assunzione di indipendenza dei residui è soddisfatta. Possiamo quindi procedere a verificare la successiva assunzione, cioè quella di linearità.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Riparazione affettiva post-infedeltà</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/02_clement_2022.html#linearità",
    "href": "chapters/path_analysis/02_clement_2022.html#linearità",
    "title": "23  Riparazione affettiva post-infedeltà",
    "section": "23.7 Linearità",
    "text": "23.7 Linearità\nUn’ipotesi fondamentale nella regressione è che le variabili indipendenti (IV) siano correlate con la variabile dipendente (DV) in modo lineare. Questo significa che all’aumentare o al diminuire delle IV, i valori della DV aumentano o diminuiscono in modo proporzionale. Per verificare questa ipotesi, si può tracciare un grafico tra le IV e la DV, oppure utilizzare grafici parziali per verificare la relazione lineare tra ciascuna IV e la DV.\nNel suo tutorial, Clement & Bradley-Garcia (2022) verifica l’assunzione di linearità tra tutte le variabili usando un grafico di dispersione che confronta i residui studentizzati (SRE_1) e i valori previsti non standardizzati (PRE_1).\n\n# Calcolare i residui studentizzati e i valori previsti\nSRE_1 &lt;- rstudent(model) # Residui studentizzati\nPRE_1 &lt;- predict(model) # Valori previsti non standardizzati\n\n# Creare un grafico di dispersione per verificare la linearità\nplot(PRE_1, SRE_1,\n    main = \"Verifica della linearità: residui vs valori previsti\",\n    xlab = \"Valori previsti (PRE_1)\",\n    ylab = \"Residui studentizzati (SRE_1)\",\n    pch = 19, col = \"blue\"\n)\nabline(h = 0, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nSe i punti nel grafico sono distribuiti in modo casuale attorno alla linea orizzontale (che rappresenta i residui pari a 0), l’assunzione di linearità è soddisfatta. Se invece si notano dei pattern, questo potrebbe essere indicativo di una violazione della linearità.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Riparazione affettiva post-infedeltà</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/02_clement_2022.html#omoschedasticità",
    "href": "chapters/path_analysis/02_clement_2022.html#omoschedasticità",
    "title": "23  Riparazione affettiva post-infedeltà",
    "section": "23.8 Omoschedasticità",
    "text": "23.8 Omoschedasticità\nL’assunzione di omoschedasticità richiede che l’errore nella relazione tra le variabili indipendenti (IV) e la variabile dipendente (DV) sia costante su tutti i valori delle IV. Quando questa assunzione è violata, si verifica eteroschedasticità, che significa che l’errore (la variabilità nei punteggi della DV non spiegata dalle IV) varia in base ai punteggi della DV (Osborne & Waters, 2002). Per verificare se questa assunzione è soddisfatta, possiamo utilizzare lo stesso grafico di dispersione dei residui che abbiamo creato prima. Se i residui sono distribuiti in modo casuale attorno ai valori previsti e formano una forma rettangolare, possiamo concludere che l’assunzione di omoschedasticità è soddisfatta.\n\n # Calcolo dei residui e dei valori previsti\n residuals &lt;- resid(model)\n predicted_values &lt;- predict(model)\n\n # Grafico di dispersione per verificare l'omoschedasticità\n plot(predicted_values, residuals,\n     main = \"Verifica dell'omoschedasticità: Residui vs Valori previsti\",\n     xlab = \"Valori previsti\",\n     ylab = \"Residui\",\n     pch = 19, col = \"blue\"\n )\n abline(h = 0, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\nSe i punti nel grafico sono distribuiti casualmente attorno alla linea orizzontale (che rappresenta residui pari a 0) e non mostrano alcun pattern evidente (ad esempio, non formano un cono o una forma curva), allora l’assunzione di omoschedasticità è soddisfatta.\nSe invece i punti mostrano una dispersione non uniforme (ad esempio, aumentano o diminuiscono con i valori previsti), potrebbe esserci eteroschedasticità, e l’assunzione sarebbe violata.\n\nÈ anche possibile eseguire un test statistico per l’omoschedasticità, come il test di Breusch-Pagan, utilizzando il pacchetto lmtest:\n\nbptest(model)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model\nBP = 3.4348, df = 3, p-value = 0.3293\n\n\n\nSe il p-value è alto (di solito &gt; 0.05), non ci sono evidenze di eteroschedasticità e l’assunzione di omoschedasticità è soddisfatta.\nSe il p-value è basso (di solito &lt; 0.05), c’è eteroschedasticità e l’assunzione è violata.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Riparazione affettiva post-infedeltà</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/02_clement_2022.html#multicollinearità",
    "href": "chapters/path_analysis/02_clement_2022.html#multicollinearità",
    "title": "23  Riparazione affettiva post-infedeltà",
    "section": "23.9 Multicollinearità",
    "text": "23.9 Multicollinearità\nQuando due o più variabili indipendenti (IV) sono fortemente correlate tra loro, potrebbe esserci multicollinearità, il che significa che i punteggi di una o più IV sono determinati da altre IV nel modello (Kim, 2019). Per verificare se esiste multicollinearità, possiamo calcolare il fattore di inflazione della varianza (VIF) o la statistica di tolleranza. Queste due misure sono reciproche, quindi è sufficiente controllarne una.\n\nLa tolleranza dovrebbe essere superiore a 0.1.\nIl VIF non dovrebbe superare il valore di 10 (Miles, 2005).\n\n\n # Calcolo del VIF per le variabili indipendenti\n vif_values &lt;- vif(model)\n\n # Visualizzazione dei valori VIF\n print(vif_values)\n\n\nSe tutti i valori di VIF sono inferiori a 10, non c’è multicollinearità nel modello.\nSe trovi valori di VIF superiori a 10, potresti avere un problema di multicollinearità e dovresti considerare di rimuovere o combinare alcune variabili indipendenti.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Riparazione affettiva post-infedeltà</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/02_clement_2022.html#normalità",
    "href": "chapters/path_analysis/02_clement_2022.html#normalità",
    "title": "23  Riparazione affettiva post-infedeltà",
    "section": "23.10 Normalità",
    "text": "23.10 Normalità\nL’assunzione di normalità richiede che i residui, ossia gli errori nella stima, siano distribuiti normalmente (Hayes, 2018). Per verificare questa assunzione, possiamo ispezionare visivamente l’istogramma dei residui sovrapposto a una curva di normalità e un grafico P-P (probability-probability plot).\n\nIstogramma: Se i residui sono distribuiti normalmente, l’istogramma avrà una forma simile a una campana.\nGrafico P-P: Se i punti si allineano bene lungo la diagonale, significa che la distribuzione è normale.\n\nAnche se ci sono piccole deviazioni dalla normalità, la regressione è robusta a violazioni non gravi della normalità, quindi possiamo comunque procedere con l’analisi.\nVerificare la normalità visivamente\nCreeremo un istogramma dei residui e sovrapporremo una curva di densità normale per vedere se i residui seguono una distribuzione normale.\n\n# Calcolo dei residui\nresiduals &lt;- resid(model)\n\n# Creare l'istogramma con curva di normalità sovrapposta\nhist(residuals,\n    prob = TRUE, main = \"Istogramma dei residui\",\n    xlab = \"Residui\", col = \"lightblue\", border = \"black\"\n)\n\n# Sovrapporre la curva di normalità\nlines(density(residuals), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nSe la curva rossa (curva di densità) segue bene la forma dell’istogramma, possiamo concludere che i residui sono distribuiti normalmente.\nUn altro metodo per verificare la normalità è creare un grafico P-P (probability-probability plot).\n\n# Creazione del P-P plot\nqqnorm(residuals, main = \"Grafico P-P dei residui\")\nqqline(residuals, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nSe i punti nel grafico P-P seguono da vicino la linea diagonale, i residui sono distribuiti normalmente. Se ci sono alcune deviazioni, la regressione è comunque robusta a violazioni lievi della normalità.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Riparazione affettiva post-infedeltà</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/02_clement_2022.html#analisi-di-moderazione",
    "href": "chapters/path_analysis/02_clement_2022.html#analisi-di-moderazione",
    "title": "23  Riparazione affettiva post-infedeltà",
    "section": "23.11 Analisi di moderazione",
    "text": "23.11 Analisi di moderazione\nPer determinare se esiste una relazione di moderazione tra DTST (fiducia di coppia) e RHSTOT (umiltà percepita del partner), possiamo fare riferimento ai risultati dell’output del seguente modello di regressione lineare:\n\n# Creare il modello di moderazione\nmodel &lt;- lm(CS_TOT ~ DTST * RHSTOT, data = data)\nsummary(model)\n\n\nCall:\nlm(formula = CS_TOT ~ DTST * RHSTOT, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-25.8585  -4.1310   0.8514   5.9536  15.0329 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 74.858218   9.903011   7.559 5.73e-12 ***\nDTST        -0.516985   0.279163  -1.852   0.0662 .  \nRHSTOT      -0.354859   0.241398  -1.470   0.1439    \nDTST:RHSTOT  0.014269   0.006175   2.311   0.0224 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.103 on 134 degrees of freedom\nMultiple R-squared:  0.1174,    Adjusted R-squared:  0.09759 \nF-statistic: 5.939 on 3 and 134 DF,  p-value: 0.0007811\n\n\nDalla tabella dei coefficienti, possiamo notare che:\n\nL’intercetta ha un valore stimato di 74.86, con un errore standard di 9.90, risultando altamente significativa (p &lt; 0.001).\nL’effetto principale di DTST (fiducia di coppia) è negativo (b = -0.517), ma non significativo al 5% (p = 0.0662).\nL’effetto principale di RHSTOT (percezione dell’umiltà del partner) è anch’esso negativo (b = -0.355), ma non significativo (p = 0.1439).\nTuttavia, l’interazione tra DTST e RHSTOT risulta significativa (b = 0.0143, p = 0.0224), suggerendo che l’effetto di DTST su CS_TOT varia in base ai livelli di RHSTOT.\n\n\n23.11.1 Interpretazione dell’interazione\nPoiché l’interazione è significativa, significa che l’associazione tra la fiducia di coppia (DTST) e la compassione verso il partner che ha ferito (CS_TOT) dipende dalla percezione dell’umiltà del partner (RHSTOT). Questo implica che la fiducia di coppia avrà un effetto diverso sulla compassione a seconda del livello di umiltà percepita.\n\n\n23.11.2 Pendenze semplici\nPer visualizzare questa interazione, possiamo usare un grafico che mostra l’effetto di DTST (fiducia di coppia) su CS_TOT (compassione) a tre livelli di RHSTOT (umiltà percepita del partner): alla media, e a ±1 deviazione standard dalla media. Il codice fornito per visualizzare le pendenze semplici è:\n\n# Calcolo della media e delle deviazioni standard di RHSTOT\nmean_RHSTOT &lt;- mean(data$RHSTOT, na.rm = TRUE)\nsd_RHSTOT &lt;- sd(data$RHSTOT, na.rm = TRUE)\n\n# Stampa dei valori\nmean_RHSTOT\nsd_RHSTOT\n\n# Visualizzare le semplici pendenze dell'interazione usando la media e ± 1 SD\ninteract_plot(model,\n    pred = DTST, modx = RHSTOT,\n    modx.values = c(mean_RHSTOT - sd_RHSTOT, mean_RHSTOT, mean_RHSTOT + sd_RHSTOT),\n    plot.points = TRUE,\n    main.title = \"Interazione tra DTST e RHSTOT su CS_TOT\",\n    x.label = \"Fiducia di coppia (DTST)\",\n    y.label = \"Compassione (CS_TOT)\",\n    legend.main = \"Umiltà del partner (RHSTOT)\"\n)\n\n42.9202898550725\n\n\n10.1916393344805\n\n\n\n\n\n\n\n\n\nDal grafico risultante, possiamo visualizzare le pendenze dell’interazione, mostrando come l’effetto della fiducia di coppia sulla compassione cambi in base ai diversi livelli di umiltà percepita del partner. Questo tipo di visualizzazione permette di vedere chiaramente che quando RHSTOT è alto (partner percepito come molto umile), l’effetto di DTST su CS_TOT aumenta.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Riparazione affettiva post-infedeltà</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/02_clement_2022.html#analisi-della-mediazione",
    "href": "chapters/path_analysis/02_clement_2022.html#analisi-della-mediazione",
    "title": "23  Riparazione affettiva post-infedeltà",
    "section": "23.12 Analisi della Mediazione",
    "text": "23.12 Analisi della Mediazione\nOra dobbiamo capire se c’è un’associazione diretta tra la variabile indipendente (X, ad esempio, la fiducia di coppia) e la variabile dipendente (Y, ad esempio, il perdono). Per farlo, analizziamo il riepilogo del modello per la variabile di esito TRIM_Ben (che rappresenta il perdono).\n\n# Definizione del modello di mediazione\nmodel &lt;- \"\n  # Effetto diretto di DTST su TRIM_Ben\n  TRIM_Ben ~ c_prime*DTST\n\n  # Effetto di DTST su CS_TOT (mediazione)\n  CS_TOT ~ a*DTST\n\n  # Effetto di CS_TOT su TRIM_Ben (mediazione)\n  TRIM_Ben ~ b*CS_TOT\n\n  # Effetto indiretto (a * b)\n  ab := a*b\n\n  # Effetto totale di DTST su TRIM_Ben\n  total := c_prime + ab\n\"\n\n# Esegui il modello con i dati\nfit &lt;- sem(model, data = data)\n\n# Riassunto dei risultati\nsummary(fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) |&gt;\n  print()\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         5\n\n  Number of observations                           138\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nModel Test Baseline Model:\n\n  Test statistic                                60.916\n  Degrees of freedom                                 3\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.000\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)               -895.673\n  Loglikelihood unrestricted model (H1)       -895.673\n                                                      \n  Akaike (AIC)                                1801.345\n  Bayesian (BIC)                              1815.981\n  Sample-size adjusted Bayesian (SABIC)       1800.163\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: RMSEA &lt;= 0.050                       NA\n  P-value H_0: RMSEA &gt;= 0.080                       NA\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n  TRIM_Ben ~                                                   \n    DTST    (c_pr)    0.148    0.040    3.691    0.000    0.148\n  CS_TOT ~                                                     \n    DTST       (a)    0.197    0.069    2.864    0.004    0.197\n  TRIM_Ben ~                                                   \n    CS_TOT     (b)    0.292    0.048    6.058    0.000    0.292\n  Std.all\n         \n    0.267\n         \n    0.237\n         \n    0.438\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n   .TRIM_Ben         21.819    2.627    8.307    0.000   21.819\n   .CS_TOT           68.186    8.209    8.307    0.000   68.186\n  Std.all\n    0.681\n    0.944\n\nR-Square:\n                   Estimate\n    TRIM_Ben          0.319\n    CS_TOT            0.056\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n    ab                0.058    0.022    2.589    0.010    0.058\n    total             0.205    0.044    4.690    0.000    0.205\n  Std.all\n    0.104\n    0.371\n\n\n\n\n23.12.1 Interpretazione del Modello di Mediazione\nL’output di lavaan fornisce tutte le informazioni necessarie per interpretare il modello di mediazione, con particolare attenzione ai coefficienti standardizzati e non standardizzati. Poiché il modello presenta zero gradi di libertà, non è possibile interpretare gli indici di adattamento (CFI, TLI, RMSEA). L’interpretazione si concentra quindi sui coefficienti del modello di percorso.\n\n\n23.12.2 Effetto Diretto e Indiretto\n\nEffetto diretto (c’ path): Il coefficiente di percorso standardizzato dell’effetto diretto di DTST (fiducia di coppia) su TRIM_Ben (perdono) è pari a 0.267, con un coefficiente non standardizzato di 0.148 (SE = 0.040, z = 3.691, p &lt; 0.001). Questo risultato indica che la fiducia di coppia ha un effetto positivo diretto sul perdono. Le persone che hanno maggiore fiducia nel partner tendono a mostrare un livello più elevato di perdono, indipendentemente dalla compassione verso il partner.\nEffetto indiretto (ab path): Il coefficiente standardizzato per l’effetto indiretto di DTST su TRIM_Ben, mediato da CS_TOT (compassione), è pari a 0.104, con un coefficiente non standardizzato di 0.058 (SE = 0.022, z = 2.589, p = 0.010). Questo suggerisce che la fiducia di coppia influisce sul perdono anche attraverso la compassione. In altre parole, una maggiore fiducia nel partner porta a un aumento della compassione verso di lui, e questo incremento di compassione favorisce ulteriormente il perdono.\n\n\n\n23.12.3 Effetto Totale\n\nEffetto totale: Sommando gli effetti diretto e indiretto, l’effetto totale di DTST su TRIM_Ben è pari a 0.371, con un coefficiente non standardizzato di 0.205 (SE = 0.044, z = 4.690, p &lt; 0.001). Ciò indica che la fiducia di coppia ha un’influenza complessiva positiva sul perdono, sia direttamente che attraverso la mediazione della compassione.\n\n\n\n23.12.4 Varianza Spiegata (R²)\n\nLa fiducia di coppia e la compassione spiegano il 31.9% della varianza in TRIM_Ben (perdono), suggerendo che questi predittori contribuiscono in modo sostanziale a spiegare il perdono.\nInoltre, DTST spiega il 5.6% della varianza in CS_TOT (compassione), indicando che una parte dell’influenza della fiducia di coppia è esercitata attraverso la compassione.\n\n\n\n23.12.5 Conclusioni\nIn sintesi:\n\nEffetto diretto: La fiducia di coppia (DTST) ha un’influenza positiva diretta sul perdono (TRIM_Ben), con un coefficiente standardizzato pari a 0.267. Questo significa che, indipendentemente da altri fattori, le persone che hanno più fiducia nel partner sono più inclini a perdonare.\nEffetto indiretto: Oltre all’effetto diretto, c’è un’influenza indiretta mediata dalla compassione (CS_TOT), con un coefficiente standardizzato pari a 0.104. Questo implica che la fiducia di coppia aumenta la compassione verso il partner, e la maggiore compassione favorisce ulteriormente il perdono.\nEffetto totale: L’effetto complessivo della fiducia di coppia sul perdono, che include sia l’effetto diretto che quello mediato dalla compassione, è pari a 0.371, evidenziando un’influenza positiva e significativa.\n\n\n\n23.12.6 Implicazioni\nQuesti risultati suggeriscono che per promuovere il perdono, la fiducia nel partner è un fattore chiave. Tuttavia, anche la compassione verso il partner gioca un ruolo essenziale, mediando parte dell’effetto della fiducia sul perdono. Pertanto, lavorare sia sulla fiducia che sulla compassione potrebbe essere utile per migliorare la capacità di perdonare in una relazione di coppia.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Riparazione affettiva post-infedeltà</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/02_clement_2022.html#mediazione-moderata",
    "href": "chapters/path_analysis/02_clement_2022.html#mediazione-moderata",
    "title": "23  Riparazione affettiva post-infedeltà",
    "section": "23.13 Mediazione Moderata",
    "text": "23.13 Mediazione Moderata\n\n# Define the moderated mediation model (PROCESS model 7 equivalent)\n# Here, DTST is the independent variable (X), CS_TOT is the mediator (M),\n# TRIM_Ben is the outcome (Y), and RHSTOT is the moderator (W).\n\n# Specify the model in lavaan syntax\nmodel &lt;- \"\n  # Direct effect of X on Y\n  TRIM_Ben ~ c*DTST + b*CS_TOT + d*RHSTOT + e*DTST:RHSTOT\n\n  # Mediation pathway (X -&gt; M -&gt; Y)\n  CS_TOT ~ a*DTST + f*RHSTOT + g*DTST:RHSTOT\n\n  # Indirect effect of X on Y through M (CS_TOT)\n  indirect := a*b\n  direct := c\n  total := indirect + direct\n\"\n\n# Fit the model\nfit &lt;- sem(model, data = data, se = \"bootstrap\", bootstrap = 100)\n\n# Summary of the model\nsummary(fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n\n  Number of observations                           138\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nModel Test Baseline Model:\n\n  Test statistic                                71.556\n  Degrees of freedom                                 7\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.000\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)               -890.353\n  Loglikelihood unrestricted model (H1)       -890.353\n                                                      \n  Akaike (AIC)                                1798.705\n  Bayesian (BIC)                              1825.050\n  Sample-size adjusted Bayesian (SABIC)       1796.577\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: RMSEA &lt;= 0.050                       NA\n  P-value H_0: RMSEA &gt;= 0.080                       NA\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.000\n\nParameter Estimates:\n\n  Standard errors                            Bootstrap\n  Number of requested bootstrap draws              100\n  Number of successful bootstrap draws             100\n\nRegressions:\n                   Estimate    Std.Err  z-value  P(&gt;|z|)   Std.lv  \n  TRIM_Ben ~                                                       \n    DTST       (c)      0.203    0.147    1.378    0.168      0.203\n    CS_TOT     (b)      0.289    0.049    5.929    0.000      0.289\n    RHSTOT     (d)      0.122    0.125    0.976    0.329      0.122\n    DTST:RHSTO (e)     -0.002    0.003   -0.625    0.532     -0.002\n  CS_TOT ~                                                         \n    DTST       (a)     -0.517    0.324   -1.597    0.110     -0.517\n    RHSTOT     (f)     -0.355    0.262   -1.353    0.176     -0.355\n    DTST:RHSTO (g)      0.014    0.007    2.162    0.031      0.014\n  Std.all\n         \n    0.367\n    0.433\n    0.219\n   -0.254\n         \n   -0.621\n   -0.424\n    1.232\n\nVariances:\n                   Estimate    Std.Err  z-value  P(&gt;|z|)   Std.lv  \n   .TRIM_Ben           21.601    3.336    6.476    0.000     21.601\n   .CS_TOT             63.762    8.415    7.577    0.000     63.762\n  Std.all\n    0.675\n    0.883\n\nR-Square:\n                   Estimate  \n    TRIM_Ben            0.325\n    CS_TOT              0.117\n\nDefined Parameters:\n                   Estimate    Std.Err  z-value  P(&gt;|z|)   Std.lv  \n    indirect           -0.149    0.104   -1.435    0.151     -0.149\n    direct              0.203    0.148    1.371    0.170      0.203\n    total               0.054    0.142    0.380    0.704      0.054\n  Std.all\n   -0.269\n    0.367\n    0.097\n\n\n\n\n23.13.1 Interpretazione dell’Analisi di Mediazione Moderata\nL’output del modello mostra un’analisi di mediazione moderata, dove la variabile RHSTOT (percezione dell’umiltà del partner) modera la relazione tra DTST (fiducia di coppia) e TRIM_Ben (perdono) tramite CS_TOT (compassione verso il partner).\nDall’indice di mediazione moderata, possiamo verificare se RHSTOT influenza l’effetto indiretto di DTST su TRIM_Ben attraverso CS_TOT. L’intervallo di confidenza per l’effetto moderato dell’umiltà del partner sull’effetto indiretto della fiducia di coppia va da 0.0001 a 0.0077, il che indica che la percezione dell’umiltà modera effettivamente l’effetto indiretto.\n\n\n23.13.2 Effetto Diretto e Indiretto\n\nEffetto diretto (c path): Il coefficiente per l’effetto diretto di DTST su TRIM_Ben è pari a 0.203, con un intervallo di confidenza che non include valori sufficientemente vicini allo zero (z = 1.378, p = 0.168), suggerendo che l’effetto diretto non è robusto. Tuttavia, il coefficiente indica una possibile relazione positiva tra la fiducia di coppia e il perdono.\nEffetto indiretto (ab path): L’effetto indiretto di DTST su TRIM_Ben, mediato da CS_TOT, è pari a -0.149, con un intervallo di confidenza che include valori vicini allo zero (z = -1.435, p = 0.151). Questo indica che l’effetto indiretto non è rilevante in questo modello.\nInterazione moderata (g path): La percezione dell’umiltà del partner modera l’effetto di DTST su CS_TOT, con un coefficiente di interazione pari a 0.014 (z = 2.162, p = 0.031). Questo suggerisce che la fiducia di coppia influenza la compassione in maniera diversa a seconda del livello di umiltà percepita del partner.\n\n\n\n23.13.3 Effetto Totale\nL’effetto totale di DTST su TRIM_Ben (includendo l’effetto diretto e indiretto) è di 0.054 (z = 0.380, p = 0.704), il che suggerisce che la fiducia di coppia ha un’influenza complessiva molto ridotta sul perdono, in parte dovuta alla mancanza di un effetto indiretto rilevante.\n\n\n23.13.4 Varianza Spiegata (R²)\n\nLa fiducia di coppia, la compassione, e l’umiltà percepita spiegano il 32.5% della varianza in TRIM_Ben (perdono).\nLe stesse variabili spiegano il 11.7% della varianza in CS_TOT (compassione).\n\n\n\n23.13.5 Conclusioni\nIn sintesi:\n\nLa fiducia di coppia (DTST) ha un’influenza diretta positiva, anche se debole, sul perdono (TRIM_Ben), ma l’effetto non è forte né chiaramente rilevante.\nL’effetto indiretto della fiducia di coppia sul perdono, mediato dalla compassione (CS_TOT), non è particolarmente rilevante in questo modello.\nL’umiltà percepita del partner (RHSTOT) modera significativamente la relazione tra fiducia di coppia e compassione, suggerendo che l’effetto della fiducia sul perdono può dipendere dal livello di umiltà percepita nel partner.\n\nNel complesso, questo modello di mediazione moderata mostra che l’umiltà del partner influenza la relazione tra fiducia e compassione, ma non ci sono prove chiare di un effetto indiretto significativo di DTST su TRIM_Ben attraverso CS_TOT.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Riparazione affettiva post-infedeltà</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/02_clement_2022.html#considerazioni-conclusive",
    "href": "chapters/path_analysis/02_clement_2022.html#considerazioni-conclusive",
    "title": "23  Riparazione affettiva post-infedeltà",
    "section": "23.14 Considerazioni conclusive",
    "text": "23.14 Considerazioni conclusive\nDalle tre analisi condotte emerge un quadro complesso della relazione tra fiducia di coppia (DTST), compassione verso il partner che ha ferito (CS_TOT) e perdono (TRIM_Ben). I risultati indicano che la fiducia di coppia ha un’influenza diretta e positiva sul perdono, anche se in alcuni modelli questo effetto non risulta sufficientemente robusto. Tuttavia, la fiducia di coppia sembra agire in modo più consistente attraverso il suo effetto mediato dalla compassione verso il partner.\nUn aspetto chiave emerso dalle analisi è il ruolo della percezione dell’umiltà del partner (RHSTOT). Questa variabile non solo modera l’effetto della fiducia di coppia sulla compassione, ma contribuisce anche a modellare l’effetto complessivo della fiducia sul perdono. L’interazione tra fiducia di coppia e umiltà percepita evidenzia che, in contesti in cui il partner è percepito come umile, l’effetto della fiducia sulla compassione aumenta, il che a sua volta rafforza la propensione al perdono.\nIn sintesi, i risultati suggeriscono che: 1. La fiducia di coppia è un fattore rilevante per il perdono, ma la sua influenza è amplificata quando la compassione verso il partner funge da mediatore. 2. La percezione dell’umiltà del partner gioca un ruolo cruciale nel rafforzare il legame tra fiducia e compassione, sottolineando l’importanza del contesto relazionale. 3. Il perdono non è semplicemente una funzione della fiducia, ma è anche profondamente influenzato dalla capacità di sviluppare compassione verso il partner, soprattutto quando quest’ultimo è visto come umile e disposto a riconoscere il proprio errore.\nQuesti risultati offrono una visione sfumata della dinamica tra fiducia, compassione e perdono, suggerendo che interventi volti a promuovere il perdono in coppia dovrebbero considerare non solo il livello di fiducia tra i partner, ma anche la percezione di umiltà e la capacità di sviluppare compassione reciproca.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Riparazione affettiva post-infedeltà</span>"
    ]
  },
  {
    "objectID": "chapters/path_analysis/02_clement_2022.html#session-info",
    "href": "chapters/path_analysis/02_clement_2022.html#session-info",
    "title": "23  Riparazione affettiva post-infedeltà",
    "section": "23.15 Session Info",
    "text": "23.15 Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] sjPlot_2.8.16      interactions_1.2.0 lmtest_0.9-40     \n [4] zoo_1.8-12         car_3.1-3          carData_3.0-5     \n [7] performance_0.12.3 mediation_4.5.0    sandwich_3.1-1    \n[10] mvtnorm_1.3-1      Matrix_1.7-0       haven_2.5.4       \n[13] lavaanPlot_0.8.1   lavaanExtra_0.2.1  nortest_1.0-4     \n[16] MASS_7.3-61        ggokabeito_0.1.0   viridis_0.6.5     \n[19] viridisLite_0.4.2  ggpubr_0.6.0       ggExtra_0.10.1    \n[22] gridExtra_2.3      patchwork_1.3.0    bayesplot_1.11.1  \n[25] semTools_0.5-6     semPlot_1.1.6      lavaan_0.6-19     \n[28] psych_2.4.6.26     scales_1.3.0       markdown_1.13     \n[31] knitr_1.48         lubridate_1.9.3    forcats_1.0.0     \n[34] stringr_1.5.1      dplyr_1.1.4        purrr_1.0.2       \n[37] readr_2.1.5        tidyr_1.3.1        tibble_3.2.1      \n[40] ggplot2_3.5.1      tidyverse_2.0.0    here_1.0.1        \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.1       later_1.3.2         pbdZMQ_0.3-13      \n  [4] datawizard_0.13.0   XML_3.99-0.17       rpart_4.1.23       \n  [7] lifecycle_1.0.4     rstatix_0.7.2       rprojroot_2.0.4    \n [10] globals_0.16.3      lattice_0.22-6      insight_0.20.5     \n [13] rockchalk_1.8.157   backports_1.5.0     magrittr_2.0.3     \n [16] openxlsx_4.2.7.1    Hmisc_5.1-3         rmarkdown_2.28     \n [19] httpuv_1.6.15       qgraph_1.9.8        zip_2.3.1          \n [22] pbapply_1.7-2       minqa_1.2.8         RColorBrewer_1.1-3 \n [25] multcomp_1.4-26     abind_1.4-8         quadprog_1.5-8     \n [28] nnet_7.3-19         TH.data_1.1-2       jtools_2.3.0       \n [31] listenv_0.9.1       arm_1.14-4          parallelly_1.38.0  \n [34] codetools_0.2-20    tidyselect_1.2.1    ggeffects_1.7.1    \n [37] farver_2.1.2        lme4_1.1-35.5       broom.mixed_0.2.9.5\n [40] stats4_4.4.1        base64enc_0.1-3     jsonlite_1.8.9     \n [43] Formula_1.2-5       survival_3.7-0      emmeans_1.10.4     \n [46] tools_4.4.1         Rcpp_1.0.13         glue_1.8.0         \n [49] mnormt_2.1.1        xfun_0.48           IRdisplay_1.1      \n [52] withr_3.0.1         fastmap_1.2.0       boot_1.3-31        \n [55] fansi_1.0.6         digest_0.6.37       mi_1.1             \n [58] timechange_0.3.0    R6_2.5.1            mime_0.12          \n [61] estimability_1.5.1  colorspace_2.1-1    Cairo_1.6-2        \n [64] lpSolve_5.6.21      gtools_3.9.5        jpeg_0.1-10        \n [67] DiagrammeR_1.0.11   utf8_1.2.4          generics_0.1.3     \n [70] data.table_1.16.0   corpcor_1.6.10      htmlwidgets_1.6.4  \n [73] pkgconfig_2.0.3     sem_3.1-16          gtable_0.3.5       \n [76] furrr_0.3.1         htmltools_0.5.8.1   png_0.1-8          \n [79] rstudioapi_0.16.0   tzdb_0.4.0          reshape2_1.4.4     \n [82] uuid_1.2-1          coda_0.19-4.1       visNetwork_2.1.2   \n [85] checkmate_2.3.2     nlme_3.1-166        nloptr_2.1.1       \n [88] repr_1.1.7          sjlabelled_1.2.0    parallel_4.4.1     \n [91] miniUI_0.1.1.1      foreign_0.8-87      pillar_1.9.0       \n [94] grid_4.4.1          vctrs_0.6.5         promises_1.3.0     \n [97] OpenMx_2.21.12      xtable_1.8-4        cluster_2.1.6      \n[100] htmlTable_2.4.3     evaluate_1.0.0      pbivnorm_0.6.0     \n[103] cli_3.6.3           kutils_1.73         compiler_4.4.1     \n[106] rlang_1.1.4         crayon_1.5.3        ggsignif_0.6.4     \n[109] fdrtool_1.2.18      sjmisc_2.8.10       plyr_1.8.9         \n[112] pander_0.6.5        stringi_1.8.4       munsell_0.5.1      \n[115] lisrelToR_0.3       pacman_0.5.1        sjstats_0.19.0     \n[118] IRkernel_1.3.2      hms_1.1.3           glasso_1.11        \n[121] future_1.34.0       shiny_1.9.1         igraph_2.0.3       \n[124] broom_1.0.7         RcppParallel_5.1.9 \n\n\n\n\n\n\nClement, L. M., & Bradley-Garcia, M. (2022). A step-by-step tutorial for performing a moderated mediation analysis using PROCESS. The Quantitative Methods for Psychology, 18(3), 258–271.\n\n\nHargrave, T. D., & Hammer, M. Y. (2016). Restoration of Relationships After Affairs. In Techniques for the Couple Therapist (pp. 190–193). Routledge.",
    "crumbs": [
      "Analisi dei percorsi",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Riparazione affettiva post-infedeltà</span>"
    ]
  },
  {
    "objectID": "chapters/fa/01_intro_fa.html",
    "href": "chapters/fa/01_intro_fa.html",
    "title": "24  Introduzione all’analisi fattoriale",
    "section": "",
    "text": "24.1 Analisi Fattoriale Esplorativa (EFA)\nL’EFA viene utilizzata quando il ricercatore non ha ipotesi a priori su come un gruppo di variabili si strutturi. Il suo scopo è identificare empiricamente il modello che meglio si adatta ai dati, bilanciando precisione e semplicità. Questa tecnica esplora la struttura sottostante ai dati, permettendo di individuare fattori latenti che spiegano la varianza comune tra le variabili osservate. È particolarmente utile nei primi stadi di sviluppo di test psicometrici, quando si desidera identificare le dimensioni latenti sottostanti a un nuovo insieme di item. Tuttavia, la scelta di parametri e metodi di estrazione influisce pesantemente sul risultato finale.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Introduzione all'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/01_intro_fa.html#analisi-fattoriale-confermativa-cfa",
    "href": "chapters/fa/01_intro_fa.html#analisi-fattoriale-confermativa-cfa",
    "title": "24  Introduzione all’analisi fattoriale",
    "section": "24.2 Analisi Fattoriale Confermativa (CFA)",
    "text": "24.2 Analisi Fattoriale Confermativa (CFA)\nL’CFA è un approccio utilizzato quando il ricercatore ha un modello teorico ben definito e desidera valutare quanto questo modello ipotizzato si adatti ai dati osservati. La CFA consente di confrontare modelli teorici alternativi e valutare quale meglio spiega i dati, tenendo conto di vari fattori come i carichi fattoriali, gli errori e le covarianze. In psicometria, viene comunemente utilizzata per verificare la validità strutturale di un test o questionario, valutando se i dati empirici supportano il modello teorico ipotizzato.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Introduzione all'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/01_intro_fa.html#struttura-e-componenti-dellanalisi-fattoriale",
    "href": "chapters/fa/01_intro_fa.html#struttura-e-componenti-dellanalisi-fattoriale",
    "title": "24  Introduzione all’analisi fattoriale",
    "section": "24.3 Struttura e Componenti dell’Analisi Fattoriale",
    "text": "24.3 Struttura e Componenti dell’Analisi Fattoriale\nIndipendentemente dal tipo di analisi, l’analisi fattoriale si basa sulla distinzione tra variabili osservate (o manifest) e variabili latenti (o fattori). Le variabili latenti rappresentano costrutti teorici non direttamente osservabili, mentre le variabili osservate sono i punteggi effettivi ottenuti da misure dirette. Un modello fattoriale può includere carichi fattoriali, errori, covarianze e percorsi di regressione.\nUn carico fattoriale rappresenta la forza della relazione tra una variabile osservata e il fattore latente, mentre il residuo o errore rappresenta la varianza non spiegata dal fattore latente. Le covarianze esprimono le relazioni tra le variabili o tra i fattori latenti. L’equazione generale di un indicatore osservato \\(X\\) in relazione a un fattore latente \\(F\\) può essere espressa come:\n\\[\nX = \\lambda \\cdot F + \\text{Intercept} + \\text{Errore}\n\\]\ndove:\n\n\\(X\\) è il valore osservato dell’indicatore;\n\\(\\lambda\\) è il carico fattoriale;\n\\(F\\) è il valore del fattore latente;\nIntercept è il valore atteso dell’indicatore quando il fattore latente è zero;\nErrore è la parte di varianza non spiegata dal fattore latente.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Introduzione all'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/01_intro_fa.html#modelli-fattoriali-gerarchici-e-bifattoriali",
    "href": "chapters/fa/01_intro_fa.html#modelli-fattoriali-gerarchici-e-bifattoriali",
    "title": "24  Introduzione all’analisi fattoriale",
    "section": "24.4 Modelli Fattoriali Gerarchici e Bifattoriali",
    "text": "24.4 Modelli Fattoriali Gerarchici e Bifattoriali\nEsistono varianti più avanzate dell’analisi fattoriale, come i modelli gerarchici e i modelli bifattoriali, che permettono di rappresentare strutture latenti più complesse. In particolare, i modelli bifattoriali sono utili quando si ritiene che un insieme di variabili possa essere spiegato sia da un fattore generale che da fattori specifici. Ad esempio, nel contesto della misurazione dell’intelligenza, un modello bifattoriale potrebbe includere un fattore generale (g) che spiega la varianza comune tra tutte le variabili, e fattori specifici che spiegano varianze più circoscritte a singoli domini cognitivi.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Introduzione all'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/01_intro_fa.html#sviluppo-storico-dellanalisi-fattoriale",
    "href": "chapters/fa/01_intro_fa.html#sviluppo-storico-dellanalisi-fattoriale",
    "title": "24  Introduzione all’analisi fattoriale",
    "section": "24.5 Sviluppo Storico dell’Analisi Fattoriale",
    "text": "24.5 Sviluppo Storico dell’Analisi Fattoriale\nL’analisi fattoriale è stata sviluppata all’inizio del XX secolo da Charles Spearman per studiare la struttura dell’intelligenza. Spearman introdusse il concetto di fattore generale (g), che rappresentava la dimensione comune che spiegava la covarianza tra diverse abilità cognitive. Successivamente, psicologi come Thurstone criticarono il modello unifattoriale di Spearman e proposero un modello multifattoriale, che permetteva di individuare più fattori specifici, ciascuno dei quali spiegava una dimensione distinta dell’intelligenza.\nNegli anni ’60 e ’70, l’analisi fattoriale subì una trasformazione con lo sviluppo dei modelli di equazioni strutturali (SEM), che combinavano l’analisi fattoriale con la path analysis per rappresentare relazioni più complesse tra variabili osservate e latenti. Questo sviluppo permise ai ricercatori di verificare ipotesi teoriche più articolate riguardanti la struttura di costrutti psicologici complessi.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Introduzione all'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/01_intro_fa.html#applicazioni-dellanalisi-fattoriale",
    "href": "chapters/fa/01_intro_fa.html#applicazioni-dellanalisi-fattoriale",
    "title": "24  Introduzione all’analisi fattoriale",
    "section": "24.6 Applicazioni dell’Analisi Fattoriale",
    "text": "24.6 Applicazioni dell’Analisi Fattoriale\nL’analisi fattoriale è ampiamente utilizzata nello sviluppo di strumenti psicometrici, come i test di intelligenza, le scale di personalità e i questionari di auto-valutazione. Viene utilizzata per valutare la validità di costrutto, ovvero la capacità di uno strumento di misurare effettivamente il costrutto teorico che si propone di valutare. Inoltre, l’analisi fattoriale può essere impiegata per esaminare la validità discriminante, ovvero la capacità di uno strumento di distinguere tra costrutti correlati ma distinti.\nInfine, l’analisi fattoriale è uno strumento fondamentale per individuare variabili latenti sottostanti e semplificare i dati complessi, consentendo ai ricercatori di ridurre grandi set di variabili osservate a un insieme più ristretto di fattori interpretabili. La sua applicazione, tuttavia, richiede attenzione nella scelta dei parametri e delle assunzioni, poiché le decisioni prese nel processo di analisi possono influenzare significativamente i risultati finali.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Introduzione all'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html",
    "href": "chapters/fa/02_analisi_fattoriale_1.html",
    "title": "25  Il modello unifattoriale",
    "section": "",
    "text": "25.1 Introduzione\nNel contesto dell’analisi fattoriale, i costrutti teorici di interesse vengono rappresentati da variabili latenti, che riflettono le comunanze sottostanti tra un insieme di variabili manifeste (ovvero misurabili). Le variabili manifeste, come gli item di un questionario o gli indici derivati da un compito comportamentale, sono direttamente osservabili e vengono solitamente rappresentate graficamente come quadrati. Al contrario, le variabili latenti descrivono costrutti ipotetici non direttamente osservabili, come un fattore latente di intelligenza o memoria, e sono rappresentate come cerchi nei diagrammi.\nIl legame tra variabili latenti e manifeste si esprime tramite i carichi fattoriali, cioè percorsi che collegano una variabile latente a una variabile osservabile. L’intensità di un carico fattoriale indica la quota di varianza osservata spiegata dal fattore latente. In altre parole, il carico fattoriale riflette la capacità di una data variabile manifesta di rappresentare il costrutto latente.\nDal punto di vista matematico, l’analisi fattoriale descrive ogni misura osservabile \\(y\\) come la combinazione lineare del punteggio latente \\(\\xi\\) relativo al costrutto di interesse e di un elemento di errore non osservato \\(\\delta\\). In questo modello, il valore di \\(y\\) è interpretato come il prodotto del punteggio latente, ponderato da un coefficiente di carico \\(\\lambda\\), a cui si somma un termine di errore specifico \\(\\delta_y\\):\n\\[\ny = \\lambda \\xi + \\delta_y.\n\\]\nUn esempio concreto può aiutare a chiarire questo concetto: supponiamo di usare una bilancia non perfettamente affidabile per misurare il peso corporeo. Ogni lettura non rifletterà solo il peso reale della persona, ma includerà anche una componente di errore dovuta alla bilancia stessa, manifestando variazioni casuali tra una misurazione e l’altra.\nQuando si dispone di più misure osservabili \\(y\\) che rappresentano il medesimo costrutto latente \\(\\xi\\), diventa possibile stimare con maggiore accuratezza sia il punteggio reale latente \\(\\xi\\) sia la componente di errore di misura \\(\\delta\\). Questo permette di migliorare la precisione interpretativa dei dati e di ottenere una rappresentazione più affidabile del costrutto teorico di interesse.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#modello-monofattoriale",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#modello-monofattoriale",
    "title": "25  Il modello unifattoriale",
    "section": "25.2 Modello monofattoriale",
    "text": "25.2 Modello monofattoriale\nCon \\(p\\) variabili manifeste \\(y_i\\), il caso più semplice è quello di un solo fattore comune:\n\\[\n\\begin{equation}\ny_i = \\mu_i + \\lambda_{i} \\xi +  1 \\cdot \\delta_i \\qquad i=1, \\dots, p,\n\\end{equation}\n\\tag{25.1}\\]\ndove \\(\\xi\\) rappresenta il fattore comune a tutte le \\(y_i\\), \\(\\delta_i\\) sono i fattori specifici o unici di ogni variabile osservata e \\(\\lambda_i\\) sono le saturazioni (o pesi) fattoriali le quali stabiliscono il peso del fattore latente su ciascuna variabile osservata.\nIl modello di analisi fattoriale e il modello di regressione possono sembrare simili, ma presentano alcune differenze importanti. In primo luogo, sia il fattore comune \\(\\xi\\) sia i fattori specifici \\(\\delta_i\\) sono inosservabili, il che rende tutto ciò che si trova a destra dell’uguaglianza incognito. In secondo luogo, l’analisi di regressione e l’analisi fattoriale hanno obiettivi diversi. L’analisi di regressione mira a individuare le variabili esplicative, osservabili direttamente, che sono in grado di spiegare la maggior parte della varianza della variabile dipendente. Al contrario, il problema dell’analisi unifattoriale consiste nell’identificare la variabile esplicativa inosservabile che è in grado di spiegare la maggior parte della covarianza tra le variabili osservate.\nSolitamente, per comodità, si assume che la media delle variabili osservate \\(y_i\\) sia zero, ovvero \\(\\mu_i=0\\). Ciò equivale a considerare gli scarti delle variabili rispetto alle rispettive medie. Il modello unifattoriale assume che le variabili osservate siano il risultato della combinazione lineare di un fattore comune \\(\\xi\\) e dei fattori specifici \\(\\delta_i\\), ovvero:\n\\[\n\\begin{equation}\ny_i -\\mu_i = \\lambda_i \\xi + 1 \\cdot \\delta_i,\n\\end{equation}\n\\tag{25.2}\\]\ndove \\(\\lambda_i\\) è la saturazione o il peso della variabile \\(i\\)-esima sul fattore comune e \\(\\delta_i\\) rappresenta il fattore specifico della variabile \\(i\\)-esima. Si assume che il fattore comune abbia media zero e varianza unitaria, mentre i fattori specifici abbiano media zero, varianza \\(\\psi_{i}\\) e siano incorrelati tra loro e con il fattore comune. Nel modello unifattoriale, l’interdipendenza tra le variabili è completamente spiegata dal fattore comune.\nLe ipotesi precedenti consentono di ricavare la covarianza tra la variabile osservata \\(y_i\\) e il fattore comune, la varianza della variabile osservata \\(y_i\\) e la covarianza tra due variabili osservate \\(y_i\\) e \\(y_k\\). L’obiettivo della discussione in questo capitolo è appunto quello di analizzare tali grandezze statistiche.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#correlazione-parziale",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#correlazione-parziale",
    "title": "25  Il modello unifattoriale",
    "section": "25.3 Correlazione parziale",
    "text": "25.3 Correlazione parziale\nPrima di entrare nel dettaglio del modello statistico dell’analisi fattoriale, è importante chiarire il concetto di correlazione parziale. Si attribuisce spesso a Charles Spearman la nascita dell’analisi fattoriale. Nel 1904, Spearman pubblicò un articolo intitolato “General Intelligence, Objectively Determined and Measured” in cui propose la Teoria dei Due Fattori. In questo articolo, dimostrò come fosse possibile identificare un fattore inosservabile a partire da una matrice di correlazioni, utilizzando il metodo dell’annullamento della tetrade (tetrad differences). L’annullamento della tetrade è un’applicazione della teoria della correlazione parziale che mira a stabilire se, controllando un insieme di variabili inosservabili chiamate fattori \\(\\xi_j\\), le correlazioni tra le variabili osservabili \\(Y_i\\), al netto degli effetti lineari delle \\(\\xi_j\\), diventino statisticamente nulle.\nPossiamo considerare un esempio con tre variabili: \\(Y_1\\), \\(Y_2\\) e \\(F\\). La correlazione tra \\(Y_1\\) e \\(Y_2\\), \\(r_{1,2}\\), può essere influenzata dalla presenza di \\(F\\). Per calcolare la correlazione parziale tra \\(Y_1\\) e \\(Y_2\\) al netto dell’effetto lineare di \\(F\\), dobbiamo trovare le componenti di \\(Y_1\\) e \\(Y_2\\) che sono linearmente indipendenti da \\(F\\).\nPer fare ciò, dobbiamo trovare la componente di \\(Y_1\\) che è ortogonale a \\(F\\). Possiamo calcolare i residui \\(E_1\\) del modello:\n\\[\nY_1 = b_{01} + b_{11}F + E_1.\n\\tag{25.3}\\]\nLa componente di \\(Y_1\\) linearmente indipendente da \\(F\\) è quindi data dai residui \\(E_1\\). Possiamo eseguire un’operazione analoga per \\(Y_2\\) per trovare la sua componente ortogonale a \\(F\\). Calcolando la correlazione tra le due componenti così ottenute si ottiene la correlazione parziale tra \\(Y_1\\) e \\(Y_2\\) al netto dell’effetto lineare di \\(F\\).\nL’Equazione 25.4 consente di calcolare la correlazione parziale tra \\(Y_1\\) e \\(Y_2\\) al netto dell’effetto di \\(F\\) a partire dalle correlazioni semplici tra le tre variabili \\(Y_1\\), \\(Y_2\\) e \\(F\\).\n\\[\n\\begin{equation}\nr_{1,2 \\mid F} = \\frac{r_{12} - r_{1F}r_{2F}}{\\sqrt{(1-r_{1F}^2)(1-r_{2F}^2)}}.\n\\end{equation}\n\\tag{25.4}\\]\nIn particolare, la correlazione parziale \\(r_{1,2 \\mid F}\\) è data dalla differenza tra la correlazione \\(r_{12}\\) tra \\(Y_1\\) e \\(Y_2\\) e il prodotto tra le correlazioni \\(r_{1F}\\) e \\(r_{2F}\\) tra ciascuna delle due variabili e \\(F\\), il tutto diviso per la radice quadrata del prodotto delle differenze tra 1 e i quadrati delle correlazioni tra \\(Y_1\\) e \\(F\\) e tra \\(Y_2\\) e \\(F\\). In altre parole, la formula tiene conto dell’effetto di \\(F\\) sulle correlazioni tra \\(Y_1\\) e \\(Y_2\\) per ottenere una stima della relazione diretta tra le due variabili, eliminando l’effetto del fattore comune.\nConsideriamo un esempio numerico. Sia \\(f\\) una variabile su cui misuriamo \\(n\\) valori\n\nset.seed(123)\nn &lt;- 1000\nf &lt;- rnorm(n, 24, 12)\n\nSiano \\(y_1\\) e \\(y_2\\) funzioni lineari di \\(f\\), a cui viene aggiunta una componente d’errore gaussiano:\n\ny1 &lt;- 10 + 7 * f + rnorm(n, 0, 50)\ny2 &lt;- 3  + 2 * f + rnorm(n, 0, 50)\n\nLa correlazione tra \\(y_1\\) e \\(y_2\\) (\\(r_{12}= 0.355\\)) deriva dal fatto che \\(\\hat{y}_1\\) e \\(\\hat{y}_2\\) sono entrambe funzioni lineari di \\(f\\):\n\nY &lt;- cbind(y1, y2, f)\ncor(Y) |&gt;\n    round(3)\n\n\nA matrix: 3 x 3 of type dbl\n\n\n\ny1\ny2\nf\n\n\n\n\ny1\n1.000\n0.380\n0.867\n\n\ny2\n0.380\n1.000\n0.423\n\n\nf\n0.867\n0.423\n1.000\n\n\n\n\n\nEseguiamo le regressioni di \\(y_1\\) su \\(f\\) e di \\(y_2\\) su \\(F\\):\n\nfm1 &lt;- lm(y1 ~ f)\nfm2 &lt;- lm(y2 ~ f)\n\nNella regressione, ciascuna osservazione \\(y_{i1}\\) viene scomposta in due componenti linearmente indipendenti, i valori adattati \\(\\hat{y}_{i}\\) e i residui, \\(e_{i}\\): \\(y_i = \\hat{y}_i + e_1\\). Nel caso di \\(y_1\\) abbiamo\n\ncbind(y1, y1.hat=fm1$fit, e=fm1$res, fm1$fit+fm1$res) |&gt;\n    head() |&gt;\n    round(3)\n\n\nA matrix: 6 x 4 of type dbl\n\n\n\ny1\ny1.hat\ne\n\n\n\n\n\n1\n81.130\n130.505\n-49.375\n81.130\n\n\n2\n106.667\n159.704\n-53.037\n106.667\n\n\n3\n308.032\n317.846\n-9.813\n308.032\n\n\n4\n177.314\n186.285\n-8.971\n177.314\n\n\n5\n61.393\n191.482\n-130.089\n61.393\n\n\n6\n374.094\n331.668\n42.426\n374.094\n\n\n\n\n\nLo stesso può dirsi di \\(y_2\\). La correlazione parziale \\(r_{12 \\mid f}\\) tra \\(y_1\\) e \\(y_2\\) dato \\(f\\) è uguale alla correlazione di Pearson tra i residui \\(e_1\\) e \\(e_2\\) calcolati mediante i due modelli di regressione descritti sopra:\n\ncor(fm1$res, fm2$res)\n\n0.0282861771586006\n\n\n\nLa correlazione parziale tra \\(y_1\\) e \\(y_2\\) al netto di \\(f\\) è .02829. Per i dati esaminati sopra, dunque, la correlazione parziale tra le variabili \\(y_1\\) e \\(y_2\\) diventa uguale a zero se la variabile \\(f\\) viene controllata (ovvero, se escludiamo da \\(y_1\\) e da \\(y_2\\) l’effetto lineare di \\(f\\)).\nIl fatto che la correlazione parziale sia zero significa che la correlazione che abbiamo osservato tra \\(y_1\\) e \\(y_2\\) (\\(r = 0.355\\)) non dipendeva dall’effetto che una variabile \\(y\\) esercitava sull’altra, ma bensì dal fatto che c’era una terza variabile, \\(f\\), che influenzava sia \\(y_1\\) sia \\(y_2\\). In altre parole, le variabili \\(y_1\\) e \\(y_2\\) sono condizionalmente indipendenti dato \\(f\\). Ciò significa, come abbiamo visto sopra, che la componente di \\(y_1\\) linearmente indipendente da \\(f\\) è incorrelata con la componente di \\(y_2\\) linearmente indipendente da \\(f\\).\nLa correlazione che abbiamo calcolato tra i residui di due modelli di regressione è identica alla correlazione che viene calcolata applicando l’Equazione 25.4:\n\n(R[1, 2] - R[1, 3] * R[2, 3]) / \n  sqrt((1 - R[1, 3]^2) * (1- R[2, 3]^2)) |&gt;\n  round(3)\n\n0.0282751285315664",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#principio-base-dellanalisi-fattoriale",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#principio-base-dellanalisi-fattoriale",
    "title": "25  Il modello unifattoriale",
    "section": "25.4 Principio base dell’analisi fattoriale",
    "text": "25.4 Principio base dell’analisi fattoriale\nAttualmente, l’inferenza statistica nell’analisi fattoriale spesso si svolge mediante il calcolo di stime della massima verosimiglianza ottenute mediante procedure iterative. All’inizio dell’analisi fattoriale, tuttavia, la procedura di estrazione dei fattori faceva leva sulle relazioni invarianti che il modello fattoriale impone agli elementi della matrice di covarianza delle variabili osservate. Il più conosciuto tra tali invarianti è la tetrade che si presenta nei modelli ad un fattore.\nLa tetrade è una combinazione di quattro correlazioni. Se l’associazione osservata tra le variabili dipende effettivamente dal fatto che le variabili in questione sono state causalmente generate da un fattore comune inosservabile, allora è possibile generare una combinazione delle correlazioni tra le variabili che porta all’annullamento della tetrade. In altre parole, l’analisi fattoriale si chiede se esiste un insieme esiguo di \\(m&lt;p\\) variabili inosservabili che rendono significativamente nulle tutte le correlazioni parziali tra le \\(p\\) variabili osservate al netto dei fattori comuni. Se il metodo della correlazione parziale consente di identificare \\(m\\) variabili latenti, allora lo psicologo conclude che tali fattori corrispondono agli \\(m\\) costrutti che intende misurare.\nPer chiarire il metodo dell’annullamento della tetrade consideriamo la matrice di correlazioni riportata nella Tabella successiva. Nella tabella, la correlazione parziale tra ciascuna coppia di variabili \\(y_i\\), \\(y_j\\) (con \\(i \\neq j\\)) dato \\(\\xi\\) è sempre uguale a zero. Ad esempio, la correlazione parziale tra \\(y_3\\) e \\(y_5\\) dato \\(\\xi\\) è:\n\\[\n\\begin{align}\n  r_{35 \\mid \\xi} &= \\frac{r_{35} - r_{3\\xi}r_{5\\xi}}\n  {\\sqrt{(1-r_{3\\xi}^2)(1-r_{5\\xi}^2)}} \\notag \\\\[12pt]\n  &= \\frac{0.35 - 0.7 \\times 0.5}\n  {\\sqrt{(1-0.7^2)(1-0.5^2)}} = 0. \\notag\n\\end{align}\n\\]\nLo stesso risultato si trova per qualunque altra coppia di variabili \\(y_i\\) e \\(y_j\\), ovvero \\(r_{ij \\mid \\xi} = 0\\).\n\n\n\n\n\\(\\xi\\)\n\\(y_1\\)\n\\(y_2\\)\n\\(y_3\\)\n\\(y_4\\)\n\\(y_5\\)\n\n\n\n\n\\(\\xi\\)\n1.00\n\n\n\n\n\n\n\n\\(y_1\\)\n0.90\n1.00\n\n\n\n\n\n\n\\(y_2\\)\n0.80\n0.72\n1.00\n\n\n\n\n\n\\(y_3\\)\n0.70\n0.63\n0.56\n1.00\n\n\n\n\n\\(y_4\\)\n0.60\n0.54\n0.48\n0.42\n1.00\n\n\n\n\\(y_5\\)\n0.50\n0.45\n0.40\n0.35\n0.30\n1.00\n\n\n\nPossiamo dunque dire che, per la matrice di correlazioni della Tabella, esiste un’unica variabile \\(\\xi\\) la quale, quando viene controllata, spiega tutte le\n\\[p(p-1)/2 = 5(5-1)/2=10\\]\ncorrelazioni tra le variabili \\(y\\). Questo risultato non è sorprendente, in quanto la matrice di correlazioni della Tabella è stata costruita in modo tale da possedere tale proprietà.\nMa supponiamo di essere in una situazione diversa, ovvero di avere osservato soltanto le variabili \\(y_i\\) e di non conoscere \\(\\xi\\). In tali circostanze ci possiamo porre la seguente domanda: Esiste una variabile inosservabile \\(\\xi\\) la quale, se venisse controllata, renderebbe uguali a zero tutte le correlazioni parziali tra le variabili \\(y\\)? Se una tale variabile inosservabile esiste, ed è in grado di spiegare tutte le correlazioni tra le variabili osservate \\(y\\), allora essa viene chiamata fattore. Arriviamo dunque alla seguente definizione:\nUn fattore è una variabile inosservabile in grado di rendere significativamente nulle tutte le correlazioni parziali tra le variabili manifeste.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#vincoli-sulle-correlazioni",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#vincoli-sulle-correlazioni",
    "title": "25  Il modello unifattoriale",
    "section": "25.5 Vincoli sulle correlazioni",
    "text": "25.5 Vincoli sulle correlazioni\nCome si può stabilire se esiste una variabile inosservabile in grado di rendere nulle tutte le correlazioni parziali tra le variabili osservate? Riscriviamo l’Equazione 25.4 per specificare la correlazione parziale tra le variabili \\(y_i\\) e \\(y_j\\) dato \\(\\xi\\):\n\\[\n\\begin{align}\n  r_{ij \\mid \\xi} &= \\frac{r_{ij} - r_{i\\xi}r_{j\\xi}}\n  {\\sqrt{(1-r_{i\\xi}^2)(1-r_{j\\xi}^2)}}\n\\end{align}\n\\]\nAffinché \\(r_{ij \\mid \\xi}\\) sia uguale a zero è necessario che\n\\[\nr_{ij} - r_{i\\xi}r_{j\\xi}=0\n\\]\novvero\n\\[\n\\begin{equation}\nr_{ij} = r_{i\\xi}r_{j\\xi}.\n\\end{equation}\n\\]\nIn altri termini, se esiste un fattore non osservato \\(\\xi\\) in grado di rendere uguali a zero tutte le correlazioni parziali \\(r_{ih \\mid \\xi}\\), allora la correlazione tra ciascuna coppia di variabili \\(y\\) deve essere uguale al prodotto delle correlazioni tra ciascuna \\(y\\) e il fattore latente \\(\\xi\\). Questo è il principio base dell’analisi fattoriale.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#teoria-dei-due-fattori",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#teoria-dei-due-fattori",
    "title": "25  Il modello unifattoriale",
    "section": "25.6 Teoria dei Due Fattori",
    "text": "25.6 Teoria dei Due Fattori\nPer fare un esempio concreto relativo al metodo dell’annullamento della tetrade, esaminiamo la matrice di correlazioni originariamente analizzata da Spearman. Spearman (1904) raccolse alcune misure di capacità intellettuale su un piccolo numero di studenti di una scuola superiore. Nello specifico, esaminò i voti di tali studenti nelle seguenti materie: studio dei classici (\\(c\\)), letteratura inglese (\\(e\\)) e abilità matematiche (\\(m\\)). Considerò anche la prestazione in un compito di discriminazione dell’altezza di suoni (“pitch discrimination”) (\\(p\\)), ovvero un’abilità diversa da quelle richieste nei test scolastici.\nSecondo la Teoria dei Due Fattori, le prestazioni relative ad un determinato compito intellettuale possiedono una componente comune (detta fattore ‘g’) con le prestazioni in un qualunque altro compito intellettuale e una componente specifica a quel determinato compito. Il modello dell’intelligenza di Spearman prevede dunque due fattori, uno generale e uno specifico (detto fattore ‘s’). Il fattore ‘g’ costituisce la componente invariante dell’abilità intellettiva, mente il fattore ‘s’ è una componente che varia da condizione a condizione.\nCome è possibile stabilire se esiste una variabile latente in grado di spiegare le correlazioni tra le variabili osservate da Spearman? Lo strumento proposto da Spearman per rispondere a questa domanda è l’annullamento della tetrade. L’annullamento della tetrade utilizza i vincoli sulle correlazioni che derivano dalla definizione di correlazione parziale. In precedenza abbiamo visto che la correlazione parziale tra le variabili \\(y\\) indicizzate da \\(i\\) e \\(j\\), al netto dell’effetto di \\(\\xi\\), è nulla se\n\\[\nr_{ij} = r_{i\\xi}r_{j\\xi}.\n\\]\nNel caso dei dati di Spearman, dunque, le correlazioni parziali sono nulle se la correlazione tra ‘’studi classici’’ e ‘’letteratura inglese’’ è uguale al prodotto della correlazione tra ‘’studi classici’’ e il fattore \\(\\xi\\) e della correlazione tra ‘’letteratura inglese’’ e il fattore \\(\\xi\\). Inoltre, la correlazione tra ‘’studi classici’’ e ‘’abilità matematica’’ deve essere uguale al prodotto della correlazione tra ‘’studi classici’’ e il fattore \\(\\xi\\) e della correlazione tra ‘’abilità matematica’’ e il fattore \\(\\xi\\); e così via.\nLe correlazioni tra le variabili manifeste e il fattore latente sono dette e vengono denotate con la lettera \\(\\lambda\\). Se il modello di Spearman è corretto, avremo che\n\\[r_{ec}=\\lambda_e \\times \\lambda_{c},\\]\ndove \\(r_{ec}\\) è la correlazione tra ‘’letteratura inglese’’ (e) e ‘’studi classici’’ (c), \\(\\lambda_e\\) è la correlazione tra ‘’letteratura inglese’’ e \\(\\xi\\), e \\(\\lambda_{c}\\) è la correlazione tra ‘’studi classici’’ e \\(\\xi\\).\nAllo stesso modo, la correlazione tra ‘’studi classici’’ e ‘’matematica’’ (m) dovrà essere uguale a\n\\[\\lambda_c \\times \\lambda_m,\\]\neccetera.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#annullamento-della-tetrade",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#annullamento-della-tetrade",
    "title": "25  Il modello unifattoriale",
    "section": "25.7 Annullamento della tetrade",
    "text": "25.7 Annullamento della tetrade\nDate le correlazioni tra tre coppie di variabili manifeste, il metodo dell’annullamento della tetrade\n\nin una matrice di correlazione, si selezionino quattro coefficienti nelle posizioni che marcano gli angoli di un rettangolo. La differenza tra i prodotti dei coefficienti che giacciono sulle due diagonali di tale rettangolo costituisce la differenza delle tetradi e deve essere uguale a zero.\n\nrende possibile stimare i valori delle saturazioni fattoriali \\(\\lambda\\). Ad esempio, per le variabili \\(c\\), \\(m\\) ed \\(e\\), possiamo scrivere le seguenti tre equazioni in tre incognite:\n\\[\n\\begin{align}\n  r_{cm} &= \\lambda_c \\times \\lambda_m, \\notag \\\\\n  r_{em} &= \\lambda_e \\times \\lambda_m,  \\\\\n  r_{ce} &= \\lambda_c \\times \\lambda_e. \\notag\n\\end{align}\n\\]\nRisolvendo il precedente sistema di equazioni lineari, il coefficiente di saturazione \\(\\lambda_m\\) della variabile \\(y_m\\) nel fattore comune \\(\\xi\\), ad esempio, pu{`o} essere calcolato a partire dalle correlazioni tra le variabili manifeste \\(c\\), \\(m\\), ed \\(e\\) nel modo seguente\\footnote{ La terza delle equazioni del sistema lineare può essere riscritta come \\(\\lambda_c = \\frac{r_{ce}}{\\lambda_e}\\).\nUtilizzando tale risultato, la prima equazione diventa \\(r_{cm} = \\frac{r_{ce}}{\\lambda_e}\\lambda_m\\). Dalla seconda equazione otteniamo \\(\\lambda_e = \\frac{r_{em}}{\\lambda_m}\\). Sostituendo questo risultato nell’equazione precedente otteniamo \\(r_{cm} = \\frac{r_{ce}}{r_{em}}\\lambda_m^2\\), quindi \\(\\lambda_m^2 = \\frac{r_{cm} r_{em} }{r_{ce}}\\).\nVerifichiamo: \\(\\frac{r_{cm} r_{em}}{r_{ce}} = \\frac{\\lambda_c \\lambda_m \\lambda_e \\lambda_m}{\\lambda_c \\lambda_e} = \\lambda_m^2\\).\n\\[\n\\begin{align}\n  \\lambda_m &= \\sqrt{\n    \\frac{r_{cm} r_{em}}{r_{ce}}\n    }.\n\\end{align}\n\\tag{25.5}\\]\nLo stesso vale per le altre due saturazioni \\(\\lambda_c\\) e \\(\\lambda_e\\).\nNel suo articolo del 1904, Spearman osservò le seguenti correlazioni tra le variabili \\(Y_c\\), \\(Y_e\\), \\(Y_m\\) e \\(Y_p\\):\n\\[\n\\begin{array}{ccccc}\n  \\hline\n    & Y_C & Y_E & Y_M & Y_P \\\\\n  \\hline\n  Y_C & 1.00 & 0.78 & 0.70 & 0.66 \\\\\n  Y_E &   & 1.00 & 0.64 & 0.54 \\\\\n  Y_M &   &   & 1.00 & 0.45 \\\\\n  Y_P &   &   &   & 1.00 \\\\\n  \\hline\n\\end{array}\n\\]\nUtilizzando l’Equazione 25.5, mediante le correlazioni \\(r_{cm}\\), \\(r_{em}\\), e \\(r_{ce}\\) fornite dalla tabella precedente, la saturazione \\(\\lambda_m\\) diventa uguale a:\n\\[\n\\begin{align}\n  \\hat{\\lambda}_m &= \\sqrt{ \\frac{r_{cm} r_{em}}{r_{ce}} } = \\sqrt{\n    \\frac{0.70 \\times 0.64}{0.78} } = 0.76. \\notag\n\\end{align}\n\\]\nÈ importante notare che il metodo dell’annullamento della tetrade produce risultati falsificabili. Infatti, ci sono modi diversi per calcolare la stessa saturazione fattoriale. Se il modello fattoriale è corretto si deve ottenere lo stesso risultato in tutti i casi.\nNel caso presente, la saturazione fattoriale \\(\\lambda_m\\) può essere calcolata in altri due modi:\n\\[\n\\begin{align}\n  \\hat{\\lambda}_m &= \\sqrt{ \\frac{r_{cm} r_{mp}}{r_{cp}} } = \\sqrt{ \\frac{0.78 \\times 0.45}{0.66} } = 0.69, \\notag \\\\\n  \\hat{\\lambda}_m &= \\sqrt{ \\frac{r_{em} r_{mp}}{r_{ep}} } = \\sqrt{\n    \\frac{0.64 \\times 0.45}{0.54} } = 0.73. \\notag\n\\end{align}\n\\]\nI tre valori che sono stati ottenuti sono molto simili. Qual è allora la stima migliore di \\(\\lambda_m\\)?",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#metodo-del-centroide",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#metodo-del-centroide",
    "title": "25  Il modello unifattoriale",
    "section": "25.8 Metodo del centroide",
    "text": "25.8 Metodo del centroide\nLa soluzione più semplice è quella di fare la media di questi tre valori (\\(\\bar{\\lambda}_m = 0.73\\)). Un metodo migliore (meno vulnerabile ai valori anomali) è dato dal rapporto tra la somma dei numeratori e dei denominatori:\n\\[\n\\begin{align}\n  \\hat{\\lambda}_m &= \\sqrt{ \\frac{0.70 \\times 0.64 + 0.78 \\times 0.45 + 0.64\n      \\times 0.45}{0.78+0.66+0.54} } = 0.73 \\notag\n\\end{align}\n\\]\nIn questo caso, i due metodi danno lo stesso risultato. Le altre tre saturazioni fattoriali trovate mediante il metodo del centroide sono:\n\\[\\hat{\\lambda}_c = 0.97, \\quad \\hat{\\lambda}_e = 0.84, \\quad \\hat{\\lambda}_p = 0.65.\\]\nIn conclusione,\n\\[\n\\boldsymbol{\\hat{\\Lambda}}'=\n(\\hat{\\lambda}_c, \\hat{\\lambda}_e, \\hat{\\lambda}_m, \\hat{\\lambda}_p) = (0.97, 0.84, 0.73, 0.65).\n\\]\nQuesto risultato è la soluzione proposta da Spearman nel suo articolo del 1904 per risolvere il problema di determinare le saturazioni fattoriali di un modello con un fattore comune latente.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#introduzione-a-lavaan",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#introduzione-a-lavaan",
    "title": "25  Il modello unifattoriale",
    "section": "25.9 Introduzione a lavaan",
    "text": "25.9 Introduzione a lavaan\nAttualmente, l’analisi fattoriale viene svolta mediante software. Il pacchetto R più ampiamente utilizzato per condurre l’analisi fattoriale è lavaan.\n\n25.9.1 Sintassi del modello\nAl cuore del pacchetto lavaan si trova la “sintassi del modello”. La sintassi del modello è una descrizione del modello da stimare. In questa sezione, spieghiamo brevemente gli elementi della sintassi del modello lavaan.\nNell’ambiente R, una formula di regressione ha la seguente forma:\ny ~ x1 + x2 + x3 + x4\nIn questa formula, la tilde (“~”) è l’operatore di regressione. Sul lato sinistro dell’operatore, abbiamo la variabile dipendente (y), e sul lato destro abbiamo le variabili indipendenti, separate dall’operatore “+” . In lavaan, un modello tipico è semplicemente un insieme (o sistema) di formule di regressione, in cui alcune variabili (che iniziano con una ‘f’ qui sotto) possono essere latenti. Ad esempio:\ny ~ f1 + f2 + x1 + x2\nf1 ~ f2 + f3\nf2 ~ f3 + x1 + x2\nSe abbiamo variabili latenti in una qualsiasi delle formule di regressione, dobbiamo “definirle” elencando i loro indicatori (manifesti o latenti). Lo facciamo utilizzando l’operatore speciale “=~”, che può essere letto come “è misurato da”. Ad esempio, per definire le tre variabili latenti f1, f2 e f3, possiamo usare la sintassi seguente:\nf1 =~ y1 + y2 + y3\nf2 =~ y4 + y5 + y6\nf3 =~ y7 + y8 + y9 + y10\nInoltre, le varianze e le covarianze sono specificate utilizzando un operatore “doppia tilde”, ad esempio:\ny1 ~~ y1 # varianza\ny1 ~~ y2 # covarianza\nf1 ~~ f2 # covarianza\nE infine, le intercette per le variabili osservate e latenti sono semplici formule di regressione con solo una intercetta (esplicitamente indicato dal numero “1”) come unico predittore:\ny1 ~ 1\nf1 ~ 1\nUtilizzando questi quattro tipi di formule, è possibile descrivere una vasta gamma di modelli di variabili latenti. L’attuale insieme di tipi di formula è riassunto nella tabella sottostante.\n\n\n\ntipo di formula\noperatore\nmnemonic\n\n\n\n\ndefinizione variabile latente\n=~\nè misurato da\n\n\nregressione\n~\nviene regredito su\n\n\n(co)varianza (residuale)\n~~\nè correlato con\n\n\nintercetta\n~ 1\nintercetta\n\n\n\nUna sintassi completa del modello lavaan è semplicemente una combinazione di questi tipi di formule, racchiusi tra virgolette singole. Ad esempio:\nmy_model &lt;- ' \n  # regressions\n  y1 + y2 ~ f1 + f2 + x1 + x2\n  f1 ~ f2 + f3\n  f2 ~ f3 + x1 + x2\n\n  # latent variable definitions \n  f1 =~ y1 + y2 + y3 \n  f2 =~ y4 + y5 + y6 \n  f3 =~ y7 + y8 + y9\n  \n  # variances and covariances \n  y1 ~~ y1 \n  y1 ~~ y2 \n  f1 ~~ f2\n\n  # intercepts \n  y1 ~ 1 \n  f1 ~ 1\n'\nPer adattare il modello ai dati usiamo la seguente sintassi.\nfit &lt;- cfa(model = my_model, data = my_data)\n\n\n25.9.2 Un esempio concreto\nAnalizziamo nuovamente i dati di Spearman che abbiamo esaminato in precedenza usando lavaan. La matrice completa dei dati di Spearman è messa a disposizione da Kan et al. (2019).\nSpecifichiamo il nome delle variabili manifeste\n\nvarnames &lt;- c(\n  \"Classics\", \"French\", \"English\", \"Math\", \"Pitch\", \"Music\"\n)\n\ne il loro numero\n\nny &lt;- length(varnames)\n\nCreiamo la matrice di correlazione:\n\nspearman_cor_mat &lt;- matrix(\n  c(\n    1.00,  .83,  .78,  .70,  .66,  .63,\n     .83, 1.00,  .67,  .67,  .65,  .57,\n     .78,  .67, 1.00,  .64,  .54,  .51,\n     .70,  .67,  .64, 1.00,  .45,  .51,\n     .66,  .65,  .54,  .45, 1.00,  .40,\n     .63,  .57,  .51,  .51,  .40, 1.00\n  ),\n  ny, ny,\n  byrow = TRUE,\n  dimnames = list(varnames, varnames)\n)\nspearman_cor_mat\n\n\nA matrix: 6 x 6 of type dbl\n\n\n\nClassics\nFrench\nEnglish\nMath\nPitch\nMusic\n\n\n\n\nClassics\n1.00\n0.83\n0.78\n0.70\n0.66\n0.63\n\n\nFrench\n0.83\n1.00\n0.67\n0.67\n0.65\n0.57\n\n\nEnglish\n0.78\n0.67\n1.00\n0.64\n0.54\n0.51\n\n\nMath\n0.70\n0.67\n0.64\n1.00\n0.45\n0.51\n\n\nPitch\n0.66\n0.65\n0.54\n0.45\n1.00\n0.40\n\n\nMusic\n0.63\n0.57\n0.51\n0.51\n0.40\n1.00\n\n\n\n\n\nSpecifichiamo l’ampiezza campionaria:\n\nn &lt;- 33\n\nDefiniamo il modello unifattoriale in lavaan. L’operatore =~ si può leggere dicendo che la variabile latente a sinistra dell’operatore viene identificata dalle variabili manifeste elencate a destra dell’operatore e separate dal segno +. Per il caso presente, il modello dei due fattori di Spearman può essere specificato come segue.\n\nspearman_mod &lt;- \"\n  g =~ Classics + French + English + Math + Pitch + Music\n\"\n\nAdattiamo il modello ai dati con la funzione cfa():\n\nfit1 &lt;- lavaan::cfa(\n  spearman_mod,\n  sample.cov = spearman_cor_mat,\n  sample.nobs = n,\n  std.lv = TRUE\n)\n\nLa funzione cfa() è una funzione dedicata per adattare modelli di analisi fattoriale confermativa. Il primo argomento è il modello specificato dall’utente. Il secondo argomento è il dataset che contiene le variabili osservate. L’argomento std.lv = TRUE specifica che imponiamo una varianza pari a 1 a tutte le variabili latenti comuni (nel caso presente, solo una). Ciò consente di stimare le saturazioni fattoriali.\nUna volta adattato il modello, la funzione summary() ci consente di esaminare la soluzione ottenuta:\n\nout = summary(\n  fit1, \n  fit.measures = TRUE, \n  standardized = TRUE\n)\nprint(out)\n\nlavaan 0.6-19 ended normally after 23 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n\n  Number of observations                            33\n\nModel Test User Model:\n                                                      \n  Test statistic                                 2.913\n  Degrees of freedom                                 9\n  P-value (Chi-square)                           0.968\n\nModel Test Baseline Model:\n\n  Test statistic                               133.625\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.086\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)               -212.547\n  Loglikelihood unrestricted model (H1)       -211.091\n                                                      \n  Akaike (AIC)                                 449.094\n  Bayesian (BIC)                               467.052\n  Sample-size adjusted Bayesian (SABIC)        429.622\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: RMSEA &lt;= 0.050                    0.976\n  P-value H_0: RMSEA &gt;= 0.080                    0.016\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.025\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n  g =~                                                         \n    Classics          0.942    0.129    7.314    0.000    0.942\n    French            0.857    0.137    6.239    0.000    0.857\n    English           0.795    0.143    5.545    0.000    0.795\n    Math              0.732    0.149    4.923    0.000    0.732\n    Pitch             0.678    0.153    4.438    0.000    0.678\n    Music             0.643    0.155    4.142    0.000    0.643\n  Std.all\n         \n    0.956\n    0.871\n    0.807\n    0.743\n    0.689\n    0.653\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n   .Classics          0.083    0.051    1.629    0.103    0.083\n   .French            0.234    0.072    3.244    0.001    0.234\n   .English           0.338    0.094    3.610    0.000    0.338\n   .Math              0.434    0.115    3.773    0.000    0.434\n   .Pitch             0.510    0.132    3.855    0.000    0.510\n   .Music             0.556    0.143    3.893    0.000    0.556\n    g                 1.000                               1.000\n  Std.all\n    0.086\n    0.242\n    0.349\n    0.447\n    0.526\n    0.573\n    1.000\n\n\n\nL’output di lavaan si divide in tre sezioni principali:\n\nIntestazione: Le prime nove righe dell’output costituiscono l’intestazione, che fornisce informazioni chiave sul modello e sull’analisi, tra cui:\n\nVersione di lavaan: specifica la versione del pacchetto utilizzata.\nEsito dell’ottimizzazione: indica se l’algoritmo di ottimizzazione è terminato correttamente e il numero di iterazioni necessarie.\nStimatore: mostra il metodo di stima utilizzato, come ML (massima verosimiglianza).\nOttimizzatore: specifica l’algoritmo di ottimizzazione (es., NLMINB) utilizzato per trovare i parametri migliori per lo stimatore selezionato.\nNumero di parametri del modello: fornisce il totale dei parametri stimati nel modello (es., 12).\nNumero di osservazioni: indica il numero di dati effettivamente utilizzati nell’analisi (es., 33).\n“Model Test User Model”: contiene la statistica di test, i gradi di libertà e il valore p per il modello specificato dall’utente.\n\nMisure di adattamento: Questa sezione, visibile solo con fit.measures = TRUE, presenta una serie di indicatori di adattamento del modello, iniziando dalla riga “Model Test Baseline Model” e terminando con il valore per l’SRMR. Questi indicatori forniscono informazioni sulla bontà del modello rispetto ai dati.\nStime dei parametri: Questa sezione contiene le stime dei parametri e inizia con dettagli tecnici sul metodo utilizzato per calcolare gli errori standard. Di seguito, sono elencati i parametri liberi e fissi del modello, in genere con ordine che parte dalle variabili latenti, seguito dalle covarianze e dalle varianze residue. Le colonne includono:\n\nEstimate: indica il valore stimato (non standardizzato) per ciascun parametro, rappresentando il peso del collegamento tra il costrutto latente (es., g) e le variabili osservate.\nStd.err: l’errore standard per ogni stima, utile per valutare l’accuratezza della stima.\nZ-value: la statistica di Wald, calcolata dividendo la stima per il suo errore standard.\nP(&gt;|z|): il valore p, utilizzato per testare l’ipotesi nulla che la stima sia zero nella popolazione.\n\n\nUlteriori dettagli sulle colonne:\n\nEstimate: Fornisce lo stimatore di massima verosimiglianza per i pesi dei percorsi, rappresentando l’effetto diretto di ogni costrutto latente sulle variabili osservate.\nStd.lv: Questi valori sono standardizzati solo rispetto alle variabili latenti, permettendo un confronto all’interno del modello indipendentemente dalle unità di misura originali.\nStd.all: Fornisce le stime completamente standardizzate, considerando sia le variabili latenti sia quelle osservate, facilitando un confronto dei coefficienti in termini di deviazioni standard.\n\nNella sezione delle Varianze, si osserva un punto prima dei nomi delle variabili osservate. Questo formato indica che sono variabili endogene, cioè predette dalle variabili latenti, e che il valore riportato rappresenta la varianza residua non spiegata dai predittori. Invece, le variabili latenti, che non hanno un punto prima del loro nome, sono considerate esogene; i valori riportati sono le loro varianze totali stimate.\nÈ possibile semplificare l’output dalla funzione summary() in maniera tale da stampare solo la tabella completa delle stime dei parametri e degli errori standard. Qui usiamo coef(fit1).\n\ncoef(fit1) |&gt;\n    print()  \n\n       g=~Classics          g=~French         g=~English \n             0.942              0.857              0.795 \n           g=~Math           g=~Pitch           g=~Music \n             0.732              0.678              0.643 \nClassics~~Classics     French~~French   English~~English \n             0.083              0.234              0.338 \n        Math~~Math       Pitch~~Pitch       Music~~Music \n             0.434              0.510              0.556 \n\n\nUsando parameterEstimates, l’output diventa il seguente.\n\nparameterEstimates(fit1, standardized = TRUE) |&gt;\n    print()\n\n        lhs op      rhs   est    se     z pvalue ci.lower ci.upper\n1         g =~ Classics 0.942 0.129 7.314  0.000    0.689    1.194\n2         g =~   French 0.857 0.137 6.239  0.000    0.588    1.127\n3         g =~  English 0.795 0.143 5.545  0.000    0.514    1.076\n4         g =~     Math 0.732 0.149 4.923  0.000    0.441    1.024\n5         g =~    Pitch 0.678 0.153 4.438  0.000    0.379    0.978\n6         g =~    Music 0.643 0.155 4.142  0.000    0.339    0.948\n7  Classics ~~ Classics 0.083 0.051 1.629  0.103   -0.017    0.183\n8    French ~~   French 0.234 0.072 3.244  0.001    0.093    0.376\n9   English ~~  English 0.338 0.094 3.610  0.000    0.154    0.522\n10     Math ~~     Math 0.434 0.115 3.773  0.000    0.208    0.659\n11    Pitch ~~    Pitch 0.510 0.132 3.855  0.000    0.251    0.769\n12    Music ~~    Music 0.556 0.143 3.893  0.000    0.276    0.836\n13        g ~~        g 1.000 0.000    NA     NA    1.000    1.000\n   std.lv std.all\n1   0.942   0.956\n2   0.857   0.871\n3   0.795   0.807\n4   0.732   0.743\n5   0.678   0.689\n6   0.643   0.653\n7   0.083   0.086\n8   0.234   0.242\n9   0.338   0.349\n10  0.434   0.447\n11  0.510   0.526\n12  0.556   0.573\n13  1.000   1.000\n\n\nCon opportuni parametri possiamo semplificare l’output nel modo seguente.\n\nparameterEstimates(fit1, standardized = TRUE) |&gt;\n  dplyr::filter(op == \"=~\") |&gt;\n  dplyr::select(\n    \"Latent Factor\" = lhs,\n    Indicator = rhs,\n    B = est,\n    SE = se,\n    Z = z,\n    \"p-value\" = pvalue,\n    Beta = std.all\n  ) |&gt;\n    print()\n\n  Latent.Factor Indicator     B    SE     Z p.value  Beta\n1             g  Classics 0.942 0.129 7.314       0 0.956\n2             g    French 0.857 0.137 6.239       0 0.871\n3             g   English 0.795 0.143 5.545       0 0.807\n4             g      Math 0.732 0.149 4.923       0 0.743\n5             g     Pitch 0.678 0.153 4.438       0 0.689\n6             g     Music 0.643 0.155 4.142       0 0.653\n\n\nEsaminiamo la matrice delle correlazioni residue:\n\nresiduals(fit1, type = \"cor\")$cov |&gt;\n    print()\n\n         Clsscs French Englsh   Math  Pitch  Music\nClassics  0.000                                   \nFrench   -0.003  0.000                            \nEnglish   0.008 -0.033  0.000                     \nMath     -0.011  0.023  0.040  0.000              \nPitch     0.001  0.050 -0.016 -0.062  0.000       \nMusic     0.005  0.001 -0.017  0.024 -0.050  0.000\n\n\nCreiamo un qq-plot dei residui:\n\nres1 &lt;- residuals(fit1, type = \"cor\")$cov\nres1[upper.tri(res1, diag = TRUE)] &lt;- NA\nv1 &lt;- as.vector(res1)\nv2 &lt;- v1[!is.na(v1)]\n\ntibble(v2) %&gt;% \n  ggplot(aes(sample = v2)) + \n  stat_qq() + \n  stat_qq_line()",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#diagrammi-di-percorso",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#diagrammi-di-percorso",
    "title": "25  Il modello unifattoriale",
    "section": "25.10 Diagrammi di percorso",
    "text": "25.10 Diagrammi di percorso\nIl pacchetto semPlot consente di disegnare diagrammi di percorso per vari modelli SEM. La funzione semPaths prende in input un oggetto creato da lavaan e disegna il diagramma, con diverse opzioni disponibili. Il diagramma prodotto controlla le dimensioni dei caratteri/etichette, la visualizzazione dei residui e il colore dei percorsi/coefficienti. Sono disponibili queste e molte altre opzioni di controllo.\n\nsemPaths(\n    fit1,\n    \"std\",\n    posCol = c(\"black\"),\n    edge.label.cex = 1.2,\n    whatLabels = \"std\", \n    edge.width = 0.3, # Imposta lo spessore delle linee \n    fade = FALSE # Disabilita il fading\n)\n\n\n\n\n\n\n\n\nIl calcolo delle saturazioni fattoriali con il metodo del centroide aveva prodotto il seguente risultato:\n\nclassici (Cls): 0.97\ninglese (Eng): 0.84\nmatematica (Mth): 0.73\npitch discrimination (Ptc): 0.65\n\nSi noti la somiglianza con i valori ottenuti mediante il metodo di massima verosimiglianza riportati nella figura.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#analisi-fattoriale-esplorativa",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#analisi-fattoriale-esplorativa",
    "title": "25  Il modello unifattoriale",
    "section": "25.11 Analisi fattoriale esplorativa",
    "text": "25.11 Analisi fattoriale esplorativa\nQuando abbiamo un’unica variabile latente, l’analisi fattoriale confermativa si riduce al caso dell’analisi fattoriale esplorativa. Esaminiamo qui sotto la sintassi per l’analisi fattoriale esplorativa in lavaan.\nSpecifichiamo il modello.\n\nefa_model &lt;- '\n    efa(\"efa\")*g =~ Classics + French + English + Math + Pitch + Music\n'\n\nAdattiamo il modello ai dati.\n\nfit2 &lt;- lavaan::cfa(\n  efa_model,\n  sample.cov = spearman_cor_mat,\n  sample.nobs = n,\n  std.lv = TRUE\n)\n\nEsaminiamo la soluzione ottenuta.\n\nsummary(fit2, standardized = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 3 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n\n  Rotation method                       GEOMIN OBLIQUE\n  Geomin epsilon                                 0.001\n  Rotation algorithm (rstarts)                GPA (30)\n  Standardized metric                             TRUE\n  Row weights                                     None\n\n  Number of observations                            33\n\nModel Test User Model:\n                                                      \n  Test statistic                                 2.913\n  Degrees of freedom                                 9\n  P-value (Chi-square)                           0.968\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n  g =~ efa                                                     \n    Classics          0.942    0.129    7.314    0.000    0.942\n    French            0.857    0.137    6.239    0.000    0.857\n    English           0.795    0.143    5.545    0.000    0.795\n    Math              0.732    0.149    4.923    0.000    0.732\n    Pitch             0.678    0.153    4.438    0.000    0.678\n    Music             0.643    0.155    4.142    0.000    0.643\n  Std.all\n         \n    0.956\n    0.871\n    0.807\n    0.743\n    0.689\n    0.653\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n   .Classics          0.083    0.051    1.629    0.103    0.083\n   .French            0.234    0.072    3.244    0.001    0.234\n   .English           0.338    0.094    3.610    0.000    0.338\n   .Math              0.434    0.115    3.773    0.000    0.434\n   .Pitch             0.510    0.132    3.855    0.000    0.510\n   .Music             0.556    0.143    3.893    0.000    0.556\n    g                 1.000                               1.000\n  Std.all\n    0.086\n    0.242\n    0.349\n    0.447\n    0.526\n    0.573\n    1.000\n\n\n\n\nsemPaths(\n    fit2,\n    \"std\",\n    posCol = c(\"black\"),\n    edge.label.cex = 1.2,\n    whatLabels = \"std\",\n    edge.width = 0.3, # Imposta lo spessore delle linee\n    fade = FALSE # Disabilita il fading\n)",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#riflessioni-conclusive",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#riflessioni-conclusive",
    "title": "25  Il modello unifattoriale",
    "section": "25.12 Riflessioni Conclusive",
    "text": "25.12 Riflessioni Conclusive\nIn questo capitolo, abbiamo introdotto il metodo dell’annullamento della tetrade, che permette di stimare le saturazioni in un modello monofattoriale. Abbiamo anche illustrato come questo metodo sia, in effetti, un’applicazione del concetto di correlazione parziale.\nUn aspetto fondamentale nella costruzione dei test psicologici riguarda la determinazione del numero di fattori o tratti sottostanti al set di indicatori in esame. La teoria classica dei test presuppone che un test sia monofattoriale, cioè che gli indicatori riflettano un unico tratto latente. La mancata monodimensionalità introduce difficoltà nell’applicare i principi della teoria classica ai punteggi di un test che non soddisfa tale proprietà.\nL’analisi della dimensionalità di un insieme di indicatori rappresenta, quindi, una fase cruciale nel processo di costruzione di un test. Solitamente, questa valutazione viene effettuata attraverso l’analisi fattoriale. In questo capitolo, abbiamo descritto le proprietà di base del modello unifattoriale, gettando le fondamenta per una comprensione più approfondita della dimensionalità e dell’influenza di un singolo tratto latente sugli indicatori.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/02_analisi_fattoriale_1.html#session-info",
    "href": "chapters/fa/02_analisi_fattoriale_1.html#session-info",
    "title": "25  Il modello unifattoriale",
    "section": "25.13 Session Info",
    "text": "25.13 Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] lavaanExtra_0.2.1 lavaanPlot_0.8.1  kableExtra_1.4.0 \n [4] corrplot_0.94     nortest_1.0-4     MASS_7.3-61      \n [7] ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2\n[10] ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n[13] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6   \n[16] semPlot_1.1.6     lavaan_0.6-19     psych_2.4.6.26   \n[19] scales_1.3.0      markdown_1.13     knitr_1.48       \n[22] lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n[25] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5      \n[28] tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n[31] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] RColorBrewer_1.1-3  rstudioapi_0.16.0   jsonlite_1.8.9     \n  [4] magrittr_2.0.3      TH.data_1.1-2       estimability_1.5.1 \n  [7] farver_2.1.2        nloptr_2.1.1        rmarkdown_2.28     \n [10] vctrs_0.6.5         Cairo_1.6-2         minqa_1.2.8        \n [13] base64enc_0.1-3     rstatix_0.7.2       htmltools_0.5.8.1  \n [16] broom_1.0.7         Formula_1.2-5       htmlwidgets_1.6.4  \n [19] plyr_1.8.9          sandwich_3.1-1      emmeans_1.10.4     \n [22] zoo_1.8-12          uuid_1.2-1          igraph_2.0.3       \n [25] mime_0.12           lifecycle_1.0.4     pkgconfig_2.0.3    \n [28] Matrix_1.7-0        R6_2.5.1            fastmap_1.2.0      \n [31] shiny_1.9.1         numDeriv_2016.8-1.1 digest_0.6.37      \n [34] OpenMx_2.21.12      fdrtool_1.2.18      colorspace_2.1-1   \n [37] rprojroot_2.0.4     Hmisc_5.1-3         labeling_0.4.3     \n [40] fansi_1.0.6         timechange_0.3.0    abind_1.4-8        \n [43] compiler_4.4.1      withr_3.0.1         glasso_1.11        \n [46] htmlTable_2.4.3     backports_1.5.0     carData_3.0-5      \n [49] ggsignif_0.6.4      corpcor_1.6.10      gtools_3.9.5       \n [52] tools_4.4.1         pbivnorm_0.6.0      foreign_0.8-87     \n [55] zip_2.3.1           httpuv_1.6.15       nnet_7.3-19        \n [58] glue_1.8.0          quadprog_1.5-8      DiagrammeR_1.0.11  \n [61] promises_1.3.0      nlme_3.1-166        lisrelToR_0.3      \n [64] grid_4.4.1          pbdZMQ_0.3-13       checkmate_2.3.2    \n [67] cluster_2.1.6       reshape2_1.4.4      generics_0.1.3     \n [70] gtable_0.3.5        tzdb_0.4.0          data.table_1.16.0  \n [73] hms_1.1.3           xml2_1.3.6          car_3.1-3          \n [76] utf8_1.2.4          sem_3.1-16          pillar_1.9.0       \n [79] IRdisplay_1.1       rockchalk_1.8.157   later_1.3.2        \n [82] splines_4.4.1       lattice_0.22-6      survival_3.7-0     \n [85] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n [88] pbapply_1.7-2       svglite_2.1.3       stats4_4.4.1       \n [91] xfun_0.48           qgraph_1.9.8        arm_1.14-4         \n [94] visNetwork_2.1.2    stringi_1.8.4       pacman_0.5.1       \n [97] boot_1.3-31         evaluate_1.0.0      codetools_0.2-20   \n[100] mi_1.1              cli_3.6.3           RcppParallel_5.1.9 \n[103] IRkernel_1.3.2      rpart_4.1.23        systemfonts_1.1.0  \n[106] xtable_1.8-4        repr_1.1.7          munsell_0.5.1      \n[109] Rcpp_1.0.13         coda_0.19-4.1       png_0.1-8          \n[112] XML_3.99-0.17       parallel_4.4.1      jpeg_0.1-10        \n[115] lme4_1.1-35.5       mvtnorm_1.3-1       openxlsx_4.2.7.1   \n[118] crayon_1.5.3        rlang_1.1.4         multcomp_1.4-26    \n[121] mnormt_2.1.1       \n\n\n\n\n\n\nKan, K.-J., Maas, H. L. van der, & Levine, S. Z. (2019). Extending psychometric network analysis: Empirical evidence against g in favor of mutualism? Intelligence, 73, 52–62.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Il modello unifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html",
    "href": "chapters/fa/03_analisi_fattoriale_2.html",
    "title": "26  Il modello statistico dell’analisi fattoriale",
    "section": "",
    "text": "26.1 Modello monofattoriale\nIl punto di partenza dell’analisi fattoriale esplorativa è rappresentato da una marice di dimensioni \\(p \\times p\\) (dove \\(p\\) è il numero di variabili osservate) che contiene i coefficienti di correlazione (o di covarianza) tra le variabili. Il punto di arrivo è rappresentato da una matrice di dimensioni \\(p \\times k\\) (dove \\(k\\)) è il numero di fattori comuni che contiene i coefficienti (le saturazioni) che esprimono la relazione tra i fattori e le variabili osservate. Considereremo ora il modello matematico dell’analisi fattoriale esplorativa, con un solo fattore comune, che rappresenta il caso più semplice.\nCon \\(p\\) variabili manifeste \\(Y_i\\), il modello ad un fattore comune può essere espresso algebricamente nel modo seguente:\n\\[\nY_i = \\mu_i + \\lambda_{i} \\xi + \\delta_i \\qquad i=1, \\dots, p\n\\]\ndove \\(\\xi\\) rappresenta il fattore latente, chiamato anche fattore comune, poiché è comune a tutte le \\(Y_i\\), i \\(\\delta_i\\) sono invece specifici di ogni variabile osservata e per tale ragione vengono chiamati fattori specifici o unici, e infine i \\(\\lambda_i\\) sono detti saturazioni (o pesi) fattoriali poiché consentono di valutare il peso del fattore latente su ciascuna variabile osservata. Si suole assumere per comodità che \\(\\mu=0\\), il che corrisponde a considerare le variabili \\(Y_i\\) come ottenute dagli scarti dalle medie \\(\\mu_i\\) per \\(i = 1, \\dots, p\\):\n\\[\nY_i -\\mu_i = \\lambda_i \\xi + \\delta_i.\n\\]\nSi assume che il fattore comune abbia media zero, \\(\\mathbb{E}(\\xi)=0\\), e varianza unitaria, \\(\\mathbb{V}(\\xi)=1\\), che i fattori specifici abbiano media zero, \\(\\mathbb{E}(\\delta_j)=0\\), e varianza \\(\\mathbb{V}(\\delta_j)=\\psi_{i}\\), che i fattori specifici siano incorrelati tra loro, \\(\\mathbb{E}(\\delta_i \\delta_k)=0\\), e che i fattori specifici siano incorrelati con il fattore comune, \\(\\mathbb{E}(\\delta_i \\xi)=0\\).\nIn questo modello, poiché i fattori specifici sono tra loro incorrelati, l’interdipendenza tra le variabili manifeste è completamente spiegata dal fattore comune. Dalle ipotesi precedenti è possibile ricavare la covarianza tra \\(Y_i\\) e il fattore comune, la varianza della \\(i\\)-esima variabile manifesta \\(Y_i\\) e la covarianza tra due variabili manifeste \\(Y_i\\) e \\(Y_k\\).",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#covarianza-tra-un-indicatore-e-il-fattore-comune",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#covarianza-tra-un-indicatore-e-il-fattore-comune",
    "title": "26  Il modello statistico dell’analisi fattoriale",
    "section": "26.2 Covarianza tra un indicatore e il fattore comune",
    "text": "26.2 Covarianza tra un indicatore e il fattore comune\nDal modello monofattoriale è possibile determinare l’espressione della covarianza teorica tra una variabile manifesta \\(Y_i\\) e il fattore comune \\(\\xi\\):\n\\[\nCov(Y_i,\\xi)=\\mathbb{E}(Y_i \\xi)-\\mathbb{E}(Y_i)\\mathbb{E}(\\xi).\n\\]\nDato che \\(\\mathbb{E}(\\xi)=0\\), possiamo scrivere\n\\[\n\\begin{equation}\n\\begin{aligned}\n  Cov(Y_i,\\xi) &= \\mathbb{E}(Y_i \\xi)=\\mathbb{E}[(\\lambda_i \\xi + \\delta_i) \\xi]\\notag\\\\\n  &=\\mathbb{E}(\\lambda_i \\xi^2 + \\delta_i \\xi)\\notag\\\\\n  &=\\lambda_i\\underbrace{\\mathbb{E}(\\xi^2)}_{\\mathbb{V}(\\xi)=1} + \\underbrace{\\mathbb{E}(\\delta_i \\xi)}_{Cov(\\delta_i, \\xi)=0}\\notag\\\\\n  &= \\lambda_i.\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nNel modello a un solo fattore, dunque, la saturazione \\(\\lambda_j\\) rappresenta la covarianza la variabile manifesta \\(Y_i\\) e il fattore comune \\(\\xi\\) e indica l’importanza del fattore nel determinare il punteggio osservato. Se le variabili \\(Y_i\\) sono standardizzate, la saturazione fattoriale \\(\\lambda_i\\) corrisponde alla correlazione tra \\(Y_i\\) e \\(\\xi\\).",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#espressione-fattoriale-della-varianza",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#espressione-fattoriale-della-varianza",
    "title": "26  Il modello statistico dell’analisi fattoriale",
    "section": "26.3 Espressione fattoriale della varianza",
    "text": "26.3 Espressione fattoriale della varianza\nNell’ipotesi che le variabili \\(Y_i\\) abbiano media nulla, la varianza di \\(Y_i\\)\n\\[\n\\begin{equation}\n  \\mathbb{V}(Y_i) = \\mathbb{E}(Y_i^2) -[\\mathbb{E}(Y_i)]^2 = \\mathbb{E}(Y_i^2)\\notag\n\\end{equation}\n\\]\nè data da\n\\[\n\\begin{equation}\n\\begin{aligned}\n  \\mathbb{V}(Y_i) &= \\mathbb{E}[(\\lambda_i \\xi + \\delta_i)^2 ]\\notag\\\\\n  &=\\lambda_i^2 \\underbrace{\\mathbb{E}(\\xi^2) }_{\\mathbb{V}(\\xi)=1} + \\underbrace{\\mathbb{E}(\\delta_i^2) }_{\\mathbb{V}(\\delta_i)=\\psi_{i}} + 2\\lambda_i \\underbrace{\\mathbb{E}(\\xi \\delta_i) }_{Cov(\\xi, \\delta_{i})=0}\\notag\\\\\n  &=\\lambda^2_i + \\psi_{i}.\n\\end{aligned}\n\\end{equation}\n\\]\nLa quantità \\(\\lambda^2_i\\) è denominata comunalità della \\(i\\)-esima variabile manifesta e corrisponde alla quota della varianza della \\(Y_i\\) spiegata dal fattore comune. Di conseguenza \\(\\psi_{i}\\) è la parte residua della varianza di \\(Y_i\\) non spiegata dal fattore comune ed è denominata unicità di \\(Y_i\\). Nel caso di variabili standardizzate, l’unicità diventa uguale a\n\\[\n\\psi_{i}=1-\\lambda^2_i.\n\\]\nIn definitiva, la varianza totale di una variabile osservata può essere divisa in una quota che ciascuna variabile condivide con le altre variabili ed è spiegata dal fattore comune (questa quota è chiamata comunalità ed è uguale uguale al quadrato della saturazione della variabile osservata nel fattore comune, ovvero \\(h^2_i = \\lambda_i^2\\)), e in una quota che è spiegata dal fattore specifico (questa parte è chiamata unicità ed è uguale a \\(u_i = \\psi_{i}\\)).\nEsempio. Riprendiamo l’analisi della matrice di correlazioni di Spearman. Nell’output prodotto dalla funzione factanal() viene riportata la quantità denominata SS loadings. Tale quantità indica la porzione della varianza totale delle 4 variabili manifeste che viene spiegata dal fattore comune. Ciascuna variabile standardizzata contribuisce con un’unità di varianza; nel caso presente, dunque la varianza totale è uguale a 4. Si ricordi che, nella statistica multivariata, per varianza totale si intende la somma delle varianze delle variabili manifeste (nel linguaggio dell’algebra matriciale questa quantità corrisponde alla traccia della matrice di covarianze). La quota della varianza totale spiegata dal modello, invece, è data dalla somma delle comunalità delle quattro variabili, ovvero dalla somma delle saturazioni fattoriali innalzate al quadrato.\n\nSpearman &lt;- matrix(c(\n  1.0, .78, .70, .66,\n  .78, 1.0, .64, .54,\n  .70, .64, 1.0, .45,\n  .66, .54, .45, 1.0\n),\nbyrow = TRUE, ncol = 4\n)\nrownames(Spearman) &lt;- c(\"C\", \"E\", \"M\", \"P\")\ncolnames(Spearman) &lt;- c(\"C\", \"E\", \"M\", \"P\")\nSpearman |&gt;\n  print()\n\n     C    E    M    P\nC 1.00 0.78 0.70 0.66\nE 0.78 1.00 0.64 0.54\nM 0.70 0.64 1.00 0.45\nP 0.66 0.54 0.45 1.00\n\n\nEseguiamo l’analisi fattoriale:\n\nfm &lt;- factanal(covmat = Spearman, factors = 1)\nfm |&gt;\n    print()\n\n\nCall:\nfactanal(factors = 1, covmat = Spearman)\n\nUniquenesses:\n    C     E     M     P \n0.086 0.329 0.460 0.539 \n\nLoadings:\n  Factor1\nC 0.956  \nE 0.819  \nM 0.735  \nP 0.679  \n\n               Factor1\nSS loadings      2.587\nProportion Var   0.647\n\nThe degrees of freedom for the model is 2 and the fit was 0.023 \n\n\nLe saturazioni fattoriali sono:\n\nL &lt;- c(fm$load[1], fm$load[2], fm$load[3], fm$load[4])\nprint(L)\n\n[1] 0.9562592 0.8193902 0.7350316 0.6790212\n\n\nFacendo il prodotto interno otteniamo:\n\nt(L) %*% L \n\n\nA matrix: 1 x 1 of type dbl\n\n\n2.587173\n\n\n\n\n\nIn termini proporzionali, la quota della varianza totale delle variabile manifeste che viene spiegata dal modello ad un fattore comune è dunque uguale a \\(2.587 / 4 = 0.647\\). Questa quantità è indicata nell’output con la denominazione Proportion Var.\nSi dice unicità (uniqueness) la quota della varianza della variabile considerata che non viene spiegata dalla soluzione fattoriale:\n\nround(fm$uniqueness, 3) |&gt;\n    print()\n\n    C     E     M     P \n0.086 0.329 0.460 0.539 \n\n\nLa comunalità (ovvero, la quota di varianza di ciascuna variabile manifesta che viene spiegata dal fattore comune) può essere trovata come:\n\nround(1 - fm$uniqueness, 3) |&gt;\n    print()\n\n    C     E     M     P \n0.914 0.671 0.540 0.461 \n\n\noppure con\n\nL^2 |&gt;\n    round(3) |&gt;\n    print()\n\n[1] 0.914 0.671 0.540 0.461",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#covarianza-tra-due-variabili-manifeste",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#covarianza-tra-due-variabili-manifeste",
    "title": "26  Il modello statistico dell’analisi fattoriale",
    "section": "26.4 Covarianza tra due variabili manifeste",
    "text": "26.4 Covarianza tra due variabili manifeste\nNell’ipotesi che le variabili \\(Y_i\\) abbiano media nulla, la covarianza tra \\(Y_i\\) e \\(Y_k\\)\n\\[\nCov(Y_i, Y_k)=\\mathbb{E}(Y_i Y_k) -\n\\mathbb{E}(Y_i)\\mathbb{E}(Y_k)=\\mathbb{E}(Y_i Y_k)\n\\]\nè uguale al prodotto delle corrispondenti saturazioni fattoriali:\n\\[\n\\begin{equation}\n\\begin{aligned}\nCov(Y_i, Y_k) &= \\mathbb{E}(Y_i Y_k) \\notag\\\\\n  & =\\mathbb{E}[(\\lambda_i \\xi + \\delta_i)(\\lambda_k \\xi +  \\delta_k)]\\notag\\\\\n  &=\\mathbb{E}(\\lambda_i\\lambda_k\\xi^2 + \\lambda_i  \\xi \\delta_k + \\lambda_k \\delta_i \\xi + \\delta_i \\delta_k)\\notag\\\\\n  &=\\lambda_i\\lambda_k\\underbrace{\\mathbb{E}(\\xi^2)}_{\\mathbb{V}(\\xi)=1}+\\lambda_i\\underbrace{\\mathbb{E}(\\xi \\delta_k)}_{Cov(\\xi, \\delta_k) =0}+\\notag\\\\ \\;&+\\lambda_k\\underbrace{\\mathbb{E}(\\delta_i \\xi)}_{Cov(\\delta_i, \\xi) =0} +\\underbrace{\\mathbb{E}(\\delta_i \\delta_k)}_{Cov(\\delta_i, \\delta_k)=0}\\notag\\\\\n  &=\\lambda_i\\lambda_k.\n\\end{aligned}\n\\end{equation}\n\\]",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#correlazioni-osservate-e-correlazioni-riprodotte-dal-modello",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#correlazioni-osservate-e-correlazioni-riprodotte-dal-modello",
    "title": "26  Il modello statistico dell’analisi fattoriale",
    "section": "26.5 Correlazioni osservate e correlazioni riprodotte dal modello",
    "text": "26.5 Correlazioni osservate e correlazioni riprodotte dal modello\nIn generale possiamo affermare che il modello monofattoriale è adeguato se si verifica che \\(Cov(Y_i, Y_k \\mid \\xi) = 0\\) (\\(i, k = 1, \\dots,p; \\; i\\neq k\\)), ossia se il fattore comune spiega tutta la covarianza tra le variabili osservate. La matrice di correlazioni riprodotte dal modello è chiamata \\(\\boldsymbol{\\Sigma}\\) e può essere espressa come:\n\\[\n\\boldsymbol{\\Sigma} = \\boldsymbol{\\Lambda} \\boldsymbol{\\Lambda}^\\prime + \\boldsymbol{\\Psi}\n\\]\nIn altri termini, il modello monofattoriale è adeguato se è nulla la differenza tra la matrice di correlazioni osservate e la matrice di correlazioni riprodotte dal modello. Per i dati di Spearman, le correlazioni riprodotte dal modello ad un fattore sono\n\nround(L %*% t(L) + diag(fm$uniq), 3)\n\n\nA matrix: 4 x 4 of type dbl\n\n\n1.000\n0.784\n0.703\n0.649\n\n\n0.784\n1.000\n0.602\n0.556\n\n\n0.703\n0.602\n1.000\n0.499\n\n\n0.649\n0.556\n0.499\n1.000\n\n\n\n\n\nLa matrice delle differenze tra le correlazioni campionarie e quelle riprodotte è\n\nround(Spearman - (L %*% t(L) + diag(fm$uniq)), 3) \n\n\nA matrix: 4 x 4 of type dbl\n\n\n\nC\nE\nM\nP\n\n\n\n\nC\n0.000\n-0.004\n-0.003\n0.011\n\n\nE\n-0.004\n0.000\n0.038\n-0.016\n\n\nM\n-0.003\n0.038\n0.000\n-0.049\n\n\nP\n0.011\n-0.016\n-0.049\n0.000\n\n\n\n\n\nLo scarto maggiore tra le correlazioni campionarie e quelle riprodotte è uguale a 0.049. Si può dunque concludere che il modello monofattoriale spiega in maniera ragionevole i dati di Spearman.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#bontà-di-adattamento-del-modello-ai-dati",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#bontà-di-adattamento-del-modello-ai-dati",
    "title": "26  Il modello statistico dell’analisi fattoriale",
    "section": "26.6 Bontà di adattamento del modello ai dati",
    "text": "26.6 Bontà di adattamento del modello ai dati\nLa verifica della bontà di adattamento del modello ai dati si determina mediante un test statistico che valuta la differenza tra la matrice di correlazioni (o di covarianze) osservata e la matrice di correlazioni (o covarianze) predetta dal modello fattoriale. L’ipotesi nulla che viene valutata è che la matrice delle correlazioni residue sia dovuta semplicemente agli errori di campionamento, ovvero che la matrice di correlazioni predetta dal modello \\(\\boldsymbol{\\Sigma}(\\theta)\\) sia uguale alla matrice di correlazioni \\(\\boldsymbol{\\Sigma}\\) nella popolazione.\nLa statistica test \\(v\\) è una funzione della differenza tra la matrice riprodotta \\(\\boldsymbol{S}(\\theta)\\) e quella osservata \\(\\boldsymbol{S}\\)\n\\[\nv = f\\left[\\boldsymbol{S}(\\theta) - \\boldsymbol{S}\\right]\n\\]\ne si distribuisce come una \\(\\chi^2\\) con \\(\\nu\\) gradi di libertà\n\\[\n\\nu = p(p+1)/ 2 - q,\n\\]\ndove \\(p\\) è il numero di variabili manifeste e \\(q\\) è il numero di parametri stimati dal modello fattoriale (ovvero, \\(\\lambda\\) e \\(\\psi\\)).\nLa statistica \\(v\\) assume valore 0 se i parametri del modello riproducono esattamente la matrice di correlazioni tra le variabili nella popolazione. Tanto maggiore è la statistica \\(v\\) tanto maggiore è la discrepanza tra le correlazioni osservate e quelle predette dal modello fattoriale.\nUn risultato statisticamente significativo (es., \\(p\\) &lt; .05) – il quale suggerisce che una tale differenza non è uguale a zero – rivela dunque una discrepanza tra il modello e i dati. Il test del modello fattoriale mediante la statistica \\(\\chi^2\\) segue dunque una logica diversa da quella utilizzata nei normali test di ipotesi statistiche: un risultato statisticamente significativo indica una mancanza di adattamento del modello ai dati.\nL’applicazione del test \\(\\chi^2\\) per valutare la bontà di adattamento del modello ai dati richiede che ciascuna variabile manifesta sia distribuita normalmente – più precisamente, richiede che le variabili manifeste siano un campione casuale che deriva da una normale multivariata. Questo requisito non è facile da rispettare in pratica.\nTuttavia, il limite principale della statistica \\(\\chi^2\\) è che essa dipende fortemente dalle dimensioni del campione: al crescere delle dimensioni campionarie è più facile ottenere un risultato statisticamente significativo (ovvero, concludere che vi è un cattivo adattamento del modello ai dati). Per questa ragione, la bontà di adattamento del modello ai dati viene valutata da molteplici indici, non soltanto dalla statistica \\(\\chi^2\\). Più comune è calcolare il rapporto \\(\\chi^2 / \\nu\\) e usare tale rapporto per valutare la bontà dell’adattamento. Valori minori di 3 o 4 suggeriscono che il modello ben si adatta ai dati.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#lerrore-standard-della-misurazione-e-il-modello-fattoriale",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#lerrore-standard-della-misurazione-e-il-modello-fattoriale",
    "title": "26  Il modello statistico dell’analisi fattoriale",
    "section": "26.7 L’errore standard della misurazione e il modello fattoriale",
    "text": "26.7 L’errore standard della misurazione e il modello fattoriale\nIn questa sezione, approfondiamo la connessione tra l’errore standard di misurazione, un concetto fondamentale della Classical Test Theory (CTT), e l’applicazione del modello fattoriale. Questa connessione ci permette di reinterpretare l’errore standard di misurazione attraverso il prisma dell’analisi fattoriale. Procediamo con un’esposizione dettagliata.\nAll’interno della CTT, si afferma che il punteggio ottenuto (\\(X\\)) in un test corrisponde alla somma del valore vero (\\(T\\)) e dell’errore di misurazione (\\(E\\)), dove \\(E\\) è considerato una variabile casuale indipendente da \\(T\\). Se focalizziamo l’attenzione sul soggetto \\(i\\)-esimo, la formula diventa \\(X_i = T_i + E_i\\), con \\(T_i\\) rappresentante il valore vero e \\(E_i\\) l’errore di misurazione, quest’ultimo avente media zero.\nTrasformiamo questa relazione nel contesto di un modello fattoriale monofattoriale che coinvolge \\(p\\) variabili osservate (o item). Per ogni item, la relazione è espressa come:\n\\[\n\\begin{equation}\n\\begin{aligned}\nY_{1i} &=  \\lambda_1 \\xi_i + \\delta_{1i} \\notag\\\\\nY_{2i} &=  \\lambda_2 \\xi_i + \\delta_{2i} \\notag\\\\\n  \\dots\\notag\\\\\nY_{pi} &=  \\lambda_p \\xi_i + \\delta_{pi}, \\notag\n\\end{aligned}\n\\end{equation}\n\\]\ndove \\(Y_{ji}\\) rappresenta il punteggio osservato per l’item \\(j\\) del soggetto \\(i\\), \\(\\lambda_j\\) è il carico fattoriale dell’item \\(j\\) sul fattore comune \\(\\xi_i\\), e \\(\\delta_{ji}\\) è l’errore unico associato all’item \\(j\\) per il soggetto \\(i\\).\nIl punteggio totale \\(X_i\\) per il soggetto \\(i\\)-esimo deriva dalla somma dei punteggi di ciascun item, il che si traduce in:\n\\[\n\\begin{equation}\n\\begin{aligned}\nX_i &= \\sum_{j=1}^p Y_{ji} = \\sum_{j=1}^p \\lambda_j \\xi_i + \\sum_{j=1}^p \\delta_{ji}\\notag\\\\[12pt]\n  &=  \\left( \\sum_{j=1}^p \\lambda_j \\right) \\xi_i  +  \\sum_{j=1}^p \\delta_{ji} \\notag\\\\[12pt]\n  &= T_i + E_i\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nRispettando la struttura della CTT, la varianza del punteggio osservato \\(X_i\\) si decompone in due componenti fondamentali: la varianza del valore vero \\(\\sigma^2_{T_i}\\) e la varianza dell’errore \\(\\sigma^2_{E_i}\\). Nel contesto dell’analisi fattoriale, \\(\\sigma^2_{T_i}\\) corrisponde al quadrato della somma dei carichi fattoriali:\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\sigma^2_{T_i} &= \\mathbb{V}\\left[ \\left( \\sum_{j=1}^p \\lambda_j \\right) \\xi_i \\right]\\notag\\\\\n&= \\left( \\sum_{j=1}^p \\lambda_j \\right)^2 \\mathbb{V}(\\xi_i)\\notag\\\\\n&= \\left( \\sum_{j=1}^p \\lambda_j \\right)^2 \\notag\n\\end{aligned}\n\\end{equation}\n\\]\nInoltre, considerando la varianza dell’errore di misurazione \\(\\sigma^2_{E_i}\\) nel contesto fattoriale, questa è equivalente alla somma delle varianze degli errori unici (\\(\\delta_{ji}\\)), ovvero le unicità:\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\sigma^2_{E_i} &= \\mathbb{V}\\left( \\sum_{j=1}^p \\delta_{ji} \\right)\\notag\\\\\n&= \\sum_{j=1}^p \\mathbb{V}\\left( \\delta_{ji} \\right)\\notag\\\\\n&= \\sum_{j=1}^p \\Psi_j\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nPertanto, nel contesto dell’analisi fattoriale, l’errore standard di misurazione per il punteggio totale del test è quantificabile come la radice quadrata della somma delle unicità:\n\\[\n\\begin{equation}\n\\sigma_{E} = \\sqrt{\\sum_{j=1}^p \\Psi_j}\n\\end{equation}\n\\](eq-err-stnd-meas-FA)\nQuesto collegamento tra la CTT e l’analisi fattoriale offre una prospettiva rinnovata sull’errore standard di misurazione, arricchendo la nostra comprensione della precisione dei test psicometrici.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#un-esempio-concreto",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#un-esempio-concreto",
    "title": "26  Il modello statistico dell’analisi fattoriale",
    "section": "26.8 Un esempio concreto",
    "text": "26.8 Un esempio concreto\nApplichiamo ora il risultato precedente ad un caso concreto. Consideriamo i dati utilizzati nella validazione italiana del Cognitive Style Questionnaire - Short Form (CSQ-SF, Meins et al. 2012). Il CSQ-SF viene utilizzato per misurare la vulnerabilità all’ansia e alla depressione. È costituito da cinque sottoscale: Internality, Globality, Stability, Negative consequences e Self-worth.\nLeggiamo i dati in \\(\\textsf{R}\\):\n\ncsq &lt;- rio::import(here::here(\"data\", \"csq540.csv\"))\n\nIl numero di partecipanti è\n\nn &lt;- nrow(csq)\nn\n\n540\n\n\nLe statistiche descrittive si ottengono con la seguente istruzione:\n\npsych::describe(csq, type = 2) \n\n\nA psych: 5 x 13\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nI\n1\n540\n47.75741\n5.781620\n48\n47.87037\n4.4478\n21\n64\n43\n-0.3065318\n1.0724845\n0.2488013\n\n\nG\n2\n540\n45.00000\n11.936909\n42\n44.54630\n11.8608\n16\n78\n62\n0.3393144\n-0.6990477\n0.5136828\n\n\nS\n3\n540\n44.60370\n12.183141\n42\n44.23843\n13.3434\n16\n77\n61\n0.2671381\n-0.7650876\n0.5242789\n\n\nN\n4\n540\n22.00741\n6.922305\n21\n21.85648\n7.4130\n8\n39\n31\n0.2070885\n-0.7433740\n0.2978886\n\n\nW\n5\n540\n44.04630\n13.103969\n43\n43.65972\n13.3434\n16\n79\n63\n0.3051253\n-0.5305164\n0.5639050\n\n\n\n\n\nEsaminiamo la matrice di correlazione:\n\npsych::pairs.panels(csq) \n\n\n\n\n\n\n\n\nLa sottoscala di Internality è problematica, come messo anche in evidenza dall’autore del test. La consideriamo comunque in questa analisi statistica.\nSpecifichiamo il modello unifattoriale nella sintassi di lavaan:\n\nmod_csq &lt;- \"\n   F =~ NA*I + G + S + N + W\n   F ~~ 1*F\n\" \n\nAdattiamo il modello ai dati:\n\nfit &lt;- lavaan:::cfa(\n  mod_csq,\n  data = csq\n)\n\nEsaminiamo i risultati:\n\nsummary(\n  fit, \n  standardized = TRUE,\n  fit.measures = TRUE\n) |&gt;\n  print()\n\nlavaan 0.6-19 ended normally after 26 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n  Number of observations                           540\n\nModel Test User Model:\n                                                      \n  Test statistic                                46.716\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              2361.816\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.982\n  Tucker-Lewis Index (TLI)                       0.965\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -8741.781\n  Loglikelihood unrestricted model (H1)      -8718.423\n                                                      \n  Akaike (AIC)                               17503.562\n  Bayesian (BIC)                             17546.478\n  Sample-size adjusted Bayesian (SABIC)      17514.734\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.124\n  90 Percent confidence interval - lower         0.093\n  90 Percent confidence interval - upper         0.158\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.989\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.033\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n  F =~                                                         \n    I                 0.725    0.253    2.867    0.004    0.725\n    G               -11.322    0.384  -29.481    0.000  -11.322\n    S               -11.342    0.398  -28.513    0.000  -11.342\n    N                -6.163    0.233  -26.398    0.000   -6.163\n    W               -11.598    0.444  -26.137    0.000  -11.598\n  Std.all\n         \n    0.126\n   -0.949\n   -0.932\n   -0.891\n   -0.886\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n    F                 1.000                               1.000\n   .I                32.840    2.000   16.420    0.000   32.840\n   .G                14.038    1.473    9.532    0.000   14.038\n   .S                19.508    1.718   11.353    0.000   19.508\n   .N                 9.847    0.725   13.573    0.000    9.847\n   .W                36.892    2.685   13.737    0.000   36.892\n  Std.all\n    1.000\n    0.984\n    0.099\n    0.132\n    0.206\n    0.215\n\n\n\nEsaminiamo solo le stime dei parametri del modello:\n\nparameterEstimates(fit) |&gt;\n    print()\n\n   lhs op rhs     est    se       z pvalue ci.lower ci.upper\n1    F =~   I   0.725 0.253   2.867  0.004    0.229    1.220\n2    F =~   G -11.322 0.384 -29.481  0.000  -12.075  -10.569\n3    F =~   S -11.342 0.398 -28.513  0.000  -12.122  -10.563\n4    F =~   N  -6.163 0.233 -26.398  0.000   -6.621   -5.705\n5    F =~   W -11.598 0.444 -26.137  0.000  -12.467  -10.728\n6    F ~~   F   1.000 0.000      NA     NA    1.000    1.000\n7    I ~~   I  32.840 2.000  16.420  0.000   28.920   36.759\n8    G ~~   G  14.038 1.473   9.532  0.000   11.151   16.924\n9    S ~~   S  19.508 1.718  11.353  0.000   16.140   22.876\n10   N ~~   N   9.847 0.725  13.573  0.000    8.425   11.269\n11   W ~~   W  36.892 2.685  13.737  0.000   31.628   42.155\n\n\nRecuperiamo le specificità:\n\npsi &lt;- parameterEstimates(fit)$est[7:11]\npsi |&gt;\n    print()\n\n[1] 32.839665 14.037578 19.508119  9.846927 36.891617\n\n\nStimiamo l’errore standard della misurazione con la @ref(eq:err-stnd-meas-FA):\n\nsqrt(sum(psi)) |&gt;\n    print()\n\n[1] 10.63597\n\n\nApplichiamo ora la formula della TCT:\n\\[\n\\sigma_E = \\sigma_X \\sqrt{1 -\\rho_{XX^\\prime}}.\n\\]\nPer trovare \\(\\sigma\\) calcoliamo prima il punteggio totale:\n\ntot_score &lt;- rowSums(csq)\n\nLa deviazione standard di tot_score ci fornisce una stima di \\(\\sigma_X\\):\n\nsigma &lt;- sd(tot_score)\nsigma |&gt;\n    print()\n\n[1] 41.26414\n\n\nPer applicare la formula della TCT abbiamo bisogno dell’attendibilità. La stimiamo usando la funzione reliability del pacchetto semTools dall’oggetto creato da lavaan:::cfa():\n\nrel &lt;- semTools::reliability(fit)\nrel |&gt;\n    print()\n\n               F\nalpha  0.8506572\nomega  0.9330313\nomega2 0.9330313\nomega3 0.9273385\navevar 0.7916575\n\n\nUtilizzando \\(\\Omega\\) otteniamo:\n\nsigma * sqrt(1- rel[2]) |&gt;\n    print()\n\n[1] 0.2587831\n\n\n10.67846074554\n\n\nSi noti come il risultato sia molto simile a quello trovato con la formula della TCT.\n\n26.8.1 Correlazioni osservate e riprodotte\nLe correlazioni riprodotte dal modello si ottengono nel modo seguente dall’oggetto fit.\n\ncor_mat &lt;- lavInspect(fit, \"cor.ov\")\ncor_mat |&gt;\n    print()\n\n       I      G      S      N      W\nI  1.000                            \nG -0.119  1.000                     \nS -0.117  0.885  1.000              \nN -0.112  0.846  0.830  1.000       \nW -0.111  0.841  0.825  0.789  1.000\n\n\nAbbiamo visto come il modello unifattoriale predice che la correlazione tra due variabili manifeste sia il prodotto delle rispettive correlazioni fattoriali. Estraiamo le saturazioni fattoriali.\n\nl &lt;- inspect(fit, what=\"std\")$lambda\nl |&gt;\n    print()\n\n       F\nI  0.126\nG -0.949\nS -0.932\nN -0.891\nW -0.886\n\n\nPer esempio, se consideriamo I e G, la correlazione predetta dal modello fattoriale tra queste due sottoscale è data dal prodotto delle rispettive saturazioni fattoriali.\n\nl[1] * l[2] \n\n-0.11915121205349\n\n\nLa matrice di correlazioni riprodotte riportata sopra mostra il risultato di questo prodotto per ciascuna coppia di variabili manifeste.\n\nl %*% t(l) |&gt; round(3) |&gt;\n    print()\n\n       I      G      S      N      W\nI  0.016 -0.119 -0.117 -0.112 -0.111\nG -0.119  0.901  0.885  0.846  0.841\nS -0.117  0.885  0.868  0.830  0.825\nN -0.112  0.846  0.830  0.794  0.789\nW -0.111  0.841  0.825  0.789  0.785\n\n\n\n\n26.8.2 Scomposizione della varianza\nConsideriamo la variabile manifesta W. Calcoliamo la varianza.\n\nvar(csq$W) |&gt; print()\n\n[1] 171.714\n\n\nLa varianza riprodotta di questa variabile, secondo il modello fattoriale, dovrebbe esere uguale alla somma di due componenti: la varianza predetta dall’effetto causale del fattore latente e la varianza residua. La varianza predetta dall’effetto causale del fattore latente è uguale alla saturazione elevata al quadrato:\n\n(-11.598)^2 \n\n134.513604\n\n\nCalcolo ora la proporzione di varianza residua normalizzando rispetto alla varianza osservata (non a quella riprodotta dal modello):\n\n1 - (-11.598)^2 / var(csq$W) \n\n0.216641572893455\n\n\nIl valore così ottenuto è molto simile al valore della varianza residua di W.\nRipeto i calcoli per la variabile G\n\n1 - (-11.322)^2 / var(csq$G) \n\n0.1003728851332\n\n\ne per la variabile I\n\n1 - (0.725)^2 / var(csq$I) \n\n0.984275494822392\n\n\nIn tutti i casi, i valori ottenuti sono molto simili alle varianze residue ipotizzate dal modello unifattoriale.\n\n\n26.8.3 Correlazione tra variabili manifeste e fattore comune\nUn modo per verificare il fatto che, nel modello unifattoriale, la saturazione fattoriale della \\(i\\)-esima variabile manifesta è uguale alla correlazione tra i punteggi osservati sulla i$-esima variabile manifesta e il fattore latente è quella di calcolare le correlazioni tra le variabili manifeste e i punteggi fattoriali. I punteggi fattoriali rappresentano una stima del punteggio “vero”, ovvero del punteggio che ciascun rispondente otterrebbe in assenza di errori di misurazione. Vedremo in seguito come si possono stimare i punteggi fattoriali. Per ora ci limitiamo a calcolarli usando lavaan.\n\nhead(lavPredict(fit)) |&gt;\n    print()\n\n              F\n[1,]  0.2693790\n[2,] -0.9110820\n[3,]  0.1871406\n[4,] -0.3315541\n[5,]  0.8306627\n[6,]  1.1534515\n\n\nAbbiamo un punteggio diverso per ciascuno dei 540 individui che appartengono al campione di dati esaminato.\n\ndim(lavPredict(fit))\n\n\n5401\n\n\nCalcoliamo ora le correlazioni tra i valori osservati su ciascuna delle cinque scale del CSQ e le stime dei punteggi veri.\n\nc(\n  cor(csq$I, lavPredict(fit)),\n  cor(csq$G, lavPredict(fit)),\n  cor(csq$S, lavPredict(fit)),\n  cor(csq$N, lavPredict(fit)),\n  cor(csq$W, lavPredict(fit))\n) |&gt; \n  round(3) |&gt;\n    print()\n\n[1]  0.128 -0.970 -0.952 -0.910 -0.905\n\n\nSi noti che i valori ottenui sono molto simili ai valori delle saturazioni fattoriali. La piccola differenza tra le correlazioni ottenute e i valori delle saturazioni fattoriali dipende dal fatto che abbiamo stimato i punteggi fattoriali.\n\ninspect(fit, what=\"std\")$lambda |&gt;\n    print()\n\n       F\nI  0.126\nG -0.949\nS -0.932\nN -0.891\nW -0.886",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/03_analisi_fattoriale_2.html#session-info",
    "href": "chapters/fa/03_analisi_fattoriale_2.html#session-info",
    "title": "26  Il modello statistico dell’analisi fattoriale",
    "section": "26.9 Session Info",
    "text": "26.9 Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] nortest_1.0-4     MASS_7.3-61       ggokabeito_0.1.0 \n [4] viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [7] ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0  \n[10] bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n[13] lavaan_0.6-19     psych_2.4.6.26    scales_1.3.0     \n[16] markdown_1.13     knitr_1.48        lubridate_1.9.3  \n[19] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n[22] purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n[25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n[28] here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.16.0  jsonlite_1.8.9     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5.1 farver_2.1.2      \n  [7] nloptr_2.1.1       rmarkdown_2.28     vctrs_0.6.5       \n [10] Cairo_1.6-2        minqa_1.2.8        base64enc_0.1-3   \n [13] rstatix_0.7.2      htmltools_0.5.8.1  broom_1.0.7       \n [16] Formula_1.2-5      htmlwidgets_1.6.4  plyr_1.8.9        \n [19] sandwich_3.1-1     rio_1.2.3          emmeans_1.10.4    \n [22] zoo_1.8-12         uuid_1.2-1         igraph_2.0.3      \n [25] mime_0.12          lifecycle_1.0.4    pkgconfig_2.0.3   \n [28] Matrix_1.7-0       R6_2.5.1           fastmap_1.2.0     \n [31] shiny_1.9.1        digest_0.6.37      OpenMx_2.21.12    \n [34] fdrtool_1.2.18     colorspace_2.1-1   rprojroot_2.0.4   \n [37] Hmisc_5.1-3        fansi_1.0.6        timechange_0.3.0  \n [40] abind_1.4-8        compiler_4.4.1     withr_3.0.1       \n [43] glasso_1.11        htmlTable_2.4.3    backports_1.5.0   \n [46] carData_3.0-5      R.utils_2.12.3     ggsignif_0.6.4    \n [49] corpcor_1.6.10     gtools_3.9.5       tools_4.4.1       \n [52] pbivnorm_0.6.0     foreign_0.8-87     zip_2.3.1         \n [55] httpuv_1.6.15      nnet_7.3-19        R.oo_1.26.0       \n [58] glue_1.8.0         quadprog_1.5-8     promises_1.3.0    \n [61] nlme_3.1-166       lisrelToR_0.3      grid_4.4.1        \n [64] pbdZMQ_0.3-13      checkmate_2.3.2    cluster_2.1.6     \n [67] reshape2_1.4.4     generics_0.1.3     gtable_0.3.5      \n [70] tzdb_0.4.0         R.methodsS3_1.8.2  data.table_1.16.0 \n [73] hms_1.1.3          car_3.1-3          utf8_1.2.4        \n [76] sem_3.1-16         pillar_1.9.0       IRdisplay_1.1     \n [79] rockchalk_1.8.157  later_1.3.2        splines_4.4.1     \n [82] lattice_0.22-6     survival_3.7-0     kutils_1.73       \n [85] tidyselect_1.2.1   miniUI_0.1.1.1     pbapply_1.7-2     \n [88] stats4_4.4.1       xfun_0.48          qgraph_1.9.8      \n [91] arm_1.14-4         stringi_1.8.4      pacman_0.5.1      \n [94] boot_1.3-31        evaluate_1.0.0     codetools_0.2-20  \n [97] mi_1.1             cli_3.6.3          RcppParallel_5.1.9\n[100] IRkernel_1.3.2     rpart_4.1.23       xtable_1.8-4      \n[103] repr_1.1.7         munsell_0.5.1      Rcpp_1.0.13       \n[106] coda_0.19-4.1      png_0.1-8          XML_3.99-0.17     \n[109] parallel_4.4.1     jpeg_0.1-10        lme4_1.1-35.5     \n[112] mvtnorm_1.3-1      openxlsx_4.2.7.1   crayon_1.5.3      \n[115] rlang_1.1.4        multcomp_1.4-26    mnormt_2.1.1      \n\n\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Il modello statistico dell'analisi fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/04_analisi_fattoriale_3.html",
    "href": "chapters/fa/04_analisi_fattoriale_3.html",
    "title": "27  Il modello multifattoriale",
    "section": "",
    "text": "27.1 Fattori ortogonali\nLa teoria dei due fattori ha orientato per diversi anni le ricerche sull’intelligenza, finché Thurstone (1945) non propose una sua modifica, conosciuta come teoria multifattoriale. Secondo Thurstone la covariazione tra le variabili manifeste non può essere spiegata da un unico fattore generale. Invece è necessario ipotizzare l’azione causale di diversi fattori, definiti comuni, i quali si riferiscono solo ad alcune delle variabili considerate.\nIl modello plurifattoriale assume che ciascuna variabile manifesta sia espressa come funzione lineare di un certo numero \\(m\\) di fattori comuni, \\(\\xi_1, \\xi_2, \\dots, \\xi_m\\), responsabili della correlazione con le altre variabili, e di un solo fattore specifico (termine d’errore), responsabile della variabilità della variabile stessa. Per \\(p\\) variabili manifeste, \\(Y_1, Y_2, \\dots, Y_p\\), il modello fattoriale diventa quello indicato dal sistema di equazioni lineari descritto di seguito. Idealmente, \\(m\\) dovrebbe essere molto più piccolo di \\(p\\) così da consentire una descrizione parsimoniosa delle variabili manifeste in funzione di pochi fattori soggiacenti.\nLe variabili manifeste \\(Y\\) sono indicizzate da \\(i = 1, \\dots, p.\\) Le variabili latenti \\(\\xi\\) (fattori) sono indicizzate da \\(j = 1, \\dots, m.\\) I fattori specifici \\(\\delta\\) sono indicizzati da \\(i = 1, \\dots, p.\\) Le saturazioni fattoriali si distinguono dunque tramite due indici, \\(i\\) e \\(j\\): il primo indice si riferisce alle variabili manifeste, il secondo si riferisce ai fattori latenti.\nIndichiamo con \\(\\mu_i\\), con \\(i=1, \\dots, p\\) le medie delle \\(p\\) variabili manifeste \\(Y_1, Y_2, \\dots, Y_p\\). Se non vi è alcun effetto delle variabili comuni latenti, allora la variabile \\(Y_{ijk}\\), dove \\(k\\) è l’indice usato per i soggetti, sarà uguale a:\n\\[\n\\begin{equation}\n\\begin{cases}\n  Y_{1k}    &= \\mu_1 + \\delta_{1k} \\\\\n&\\vdots\\\\\nY_{ik}   &= \\mu_i + \\delta_{ik}\\\\\n&\\vdots\\\\\nY_{pk}   &= \\mu_p + \\delta_{pk} \\notag\n\\end{cases}\n\\end{equation}\n\\]\nSe invece le variabili manifeste rappresentano la somma dell’effetto causale di \\(m\\) fattori comuni e di \\(p\\) fattori specifici, allora possiamo scrivere:\n\\[\n\\begin{equation}\n\\begin{cases}\n  Y_1  - \\mu_1  &= \\lambda_{11}\\xi_1 + \\dots + \\lambda_{1k}\\xi_k \\dots +\\lambda_{1m}\\xi_m + \\delta_1 \\\\\n&\\vdots\\\\\nY_i -  \\mu_i  &= \\lambda_{i1}\\xi_1 + \\dots +  \\lambda_{ik}\\xi_k \\dots +\\lambda_{im}\\xi_m + \\delta_i\\\\\n&\\vdots\\\\\nY_p - \\mu_p  &= \\lambda_{p1}\\xi_1 + \\dots +  \\lambda_{pk}\\xi_k \\dots +\\lambda_{pm}\\xi_m + \\delta_p \\notag\n\\end{cases}\n\\end{equation}\n\\]\nNel precedente sistema di equazioni lineari,\nIn conclusione, secondo il modello multifattoriale, le variabili manifeste \\(Y_i\\), con \\(i=1, \\dots, p\\), sono il risultato di una combinazione lineare di \\(m &lt; p\\) fattori inosservabili ad esse comuni \\(\\xi_j\\), con \\(j=1, \\dots, m\\), e di \\(p\\) fattori specifici \\(\\delta_i\\), con \\(i=1, \\dots, p\\), anch’essi inosservabili e di natura residua.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Il modello multifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/04_analisi_fattoriale_3.html#fattori-ortogonali",
    "href": "chapters/fa/04_analisi_fattoriale_3.html#fattori-ortogonali",
    "title": "27  Il modello multifattoriale",
    "section": "",
    "text": "\\(\\xi_j\\), con \\(j=1, \\dots, m\\), rappresenta la \\(j\\)-esima variabile inosservabile a fattore comune (ossia il \\(j\\)-esimo fattore comune a tutte le variabili \\(Y_i\\));\n\\(\\lambda_{ij}\\) rappresenta il parametro, detto saturazione o peso fattoriale, che riflette l’importanza del \\(j\\)-esimo fattore comune nella composizione della \\(i\\)-esima variabile osservabile;\n\\(\\delta_i\\) rappresenta il fattore specifico (o unico) di ogni variabile manifesta \\(Y_i\\).\n\n\n\n27.1.1 Assunzioni del modello multifattoriale\nLe variabili inosservabili a fattore comune \\(\\xi_j\\), con \\(j=1, \\dots, m\\), in quanto latenti, non possiedono unità di misura. Pertanto, per semplicità si assume che abbiano media zero, \\(\\mathbb{E}(\\xi_j)=0\\), abbiano varianza unitaria, \\(\\mathbb{V} (\\xi_j)= \\mathbb{E}(\\xi_j^2) - [\\mathbb{E}(\\xi_j)]^2=1\\), e siano incorrelate tra loro, \\(Cov(\\xi_j, \\xi_h)=0\\), con \\(j, h = 1, \\dots, m; \\;j \\neq h\\). Si assume inoltre che le variabili a fattore specifico \\(\\delta_i\\) siano tra loro incorrelate, \\(Cov(\\delta_i,\\delta_k)=0\\), con \\(i, k = 1, \\dots, p, \\; i \\neq k\\), abbiano media zero, \\(\\mathbb{E}(\\delta_i)=0\\), e varianza uguale a \\(\\mathbb{V} (\\delta_i) = \\psi_{ii}\\). La varianza \\(\\psi_{ii}\\) è detta varianza specifica o unicità della \\(i\\)-esima variabile manifesta \\(Y_i\\). Si assume infine che i fattori specifici siano linearmente incorrelati con i fattori comuni, ovvero \\(Cov(\\xi_j, \\delta_i)=0\\) per ogni \\(j=1, \\dots, m\\) e per ogni \\(i=1\\dots,p\\).\n\n\n27.1.2 Interpretazione dei parametri del modello\n\n27.1.2.1 Covarianza tra variabili e fattori\nNell’ipotesi che le variabili \\(Y_i\\) abbiano media nulla, la covarianza tra \\(Y_i\\) e \\(\\xi_j\\) è uguale alla saturazione fattoriale \\(\\lambda_{ij}\\):\n\\[\n\\begin{equation}\n\\begin{aligned}\n  Cov(Y_i, \\xi_j) &= \\mathbb{E}(Y_i \\xi_j)\\notag\\\\\n  &=\\mathbb{E}\\left[(\\lambda_{i1} \\xi_1 + \\dots + \\lambda_{im} \\xi_m + \\delta_i)\\xi_j \\right]\\notag\\\\\n  &= \\lambda_{i1}\\underbrace{\\mathbb{E}(\\xi_1\\xi_j)}_{=0} + \\dots +\n\\lambda_{ij}\\underbrace{\\mathbb{E}(\\xi_j^2)}_{=1} + \\dots \\notag\\\\\n& \\; + \\lambda_{im}\\underbrace{\\mathbb{E}(\\xi_m\\xi_j)}_{=0} +\n  \\underbrace{\\mathbb{E}(\\delta_i \\xi_j)}_{=0}\\notag\\\\\n  &= \\lambda_{ij}.\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nAnche nel modello multifattoriale, dunque, le saturazioni fattoriali rappresentano le covarianze tra le variabili e i fattori:\n\\[\nCov(Y_i, \\xi_j) = \\lambda_{ij} \\qquad i=1, \\dots, p; \\quad j= 1, \\dots, m.\n\\]\nNaturalmente, se le variabili sono standardizzate, le saturazioni fattoriali diventano correlazioni:\n\\[\nr_{ij} = \\lambda_{ij}.\n\\]\n\n\n27.1.2.2 Espressione fattoriale della varianza\nCome nel modello monofattoriale, la varianza delle variabili manifeste si decompone in una componente dovuta ai fattori comuni, chiamata comunalità, e in una componente specifica alle \\(Y_i\\), chiamata unicità. Nell’ipotesi che le variabili \\(Y_i\\) abbiano media nulla, la varianza di \\(Y_i\\) è uguale a\n\\[\n\\begin{aligned}\n  \\mathbb{V} (Y_i)\n  &=\\mathbb{E}\\left[ (\\lambda_{i1} \\xi_1 + \\dots +\n    \\lambda_{im} \\xi_m + \\delta_i)^2 \\right].\n\\end{aligned}\n\\tag{27.1}\\]\nCome si sviluppa il polinomio precedente? Il quadrato di un polinomio è uguale alla somma dei quadrati di tutti i termini più il doppio prodotto di ogni termine per ciascuno di quelli che lo seguono. Il valore atteso del quadrato del primo termine è uguale a \\(\\lambda_{i1}^2\\mathbb{E}(\\xi_1^2)\\) ma, essendo la varianza di \\(\\xi_1\\) uguale a \\(1\\), otteniamo semplicemente \\(\\lambda_{i1}^2\\). Lo stesso vale per i quadrati di tutti i termini seguenti tranne l’ultimo. Infatti, \\(\\mathbb{E}(\\delta_i^2)=\\psi_{ii}\\). Per quel che riguarda i doppi prodotti, sono tutti nulli. In primo luogo perché, nel caso di fattori ortogonali, la covarianza tra i fattori comuni è nulla, \\(\\mathbb{E}(\\xi_j \\xi_h)=0\\), con \\(j \\neq h\\). In secondo luogo perché il fattori comuni cono incorrelati con i fattori specifici, quindi \\(\\mathbb{E}(\\delta_i \\xi_j)=0\\).\nIn conclusione,\n\\[\n\\begin{aligned}\n  \\mathbb{V}(Y_i) &= \\lambda_{i1}^2 + \\lambda_{i2}^2 + \\dots + \\lambda_{im}^2 + \\psi_{ii} \\notag\\\\\n  &= \\sum_{j=1}^m \\lambda_{ij}^2 + \\psi_{ii}\\notag\\\\\n  &= h_i^2 + \\psi_{ii}\\notag\\\\\n  &=\\text{communalità} + \\text{unicità},\\notag\n\\end{aligned}\n\\]\nla varianza della variabile manifesta \\(Y_i\\) è suddivisa in due parti: il primo addendo è definito comunalità poiché rappresenta la parte di variabilità della \\(Y_i\\) spiegata dai fattori comuni; il secondo addendo è invece definito varianza specifica (o unicità) poiché esprime la parte di variabilità della \\(Y_i\\) non spiegata dai fattori comuni.\n\n\n27.1.2.3 Espressione fattoriale della covarianza\nQuale esempio, consideriamo il caso di \\(p=5\\) variabili osservabili e \\(m=2\\) fattori ortogonali. Se le variabili manifeste sono ‘centrate’ (ovvero, se a ciascuna di esse sottraiamo la rispettiva media), allora il modello multifattoriale diventa\n\\[\n\\begin{aligned}\n  Y_1 &= \\lambda_{11} \\xi_1 + \\lambda_{12} \\xi_2 + \\delta_1,\\notag\\\\\n  Y_2 &= \\lambda_{21} \\xi_1 + \\lambda_{22} \\xi_2 + \\delta_2,\\notag\\\\\n  Y_3 &= \\lambda_{31} \\xi_1 + \\lambda_{32} \\xi_2 + \\delta_3,\\notag\\\\\n  Y_4 &= \\lambda_{41} \\xi_1 + \\lambda_{42} \\xi_2 + \\delta_4,\\notag\\\\\n  Y_5 &= \\lambda_{51} \\xi_1 + \\lambda_{52} \\xi_2 + \\delta_5.\\notag\n\\end{aligned}\n\\]\nNell’ipotesi che le variabili \\(Y_i\\) abbiano media nulla, la covarianza tra \\(Y_1\\) e \\(Y_2\\), ad esempio, è uguale a:\n\\[\n\\begin{aligned}\n  Cov(Y_1, Y_2) &= \\mathbb{E}\\left( Y_1 Y_2\\right) \\notag\\\\\n  &= \\mathbb{E}\\left[\n  (\\lambda_{11} \\xi_1 + \\lambda_{12} \\xi_2 + \\delta_1)\n   (\\lambda_{21} \\xi_1 + \\lambda_{22} \\xi_2 +  \\delta_2)\n  \\right]\\notag\\\\\n  &= \\lambda_{11} \\lambda_{21} \\mathbb{E}(\\xi_1^2) +\n      \\lambda_{11} \\lambda_{22} \\mathbb{E}(\\xi_1 \\xi_2) +\\notag\n      \\lambda_{11} \\mathbb{E}(\\xi_1 \\delta_2) +\\notag\\\\\n    &\\quad \\lambda_{12} \\lambda_{21}\\mathbb{E}(\\xi_1 \\xi_2)\\, +\n      \\lambda_{12} \\lambda_{22}\\mathbb{E}(\\xi^2_2)\\, +\n      \\lambda_{12} \\mathbb{E}(\\xi_2\\delta_2) +\\notag\\\\\n    &\\quad \\lambda_{21} \\mathbb{E}(\\xi_1\\delta_1) +\\notag\n     \\lambda_{22} \\mathbb{E}(\\xi_2\\delta_1) + \\mathbb{E}(\\delta_1 \\delta_2)\\notag\\\\\n   &= \\lambda_{11} \\lambda_{21} + \\lambda_{12} \\lambda_{22}.\\notag\n\\end{aligned}\n\\]\nIn conclusione, la covarianza tra le variabili manifeste \\(Y_l\\) e \\(Y_m\\) riprodotta dal modello è data dalla somma dei prodotti delle saturazioni \\(\\lambda_l \\lambda_m\\) nei due fattori.\nEsempio. Consideriamo i dati riportati da Brown (2015), ovvero otto misure di personalità raccolte su un campione di 250 pazienti che hanno concluso un programma di psicoterapia. Le scale sono le seguenti:\n\nanxiety (N1),\nhostility (N2),\ndepression (N3),\nself-consciousness (N4),\nwarmth (E1),\ngregariousness (E2),\nassertiveness (E3),\npositive emotions (E4).\n\n\nvarnames &lt;- c(\"N1\", \"N2\", \"N3\", \"N4\", \"E1\", \"E2\", \"E3\", \"E4\")\nsds &lt;- '5.7  5.6  6.4  5.7  6.0  6.2  5.7  5.6'\n\ncors &lt;- '\n    1.000\n    0.767  1.000 \n    0.731  0.709  1.000 \n    0.778  0.738  0.762  1.000 \n    -0.351  -0.302  -0.356  -0.318  1.000 \n    -0.316  -0.280  -0.300  -0.267  0.675  1.000 \n    -0.296  -0.289  -0.297  -0.296  0.634  0.651  1.000 \n    -0.282  -0.254  -0.292  -0.245  0.534  0.593  0.566  1.000\n'\n\npsychot_cor_mat &lt;- getCov(cors, names = varnames)\nn &lt;- 250\n\nEseguiamo l’analisi fattoriale esplorativa con il metodo della massima verosimiglianza ipotizzando due fattori comuni incorrelati:\n\nn_facs &lt;- 2\nfit_efa &lt;- factanal(\n  covmat = psychot_cor_mat,\n  factors = n_facs,\n  rotation = \"varimax\",\n  n.obs = n\n)\n\nEsaminiamo le saturazioni fattoriali:\n\nlambda &lt;- fit_efa$loadings\nprint(lambda)\n\n\nLoadings:\n   Factor1 Factor2\nN1  0.854  -0.228 \nN2  0.826  -0.194 \nN3  0.811  -0.233 \nN4  0.865  -0.186 \nE1 -0.202   0.773 \nE2 -0.139   0.829 \nE3 -0.158   0.771 \nE4 -0.147   0.684 \n\n               Factor1 Factor2\nSS loadings      2.923   2.526\nProportion Var   0.365   0.316\nCumulative Var   0.365   0.681\n\n\nLa soluzione fattoriale conferma la presenza di due fattori: il primo fattore satura sulle scale di neutoricismo, il secono sulle scale di estroversione.\nLa correlazione riprodotta \\(r_{12}\\) è uguale a \\(\\lambda_{11}\\lambda_{21} + \\lambda_{12}\\lambda_{22}\\)\n\nlambda[1, 1] * lambda[2, 1] + lambda[1, 2] * lambda[2, 2]\n\n0.749284399375181\n\n\ne corrisponde da vicino alla correlazione osservata 0.767.\nL’intera matrice di correlazioni riprodotte è \\(\\boldsymbol{\\Lambda} \\boldsymbol{\\Lambda}^{\\mathsf{T}} + \\boldsymbol{\\psi}\\):\n\nRr &lt;- lambda %*% t(lambda) + diag(fit_efa$uniq)\nRr %&gt;% \n  round(3)\n\n\nA matrix: 8 x 8 of type dbl\n\n\n\nN1\nN2\nN3\nN4\nE1\nE2\nE3\nE4\n\n\n\n\nN1\n1.000\n0.749\n0.745\n0.781\n-0.348\n-0.307\n-0.311\n-0.281\n\n\nN2\n0.749\n1.000\n0.715\n0.751\n-0.317\n-0.276\n-0.281\n-0.254\n\n\nN3\n0.745\n0.715\n1.000\n0.745\n-0.344\n-0.306\n-0.308\n-0.279\n\n\nN4\n0.781\n0.751\n0.745\n1.000\n-0.318\n-0.274\n-0.280\n-0.254\n\n\nE1\n-0.348\n-0.317\n-0.344\n-0.318\n1.000\n0.669\n0.628\n0.558\n\n\nE2\n-0.307\n-0.276\n-0.306\n-0.274\n0.669\n1.000\n0.661\n0.587\n\n\nE3\n-0.311\n-0.281\n-0.308\n-0.280\n0.628\n0.661\n1.000\n0.550\n\n\nE4\n-0.281\n-0.254\n-0.279\n-0.254\n0.558\n0.587\n0.550\n1.000\n\n\n\n\n\nLa differenza tra la matrice di correlazioni riprodotte e la matrice di correlazioni osservate è uguale a:\n\n(psychot_cor_mat - Rr) %&gt;% \n  round(3)\n\n\nA matrix: 8 x 8 of type dbl\n\n\n\nN1\nN2\nN3\nN4\nE1\nE2\nE3\nE4\n\n\n\n\nN1\n0.000\n0.018\n-0.014\n-0.003\n-0.003\n-0.009\n0.015\n-0.001\n\n\nN2\n0.018\n0.000\n-0.006\n-0.013\n0.015\n-0.004\n-0.008\n0.000\n\n\nN3\n-0.014\n-0.006\n0.000\n0.017\n-0.012\n0.006\n0.011\n-0.013\n\n\nN4\n-0.003\n-0.013\n0.017\n0.000\n0.000\n0.007\n-0.016\n0.009\n\n\nE1\n-0.003\n0.015\n-0.012\n0.000\n0.000\n0.006\n0.006\n-0.024\n\n\nE2\n-0.009\n-0.004\n0.006\n0.007\n0.006\n0.000\n-0.010\n0.006\n\n\nE3\n0.015\n-0.008\n0.011\n-0.016\n0.006\n-0.010\n0.000\n0.016\n\n\nE4\n-0.001\n0.000\n-0.013\n0.009\n-0.024\n0.006\n0.016\n0.000",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Il modello multifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/04_analisi_fattoriale_3.html#fattori-obliqui",
    "href": "chapters/fa/04_analisi_fattoriale_3.html#fattori-obliqui",
    "title": "27  Il modello multifattoriale",
    "section": "27.2 Fattori obliqui",
    "text": "27.2 Fattori obliqui\nAnche nel caso di fattori comuni correlati è possibile esprimere nei termini dei parametri del modello la covarianza teorica tra una variabile manifesta \\(Y_i\\) e uno dei fattori comuni, la covarianza teorica tra due variabili manifeste, e la comunalità di ciascuna variabile manifesta. Dato però che i fattori comuni risultano correlati, l’espressione fattoriale di tali quantità è più complessa che nel caso di fattori comuni ortogonali.\n\n27.2.1 Covarianza teorica tra variabili e fattori\nIn base al modello multifattoriale con \\(m\\) fattori comuni la variabile \\(Y_i\\) è\n\\[\nY_i = \\lambda_{i1} \\xi_1 + \\dots + \\lambda_{im} \\xi_m + \\delta_i.\n(\\#eq:mod-multifact)\n\\]\nPoniamoci il problema di trovare la covarianza teorica tra la variabile manifesta \\(Y_i\\) e il fattore comune \\(\\xi_j\\). Come in precedenza, il problema si riduce a quello di trovare \\(\\mathbb{E}(Y_i \\xi_j)\\). Ne segue che\n\\[\n\\begin{equation}\n\\begin{aligned}\n  Cov(Y_i, \\xi_j) &= \\mathbb{E}(Y_i \\xi_j)\\notag\\\\\n  &=\\mathbb{E}\\left[(\\lambda_{i1} \\xi_1 + \\dots + \\lambda_{ij} \\xi_j + \\dots + \\lambda_{im} \\xi_m + \\delta_i)\\xi_j \\right]\\notag\\\\\n  &= \\lambda_{i1}\\underbrace{\\mathbb{E}(\\xi_1\\xi_j)}_{\\neq 0} + \\dots + \\lambda_{ij}\\underbrace{\\mathbb{E}(\\xi_j^2)}_{=1} + \\dots \\notag\\\\\n& \\quad + \\lambda_{im}\\underbrace{\\mathbb{E}(\\xi_m\\xi_j)}_{\\neq 0} + \\underbrace{\\mathbb{E}(\\delta_i \\xi_j)}_{=0}\\notag\\\\\n  &= \\lambda_{ij} + \\lambda_{i1} Cov(\\xi_1, \\xi_j) + \\dots + \\lambda_{im} Cov(\\xi_m, \\xi_j).\n\\end{aligned}\n\\end{equation}\n\\]\nAd esempio, nel caso di tre fattori comuni \\(\\xi_1, \\xi_2, \\xi_3\\), la covarianza tra \\(Y_1\\) e \\(\\xi_{1}\\) diventa\n\\[\n\\lambda_{11} + \\lambda_{12}Cov(\\xi_1, \\xi_2) + \\lambda_{13}Cov(\\xi_1, \\xi_3).\n\\]\n\n\n27.2.2 Espressione fattoriale della varianza\nPoniamoci ora il problema di trovare la varianza teorica della variabile manifesta \\(Y_i\\). In base al modello fattoriale, la variabile \\(Y_i\\) è specificata come nella @ref(eq:mod-multifact). La varianza di \\(Y_i\\) è \\(\\mathbb{V}(Y_i) = \\mathbb{E}(Y_i^2) -[\\mathbb{E}(Y_i)]^2\\). Però, avendo espresso \\(Y_i\\) nei termini della differenza dalla sua media, l’espressione della varianza si riduce a \\(\\mathbb{V}(Y_i) = \\mathbb{E}(Y_i^2)\\). Dobbiamo dunque sviluppare l’espressione\n\\[\n\\mathbb{E}(Y_i^2) = \\mathbb{E}[(\\lambda_{i1} \\xi_1 + \\dots + \\lambda_{im} \\xi_m + \\delta_i)^2].\n\\]\nIn conclusione, la varianza teorica di \\(Y_i\\) è uguale a\n\\[\n\\begin{equation}\n\\begin{split}\n\\mathbb{V}(Y_i) &= \\lambda_{i1}^2 + \\lambda_{i2}^2 + \\dots + \\lambda_{im}^2  + \\\\\n&\\quad 2 \\lambda_{i1} \\lambda_{i2} Cov(\\xi_1, \\xi_2) + \\dots + 2 \\lambda_{i,m-1} \\lambda_{im} Cov(\\xi_{m-1}, \\xi_m) + \\\\\n&\\quad \\psi_{ii}.\\notag\n\\end{split}\n\\end{equation}\n\\]\nAd esempio, nel caso di tre fattori comuni, \\(\\xi_1, \\xi_2, \\xi_3\\), la varianza di \\(Y_1\\) è\n\\[\n\\begin{equation}\n\\begin{split}\n\\mathbb{V}(Y_1) = &\\lambda_{11}^2 + \\lambda_{12}^2 + \\lambda_{13}^2 +\\\\\n&\\quad 2 \\lambda_{11} \\lambda_{12} Cov(\\xi_1, \\xi_2) + \\\\\n&\\quad 2 \\lambda_{11} \\lambda_{13} Cov(\\xi_1, \\xi_3) + \\\\\n&\\quad 2 \\lambda_{12} \\lambda_{13} Cov(\\xi_2, \\xi_3) + \\\\\n&\\quad \\psi_{11}. \\notag\n\\end{split}\n\\end{equation}\n\\]\n\n\n27.2.3 Covarianza teorica tra due variabili\nConsideriamo ora il caso più semplice di due soli fattori comuni correlati e calcoliamo la covarianza tra \\(Y_1\\) e \\(Y_2\\):\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathbb{E}(Y_1 Y_2) =\\mathbb{E}[(&\\lambda_{11}\\xi_1 + \\lambda_{12}\\xi_2+\\delta_1) (\\lambda_{21}\\xi_1 + \\lambda_{22}\\xi_2+\\delta_2)]\\notag\\\\\n=\\mathbb{E}(\n&\\lambda_{11}\\lambda_{21}\\xi_1^2 +\n\\lambda_{11}\\lambda_{22}\\xi_1\\xi_2 +\n\\lambda_{11}\\xi_1\\delta_2 +\\notag\\\\\n+&\\lambda_{12}\\lambda_{21}\\xi_1\\xi_2 +\n\\lambda_{12}\\lambda_{22}\\xi_2^2 +\n\\lambda_{12}\\xi_2\\delta_2 +\\notag\\\\\n+&\\lambda_{21}\\xi_1\\delta_1 +\n\\lambda_{22}\\xi_2\\delta_1 +\n\\delta_1\\delta_2).\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nDistribuendo l’operatore di valore atteso, dato che \\(\\mathbb{E}(\\xi^2)=1\\) e \\(\\mathbb{E}(\\xi \\delta)=0\\), otteniamo\n\\[\nCov(Y_1, Y_2) = \\lambda_{11} \\lambda_{21} + \\lambda_{12} \\lambda_{22} +\n\\lambda_{12} \\lambda_{21}Cov(\\xi_1, \\xi_2) +\\lambda_{11} \\lambda_{22}Cov(\\xi_1, \\xi_2).\n\\]\nIn termini matriciali si scrive\n\\[\n\\boldsymbol{\\Sigma} =\\boldsymbol{\\Lambda} \\boldsymbol{\\Phi} \\boldsymbol{\\Lambda}^{\\mathsf{T}} + \\boldsymbol{\\Psi},\n\\]\ndove \\(\\boldsymbol{\\Phi}\\) è la matrice di ordine \\(m \\times m\\) di varianze e covarianze tra i fattori comuni e \\(\\boldsymbol{\\Psi}\\) è una matrice diagonale di ordine \\(p\\) con le unicità delle variabili.\nEsempio. Consideriamo nuovamente i dati esaminati negli esercizi precedenti, ma questa volta il modello consente una correlazione tra i due fattori comuni:\n\nefa_result &lt;- fa(\n    psychot_cor_mat, \n    nfactors = 2, \n    n.obs = n, \n    rotate = \"oblimin\"\n)\nprint(efa_result)\n\nFactor Analysis using method =  minres\nCall: fa(r = psychot_cor_mat, nfactors = 2, n.obs = n, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n     MR1   MR2   h2   u2 com\nN1  0.88 -0.02 0.78 0.22   1\nN2  0.85  0.01 0.72 0.28   1\nN3  0.83 -0.04 0.71 0.29   1\nN4  0.90  0.03 0.78 0.22   1\nE1 -0.05  0.77 0.63 0.37   1\nE2  0.03  0.86 0.71 0.29   1\nE3  0.00  0.79 0.63 0.37   1\nE4 -0.01  0.70 0.49 0.51   1\n\n                       MR1  MR2\nSS loadings           3.00 2.45\nProportion Var        0.37 0.31\nCumulative Var        0.37 0.68\nProportion Explained  0.55 0.45\nCumulative Proportion 0.55 1.00\n\n With factor correlations of \n      MR1   MR2\nMR1  1.00 -0.43\nMR2 -0.43  1.00\n\nMean item complexity =  1\nTest of the hypothesis that 2 factors are sufficient.\n\ndf null model =  28  with the objective function =  5.02 with Chi Square =  1231.22\ndf of  the model are 13  and the objective function was  0.04 \n\nThe root mean square of the residuals (RMSR) is  0.01 \nThe df corrected root mean square of the residuals is  0.02 \n\nThe harmonic n.obs is  250 with the empirical chi square  1.73  with prob &lt;  1 \nThe total n.obs was  250  with Likelihood Chi Square =  9.65  with prob &lt;  0.72 \n\nTucker Lewis Index of factoring reliability =  1.006\nRMSEA index =  0  and the 90 % confidence intervals are  0 0.047\nBIC =  -62.12\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   MR1  MR2\nCorrelation of (regression) scores with factors   0.96 0.94\nMultiple R square of scores with factors          0.93 0.87\nMinimum correlation of possible factor scores     0.85 0.75\n\n\n\nfa.diagram(efa_result)\n\n\n\n\n\n\n\n\nEsaminiamo la matrice delle correlazioni residue:\n\nresiduals &lt;- residuals(efa_result)\nprint(residuals)\n\n   N1    N2    N3    N4    E1    E2    E3    E4   \nN1  0.22                                          \nN2  0.02  0.28                                    \nN3 -0.01  0.00  0.29                              \nN4  0.00 -0.01  0.02  0.22                        \nE1  0.00  0.01 -0.01  0.00  0.37                  \nE2 -0.01  0.00  0.01  0.01  0.01  0.29            \nE3  0.01 -0.01  0.01 -0.02  0.01 -0.01  0.37      \nE4  0.00  0.00 -0.01  0.01 -0.02  0.01  0.01  0.51\n\n\nEsaminiamo più da vicino la matrice di correlazioni riprodotta dal modello, nel caso di fattori obliqui. Le saturazioni fattoriali sono:\n\n# Estrai i carichi fattoriali (saturazioni fattoriali)\nlambda &lt;- efa_result$loadings\n\n# Converti i carichi in una matrice 8 x 2 (assumendo 2 fattori)\n# e assegna i nomi appropriati alle righe e alle colonne\nlambda &lt;- matrix(lambda[, 1:2], nrow = 8, ncol = 2)\nrownames(lambda) &lt;- c(\"N1\", \"N2\", \"N3\", \"N4\", \"E1\", \"E2\", \"E3\", \"E4\")\ncolnames(lambda) &lt;- c(\"Factor1\", \"Factor2\")\n\n# Stampa la matrice dei carichi\nprint(lambda)\n\n        Factor1     Factor2\nN1  0.877075569 -0.01577815\nN2  0.852280802  0.01128419\nN3  0.826584447 -0.03684789\nN4  0.898762806  0.03121279\nE1 -0.048589010  0.77186847\nE2  0.034700239  0.85566012\nE3  0.002815265  0.79291602\nE4 -0.007884591  0.69545191\n\n\nLa matrice di intercorrelazoni fattoriali è\n\n# Estrai la matrice delle intercorrelazioni fattoriali\nPhi &lt;- efa_result$Phi\n\n# Stampa la matrice delle intercorrelazioni\nprint(Phi)\n\n           MR1        MR2\nMR1  1.0000000 -0.4313609\nMR2 -0.4313609  1.0000000\n\n\nLe varianze residue sono:\n\n# Estrai le varianze residue\ndiag(efa_result$uniquenesses) |&gt;\n    round(2)\n\n\nA matrix: 8 x 8 of type dbl\n\n\n0.22\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n0.00\n0.28\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n0.00\n0.00\n0.29\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n0.00\n0.00\n0.00\n0.22\n0.00\n0.00\n0.00\n0.00\n\n\n0.00\n0.00\n0.00\n0.00\n0.37\n0.00\n0.00\n0.00\n\n\n0.00\n0.00\n0.00\n0.00\n0.00\n0.29\n0.00\n0.00\n\n\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.37\n0.00\n\n\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.51\n\n\n\n\n\nMediante i parametri del modello la matrice di correlazione si riproduce nel modo seguente:\n\\[\n\\boldsymbol{\\Sigma} =\\boldsymbol{\\Lambda} \\boldsymbol{\\Phi} \\boldsymbol{\\Lambda}^{\\mathsf{T}} + \\boldsymbol{\\Psi}.\n\\]\nIn \\(\\textsf{R}\\) scriviamo:\n\nR_hat &lt;- lambda %*% Phi %*% t(lambda) + Psi\nR_hat |&gt; \n  round(2)\n\n\nA matrix: 8 x 8 of type dbl\n\n\n\nN1\nN2\nN3\nN4\nE1\nE2\nE3\nE4\n\n\n\n\nN1\n1.00\n0.75\n0.75\n0.78\n-0.35\n-0.31\n-0.31\n-0.28\n\n\nN2\n0.75\n1.00\n0.71\n0.75\n-0.32\n-0.28\n-0.28\n-0.25\n\n\nN3\n0.75\n0.71\n1.00\n0.74\n-0.34\n-0.31\n-0.31\n-0.28\n\n\nN4\n0.78\n0.75\n0.74\n1.00\n-0.32\n-0.27\n-0.28\n-0.25\n\n\nE1\n-0.35\n-0.32\n-0.34\n-0.32\n1.00\n0.67\n0.63\n0.55\n\n\nE2\n-0.31\n-0.28\n-0.31\n-0.27\n0.67\n1.00\n0.67\n0.59\n\n\nE3\n-0.31\n-0.28\n-0.31\n-0.28\n0.63\n0.67\n1.00\n0.55\n\n\nE4\n-0.28\n-0.25\n-0.28\n-0.25\n0.55\n0.59\n0.55\n1.00\n\n\n\n\n\nLe correlazioni residue sono:\n\npsychot_cor_mat - R_hat |&gt;\n  round(2)\n\n\nA matrix: 8 x 8 of type dbl\n\n\n\nN1\nN2\nN3\nN4\nE1\nE2\nE3\nE4\n\n\n\n\nN1\n0.000\n0.017\n-0.019\n-0.002\n-0.001\n-0.006\n0.014\n-0.002\n\n\nN2\n0.017\n0.000\n-0.001\n-0.012\n0.018\n0.000\n-0.009\n-0.004\n\n\nN3\n-0.019\n-0.001\n0.000\n0.022\n-0.016\n0.010\n0.013\n-0.012\n\n\nN4\n-0.002\n-0.012\n0.022\n0.000\n0.002\n0.003\n-0.016\n0.005\n\n\nE1\n-0.001\n0.018\n-0.016\n0.002\n0.000\n0.005\n0.004\n-0.016\n\n\nE2\n-0.006\n0.000\n0.010\n0.003\n0.005\n0.000\n-0.019\n0.003\n\n\nE3\n0.014\n-0.009\n0.013\n-0.016\n0.004\n-0.019\n0.000\n0.016\n\n\nE4\n-0.002\n-0.004\n-0.012\n0.005\n-0.016\n0.003\n0.016\n0.000\n\n\n\n\n\nPer fare un esempio relativo alla correlazione tra due indicatori, calcoliamo la correlazione predetta dal modello tra le variabili \\(Y_1\\) e \\(Y_2\\):\n\nlambda[1, 1] * lambda[2, 1] + lambda[1, 2] * lambda[2, 2] +\n  lambda[1, 1] * lambda[2, 2] * Phi[1, 2] + \n  lambda[1, 2] * lambda[2, 1] * Phi[1, 2] \n\n0.748868098259251\n\n\nQuesto valore si avvicina al valore contenuto dell’elemento (1, 2) della matrice di correlazioni osservate:\n\npsychot_cor_mat[1, 2]\n\n0.767\n\n\nUsando questa procedura possiamo riprodurre tutti gli elementi della matrice di correlazione osservata tramite i parametri stimati dal modello EFA replicando così il risultato che si trova con $ = ^{} + . $",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Il modello multifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/04_analisi_fattoriale_3.html#session-info",
    "href": "chapters/fa/04_analisi_fattoriale_3.html#session-info",
    "title": "27  Il modello multifattoriale",
    "section": "27.3 Session Info",
    "text": "27.3 Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] kableExtra_1.4.0  tidySEM_0.2.7     OpenMx_2.21.12   \n [4] corrplot_0.94     nortest_1.0-4     MASS_7.3-61      \n [7] ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2\n[10] ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n[13] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6   \n[16] semPlot_1.1.6     lavaan_0.6-19     psych_2.4.6.26   \n[19] scales_1.3.0      markdown_1.13     knitr_1.48       \n[22] lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n[25] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5      \n[28] tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n[31] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.1         later_1.3.2          \n  [3] pbdZMQ_0.3-13         XML_3.99-0.17        \n  [5] rpart_4.1.23          fastDummies_1.7.4    \n  [7] lifecycle_1.0.4       rstatix_0.7.2        \n  [9] StanHeaders_2.32.10   rprojroot_2.0.4      \n [11] globals_0.16.3        lattice_0.22-6       \n [13] rockchalk_1.8.157     backports_1.5.0      \n [15] magrittr_2.0.3        openxlsx_4.2.7.1     \n [17] Hmisc_5.1-3           rmarkdown_2.28       \n [19] httpuv_1.6.15         tmvnsim_1.0-2        \n [21] qgraph_1.9.8          zip_2.3.1            \n [23] pkgbuild_1.4.4        pbapply_1.7-2        \n [25] minqa_1.2.8           multcomp_1.4-26      \n [27] abind_1.4-8           quadprog_1.5-8       \n [29] nnet_7.3-19           TH.data_1.1-2        \n [31] sandwich_3.1-1        inline_0.3.19        \n [33] listenv_0.9.1         arm_1.14-4           \n [35] proto_1.0.0           parallelly_1.38.0    \n [37] texreg_1.39.4         svglite_2.1.3        \n [39] codetools_0.2-20      xml2_1.3.6           \n [41] tidyselect_1.2.1      farver_2.1.2         \n [43] lme4_1.1-35.5         matrixStats_1.4.1    \n [45] stats4_4.4.1          base64enc_0.1-3      \n [47] jsonlite_1.8.9        progressr_0.14.0     \n [49] Formula_1.2-5         survival_3.7-0       \n [51] emmeans_1.10.4        systemfonts_1.1.0    \n [53] dbscan_1.2-0          tools_4.4.1          \n [55] Rcpp_1.0.13           glue_1.8.0           \n [57] mnormt_2.1.1          xfun_0.48            \n [59] MplusAutomation_1.1.1 IRdisplay_1.1        \n [61] loo_2.8.0             withr_3.0.1          \n [63] fastmap_1.2.0         boot_1.3-31          \n [65] fansi_1.0.6           digest_0.6.37        \n [67] mi_1.1                timechange_0.3.0     \n [69] R6_2.5.1              mime_0.12            \n [71] estimability_1.5.1    colorspace_2.1-1     \n [73] Cairo_1.6-2           gtools_3.9.5         \n [75] jpeg_0.1-10           utf8_1.2.4           \n [77] generics_0.1.3        data.table_1.16.0    \n [79] corpcor_1.6.10        httr_1.4.7           \n [81] htmlwidgets_1.6.4     pkgconfig_2.0.3      \n [83] sem_3.1-16            gtable_0.3.5         \n [85] bain_0.2.11           htmltools_0.5.8.1    \n [87] carData_3.0-5         blavaan_0.5-6        \n [89] png_0.1-8             rstudioapi_0.16.0    \n [91] tzdb_0.4.0            reshape2_1.4.4       \n [93] uuid_1.2-1            curl_5.2.3           \n [95] coda_0.19-4.1         checkmate_2.3.2      \n [97] nlme_3.1-166          nloptr_2.1.1         \n [99] repr_1.1.7            zoo_1.8-12           \n[101] parallel_4.4.1        miniUI_0.1.1.1       \n[103] nonnest2_0.5-8        foreign_0.8-87       \n[105] pillar_1.9.0          grid_4.4.1           \n[107] vctrs_0.6.5           RANN_2.6.2           \n[109] promises_1.3.0        car_3.1-3            \n[111] xtable_1.8-4          cluster_2.1.6        \n[113] GPArotation_2024.3-1  htmlTable_2.4.3      \n[115] evaluate_1.0.0        pbivnorm_0.6.0       \n[117] gsubfn_0.7            mvtnorm_1.3-1        \n[119] cli_3.6.3             kutils_1.73          \n[121] compiler_4.4.1        rlang_1.1.4          \n[123] crayon_1.5.3          rstantools_2.4.0     \n[125] future.apply_1.11.2   ggsignif_0.6.4       \n[127] fdrtool_1.2.18        plyr_1.8.9           \n[129] stringi_1.8.4         rstan_2.32.6         \n[131] pander_0.6.5          QuickJSR_1.4.0       \n[133] munsell_0.5.1         lisrelToR_0.3        \n[135] CompQuadForm_1.4.3    V8_5.0.1             \n[137] pacman_0.5.1          Matrix_1.7-0         \n[139] IRkernel_1.3.2        hms_1.1.3            \n[141] glasso_1.11           future_1.34.0        \n[143] shiny_1.9.1           igraph_2.0.3         \n[145] broom_1.0.7           RcppParallel_5.1.9   \n\n\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Il modello multifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/05_factor_scores.html",
    "href": "chapters/fa/05_factor_scores.html",
    "title": "28  I punteggi fattoriali",
    "section": "",
    "text": "28.0.1 Esempio di interpretazione\nIl WISC-III (Wechsler Intelligence Scale For Children - III) valuta l’abilità intellettiva di soggetti dai 6 ai 16 anni e 11 mesi. I subtest sono stati selezionati per valutare diverse abilità mentali, che tutte insieme indicano l’abilità intellettiva generale del bambino. Alcuni gli richiedono un ragionamento astratto, altri si focalizzano sulla memoria, altri ancora richiedono certe abilità percettive e così via.\nSi consideri la matrice di correlazione tra i subtest della scala WISC-III riportata dal manuale.\nlower &lt;- '\n1\n.66      1\n.57 .55      1\n.70 .69 .54       1\n.56 .59 .47 .64      1\n.34 .34 .43 .35 .29      1\n.47 .45 .39 .45 .38 .25      1\n.21 .20 .27 .26 .25 .23 .18      1\n.40 .39 .35 .40 .35 .20 .37 .28      1\n.48 .49 .52 .46 .40 .32 .52 .27 .41      1\n.41 .42 .39 .41 .34 .26 .49 .24 .37 .61      1\n.35 .35 .41 .35 .34 .28 .33 .53 .36 .45 .38      1\n.18 .18 .22 .17 .17 .14 .24 .15 .23 .31 .29 .24     1\n'\nwisc_III_cov &lt;- getCov(\n  lower,\n  names = c(\n    \"INFO\", \"SIM\", \"ARITH\", \"VOC\", \"COMP\", \"DIGIT\", \"PICTCOM\",\n    \"CODING\", \"PICTARG\", \"BLOCK\", \"OBJECT\", \"SYMBOL\", \"MAZES\"\n  )\n)\nEseguiamo l’analisi fattoriale con il metodo delle componenti principali e una rotazione Varimax:\nf_pc &lt;- psych::principal(wisc_III_cov, nfactors = 3, rotate = \"varimax\")\nprint(f_pc)\n\nPrincipal Components Analysis\nCall: psych::principal(r = wisc_III_cov, nfactors = 3, rotate = \"varimax\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n          RC1  RC3  RC2   h2   u2 com\nINFO     0.80 0.25 0.09 0.72 0.28 1.2\nSIM      0.81 0.25 0.08 0.72 0.28 1.2\nARITH    0.65 0.26 0.28 0.57 0.43 1.7\nVOC      0.83 0.19 0.13 0.75 0.25 1.2\nCOMP     0.75 0.14 0.16 0.60 0.40 1.2\nDIGIT    0.45 0.06 0.36 0.34 0.66 2.0\nPICTCOM  0.43 0.61 0.02 0.56 0.44 1.8\nCODING   0.10 0.09 0.88 0.79 0.21 1.0\nPICTARG  0.34 0.45 0.27 0.39 0.61 2.6\nBLOCK    0.41 0.66 0.22 0.66 0.34 1.9\nOBJECT   0.31 0.71 0.14 0.62 0.38 1.5\nSYMBOL   0.23 0.32 0.74 0.70 0.30 1.6\nMAZES   -0.06 0.71 0.11 0.51 0.49 1.1\n\n                       RC1  RC3  RC2\nSS loadings           3.80 2.37 1.74\nProportion Var        0.29 0.18 0.13\nCumulative Var        0.29 0.47 0.61\nProportion Explained  0.48 0.30 0.22\nCumulative Proportion 0.48 0.78 1.00\n\nMean item complexity =  1.5\nTest of the hypothesis that 3 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.07 \n\nFit based upon off diagonal values = 0.97\nSi noti che i primi cinque subtest possiedono saturazioni maggiori di \\(0.6\\) sul primo fattore. Dato che questi test sono tutti presentati verbalmente e richiedono delle risposte verbali, tale fattore può essere denominato Comprensione Verbale.\nI subtest “Cifrario” e “Ricerca di simboli” saturano sul secondo fattore. Entrambi i subtest misurano la velocità dei processi di codifica o ricerca. Questo fattore, dunque, può essere denominato Velocità di elaborazione.\nInfine, i subtest “Completamento di figure,” “Disegno con i cubi,” “Riordinamento di storie figurate” e “Labirinti” saturano sul terzo fattore. Tutti questi test condividono una componente geometrica o configurazionale: misurano infatti le abilità necessarie per la manipolazione o la disposizione di immagini, oggetti, blocchi. Questo fattore, dunque, può essere denominato Organizzazione percettiva.\nNel caso di una rotazione ortogonale, la comunalità di ciascuna sottoscala è uguale alla somma delle saturazioni fattoriali al quadrato della sottoscala nei fattori.\nPer le 13 sottoscale del WISC-III abbiamo:\nh2 &lt;- rep(0,13)\nfor (i in 1:13) {\n  h2[i] &lt;- sum(f_pc$loadings[i, ]^2)\n}\nround(h2, 2)\n\n\n0.720.720.570.750.60.340.560.790.390.660.620.70.51\nQuesti risultati replicano quelli riportati nel manuale del test WISC-III.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>I punteggi fattoriali</span>"
    ]
  },
  {
    "objectID": "chapters/fa/05_factor_scores.html#punteggi-fattoriali",
    "href": "chapters/fa/05_factor_scores.html#punteggi-fattoriali",
    "title": "28  I punteggi fattoriali",
    "section": "28.1 Punteggi fattoriali",
    "text": "28.1 Punteggi fattoriali\nFino ad ora abbiamo considerato le strategie di costruzione del modello basate sulla stima e sull’interpretazione delle saturazioni fattoriali e delle comunalità. Questo è il primo passo nella costruzione del modello fattoriale. È però possibile compiere un passo ulteriore, ovvero quello della stima dei punteggi fattoriali (factor scores) i quali risultano utili sia per interpretare i risultati dell’analisi fattoriale che per fare diagnostica. I punteggi fattoriali forniscono le previsioni dei livelli dei fattori latenti per ogni rispondente. Esistono vari metodi di stima dei punteggi fattoriali. Tra questi troviamo il metodo di Thomson basato sulla regressione e il metodo di Bartlett basato sulla massima verosimiglianza. Entrambi questi metodi sono implementati nel software .\n\n28.1.1 Stima dei punteggi fattoriali\nSi definiscono punteggi fattoriali i valori assunti dai fattori comuni (inosservabili) in corrispondenza delle osservazioni campionarie. Il metodo di Thomson stima i punteggi fattoriali in base all’approccio della regressione multipla, ovvero, impiegando la matrice delle correlazioni tra le variabili e la matrice di struttura (ovvero, la matrice delle correlazioni delle variabili con i fattori). Per ottenere le stime dei punteggi fattoriali con il metodo di Thomson è necessario specificare nella funzione factanal() l’opzione scores = \"regression\".\n\n\n28.1.2 Dimostrazione di Thurstone\nPrima di descrivere il metodo della regressione, esaminiamo la dimostrazione che Thurstone (1947) ha fornito per illustrare il significato dei punteggi fattoriali (si veda Loehlin, 1987). L’idea è quella di esaminare la stima dei punteggi fattoriali in una situazione in cui i tali punteggi sono conosciuti, in maniera tale da potere controllare il risultato dell’analisi.\nSi consideri un insieme di 1000 scatole di cui conosciamo le dimensioni \\(x, y, z\\):\n\nset.seed(123)\nn &lt;- 1e3\nx &lt;- rnorm(n, 100, 1.5)\ny &lt;- rnorm(n, 200, 1.5)\nz &lt;- rnorm(n, 300, 1.5)\n\nIl problema è quello di stimare le dimensioni delle scatole disponendo soltanto di una serie di misure indirette, corrotte dal rumore di misura. Thurstone (1947) utilizzò le seguenti trasformazioni delle dimensioni delle scatole (si veda Jennrich, 2007).\n\ns &lt;- 40\ny1 &lt;- rnorm(n, mean(x), s)\ny2 &lt;- rnorm(n, mean(y), s)\ny3 &lt;- rnorm(n, mean(z), s)\ny4 &lt;- x * y + rnorm(n, 0, s)\ny5 &lt;- x * z + rnorm(n, 0, s)\ny6 &lt;- y * z + rnorm(n, 0, s)\ny7 &lt;- x^2 * y + rnorm(n, 0, s)\ny8 &lt;- x * y^2 + rnorm(n, 0, s)\ny9 &lt;- x^2 * z + rnorm(n, 0, s)\ny10 &lt;- x * z^2 + rnorm(n, 0, s)\ny11 &lt;- y^2 * z + rnorm(n, 0, s)\ny12 &lt;- y * z^2 + rnorm(n, 0, s)\ny13 &lt;- y^2 * z + rnorm(n, 0, s)\ny14 &lt;- y * z^2 + rnorm(n, 0, s)\ny15 &lt;- x / y + rnorm(n, 0, s)\ny16 &lt;- y / x + rnorm(n, 0, s)\ny17 &lt;- x / z + rnorm(n, 0, s)\ny18 &lt;- z / x + rnorm(n, 0, s)\ny19 &lt;- y / z + rnorm(n, 0, s)\ny20 &lt;- z / y + rnorm(n, 0, s)\ny21 &lt;- 2 * x + 2*y + rnorm(n, 0, s)\ny22 &lt;- 2 * x + 2*z + rnorm(n, 0, s)\ny23 &lt;- 2 * y + 2*z + rnorm(n, 0, s)\n\nEseguiamo l’analisi fattoriale con una soluzione a tre fattori sui dati così creati.\n\nY &lt;- cbind(\n  y1, y2, y3, y4, y5, y6, y7, y8, y9, \n  y10, y11, y12, y13, y14, y15, y16, \n  y17, y18, y19, y20, y21, y22, y23\n)\n\nfa &lt;- factanal(\n  Y, \n  factors = 3, \n  scores = \"regression\",\n  lower = 0.01\n)\n\nL’opzione scores = \"regression\" richiede il calcolo dei punteggi fattoriali con il metodo della regressione. Nel caso di una rotazione Varimax (default della funzione factanal()), i punteggi fattoriali risultano ovviamente incorrelati:\n\ncor(\n  cbind(fa$scores[, 1], fa$scores[, 2], fa$scores[, 3])\n  ) %&gt;% \n  round(3)\n\n\nA matrix: 3 x 3 of type dbl\n\n\n1.000\n0.002\n-0.001\n\n\n0.002\n1.000\n0.005\n\n\n-0.001\n0.005\n1.000\n\n\n\n\n\nGeneriamo ora i diagrammi di dispersione che mettono in relazione le dimensioni originarie delle scatole (\\(x, y, z\\)) con i punteggi fattoriali sui tre fattori. Se l’analisi ha successo, ci aspettiamo un’alta correlazione tra i punteggi fattoriali di ogni fattore e una sola delle dimensioni delle scatole \\(x\\), \\(y\\), \\(z\\).\n\np1 &lt;- tibble(x, fs1 = fa$scores[, 1]) %&gt;% \n  ggplot(aes(x, fs1)) +\n  geom_point(alpha = 0.2)\np2 &lt;- tibble(y, fs1 = fa$scores[, 1]) %&gt;% \n  ggplot(aes(y, fs1)) +\n  geom_point(alpha = 0.2)\np3 &lt;- tibble(z, fs1 = fa$scores[, 1]) %&gt;% \n  ggplot(aes(z, fs1)) +\n  geom_point(alpha = 0.2)\n\np4 &lt;- tibble(x, fs2 = fa$scores[, 2]) %&gt;% \n  ggplot(aes(x, fs2)) +\n  geom_point(alpha = 0.2)\np5 &lt;- tibble(y, fs2 = fa$scores[, 2]) %&gt;% \n  ggplot(aes(y, fs2)) +\n  geom_point(alpha = 0.2)\np6 &lt;- tibble(z, fs2 = fa$scores[, 2]) %&gt;% \n  ggplot(aes(z, fs2)) +\n  geom_point(alpha = 0.2)\n\np7 &lt;- tibble(x, fs3 = fa$scores[, 3]) %&gt;% \n  ggplot(aes(x, fs3)) +\n  geom_point(alpha = 0.2)\np8 &lt;- tibble(y, fs3 = fa$scores[, 3]) %&gt;% \n  ggplot(aes(y, fs3)) +\n  geom_point(alpha = 0.2)\np9 &lt;- tibble(z, fs3 = fa$scores[, 3]) %&gt;% \n  ggplot(aes(z, fs3)) +\n  geom_point(alpha = 0.2)\n\n\n(p1 | p2 | p3) /\n(p4 | p5 | p6) /\n(p7 | p8 | p9) \n\n\n\n\n\n\n\n\nI risultati riportati nella figura confermano le aspettative.\nIl metodo della regressione pone il problema della stima dei punteggi fattoriali nei termini di una ideale regressione di ogni fattore rispetto a tutte le variabili osservate. Per il fattore \\(j\\)-esimo, si può scrivere la seguente equazione:\n\\[\n\\begin{aligned}\nF_j =& \\beta_{1j}y_1 + \\dots + \\beta_{pm}y_p + \\varepsilon_j\n\\end{aligned}\n\\]\ndove \\(F_j\\) sono i punteggi fattoriali e \\(y\\) sono le variabili osservate standardizzate \\((Y-\\bar{Y})/s\\). In forma matriciale, il modello diventa\n\\[\n\\textbf{F} = \\textbf{y} \\textbf{B} +\n\\boldsymbol{\\varepsilon}\n\\]\nI coefficienti parziali di regressione B sono ignoti. Tuttavia, possono essere calcolati utilizzando i metodi della regressione lineare. Nel modello di regressione, infatti, i coefficienti dei minimi quadrati possono essere calcolati utilizzando due matrici di correlazioni: la matrice \\(\\textbf{R}_{xx}\\) (le correlazioni tra le variabili \\(X\\)) e la matrice \\(\\textbf{R}_{xy}\\) (le correlazioni tra le variabili \\(X\\) e la variabile \\(Y\\):\n\\[\n\\hat{\\textbf{B}} = \\textbf{R}_{xx}^{-1}\\textbf{R}_{xy}\n\\]\nNel caso dell’analisi fattoriale, \\(\\textbf{R}_{xx}\\) corrisponde alla matrice delle correlazioni tra le variabili osservate e \\(\\textbf{R}_{xy}\\) corrisponde alla matrice di struttura (la matrice delle correlazioni tra le variabili osservate e i fattori). Se i fattori sono ortogonali, la matrice di struttura coincide con la matrice dei pesi fattoriali \\(\\hat{\\boldsymbol{\\Lambda}}\\).\nI coefficienti B dell’equazione precedente possono dunque essere trovati nel modo seguente:\n\\[\n\\begin{equation}\n\\hat{\\textbf{B}} = \\textbf{R}_{yy}^{-1}\\textbf{R}_{xf}=\n\\textbf{R}^{-1}\\hat{\\boldsymbol{\\Lambda}}\n\\end{equation}\n\\]\nUna volta stimati i coefficienti \\(\\hat{\\textbf{B}}\\), i punteggi fattoriali si calcolano allo stesso modo dei punteggi teorici del modello di regressione:\n\\[\n\\begin{equation}\n\\hat{\\textbf{F}} = \\textbf{y} \\hat{\\textbf{B}} = \\textbf{y}\n\\textbf{R}^{-1}\\hat{\\boldsymbol{\\Lambda}},\n\\end{equation}\n\\]\ndove \\(\\textbf{y}\\) è la matrice delle variabili osservate standardizzate \\((Y-\\bar{Y})/s\\).\nEsercizio. Si utilizzino i dati dass21.txt che corrispondono alla somministrazione del test DASS-21 a 334 partecipanti. Lo schema di codifica si può trovare seguendo questo link. Ci si focalizzi sulla sottoscala Stress del DASS-21. Si trovino i punteggi fattoriali usando la funzione factanal() e si replichi il risultato seguendo la procedura delineata sopra.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>I punteggi fattoriali</span>"
    ]
  },
  {
    "objectID": "chapters/fa/05_factor_scores.html#session-info",
    "href": "chapters/fa/05_factor_scores.html#session-info",
    "title": "28  I punteggi fattoriali",
    "section": "28.2 Session Info",
    "text": "28.2 Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] kableExtra_1.4.0  corrplot_0.94     nortest_1.0-4    \n [4] MASS_7.3-61       ggokabeito_0.1.0  viridis_0.6.5    \n [7] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1   \n[10] gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1 \n[13] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19    \n[16] psych_2.4.6.26    scales_1.3.0      markdown_1.13    \n[19] knitr_1.48        lubridate_1.9.3   forcats_1.0.0    \n[22] stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[25] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n[28] ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.16.0  jsonlite_1.8.9     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5.1 farver_2.1.2      \n  [7] nloptr_2.1.1       rmarkdown_2.28     vctrs_0.6.5       \n [10] Cairo_1.6-2        minqa_1.2.8        base64enc_0.1-3   \n [13] rstatix_0.7.2      htmltools_0.5.8.1  broom_1.0.7       \n [16] Formula_1.2-5      htmlwidgets_1.6.4  plyr_1.8.9        \n [19] sandwich_3.1-1     emmeans_1.10.4     zoo_1.8-12        \n [22] uuid_1.2-1         igraph_2.0.3       mime_0.12         \n [25] lifecycle_1.0.4    pkgconfig_2.0.3    Matrix_1.7-0      \n [28] R6_2.5.1           fastmap_1.2.0      shiny_1.9.1       \n [31] digest_0.6.37      OpenMx_2.21.12     fdrtool_1.2.18    \n [34] colorspace_2.1-1   rprojroot_2.0.4    Hmisc_5.1-3       \n [37] labeling_0.4.3     fansi_1.0.6        timechange_0.3.0  \n [40] abind_1.4-8        compiler_4.4.1     withr_3.0.1       \n [43] glasso_1.11        htmlTable_2.4.3    backports_1.5.0   \n [46] carData_3.0-5      ggsignif_0.6.4     corpcor_1.6.10    \n [49] gtools_3.9.5       tools_4.4.1        pbivnorm_0.6.0    \n [52] foreign_0.8-87     zip_2.3.1          httpuv_1.6.15     \n [55] nnet_7.3-19        glue_1.8.0         quadprog_1.5-8    \n [58] promises_1.3.0     nlme_3.1-166       lisrelToR_0.3     \n [61] grid_4.4.1         pbdZMQ_0.3-13      checkmate_2.3.2   \n [64] cluster_2.1.6      reshape2_1.4.4     generics_0.1.3    \n [67] gtable_0.3.5       tzdb_0.4.0         data.table_1.16.0 \n [70] hms_1.1.3          xml2_1.3.6         car_3.1-3         \n [73] utf8_1.2.4         sem_3.1-16         pillar_1.9.0      \n [76] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [79] splines_4.4.1      lattice_0.22-6     survival_3.7-0    \n [82] kutils_1.73        tidyselect_1.2.1   miniUI_0.1.1.1    \n [85] pbapply_1.7-2      svglite_2.1.3      stats4_4.4.1      \n [88] xfun_0.48          qgraph_1.9.8       arm_1.14-4        \n [91] stringi_1.8.4      pacman_0.5.1       boot_1.3-31       \n [94] evaluate_1.0.0     codetools_0.2-20   mi_1.1            \n [97] cli_3.6.3          RcppParallel_5.1.9 IRkernel_1.3.2    \n[100] rpart_4.1.23       systemfonts_1.1.0  xtable_1.8-4      \n[103] repr_1.1.7         munsell_0.5.1      Rcpp_1.0.13       \n[106] coda_0.19-4.1      png_0.1-8          XML_3.99-0.17     \n[109] parallel_4.4.1     jpeg_0.1-10        lme4_1.1-35.5     \n[112] mvtnorm_1.3-1      openxlsx_4.2.7.1   crayon_1.5.3      \n[115] rlang_1.1.4        multcomp_1.4-26    mnormt_2.1.1      \n\n\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>I punteggi fattoriali</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html",
    "href": "chapters/fa/06_constraints_on_parms.html",
    "title": "29  Attendibilità e modello fattoriale",
    "section": "",
    "text": "29.1 Teoria classica dei test e analisi fattoriale\nMcDonald (2013) illustra come la teoria classica dei test possa essere correlata al modello dell’analisi fattoriale. La figura rappresenta, attraverso i termini del modello fattoriale, la relazione che sussiste tra i punteggi \\(Y\\), derivanti dalla somministrazione di un test composto da cinque item, e i punteggi veri.\nEsistono diverse strategie per stimare l’attendibilità in situazioni in cui viene somministrato un unico test. In questo contesto, analizzeremo tre metodologie che possono essere implementate attraverso l’analisi fattoriale: l’\\(\\alpha\\) di Cronbach, l’\\(\\omega\\) di McDonald e il metodo di Spearman-Brown.\nIl coefficiente \\(\\alpha\\) rappresenta il principale indice utilizzato per quantificare l’attendibilità come misura di coerenza interna o omogeneità. Approfondiremo come questo indice rappresenti il limite inferiore dell’attendibilità di un test, a condizione che siano soddisfatte alcune ipotesi. Tuttavia, se queste assunzioni non vengono rispettate, l’\\(\\alpha\\) si rivela un stimatore distorto dell’attendibilità.\nPrima di esaminare le diverse metodologie per stimare l’attendibilità in termini di coerenza interna, è essenziale distinguere tra le tre diverse forme che il modello unifattoriale può assumere. Queste tre forme corrispondono al modello con indicatori congenerici, al modello \\(\\tau\\)-equivalente e al modello parallelo.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#teoria-classica-dei-test-e-analisi-fattoriale",
    "href": "chapters/fa/06_constraints_on_parms.html#teoria-classica-dei-test-e-analisi-fattoriale",
    "title": "29  Attendibilità e modello fattoriale",
    "section": "",
    "text": "Figura 29.1: Diagramma di percorso del modello monofattoriale.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#modello-fattoriale-e-ctt",
    "href": "chapters/fa/06_constraints_on_parms.html#modello-fattoriale-e-ctt",
    "title": "29  Attendibilità e modello fattoriale",
    "section": "29.2 Modello fattoriale e CTT",
    "text": "29.2 Modello fattoriale e CTT\nConsiderando un insieme di item osservati \\(X_1, X_2, \\dots, X_p\\), con \\(p&gt;2\\), i punteggi ottenuti da questi item sono composti da due elementi distinti: una componente di punteggio vero e una componente di errore.\n\\[\n\\begin{equation}\n\\begin{aligned}\nX_1 &=T_1+E_1,\\notag\\\\\nX_2 &=T_2+E_2,\\notag\\\\\n&\\dots\\notag\\\\\nX_p &=T_p+E_p.\\notag\n\\end{aligned}\n\\end{equation}\n\\]\nIn linea con l’approccio delineato da McDonald (2013), questa decomposizione tra la componente vera e quella di errore può essere formalizzata mediante l’utilizzo dei parametri del modello fattoriale. L’equazione \\(X_i = T_i + E_i\\) può quindi essere riformulata come segue:\n\\[\nX_i = \\lambda_i \\xi + \\delta_i, \\quad{i=1, \\dots, p},\n\\]\nIn questa equazione, \\(X_i\\) rappresenta il punteggio osservato per l’item \\(i\\)-esimo (espresso in termini di scarti dalla media), \\(\\lambda_i\\) è il carico fattoriale associato all’item \\(i\\)-esimo, \\(\\xi\\) costituisce il fattore comune e \\(\\delta_i\\) è la componente residuale del punteggio osservato per l’item \\(i\\)-esimo. Tale formulazione si basa sulle assunzioni del modello monofattoriale. Nello specifico, si ipotizza che \\(\\xi\\) e \\(\\delta_i\\) siano incorrelati per ogni item \\(i\\), e che \\(\\delta_i\\) e \\(\\delta_k\\) siano incorrelati per ogni coppia \\(i \\neq k\\).",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#classi-di-modelli",
    "href": "chapters/fa/06_constraints_on_parms.html#classi-di-modelli",
    "title": "29  Attendibilità e modello fattoriale",
    "section": "29.3 Classi di modelli",
    "text": "29.3 Classi di modelli\nNell’ambito dei modelli monofattoriali, possiamo distinguere tre scenari principali:\n\nModello con indicatori congenerici: Questo modello rappresenta il caso più generale, in cui non vi sono restrizioni imposte sulla struttura degli indicatori. Gli indicatori sono correlati in quanto riflettono un fattore comune, ma possono avere carichi fattoriali diversi e specificità uniche.\nModello con indicatori \\(\\tau\\)-equivalenti: In questo scenario, tutti gli indicatori hanno lo stesso carico fattoriale, il che implica che misurano il fattore comune con la stessa forza. Tuttavia, possono differire per quanto riguarda la loro varianza e specificità.\nModello con indicatori paralleli: Qui, gli indicatori non solo condividono lo stesso carico fattoriale, ma presentano anche identica varianza degli errori. Questo indica una completa equivalenza tra gli indicatori, mostrando una struttura molto più rigida rispetto al modello \\(\\tau\\)-equivalente.\n\nIl modello con indicatori congenerici funge da base più flessibile, mentre i modelli con indicatori \\(\\tau\\)-equivalenti e paralleli introducono vincoli crescenti che specificano relazioni sempre più strette tra gli indicatori.\n\n29.3.1 Indicatori congenerici\nGli indicatori congenerici rappresentano misure di uno stesso costrutto, ma non è necessario che riflettano tale costrutto con la medesima intensità. Nel contesto degli indicatori congenerici all’interno del modello monofattoriale, non vengono introdotte limitazioni né sulle saturazioni fattoriali né sulle specificità:\n\\[\n\\lambda_1\\neq \\lambda_2 \\neq \\dots\\neq \\lambda_p,\n\\]\n\\[\n\\psi_{11}\\neq \\psi_{22} \\neq \\dots\\neq \\psi_{pp}.\n\\]\nIl modello mono-fattoriale con indicatori congenerici è dunque\n\\[\n\\begin{equation}\nX_i = \\lambda_i \\xi + \\delta_i.\n\\end{equation}\n\\tag{29.1}\\]\nDalle assunzioni precedenti possiamo derivare la matrice \\(\\boldsymbol{\\Sigma}\\) riprodotta in base al modello congenerico la quale risulta essere uguale a\n\\[\n\\boldsymbol{\\Sigma}=\\left[\n      \\begin{array}{ c c c c }\n        \\sigma_{11} & \\sigma_{12} & \\dots & \\sigma_{1p}, \\\\\n        \\sigma_{21} & \\sigma_{22} & \\dots & \\sigma_{2p}. \\\\\n        \\vdots & \\vdots & & \\vdots\\\\\n        \\sigma_{p1} & \\sigma_{p2} & \\dots & \\sigma_{pp}\n      \\end{array}\n    \\right].\n\\]\nSi noti come tutte le varianze e tutte le covarianze siano tra loro diverse.\n\n\n29.3.2 Indicatori tau-equivalenti\nNel caso di indicatori \\(\\tau\\)-equivalenti, si ha che\n\\[\n\\lambda_1=\\lambda_2=\\dots=\\lambda_p=\\lambda,\n\\]\n\\[\n\\psi_{11}\\neq \\psi_{22} \\neq \\dots\\neq \\psi_{pp}.\n\\]\nIl modello monofattoriale con indicatori \\(\\tau\\)-equivalenti diventa dunque\n\\[\n\\begin{equation}\nX_i = \\lambda \\xi + \\delta_i,\n\\end{equation}\n\\tag{29.2}\\]\novvero\n\\[\n\\begin{equation}\nX_i = \\tau + \\delta_i,\n\\end{equation}\n\\tag{29.3}\\]\ndove \\(\\tau=\\lambda \\xi\\) è l’attributo comune scalato nell’unità di misura dell’indicatore. Secondo il modello dell’Equazione 29.3, tutte le \\(p(p-1)\\) covarianze tra gli item del test devono essere uguali, ovvero\n\\[\n\\begin{equation}\n\\sigma_{ik} = \\lambda^2=\\sigma^2_T,\n\\end{equation}\n\\tag{29.4}\\]\nper \\(i\\neq k\\). Gli elementi sulla diagonale principale della matrice di varianze e covarianze saranno invece\n\\[\n\\begin{equation}\n\\sigma_{ii} = \\lambda^2 + \\psi_{ii} =\\sigma^2_T + \\psi_{ii}.\n\\end{equation}\n\\tag{29.5}\\]\nLa matrice \\(\\boldsymbol{\\Sigma}\\) riprodotta in base al modello \\(\\tau\\)-equivalente è dunque uguale a\n\\[\n\\begin{equation}\n\\boldsymbol{\\Sigma}=\\left[\n      \\begin{array}{ c c c c }\n        \\sigma_{T}^2 + \\psi_{11} & \\sigma_{T}^2 & \\dots & \\sigma_{T}^2 \\\\\n        \\sigma_{T}^2 & \\sigma_{T}^2 + \\psi_{22} & \\dots & \\sigma_{T}^2 \\\\\n        \\vdots & \\vdots & & \\vdots\\\\\n        \\sigma_{T}^2 & \\sigma_{T}^2 & \\dots & \\sigma_{T}^2 + \\psi_{pp}\n      \\end{array}\n    \\right].\n\\end{equation}\n\\tag{29.6}\\]\nTutte le covarianze sono uguali, mentre le varianze sono tra loro diverse.\n\n\n29.3.3 Indicatori paralleli\nNel caso di indicatori paralleli si ha che\n\\[\n\\lambda_1=\\lambda_2=\\dots=\\lambda_p=\\lambda,\n\\]\n\\[\n\\psi_{11}=\\psi_{22}=\\dots=\\psi_{pp}=\\psi.\n\\]\nIl modello costituito da indicatori paralleli impone dunque un’ulteriore restrizione che riguarda le varianze degli item, ovvero:\n\\[\n\\sigma_{ii} = \\lambda^2 + \\psi =\\sigma^2_T + \\sigma^2.\n\\]\nLa struttura di varianze e covarianze imposta dal modello per indicatori paralleli è dunque tale da richiedere l’uguaglianza tra tutte le covarianze tra gli item e l’uguaglianza tra tutte le varianze degli item. La matrice \\(\\boldsymbol{\\Sigma}\\) riprodotta in base al modello con indicatori paralleli è dunque uguale a\n\\[\n\\boldsymbol{\\Sigma}=\\left[\n      \\begin{array}{ c c c c }\n        \\sigma_{T}^2 + \\sigma^2 & \\sigma_{T}^2 & \\dots & \\sigma_{T}^2 \\\\\n        \\sigma_{T}^2 & \\sigma_{T}^2 + \\sigma^2 & \\dots & \\sigma_{T}^2 \\\\\n        \\vdots & \\vdots & & \\vdots\\\\\n        \\sigma_{T}^2 & \\sigma_{T}^2 & \\dots & \\sigma_{T}^2 +\\sigma^2 \\notag\n      \\end{array}\n    \\right].\n\\]",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#metodo-dei-minimi-quadrati-non-pesati",
    "href": "chapters/fa/06_constraints_on_parms.html#metodo-dei-minimi-quadrati-non-pesati",
    "title": "29  Attendibilità e modello fattoriale",
    "section": "29.4 Metodo dei minimi quadrati non pesati",
    "text": "29.4 Metodo dei minimi quadrati non pesati\nNel contesto del modello unifattoriale, la varianza di ciascun indicatore è decomposta in due componenti: la componente \\(\\sigma^2_T\\), attribuibile all’effetto del fattore latente comune, e la componente \\(\\psi\\), riferita all’influenza del fattore specifico. McDonald (2013) dimostra come sia possibile ottenere stime di tali componenti dai dati osservati. Queste stime vengono successivamente impiegate per calcolare l’affidabilità interna del test mediante le formule degli indici \\(\\alpha\\) di Cronbach e \\(\\omega\\) di McDonald.\nIn precedenza, abbiamo esaminato come la varianza del punteggio vero possa essere equivalente alla covarianza tra due forme parallele dello stesso test: \\(\\sigma^2_T = \\sigma_{XX^\\prime}\\). Nel caso di indicatori \\(\\tau\\)-equivalenti, la matrice \\(\\boldsymbol{\\Sigma}\\) prevista dal modello risulta essere:\n\\[\n\\boldsymbol{\\Sigma}=\\left[\n      \\begin{array}{ c c c c }\n        \\sigma_{T}^2 + \\psi_{11} & \\sigma_{T}^2 & \\dots & \\sigma_{T}^2 \\\\\n        \\sigma_{T}^2 & \\sigma_{T}^2 + \\psi_{22} & \\dots & \\sigma_{T}^2 \\\\\n        \\vdots & \\vdots & & \\vdots\\\\\n        \\sigma_{T}^2 & \\sigma_{T}^2 & \\dots & \\sigma_{T}^2 + \\psi_{pp} \\notag\n      \\end{array}\n    \\right],\n\\]\nossia, tutte le covarianze sono equivalenti tra loro. Nel caso degli indicatori \\(\\tau\\)-equivalenti, dunque, una stima \\(\\hat{\\sigma}^2_T\\) di \\(\\sigma^2_T\\) si ottiene calcolando la media delle covarianze della matrice S:\n\\[\n\\begin{equation}\n\\hat{\\sigma}_T^2 = \\frac{1}{p(p-1)} {\\sum \\sum}_{i \\neq k} s_{ik}.\n\\end{equation}\n\\tag{29.7}\\]\nQuesto metodo di stima di \\(\\sigma^2_T\\) è noto come “metodo dei minimi quadrati non pesati” McDonald (2013).\nInoltre, nel caso di indicatori \\(\\tau\\)-equivalenti, la stima di \\(\\psi_{ii}\\) nell’Equazione 29.5 è calcolata come:\n\\[\n\\hat{\\psi}_{ii }= s_{ii} - \\hat{\\sigma}_T^2,\n\\]\nper ogni item \\(i\\).\nPer quanto riguarda gli indicatori paralleli, la stima di \\(\\sigma^2_T\\) è ancora basata sull’Equazione 29.7, ovvero sulla media delle covarianze della matrice \\(\\boldsymbol{\\Sigma}\\). Tuttavia, la stima del valore costante \\(\\psi\\) è ottenuta tramite l’equazione:\n\\[\n\\begin{equation}\n\\hat{\\psi} = \\frac{1}{p} \\sum_i (s_{ii} - \\hat{\\sigma}_T^2)\n\\end{equation}\n\\tag{29.8}\\]",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#varianza-del-punteggio-totale-di-un-test",
    "href": "chapters/fa/06_constraints_on_parms.html#varianza-del-punteggio-totale-di-un-test",
    "title": "29  Attendibilità e modello fattoriale",
    "section": "29.5 Varianza del punteggio totale di un test",
    "text": "29.5 Varianza del punteggio totale di un test\nConsideriamo un test omogeneo costituito da \\(p\\) item, il cui punteggio totale \\(Y\\) è dato dalla somma dei punteggi individuali degli item, espressi come \\(Y = \\sum_{i=1}^p X_i\\). Analizziamo la varianza di \\(Y\\) utilizzando un modello unifattoriale.\nIn un modello congenerico con un singolo fattore comune, il punteggio di ciascun item \\(i\\), \\(X_i\\), può essere rappresentato dalla seguente equazione:\n\\[\nX_i = \\lambda_i \\xi + \\delta_i,\n\\]\ndove \\(\\lambda_i\\) rappresenta la carica fattoriale dell’item \\(i\\) sul fattore comune \\(\\xi\\), e \\(\\delta_i\\) è l’errore specifico associato all’item. Questa formulazione è analoga all’equazione \\(X_i = T_i + E_i\\) della teoria classica dei test, dove \\(T_i\\) è il vero punteggio e \\(E_i\\) l’errore di misurazione.\nIl punteggio totale, essendo la somma di tutti gli item, si esprime come \\(\\sum_i (\\lambda_i \\xi + \\delta_i)\\). La varianza del punteggio totale può quindi essere calcolata come segue:\n\\[\n\\begin{equation}\n\\begin{aligned}\n  \\mathbb{V}(Y) &= \\mathbb{V}\\left[ \\sum_i  (\\lambda_i \\xi + \\delta_i)  \\right] \\\\\n  &= \\mathbb{V}\\left[ \\left( \\sum_i \\lambda_i\\right) \\xi + \\sum_i \\delta_i\\right] \\\\\n  &=  \\left(\\sum_i \\lambda_i\\right)^2 \\mathbb{V}(\\xi) +  \\sum_i  \\mathbb{V}(\\delta_i) \\\\\n  &= \\left(\\sum_i \\lambda_i\\right)^2 + \\sum_i \\psi_{ii},\n\\end{aligned}\n\\end{equation}\n\\tag{29.9}\\]\ndove \\(\\mathbb{V}(\\xi) = 1\\) per ipotesi. La varianza di \\(Y\\) si decompone in due parti principali: la prima parte, \\((\\sum_i \\lambda_i)^2\\), rappresenta la varianza attribuibile al fattore comune, riflettendo la variazione legata all’attributo misurato dagli item; la seconda parte, \\(\\sum_i \\psi_{ii}\\), corrisponde alla somma delle varianze degli errori specifici di ciascun item, rappresentando la variazione dovuta agli errori di misurazione.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#stima-dellattendibilità",
    "href": "chapters/fa/06_constraints_on_parms.html#stima-dellattendibilità",
    "title": "29  Attendibilità e modello fattoriale",
    "section": "29.6 Stima dell’attendibilità",
    "text": "29.6 Stima dell’attendibilità\n\n29.6.1 Coefficiente Omega\nDopo aver analizzato la varianza del punteggio totale di un test come indicato nella precedente equazione:\n\\[\n\\mathbb{V}(Y) = \\left( \\sum_i \\lambda_i\\right)^2 + \\sum_i \\psi_{ii},\n\\]\nsi introduce il coefficiente di affidabilità \\(\\omega\\). McDonald (2013) definisce \\(\\omega\\) come il rapporto tra la varianza attribuibile al fattore comune e la varianza totale del punteggio. Basandosi sui parametri del modello monofattoriale, il coefficiente \\(\\omega\\) può essere formulato come segue:\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\omega &= \\frac{\\left( \\sum_{i=1}^p \\lambda_i \\right)^2}{\\mathbb{V}(Y)} \\\\\n&= \\frac{\\left( \\sum_{i=1}^p \\lambda_i \\right)^2}{\\left( \\sum_{i=1}^p \\lambda_i \\right)^2  + \\sum_{i=1}^p \\psi_{ii}}\n\\end{aligned}\n\\end{equation}\n\\tag{29.10}\\]\nQuesto coefficiente \\(\\omega\\) offre una stima quantitativa dell’affidabilità di un test, basata sui parametri del modello congenerico e utilizzando i dati raccolti da una singola somministrazione del test. La sua utilità risiede nel quantificare quanto della varianza osservata nel punteggio totale è effettivamente spiegata dal fattore comune misurato dal test.\n\n29.6.1.1 Un esempio concreto\nConsideriamo nuovamente la scala Openness del dataframe bfi discussi nel capitolo (ctt-3-notebook?). Leggiamo i dati in R.\n\ndata(bfi, package = \"psych\")\n\nÈ necessario ricodificare due item.\n\nbfi$O2r &lt;- 7 - bfi$O2\nbfi$O5r &lt;- 7 - bfi$O5\n\n\ncor(\n    bfi[c(\"O1\", \"O2r\", \"O3\", \"O4\", \"O5r\")], \n    use = \"pairwise.complete.obs\"\n) |&gt;\n    round(2)\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nO1\nO2r\nO3\nO4\nO5r\n\n\n\n\nO1\n1.00\n0.21\n0.40\n0.18\n0.24\n\n\nO2r\n0.21\n1.00\n0.26\n0.07\n0.32\n\n\nO3\n0.40\n0.26\n1.00\n0.19\n0.31\n\n\nO4\n0.18\n0.07\n0.19\n1.00\n0.18\n\n\nO5r\n0.24\n0.32\n0.31\n0.18\n1.00\n\n\n\n\n\nEseguiamo l’analisi fattoriale confermativa con lavaan.\n\nmod &lt;- \"\n    f =~ NA*O1 + O2r + O3 + O4 + O5r\n    f ~~ 1*f\n\"\n\nfit &lt;- cfa(mod, data = bfi, std.ov = TRUE, std.lv = TRUE)\n\nEstraiamo le saturazioni fattoriali e le specificità dall’oggetto fit.\n\nlambda &lt;- inspect(fit, what = \"std\")$lambda\npsy &lt;- diag(inspect(fit, what = \"est\")$theta)\n\nCalcoliamo il coefficiente \\(\\omega\\)\n\\[\n\\omega = \\frac{\\left( \\sum_{i=1}^p \\lambda_i \\right)^2}{\\left( \\sum_{i=1}^p \\lambda_i \\right)^2  + \\sum_{i=1}^p \\psi_{ii}}\n\\]\nusando i parametri del modello fattoriale.\n\nsum(lambda)^2 / (sum(lambda)^2 + sum(psy)) \n\n0.618064541281708\n\n\nRipetiamo i calcoli usando la funzione compRelSEM del pacchetto semTools.\n\nsemTools::compRelSEM(fit, tau.eq = FALSE)\n\nf: 0.618110889806653\n\n\nIl coefficiente \\(\\omega=0.62\\) può essere interpretato dicendo che il 62% della varianza del punteggio totale \\(Y\\) della sottoscala Openness viene spiegato dal fattore comune latente.\n\n\n29.6.1.2 Coefficiente \\(\\omega\\) e assunzioni della teoria classica dei test\nIl calcolo del coefficiente \\(\\omega\\) si appoggia su un’assunzione fondamentale della teoria classica dei test: che non esistano covarianze tra gli errori specifici degli item, ossia \\(\\psi_{ik}=0\\) per ogni \\(i \\neq k\\). Tuttavia, questa ipotesi potrebbe non reggere in contesti di dati empirici. Bollen (1980) sottolinea che, qualora le covarianze tra errori specifici non siano trascurabili, l’equazione per \\(\\omega\\) dovrebbe essere modificata come segue:\n\\[\n\\begin{equation}\n\\omega = \\frac{\\left( \\sum_{i=1}^p \\lambda_i \\right)^2}{\\left( \\sum_{i=1}^p \\lambda_i \\right)^2  + \\sum_{i=1}^p \\psi_{ii} + \\sum_{i, k, i\\neq k}^p \\psi_{ik}}.\n\\end{equation}\n\\]\nPer verificare la validità dell’assunzione di indipendenza tra gli errori specifici, si può ricorrere a un’analisi fattoriale confermativa. Se l’analisi rivela correlazioni significative tra molti errori specifici, potrebbe essere necessario incorporare ulteriori fattori nel modello per accomodare queste covarianze. Questo può suggerire una struttura non più unidimensionale, indicando la presenza di diverse sottoscale all’interno del test. Tuttavia, anche con l’identificazione di tali sottoscale, le covarianze tra i fattori specifici possono rimanere inesplicate. In tali casi, l’uso dell’equazione modificata per \\(\\omega\\) diventa indispensabile.\n\n\n29.6.1.3 Interpretazione del Coefficiente \\(\\omega\\)\nMcDonald (2013) propone diverse interpretazioni del coefficiente \\(\\omega\\) che aiutano a comprenderne il significato nel contesto della teoria dei test: - \\(\\omega\\) può essere visto come il quadrato della correlazione tra il punteggio totale \\(Y\\) e il fattore comune \\(\\xi\\), che rappresenta anche la correlazione tra \\(Y\\) e il punteggio vero. Questo si allinea alla definizione classica di affidabilità, espressa come \\(\\rho_{XT}^2 = \\sigma^2_{\\tau}/\\sigma^2_X\\), dove \\(\\sigma^2_{\\tau}\\) è la varianza del punteggio vero e \\(\\sigma^2_X\\) quella del punteggio osservato. - \\(\\omega\\) descrive anche la correlazione tra due applicazioni ipotetiche del test, \\(Y\\) e \\(Y'\\), che condividono le stesse somme (o medie) delle cariche fattoriali e delle varianze specifiche nel contesto di un modello a singolo fattore. - \\(\\omega\\) rappresenta il quadrato della correlazione tra il punteggio totale di \\(p\\) item e il punteggio medio di un insieme infinito di item all’interno di un dominio omogeneo, dove i \\(p\\) item analizzati sono un sottoinsieme rappresentativo.\nIn sintesi, il coefficiente \\(\\omega\\) fornisce una misura di quanto il punteggio totale di un test sia rappresentativo del fattore latente che il test intende misurare. Attraverso la correlazione, l’omogeneità e la consistenza osservata tra diverse somministrazioni o versioni di un test, \\(\\omega\\) aiuta a interpretare la qualità e l’affidabilità del test stesso.\n\n\n\n29.6.2 Coefficienti \\(\\alpha\\) e \\(\\omega\\) nel modello \\(\\tau\\)-equivalente\nNel contesto dei modelli monofattoriali, i coefficienti \\(\\omega\\) e \\(\\alpha\\) offrono stime dell’attendibilità, ma in contesti distinti. Il coefficiente \\(\\omega\\) è utile per i modelli con indicatori congenerici, mentre il coefficiente \\(\\alpha\\) è specifico per i modelli con indicatori \\(\\tau\\)-equivalenti.\nIn un modello \\(\\tau\\)-equivalente, dove ciascun item ha la stessa carica fattoriale \\(\\lambda\\), la varianza di ogni item si scompone in una parte dovuta al punteggio vero e una parte d’errore, espressa come \\(\\sigma_{ii} = \\lambda^2 + \\psi_{ii} = \\sigma^2_T + \\sigma^2_i\\). In questo scenario, la formula per il coefficiente \\(\\omega\\) si semplifica nel seguente modo:\n\\[\n\\omega = \\frac{\\left( \\sum_i \\lambda_i \\right)^2}{\\left( \\sum_i \\lambda_i \\right)^2  + \\sum_i \\psi_{ii}} = \\frac{p^2 \\lambda^2}{\\sigma^2_Y} = \\frac{p^2 \\sigma_T^2}{\\sigma_Y^2},\n\\]\ndove \\(Y\\) rappresenta il punteggio totale del test.\nApplicando il metodo dei minimi quadrati non pesati, possiamo derivare la stima seguente per \\(\\omega\\):\n\\[\n\\hat{\\omega} = \\frac{p^2 \\hat{\\sigma}_T^2}{s_Y^2},\n\\]\ndove \\(\\hat{\\sigma}_T^2\\) è stimato come:\n\\[\n\\hat{\\sigma}_T^2 = \\frac{1}{p(p-1)} \\sum \\sum_{i \\neq k} s_{ik}.\n\\]\nIntegrando questa stima nella formula precedente, otteniamo:\n\\[\n\\hat{\\omega} = \\frac{p}{p-1}\\frac{\\sum \\sum_{i \\neq k} s_{ik}}{s_Y^2}.\n\\]\nPer gli indicatori \\(\\tau\\)-equivalenti, quindi, \\(\\omega\\) può essere stimato da:\n\\[\n\\hat{\\omega} = \\frac{p}{p-1}\\left(1-\\frac{\\sum_i s_{ii}}{s_Y^2}\\right).\n\\] (eq-alpha-camp)\nQuesta stima di \\(\\omega\\) ha un parallelo nei valori di popolazione definiti da \\(\\alpha\\), che si esprime come:\n\\[\n\\alpha = \\frac{p}{p-1}\\left(1-\\frac{\\sum_{i=1}^p \\sigma_{ii}}{\\sigma_Y^2}\\right) = \\frac{p}{p-1}\\frac{\\sum_{i \\neq k}^p \\text{Cov}(X_i, X_k)}{\\mathbb{V}(Y)}.\n\\tag{29.11}\\]\nIn condizioni ideali del modello \\(\\tau\\)-equivalente, i valori di \\(\\alpha\\) e \\(\\omega\\) convergono. Tuttavia, \\(\\alpha\\) tende a sottostimare \\(\\omega\\), posizionandosi come un limite inferiore per \\(\\omega\\). Data questa natura conservativa di \\(\\alpha\\), alcuni ricercatori lo preferiscono a \\(\\omega\\), sebbene questa proprietà valga solamente quando le assunzioni del modello \\(\\tau\\)-equivalente sono rigorosamente rispettate.\n\n29.6.2.1 Un esempio concreto\nConsideriamo la matrice di varianze e covarianze della sottoscala Openness.\n\nC &lt;- cov(bfi[c(\"O1\", \"O2r\", \"O3\", \"O4\", \"O5r\")], use = \"pairwise.complete.obs\")\nC |&gt; \n    round(2)\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nO1\nO2r\nO3\nO4\nO5r\n\n\n\n\nO1\n1.28\n0.38\n0.54\n0.25\n0.36\n\n\nO2r\n0.38\n2.45\n0.50\n0.13\n0.67\n\n\nO3\n0.54\n0.50\n1.49\n0.29\n0.50\n\n\nO4\n0.25\n0.13\n0.29\n1.49\n0.29\n\n\nO5r\n0.36\n0.67\n0.50\n0.29\n1.76\n\n\n\n\n\nCalcoliamo il coefficiente \\(\\alpha\\) usando l’eq. {eq}eq-alpha-camp:\n\np &lt;- 5\nalpha &lt;- (p / (p - 1)) * (1 - tr(C) / sum(C))\nalpha\n\n0.600172514820215\n\n\n\n\n\n29.6.3 La formula “profetica” di Spearman-Brown\nLa formula profetica di Spearman-Brown è impiegata per calcolare l’affidabilità nei modelli di misurazione che utilizzano indicatori paralleli. Supponiamo di avere un test composto da \\(p\\) item paralleli, in cui ogni item ha la stessa carica fattoriale \\(\\lambda\\) e la stessa varianza dell’errore specifico \\(\\psi\\), ovvero \\(\\lambda_1=\\lambda_2=\\dots=\\lambda_p=\\lambda\\) e \\(\\psi_{11}=\\psi_{22}=\\dots=\\psi_{pp}=\\psi\\).\nLa proporzione di varianza nel punteggio totale del test spiegata dalla variabile latente è quindi:\n\\[\n\\left(\\sum_i \\lambda_i \\right)^2 = (p \\lambda)^2 = p^2 \\lambda^2.\n\\]\nDefinendo l’affidabilità di un singolo item, \\(\\rho_1\\), come\n\\[\n\\rho_1 = \\frac{\\lambda^2}{\\lambda^2 + \\psi},\n\\]\nper \\(p\\) item paralleli, l’affidabilità del test, \\(\\rho_p\\), diventa:\n\\[\n\\begin{equation}\n\\begin{aligned}\n  \\rho_p &= \\frac{p^2 \\lambda^2}{p^2 \\lambda^2 + p \\psi} \\\\\n         &= \\frac{p \\lambda^2}{ p \\lambda^2 + \\psi} \\\\\n         &= \\frac{p \\lambda^2}{(p-1) \\lambda^2 + (\\lambda^2 + \\psi)}.\n\\end{aligned}\n\\end{equation}\n\\]\nSfruttando l’affidabilità di un singolo item \\(\\rho_1\\), possiamo riformulare \\(\\rho_p\\) come:\n\\[\n\\begin{equation}\n\\begin{aligned}\n  \\rho_p &= \\frac{p \\rho_1}{(p-1)\\rho_1 + 1}.\n\\end{aligned}\n\\end{equation}\n\\] (eq-spearman-brown-der)\nQuesta espressione, derivata qui sopra, mostra come l’affidabilità \\(\\rho_p\\) di un test composto da \\(p\\) item paralleli possa essere calcolata a partire dall’affidabilità di un singolo item. Tale formula è nota come “formula di predizione” di Spearman-Brown (Spearman-Brown prophecy formula).\nIn contesti con item paralleli, è importante notare che le misure di affidabilità \\(\\omega\\), \\(\\alpha\\), e \\(\\rho_p\\) risultano equivalenti.\n\n29.6.3.1 Un esempio concreto\nPoniamoci il problema di calcolare l’attendibilità della sottoscala Openness utilizzando la formula di Spearman-Brown. Ipotizziamo dunque che gli item della scala Openness siano paralleli. La matrice di correlazione è:\n\nR &lt;- cor(bfi[c(\"O1\", \"O2r\", \"O3\", \"O4\", \"O5r\")], use = \"pairwise.complete.obs\")\nround(R, 3)\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nO1\nO2r\nO3\nO4\nO5r\n\n\n\n\nO1\n1.000\n0.214\n0.395\n0.178\n0.239\n\n\nO2r\n0.214\n1.000\n0.262\n0.068\n0.325\n\n\nO3\n0.395\n0.262\n1.000\n0.195\n0.311\n\n\nO4\n0.178\n0.068\n0.195\n1.000\n0.179\n\n\nO5r\n0.239\n0.325\n0.311\n0.179\n1.000\n\n\n\n\n\nSeguendo McDonald (2013), supponiamo di calcolare l’attendibilità di un singolo item (\\(\\rho_1\\)) come la correlazione media tra gli item:\n\nrr &lt;- NULL\np &lt;- 5\nk &lt;- 1\nfor (i in 1:p) {\n  for (j in 1:p) {\n    if (j != i) {\n      rr[k] &lt;- R[i, j]\n    }\n    k &lt;- k + 1\n  }\n}\nro_1 &lt;- mean(rr, na.rm = TRUE)\nprint(ro_1)\n\n[1] 0.2365383\n\n\nApplicando la formula di Spearman-Brown, la stima dell’attendibilità del test diventa pari a\n\n(p * ro_1) / ((p - 1) * ro_1 + 1) |&gt;\n    round(3)\n\n0.607755188979593",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#commenti-e-considerazioni-conclusive",
    "href": "chapters/fa/06_constraints_on_parms.html#commenti-e-considerazioni-conclusive",
    "title": "29  Attendibilità e modello fattoriale",
    "section": "29.7 Commenti e considerazioni conclusive",
    "text": "29.7 Commenti e considerazioni conclusive\nIl coefficiente \\(\\alpha\\) di Cronbach è uno degli indici di affidabilità più diffusi in psicometria. Tuttavia, la sua efficacia dipende strettamente dalla \\(\\tau\\)-equivalenza degli item, che presuppongono un tratto latente unidimensionale. Nella pratica, questa condizione è spesso violata: molti test misurano più di un fattore, e le comunalità degli item non sono uniformi, mettendo in discussione la validità dell’ipotesi di \\(\\tau\\)-equivalenza. Se gli errori sono incorrelati, il coefficiente \\(\\alpha\\) può sottostimare l’affidabilità; se invece gli errori sono correlati, può sovrastimarla.\nData questa limitazione, l’utilizzo del coefficiente \\(\\omega\\) di McDonald è generalmente più consigliabile. Il coefficiente \\(\\omega\\) fornisce una stima più robusta dell’affidabilità in vari contesti, inclusi quelli con assunzioni meno restrittive rispetto alla \\(\\tau\\)-equivalenza. Altri indici come il \\(glb\\) (Greatest Lower Bound), discusso da Ten Berge e Sočan (2004), e l’indice \\(\\beta\\) di Revelle (1979), rappresentano alternative valide al coefficiente \\(\\alpha\\), offrendo diversi vantaggi metodologici a seconda delle specifiche esigenze di misurazione e delle caratteristiche dei dati analizzati.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/06_constraints_on_parms.html#session-info",
    "href": "chapters/fa/06_constraints_on_parms.html#session-info",
    "title": "29  Attendibilità e modello fattoriale",
    "section": "29.8 Session Info",
    "text": "29.8 Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] modelsummary_2.2.0 nortest_1.0-4      MASS_7.3-61       \n [4] ggokabeito_0.1.0   viridis_0.6.5      viridisLite_0.4.2 \n [7] ggpubr_0.6.0       ggExtra_0.10.1     gridExtra_2.3     \n[10] patchwork_1.3.0    bayesplot_1.11.1   semTools_0.5-6    \n[13] semPlot_1.1.6      lavaan_0.6-19      psych_2.4.6.26    \n[16] scales_1.3.0       markdown_1.13      knitr_1.48        \n[19] lubridate_1.9.3    forcats_1.0.0      stringr_1.5.1     \n[22] dplyr_1.1.4        purrr_1.0.2        readr_2.1.5       \n[25] tidyr_1.3.1        tibble_3.2.1       ggplot2_3.5.1     \n[28] tidyverse_2.0.0    here_1.0.1        \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.16.0  jsonlite_1.8.9     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5.1 farver_2.1.2      \n  [7] nloptr_2.1.1       rmarkdown_2.28     vctrs_0.6.5       \n [10] minqa_1.2.8        base64enc_0.1-3    rstatix_0.7.2     \n [13] htmltools_0.5.8.1  broom_1.0.7        Formula_1.2-5     \n [16] htmlwidgets_1.6.4  plyr_1.8.9         sandwich_3.1-1    \n [19] emmeans_1.10.4     zoo_1.8-12         uuid_1.2-1        \n [22] igraph_2.0.3       mime_0.12          lifecycle_1.0.4   \n [25] pkgconfig_2.0.3    Matrix_1.7-0       R6_2.5.1          \n [28] fastmap_1.2.0      shiny_1.9.1        digest_0.6.37     \n [31] OpenMx_2.21.12     fdrtool_1.2.18     colorspace_2.1-1  \n [34] rprojroot_2.0.4    Hmisc_5.1-3        fansi_1.0.6       \n [37] timechange_0.3.0   abind_1.4-8        compiler_4.4.1    \n [40] withr_3.0.1        glasso_1.11        htmlTable_2.4.3   \n [43] backports_1.5.0    carData_3.0-5      ggsignif_0.6.4    \n [46] corpcor_1.6.10     gtools_3.9.5       tools_4.4.1       \n [49] pbivnorm_0.6.0     foreign_0.8-87     zip_2.3.1         \n [52] httpuv_1.6.15      nnet_7.3-19        glue_1.8.0        \n [55] quadprog_1.5-8     promises_1.3.0     nlme_3.1-166      \n [58] lisrelToR_0.3      grid_4.4.1         pbdZMQ_0.3-13     \n [61] checkmate_2.3.2    cluster_2.1.6      reshape2_1.4.4    \n [64] generics_0.1.3     gtable_0.3.5       tzdb_0.4.0        \n [67] data.table_1.16.0  hms_1.1.3          car_3.1-3         \n [70] utf8_1.2.4         tables_0.9.31      sem_3.1-16        \n [73] pillar_1.9.0       IRdisplay_1.1      rockchalk_1.8.157 \n [76] later_1.3.2        splines_4.4.1      lattice_0.22-6    \n [79] survival_3.7-0     kutils_1.73        tidyselect_1.2.1  \n [82] miniUI_0.1.1.1     pbapply_1.7-2      stats4_4.4.1      \n [85] xfun_0.48          qgraph_1.9.8       arm_1.14-4        \n [88] stringi_1.8.4      pacman_0.5.1       boot_1.3-31       \n [91] evaluate_1.0.0     codetools_0.2-20   mi_1.1            \n [94] cli_3.6.3          RcppParallel_5.1.9 IRkernel_1.3.2    \n [97] rpart_4.1.23       xtable_1.8-4       repr_1.1.7        \n[100] munsell_0.5.1      Rcpp_1.0.13        coda_0.19-4.1     \n[103] png_0.1-8          XML_3.99-0.17      parallel_4.4.1    \n[106] jpeg_0.1-10        lme4_1.1-35.5      mvtnorm_1.3-1     \n[109] insight_0.20.5     openxlsx_4.2.7.1   crayon_1.5.3      \n[112] rlang_1.1.4        multcomp_1.4-26    mnormt_2.1.1      \n\n\n\n\n\n\nMcDonald, R. P. (2013). Test theory: A unified treatment. Psychology Press.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Attendibilità e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/07_total_score.html",
    "href": "chapters/fa/07_total_score.html",
    "title": "30  Punteggio totale e modello fattoriale",
    "section": "",
    "text": "30.1 Punteggio totale e modello fattoriale parallelo\nMcNeish e Wolf (2020) richiamano l’attenzione sul fatto che usare il punteggio totale quale misura di un costrutto è possibile solo quando i dati soddisfano i vincoli di un modello fattoriale parallelo.\nConsideriamo l’esempio seguente, nel quale McNeish e Wolf (2020) esaminano i dati “classici” di Holzinger and Swineford (1939), i quali si riferiscono ai seguenti item:\nLeggiamo i dati in R.\nd &lt;- rio::import(\n  here::here(\"data\", \"1_Factor_Parallel.csv\")\n)\nMcNeish e Wolf (2020) sottolineano il fatto che il punteggio totale\n\\[\n\\text{Punteggio totale} = \\text{Item 1 + Item 2 + Item 3 + Item 4 + Item 5 + Item 6}\n\\]\nrappresenta l’idea che ciasun item fornisca la stessa quantità di informazione relativamente alla misura del costrutto. Ciò può essere specificato da un modello fattoriale nel quale le saturazioni fattoriali degli item sono tutte uguali a 1. Questo corrisponde al modello parallelo che abbiamo discusso in precedenza. In tali circostanze, i punteggi fattoriali del test risultano perfettamente associati al punteggio totale (correlazione uguale a 1). Dunque, se tale modello fattoriale è giustificato dai dati, questo giustifica l’uso del punteggio totale del test quale misura del costrutto.\nÈ facile verificare tali affermazioni. Implementiamo il modello parallelo.\nm_parallel &lt;-\n  \"\n  # all loadings are fixed to one\n  f1 =~ 1*X4 + 1*X5 + 1*X6 + 1*X7 + 1*X8 + 1*X9\n  \n  # all residual variances constrained to same value\n  X4 ~~ theta*X4\n  X5 ~~ theta*X5\n  X6 ~~ theta*X6\n  X7 ~~ theta*X7\n  X8 ~~ theta*X8\n  X9 ~~ theta*X9\n\"\nAdattiamo il modello parallelo ai dati forniti dagli autori.\nfit_parallel &lt;- sem(m_parallel, data=d)\nCalcoliamo il punteggio totale.\nd$ts &lt;- with(\n  d,\n  X4 + X5 + X6 + X7 + X8 + X9\n)\nCalcoliamo i punteggi fattoriali.\nscores &lt;- lavPredict(fit_parallel, method=\"regression\")\nd$scores &lt;- as.numeric(scores)\nUn diagramma a dispersione tra il punteggio totale e i punteggi fattoriali conferma che i due sono perfettamente associati. Quindi, usare il punteggio totale o i punteggi fattoriali è equivalente.\nd |&gt; \n  ggplot(aes(x=ts, y=scores)) + \n  geom_point()\nTuttavia, questa conclusione è valida solo se il modello parallelo è giustificato per i dati. Se esaminiamo l’output di lavaan vediamo che, nel caso presente, questo non è vero.\n# report output with fit measures and standardized estimates\nout = summary(fit_parallel, fit.measures = TRUE, standardized = TRUE)\nprint(out)\n\nlavaan 0.6-19 ended normally after 13 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         7\n  Number of equality constraints                     5\n\n  Number of observations                           301\n\nModel Test User Model:\n                                                      \n  Test statistic                               325.899\n  Degrees of freedom                                19\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               568.519\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.446\n  Tucker-Lewis Index (TLI)                       0.562\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2680.931\n  Loglikelihood unrestricted model (H1)      -2517.981\n                                                      \n  Akaike (AIC)                                5365.862\n  Bayesian (BIC)                              5373.276\n  Sample-size adjusted Bayesian (SABIC)       5366.933\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.232\n  90 Percent confidence interval - lower         0.210\n  90 Percent confidence interval - upper         0.254\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.206\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n  f1 =~                                                        \n    X4                1.000                               0.633\n    X5                1.000                               0.633\n    X6                1.000                               0.633\n    X7                1.000                               0.633\n    X8                1.000                               0.633\n    X9                1.000                               0.633\n  Std.all\n         \n    0.551\n    0.551\n    0.551\n    0.551\n    0.551\n    0.551\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n   .X4      (thet)    0.920    0.034   27.432    0.000    0.920\n   .X5      (thet)    0.920    0.034   27.432    0.000    0.920\n   .X6      (thet)    0.920    0.034   27.432    0.000    0.920\n   .X7      (thet)    0.920    0.034   27.432    0.000    0.920\n   .X8      (thet)    0.920    0.034   27.432    0.000    0.920\n   .X9      (thet)    0.920    0.034   27.432    0.000    0.920\n    f1                0.400    0.045    8.803    0.000    1.000\n  Std.all\n    0.697\n    0.697\n    0.697\n    0.697\n    0.697\n    0.697\n    1.000\nDunque, per questi dati, il punteggio totale può ovviamente essere calcolato. Ma non fornisce una misura adeguata del costrutto. Dunque, il punteggio totale non dovrebbe essere usato nel caso dei dati ottenuti con questo test.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Punteggio totale e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/07_total_score.html#punteggio-totale-e-modello-fattoriale-parallelo",
    "href": "chapters/fa/07_total_score.html#punteggio-totale-e-modello-fattoriale-parallelo",
    "title": "30  Punteggio totale e modello fattoriale",
    "section": "",
    "text": "Paragraph comprehension\nSentence completion\nWord definitions\nSpeeded addition\nSpeeded dot counting\nDiscrimination between curved and straight letters",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Punteggio totale e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/07_total_score.html#punteggio-totale-e-modello-fattoriale-congenerico",
    "href": "chapters/fa/07_total_score.html#punteggio-totale-e-modello-fattoriale-congenerico",
    "title": "30  Punteggio totale e modello fattoriale",
    "section": "30.2 Punteggio totale e modello fattoriale congenerico",
    "text": "30.2 Punteggio totale e modello fattoriale congenerico\nGli autori adattano ai dati un modello congenerico.\n\nm_congeneric &lt;- \n'\n  #all loadings are uniquely estimated\n  f1 =~ NA*X4 + X5 + X6 + X7 + X8 + X9\n  #constrain factor variance to 1\n  f1 ~~ 1*f1\n'\n\n\n# Fit above model\nfit_congeneric &lt;- sem(m_congeneric, data=d)\n\n\nparameterEstimates(fit_congeneric, standardized = TRUE) %&gt;%\n  dplyr::filter(op == \"=~\") %&gt;%\n  dplyr::select(\n    \"Latent Factor\" = lhs,\n    Indicator = rhs,\n    B = est,\n    SE = se,\n    Z = z,\n    \"p-value\" = pvalue,\n    Beta = std.all\n  ) %&gt;%\n  knitr::kable(\n    digits = 3, booktabs = TRUE, format = \"markdown\",\n    caption = \"Factor Loadings\"\n  )\n\n\n\nTable: Factor Loadings\n\n|Latent Factor |Indicator |     B|    SE|      Z| p-value|  Beta|\n|:-------------|:---------|-----:|-----:|------:|-------:|-----:|\n|f1            |X4        | 0.963| 0.059| 16.274|   0.000| 0.824|\n|f1            |X5        | 1.121| 0.067| 16.835|   0.000| 0.846|\n|f1            |X6        | 0.894| 0.058| 15.450|   0.000| 0.792|\n|f1            |X7        | 0.195| 0.071|  2.767|   0.006| 0.170|\n|f1            |X8        | 0.185| 0.063|  2.938|   0.003| 0.180|\n|f1            |X9        | 0.278| 0.065|  4.245|   0.000| 0.258|\n\n\nSi noti che le saturazioni fattoriali sono molto diverse tra loro, suggerendo che il punteggio del costrutto si relaziona in modo diverso con ciascun item e che sarebbe inappropriato stimare il punteggio del costrutto assegnando un peso unitario agli item.\nMcNeish e Wolf (2020) calcolano poi i punteggi fattoriali del modello congenerico.\n\nscores_cong &lt;- lavPredict(fit_congeneric, method=\"regression\")\nd$scores_cong &lt;- as.numeric(scores_cong)\n\nIl grafico seguente mostra la relazione tra i punteggi fattoriali e il punteggio totale.\n\nd |&gt; \n  ggplot(aes(x=ts, y=scores_cong)) + \n  geom_point()\n\n\n\n\n\n\n\n\nNel caso presente, il coefficiente di determinazione tra punteggio totale e punteggi fattoriali è 0.77.\n\ncor(d$ts, d$scores_cong)^2\n\n0.765992021080728\n\n\nSecondo gli autori, ciò significa che due persone con un punteggio totale identico potrebbero avere punteggi di modello congenerico potenzialmente diversi perché hanno raggiunto il loro particolare punteggio totale approvando item diversi. Poiché il modello congenerico assegna pesi diversi agli item, ciascun item contribuisce in modo diverso al punteggio fattoriale del modello congenerico, il che non è vero per il punteggio totale.\nSi noti che, per i dati di Holzinger and Swineford (1939), neppure un modello congenerico ad un fattore si dimostra adeguato.\n\nout = summary(fit_congeneric, fit.measures = TRUE, standardized = TRUE)\nprint(out)\n\nlavaan 0.6-19 ended normally after 16 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n\n  Number of observations                           301\n\nModel Test User Model:\n                                                      \n  Test statistic                               115.366\n  Degrees of freedom                                 9\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               568.519\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.808\n  Tucker-Lewis Index (TLI)                       0.680\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2575.664\n  Loglikelihood unrestricted model (H1)      -2517.981\n                                                      \n  Akaike (AIC)                                5175.328\n  Bayesian (BIC)                              5219.813\n  Sample-size adjusted Bayesian (SABIC)       5181.756\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.198\n  90 Percent confidence interval - lower         0.167\n  90 Percent confidence interval - upper         0.231\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.129\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n  f1 =~                                                        \n    X4                0.963    0.059   16.274    0.000    0.963\n    X5                1.121    0.067   16.835    0.000    1.121\n    X6                0.894    0.058   15.450    0.000    0.894\n    X7                0.195    0.071    2.767    0.006    0.195\n    X8                0.185    0.063    2.938    0.003    0.185\n    X9                0.278    0.065    4.245    0.000    0.278\n  Std.all\n         \n    0.824\n    0.846\n    0.792\n    0.170\n    0.180\n    0.258\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n    f1                1.000                               1.000\n   .X4                0.437    0.056    7.775    0.000    0.437\n   .X5                0.500    0.071    6.998    0.000    0.500\n   .X6                0.474    0.054    8.777    0.000    0.474\n   .X7                1.278    0.105   12.211    0.000    1.278\n   .X8                1.023    0.084   12.204    0.000    1.023\n   .X9                1.080    0.089   12.132    0.000    1.080\n  Std.all\n    1.000\n    0.320\n    0.285\n    0.372\n    0.971\n    0.967\n    0.933\n\n\n\nSe trascuriamo le considerazioni sulla struttura fattoriale e esaminiamo (per esempio) unicamente il coefficiente omega, finiamo per trovare una risposta accettabile, ma sbagliata.\n\npsych::omega(d[, 1:6])\n\nOmega \nCall: omegah(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip, \n    digits = digits, title = title, sl = sl, labels = labels, \n    plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option = option, \n    covar = covar)\nAlpha:                 0.72 \nG.6:                   0.76 \nOmega Hierarchical:    0.55 \nOmega H asymptotic:    0.65 \nOmega Total            0.84 \n\nSchmid Leiman Factor loadings greater than  0.2 \n      g  F1*  F2*   F3*   h2   h2   u2   p2  com\nX4 0.73            0.68 1.00 1.00 0.00 0.53 1.99\nX5 0.96                 0.92 0.92 0.08 1.00 1.01\nX6 0.69            0.22 0.54 0.54 0.46 0.90 1.22\nX7           0.56       0.33 0.33 0.67 0.03 1.15\nX8           0.75       0.59 0.59 0.41 0.05 1.12\nX9 0.22      0.49       0.29 0.29 0.71 0.16 1.41\n\nWith Sums of squares  of:\n   g  F1*  F2*  F3*   h2 \n2.02 0.00 1.11 0.54 2.67 \n\ngeneral/max  0.75   max/min =   622.07\nmean percent general =  0.44    with sd =  0.43 and cv of  0.97 \nExplained Common Variance of the general factor =  0.55 \n\nThe degrees of freedom are 0  and the fit is  0 \nThe number of observations was  301  with Chi Square =  0.03  with prob &lt;  NA\nThe root mean square of the residuals is  0 \nThe df corrected root mean square of the residuals is  NA\n\nCompare this with the adequacy of just a general factor and no group factors\nThe degrees of freedom for just the general factor are 9  and the fit is  0.48 \nThe number of observations was  301  with Chi Square =  142.26  with prob &lt;  3.5e-26\nThe root mean square of the residuals is  0.17 \nThe df corrected root mean square of the residuals is  0.21 \n\nRMSEA index =  0.222  and the 10 % confidence intervals are  0.191 0.255\nBIC =  90.9 \n\nMeasures of factor score adequacy             \n                                                 g   F1*  F2*  F3*\nCorrelation of scores with factors            0.96  0.08 0.83 0.96\nMultiple R square of scores with factors      0.93  0.01 0.68 0.91\nMinimum correlation of factor score estimates 0.86 -0.99 0.36 0.83\n\n Total, General and Subset omega for each subset\n                                                 g  F1*  F2*  F3*\nOmega total for total scores and subscales    0.84 0.92 0.66 0.86\nOmega general for total scores and subscales  0.55 0.92 0.04 0.61\nOmega group for total scores and subscales    0.27 0.00 0.61 0.25\n\n\n\n\n\n\n\n\n\nÈ invece necessario ipotizzare un modello congenerico a due fattori.\n\nm2f_cong &lt;- '\n  # all loadings are uniquely estimated on each factor\n  f1 =~ NA*X4 + X5 + X6\n  f2 =~ NA*X7 + X8 + X9\n  \n  # constrain factor variancse to 1\n  f1 ~~ 1*f1\n  f2 ~~ 1*f2\n  \n  # estimate factor covariance\n  f1 ~~ f2\n'\n\n\n# Fit above model\nfit_2f_congeneric &lt;- sem(m2f_cong, data=d)\n\nSolo questo modello fornisce un adattamento adeguato ai dati.\n\nout = summary(fit_2f_congeneric, fit.measures = TRUE, standardized = TRUE)\nprint(out)\n\nlavaan 0.6-19 ended normally after 18 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n\n  Number of observations                           301\n\nModel Test User Model:\n                                                      \n  Test statistic                                14.736\n  Degrees of freedom                                 8\n  P-value (Chi-square)                           0.064\n\nModel Test Baseline Model:\n\n  Test statistic                               568.519\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.988\n  Tucker-Lewis Index (TLI)                       0.977\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2525.349\n  Loglikelihood unrestricted model (H1)      -2517.981\n                                                      \n  Akaike (AIC)                                5076.698\n  Bayesian (BIC)                              5124.891\n  Sample-size adjusted Bayesian (SABIC)       5083.662\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.053\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.095\n  P-value H_0: RMSEA &lt;= 0.050                    0.402\n  P-value H_0: RMSEA &gt;= 0.080                    0.159\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.035\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n  f1 =~                                                        \n    X4                0.965    0.059   16.296    0.000    0.965\n    X5                1.123    0.067   16.845    0.000    1.123\n    X6                0.895    0.058   15.465    0.000    0.895\n  f2 =~                                                        \n    X7                0.659    0.080    8.218    0.000    0.659\n    X8                0.733    0.077    9.532    0.000    0.733\n    X9                0.599    0.075    8.025    0.000    0.599\n  Std.all\n         \n    0.826\n    0.847\n    0.793\n         \n    0.575\n    0.712\n    0.557\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n  f1 ~~                                                        \n    f2                0.275    0.072    3.813    0.000    0.275\n  Std.all\n         \n    0.275\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n    f1                1.000                               1.000\n    f2                1.000                               1.000\n   .X4                0.433    0.056    7.679    0.000    0.433\n   .X5                0.496    0.072    6.892    0.000    0.496\n   .X6                0.472    0.054    8.732    0.000    0.472\n   .X7                0.881    0.100    8.807    0.000    0.881\n   .X8                0.521    0.094    5.534    0.000    0.521\n   .X9                0.798    0.087    9.162    0.000    0.798\n  Std.all\n    1.000\n    1.000\n    0.318\n    0.282\n    0.371\n    0.670\n    0.492\n    0.689\n\n\n\nNel contesto di questi dati, l’utilizzo di un modello congenerico non è sufficiente a giustificare l’impiego del punteggio totale, che rappresenta la somma dei punteggi degli item. Questo perché, nel caso specifico, sommando i punteggi di tutti gli item, finiremmo per includere misurazioni di due costrutti distinti.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Punteggio totale e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/fa/07_total_score.html#session-info",
    "href": "chapters/fa/07_total_score.html#session-info",
    "title": "30  Punteggio totale e modello fattoriale",
    "section": "30.3 Session Info",
    "text": "30.3 Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] modelsummary_2.2.0 nortest_1.0-4      MASS_7.3-61       \n [4] ggokabeito_0.1.0   viridis_0.6.5      viridisLite_0.4.2 \n [7] ggpubr_0.6.0       ggExtra_0.10.1     gridExtra_2.3     \n[10] patchwork_1.3.0    bayesplot_1.11.1   semTools_0.5-6    \n[13] semPlot_1.1.6      lavaan_0.6-19      psych_2.4.6.26    \n[16] scales_1.3.0       markdown_1.13      knitr_1.48        \n[19] lubridate_1.9.3    forcats_1.0.0      stringr_1.5.1     \n[22] dplyr_1.1.4        purrr_1.0.2        readr_2.1.5       \n[25] tidyr_1.3.1        tibble_3.2.1       ggplot2_3.5.1     \n[28] tidyverse_2.0.0    here_1.0.1        \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.16.0    jsonlite_1.8.9       magrittr_2.0.3      \n  [4] TH.data_1.1-2        estimability_1.5.1   farver_2.1.2        \n  [7] nloptr_2.1.1         rmarkdown_2.28       vctrs_0.6.5         \n [10] Cairo_1.6-2          minqa_1.2.8          base64enc_0.1-3     \n [13] rstatix_0.7.2        htmltools_0.5.8.1    broom_1.0.7         \n [16] Formula_1.2-5        htmlwidgets_1.6.4    plyr_1.8.9          \n [19] sandwich_3.1-1       rio_1.2.3            emmeans_1.10.4      \n [22] zoo_1.8-12           uuid_1.2-1           igraph_2.0.3        \n [25] mime_0.12            lifecycle_1.0.4      pkgconfig_2.0.3     \n [28] Matrix_1.7-0         R6_2.5.1             fastmap_1.2.0       \n [31] shiny_1.9.1          digest_0.6.37        OpenMx_2.21.12      \n [34] fdrtool_1.2.18       colorspace_2.1-1     rprojroot_2.0.4     \n [37] Hmisc_5.1-3          labeling_0.4.3       fansi_1.0.6         \n [40] timechange_0.3.0     abind_1.4-8          compiler_4.4.1      \n [43] withr_3.0.1          glasso_1.11          htmlTable_2.4.3     \n [46] backports_1.5.0      carData_3.0-5        R.utils_2.12.3      \n [49] ggsignif_0.6.4       GPArotation_2024.3-1 corpcor_1.6.10      \n [52] gtools_3.9.5         tools_4.4.1          pbivnorm_0.6.0      \n [55] foreign_0.8-87       zip_2.3.1            httpuv_1.6.15       \n [58] nnet_7.3-19          R.oo_1.26.0          glue_1.8.0          \n [61] quadprog_1.5-8       promises_1.3.0       nlme_3.1-166        \n [64] lisrelToR_0.3        grid_4.4.1           pbdZMQ_0.3-13       \n [67] checkmate_2.3.2      cluster_2.1.6        reshape2_1.4.4      \n [70] generics_0.1.3       gtable_0.3.5         tzdb_0.4.0          \n [73] R.methodsS3_1.8.2    data.table_1.16.0    hms_1.1.3           \n [76] car_3.1-3            utf8_1.2.4           tables_0.9.31       \n [79] sem_3.1-16           pillar_1.9.0         IRdisplay_1.1       \n [82] rockchalk_1.8.157    later_1.3.2          splines_4.4.1       \n [85] lattice_0.22-6       survival_3.7-0       kutils_1.73         \n [88] tidyselect_1.2.1     miniUI_0.1.1.1       pbapply_1.7-2       \n [91] stats4_4.4.1         xfun_0.48            qgraph_1.9.8        \n [94] arm_1.14-4           stringi_1.8.4        pacman_0.5.1        \n [97] boot_1.3-31          evaluate_1.0.0       codetools_0.2-20    \n[100] mi_1.1               cli_3.6.3            RcppParallel_5.1.9  \n[103] IRkernel_1.3.2       rpart_4.1.23         xtable_1.8-4        \n[106] repr_1.1.7           munsell_0.5.1        Rcpp_1.0.13         \n[109] coda_0.19-4.1        png_0.1-8            XML_3.99-0.17       \n[112] parallel_4.4.1       jpeg_0.1-10          lme4_1.1-35.5       \n[115] mvtnorm_1.3-1        insight_0.20.5       openxlsx_4.2.7.1    \n[118] crayon_1.5.3         rlang_1.1.4          multcomp_1.4-26     \n[121] mnormt_2.1.1        \n\n\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "FA",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Punteggio totale e modello fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/01_val_matrici.html",
    "href": "chapters/extraction/01_val_matrici.html",
    "title": "31  Valutazione della matrice di correlazione",
    "section": "",
    "text": "31.1 Introduzione\nPrima di eseguire un’analisi fattoriale, è importante esaminare la matrice di correlazione tra le variabili. Se il determinante della matrice è nullo, l’analisi fattoriale non può essere eseguita. In caso contrario, è necessario valutare se le correlazioni campionarie sono sufficientemente grandi da giustificare l’analisi fattoriale. Se le correlazioni tra gli item sono modeste, la soluzione fornita dall’analisi fattoriale potrebbe non essere parsimoniosa. Per valutare questo, si può ispezionare visivamente la matrice di correlazione e utilizzare due test: il test della sfericità di Bartlett e il test Kaiser-Meyer-Olkin.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Valutazione della matrice di correlazione</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/01_val_matrici.html#ispezione-visiva-della-matrice-di-correlazione",
    "href": "chapters/extraction/01_val_matrici.html#ispezione-visiva-della-matrice-di-correlazione",
    "title": "31  Valutazione della matrice di correlazione",
    "section": "31.2 Ispezione visiva della matrice di correlazione",
    "text": "31.2 Ispezione visiva della matrice di correlazione\nL’ispezione visiva della matrice di correlazione viene effettuata per verificare la presenza di blocchi di correlazioni alte tra loro e basse con le altre variabili. Ciò suggerisce la presenza di più fattori comuni.\nPer fare un esempio, consideriamo il dataset HolzingerSwineford1939. Tale dataset contiene 301 osservazioni di punteggi di abilità mentale. Consideriao qui le variabili x1 – x9.\n\ndata(HolzingerSwineford1939)\nglimpse(HolzingerSwineford1939)\n\nRows: 301\nColumns: 15\n$ id     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17~\n$ sex    &lt;int&gt; 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1,~\n$ ageyr  &lt;int&gt; 13, 13, 13, 13, 12, 14, 12, 12, 13, 12, 12, 12, 12, 1~\n$ agemo  &lt;int&gt; 1, 7, 1, 2, 2, 1, 1, 2, 0, 5, 2, 11, 7, 8, 6, 1, 11, ~\n$ school &lt;fct&gt; Pasteur, Pasteur, Pasteur, Pasteur, Pasteur, Pasteur,~\n$ grade  &lt;int&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,~\n$ x1     &lt;dbl&gt; 3.333333, 5.333333, 4.500000, 5.333333, 4.833333, 5.3~\n$ x2     &lt;dbl&gt; 7.75, 5.25, 5.25, 7.75, 4.75, 5.00, 6.00, 6.25, 5.75,~\n$ x3     &lt;dbl&gt; 0.375, 2.125, 1.875, 3.000, 0.875, 2.250, 1.000, 1.87~\n$ x4     &lt;dbl&gt; 2.333333, 1.666667, 1.000000, 2.666667, 2.666667, 1.0~\n$ x5     &lt;dbl&gt; 5.75, 3.00, 1.75, 4.50, 4.00, 3.00, 6.00, 4.25, 5.75,~\n$ x6     &lt;dbl&gt; 1.2857143, 1.2857143, 0.4285714, 2.4285714, 2.5714286~\n$ x7     &lt;dbl&gt; 3.391304, 3.782609, 3.260870, 3.000000, 3.695652, 4.3~\n$ x8     &lt;dbl&gt; 5.75, 6.25, 3.90, 5.30, 6.30, 6.65, 6.20, 5.15, 4.65,~\n$ x9     &lt;dbl&gt; 6.361111, 7.916667, 4.416667, 4.861111, 5.916667, 7.5~\n\n\n\nhz &lt;- HolzingerSwineford1939 |&gt;\n  dplyr::select(x1:x9)\n\nhz |&gt;\n  slice(1:5) \n\n\nA data.frame: 5 x 9\n\n\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n3.333333\n7.75\n0.375\n2.333333\n5.75\n1.2857143\n3.391304\n5.75\n6.361111\n\n\n5.333333\n5.25\n2.125\n1.666667\n3.00\n1.2857143\n3.782609\n6.25\n7.916667\n\n\n4.500000\n5.25\n1.875\n1.000000\n1.75\n0.4285714\n3.260870\n3.90\n4.416667\n\n\n5.333333\n7.75\n3.000\n2.666667\n4.50\n2.4285714\n3.000000\n5.30\n4.861111\n\n\n4.833333\n4.75\n0.875\n2.666667\n4.00\n2.5714286\n3.695652\n6.30\n5.916667\n\n\n\n\n\nValutiamo la presenza di dati mancanti.\n\nmissings &lt;- colSums(is.na(hz)) # Count # missing in each column\nsummary(missings) \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n\n\nIn questo set di dati non ci sono dati mancanti.\nEsaminiamo la distribuzione delle variabili.\n\ndescribe(hz)\n\n\nA psych: 9 x 13\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nx1\n1\n301\n4.935770\n1.167432\n5.000000\n4.964730\n1.235500\n0.6666667\n8.500000\n7.833333\n-0.2543455\n0.30753382\n0.06728967\n\n\nx2\n2\n301\n6.088040\n1.177451\n6.000000\n6.017635\n1.111950\n2.2500000\n9.250000\n7.000000\n0.4700766\n0.33239397\n0.06786712\n\n\nx3\n3\n301\n2.250415\n1.130979\n2.125000\n2.199170\n1.297275\n0.2500000\n4.500000\n4.250000\n0.3834294\n-0.90752645\n0.06518857\n\n\nx4\n4\n301\n3.060908\n1.164116\n3.000000\n3.024896\n0.988400\n0.0000000\n6.333333\n6.333333\n0.2674867\n0.08012676\n0.06709855\n\n\nx5\n5\n301\n4.340532\n1.290472\n4.500000\n4.395228\n1.482600\n1.0000000\n7.000000\n6.000000\n-0.3497961\n-0.55253689\n0.07438158\n\n\nx6\n6\n301\n2.185572\n1.095603\n2.000000\n2.088322\n1.059000\n0.1428571\n6.142857\n6.000000\n0.8579486\n0.81655717\n0.06314952\n\n\nx7\n7\n301\n4.185902\n1.089534\n4.086957\n4.163630\n1.095835\n1.3043478\n7.434783\n6.130435\n0.2490881\n-0.30740386\n0.06279967\n\n\nx8\n8\n301\n5.527076\n1.012615\n5.500000\n5.492946\n0.963690\n3.0500000\n10.000000\n6.950000\n0.5252580\n1.17155564\n0.05836617\n\n\nx9\n9\n301\n5.374123\n1.009152\n5.416667\n5.366067\n0.988400\n2.7777778\n9.250000\n6.472222\n0.2038709\n0.28990791\n0.05816654\n\n\n\n\n\nI valorei di asimmetria e kurosi sono adeguati.\nConsideriamo ora le correlazioni tra le variabili usando le funzioni del pacchetto corrr:\n\ncorrr::rearrange raggruppa le variabili altamente correlate\ncorrr::rplot visualizza il risultato.\n\n\ncor_tb &lt;- correlate(hz)\n\ncor_tb |&gt;\n  rearrange() |&gt;\n  rplot(colors = c(\"red\", \"white\", \"blue\"))\n\n\n\n\n\n\n\n\nIl grafico suggerisce la presenza di tre gruppi di variabili:\n\nda x4 a x6 (primo gruppo)\nda x1 a x3 (secondo gruppo)\nda x7 a x9 (terzo gruppo).",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Valutazione della matrice di correlazione</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/01_val_matrici.html#sfericità-di-bartlett",
    "href": "chapters/extraction/01_val_matrici.html#sfericità-di-bartlett",
    "title": "31  Valutazione della matrice di correlazione",
    "section": "31.3 Sfericità di Bartlett",
    "text": "31.3 Sfericità di Bartlett\nIl test di sfericità di Bartlett verifica l’ipotesi che il campione provenga da una popolazione in cui le variabili non sono correlate. Formalmente, il test della sfericità di Bartlett verifica l’ipotesi \\(H_0 : \\boldsymbol{R} = \\boldsymbol{I}\\) tramite la formula:\n\\[\n\\chi^2 = -\\bigg[n -1 -\\frac{1}{6} (2p +5)\\bigg] \\ln |\\boldsymbol{R}|,\n\\]\nin cui \\(n\\) è il numero dei soggetti, \\(p\\) il numero delle variabili e \\(|\\boldsymbol{R}|\\) il determinante della matrice di correlazione.\nLa statistica del test di sfericità di Bartlett segue una distribuzione chi-quadro con \\(p(p - 1)/2\\) gradi di libertà. Un valore elevato della statistica indica che la matrice di correlazione R contiene valori di correlazione significativamente diversi da 0. Al contrario, un valore basso della statistica indica che le correlazioni sono basse e non si distinguono da 0.\nIl limite di questo test è che dipende dal numero delle variabili e dalla numerosità del campione, quindi tende a rigettare \\(H_0\\) all’aumentare del campione e del numero delle variabili, anche se le correlazioni sono piccole.\nApplichiamo il test di Bartlet per il dati dell’esempio in discussione.\n\ncor_mat &lt;- cor(hz)\n\nout = cortest.bartlett(R = cor_mat, n = 301)\nprint(out)\n\n$chisq\n[1] 904.0971\n\n$p.value\n[1] 1.912079e-166\n\n$df\n[1] 36\n\n\n\nIl risultato del test di Bartlett sui dati HolzingerSwineford1939 indica che esiste una correlazione tra le variabili.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Valutazione della matrice di correlazione</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/01_val_matrici.html#test-di-adeguatezza-campionaria-di-kaiser-meyer-olkin",
    "href": "chapters/extraction/01_val_matrici.html#test-di-adeguatezza-campionaria-di-kaiser-meyer-olkin",
    "title": "31  Valutazione della matrice di correlazione",
    "section": "31.4 Test di adeguatezza campionaria di Kaiser-Meyer-Olkin",
    "text": "31.4 Test di adeguatezza campionaria di Kaiser-Meyer-Olkin\nIl test di Kaiser-Meyer-Olkin (KMO) è uno strumento statistico che valuta l’adeguatezza dei dati per l’analisi fattoriale. Esso misura la proporzione di varianza tra le variabili che potrebbe essere attribuita a fattori comuni. Un valore KMO più alto indica una maggiore adattabilità dei dati all’analisi fattoriale.\nLa statistica di adeguatezza campionaria KMO è data da\n\\[\\text{KMO} = \\frac{\\sum_i\\sum_j r^2_{ij}}{\\sum_i\\sum_j r^2_{ij} +\\sum_i\\sum_jp^2_{ij}},\\]\ndove \\(r_{ij}\\) sono le correlazioni osservate e \\(p_{ij}\\) sono le correlazioni parzializzate su tutte le altre. Se le correlazioni parzializzate sono piccole, KMO tende a 1.\nSecondo Kaiser (1970), l’adeguatezza campionaria si valuta nel modo seguente:\n\nda 0.00 a 0.49: inaccettabile\nda 0.50 a 0.59: miserabile\nda 0.60 a 0.69: mediocre\nda 0.70 a 0.79: media\nda 0.80 a 0.89: meritevole\nda 0.90 a 1.00: meravigliosa.\n\nApplichiamo il test KMO ai dati HolzingerSwineford1939.\n\nout = KMO(cor_mat)\nprint(out)\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = cor_mat)\nOverall MSA =  0.75\nMSA for each item = \n  x1   x2   x3   x4   x5   x6   x7   x8   x9 \n0.81 0.78 0.73 0.76 0.74 0.81 0.59 0.68 0.79 \n\n\nPer questi dati, il risultato del test KMO indica che l’adeguatezza campionaria è media.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Valutazione della matrice di correlazione</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/01_val_matrici.html#session-info",
    "href": "chapters/extraction/01_val_matrici.html#session-info",
    "title": "31  Valutazione della matrice di correlazione",
    "section": "31.5 Session Info",
    "text": "31.5 Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] corrr_0.4.4       nortest_1.0-4     MASS_7.3-61      \n [4] ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2\n [7] ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n[10] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6   \n[13] semPlot_1.1.6     lavaan_0.6-19     psych_2.4.6.26   \n[16] scales_1.3.0      markdown_1.13     knitr_1.48       \n[19] lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n[22] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5      \n[25] tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n[28] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.16.0  jsonlite_1.8.9     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5.1 farver_2.1.2      \n  [7] nloptr_2.1.1       rmarkdown_2.28     vctrs_0.6.5       \n [10] Cairo_1.6-2        minqa_1.2.8        base64enc_0.1-3   \n [13] rstatix_0.7.2      htmltools_0.5.8.1  broom_1.0.7       \n [16] Formula_1.2-5      htmlwidgets_1.6.4  plyr_1.8.9        \n [19] sandwich_3.1-1     emmeans_1.10.4     zoo_1.8-12        \n [22] uuid_1.2-1         igraph_2.0.3       iterators_1.0.14  \n [25] mime_0.12          lifecycle_1.0.4    pkgconfig_2.0.3   \n [28] Matrix_1.7-0       R6_2.5.1           fastmap_1.2.0     \n [31] shiny_1.9.1        digest_0.6.37      OpenMx_2.21.12    \n [34] fdrtool_1.2.18     colorspace_2.1-1   rprojroot_2.0.4   \n [37] Hmisc_5.1-3        seriation_1.5.6    labeling_0.4.3    \n [40] fansi_1.0.6        timechange_0.3.0   abind_1.4-8       \n [43] compiler_4.4.1     withr_3.0.1        glasso_1.11       \n [46] htmlTable_2.4.3    backports_1.5.0    carData_3.0-5     \n [49] ggsignif_0.6.4     corpcor_1.6.10     gtools_3.9.5      \n [52] tools_4.4.1        pbivnorm_0.6.0     foreign_0.8-87    \n [55] zip_2.3.1          httpuv_1.6.15      nnet_7.3-19       \n [58] glue_1.8.0         quadprog_1.5-8     promises_1.3.0    \n [61] nlme_3.1-166       lisrelToR_0.3      grid_4.4.1        \n [64] pbdZMQ_0.3-13      checkmate_2.3.2    cluster_2.1.6     \n [67] reshape2_1.4.4     generics_0.1.3     gtable_0.3.5      \n [70] tzdb_0.4.0         ca_0.71.1          data.table_1.16.0 \n [73] hms_1.1.3          car_3.1-3          utf8_1.2.4        \n [76] sem_3.1-16         foreach_1.5.2      pillar_1.9.0      \n [79] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [82] splines_4.4.1      lattice_0.22-6     survival_3.7-0    \n [85] kutils_1.73        tidyselect_1.2.1   registry_0.5-1    \n [88] miniUI_0.1.1.1     pbapply_1.7-2      stats4_4.4.1      \n [91] xfun_0.48          qgraph_1.9.8       arm_1.14-4        \n [94] stringi_1.8.4      pacman_0.5.1       boot_1.3-31       \n [97] evaluate_1.0.0     codetools_0.2-20   mi_1.1            \n[100] cli_3.6.3          RcppParallel_5.1.9 IRkernel_1.3.2    \n[103] rpart_4.1.23       xtable_1.8-4       repr_1.1.7        \n[106] munsell_0.5.1      Rcpp_1.0.13        coda_0.19-4.1     \n[109] png_0.1-8          XML_3.99-0.17      parallel_4.4.1    \n[112] jpeg_0.1-10        lme4_1.1-35.5      mvtnorm_1.3-1     \n[115] openxlsx_4.2.7.1   crayon_1.5.3       rlang_1.1.4       \n[118] TSP_1.2-4          multcomp_1.4-26    mnormt_2.1.1",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Valutazione della matrice di correlazione</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html",
    "href": "chapters/extraction/02_estrazione.html",
    "title": "32  L’estrazione dei fattori",
    "section": "",
    "text": "32.1 Introduzione\nL’analisi fattoriale è una tecnica statistica che mira a semplificare un insieme complesso di variabili osservate, identificando un numero ridotto di fattori latenti che spieghino le correlazioni tra queste variabili. In altre parole, l’obiettivo è trovare un numero minore di variabili non osservabili (i fattori) che possano spiegare le interrelazioni tra un gran numero di variabili osservate.\nIl modello matematico dell’analisi fattoriale può essere espresso come:\n\\[\n\\boldsymbol{\\Sigma} =\\boldsymbol{\\Lambda} \\boldsymbol{\\Phi} \\boldsymbol{\\Lambda}^{\\mathsf{T}} + \\boldsymbol{\\Psi}\n\\]\ndove:\nL’estrazione dei fattori è il processo attraverso il quale si stima la matrice dei carichi fattoriali Λ. Esistono diversi metodi per estrarre i fattori, tra cui:",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#introduzione",
    "href": "chapters/extraction/02_estrazione.html#introduzione",
    "title": "32  L’estrazione dei fattori",
    "section": "",
    "text": "Σ è la matrice delle covarianze tra le variabili osservate.\nΛ è la matrice dei carichi fattoriali, che indica la relazione tra le variabili osservate e i fattori latenti.\nΦ è la matrice delle correlazioni tra i fattori latenti.\nΨ è una matrice diagonale che contiene le unicità, ovvero la parte della varianza di ciascuna variabile osservata che non è spiegata dai fattori comuni.\n\n\n\nMetodo delle componenti principali: Questo metodo massimizza la varianza spiegata dai fattori, ma non tiene conto esplicitamente dell’errore di misura.\nMetodo dei fattori principali: Questo metodo è una variante del metodo delle componenti principali, ma tiene conto dell’errore di misura stimando le comunalità iniziali.\nMetodo dei fattori principali iterato: È una versione iterativa del metodo dei fattori principali, in cui le comunalità vengono stimate ripetutamente fino a convergenza.\nMetodo di massima verosimiglianza: Questo metodo stima i parametri del modello assumendo una distribuzione normale multivariata per le variabili osservate.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#metodo-delle-componenti-principali",
    "href": "chapters/extraction/02_estrazione.html#metodo-delle-componenti-principali",
    "title": "32  L’estrazione dei fattori",
    "section": "32.2 Metodo delle componenti principali",
    "text": "32.2 Metodo delle componenti principali\nL’analisi fattoriale eseguita mediante il metodo delle componenti principali, nonostante il nome, non è un’analisi delle componenti principali. Il metodo delle componenti principali costituisce invece un’applicazione del teorema di scomposizione spettrale di una matrice. Il teorema spettrale afferma che, data la matrice simmetrica \\(\\textbf{S}_{p \\times p}\\), è sempre possibile trovare una matrice \\(\\textbf{C}_{p \\times p}\\) ortogonale tale che $ = ^{} $ con D diagonale. Il teorema specifica inoltre che gli elementi presenti sulla diagonale di D sono gli autovalori di S, mentre le colonne di C rappresentano i rispettivi autovettori normalizzati associati agli autovalori di S.\nFacciamo un esempio numerico utilizzando i dati discussi da Rencher(2002). Brown, Williams e Barlow (1984) hanno raccolto le valutazioni di una ragazza dodicenne relativamente a sette persone di sua conoscenza. Ciascuna persona veniva valutata su una scala a nove punti rispetto a cinque variabili: kind, intelligent, happy, likeable e just. La matrice di correlazione per tali variabili è riportata di seguito:\n\nR &lt;- matrix(c(\n  1.000, .296, .881, .995, .545,\n  .296, 1.000, -.022, .326, .837,\n  .881, -.022, 1.000, .867, .130,\n  .995, .326, .867, 1.000, .544,\n  .545, .837, .130, .544, 1.000\n),\nncol = 5, byrow = T, dimnames = list(\n  c(\"K\", \"I\", \"H\", \"L\", \"J\"),\n  c(\"K\", \"I\", \"H\", \"L\", \"J\")\n)\n)\nR\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nK\nI\nH\nL\nJ\n\n\n\n\nK\n1.000\n0.296\n0.881\n0.995\n0.545\n\n\nI\n0.296\n1.000\n-0.022\n0.326\n0.837\n\n\nH\n0.881\n-0.022\n1.000\n0.867\n0.130\n\n\nL\n0.995\n0.326\n0.867\n1.000\n0.544\n\n\nJ\n0.545\n0.837\n0.130\n0.544\n1.000\n\n\n\n\n\nGli autovalori e gli autovettori si calcolano con la funzione eigen():\n\ne &lt;- eigen(R)\nprint(e)\n\neigen() decomposition\n$values\n[1] 3.2633766259 1.5383820947 0.1679692814 0.0300298228 0.0002421752\n\n$vectors\n           [,1]       [,2]        [,3]       [,4]       [,5]\n[1,] -0.5367301 -0.1859819 -0.18991841 -0.1247931  0.7910052\n[2,] -0.2874672  0.6505666  0.68488713 -0.1198141  0.1034406\n[3,] -0.4343545 -0.4736877  0.40694897  0.6136634 -0.2115794\n[4,] -0.5373909 -0.1692797 -0.09532905 -0.6293896 -0.5266275\n[5,] -0.3896544  0.5377158 -0.56583170  0.4442491 -0.2037363\n\n\n\nCome indicato in precedenza, la matrice R può essere espressa come\n\\[\n\\textbf{R} = \\textbf{C}\\textbf{D}\\textbf{C}^{\\mathsf{T}}\n\\]\n\ne$vectors %*% diag(e$values) %*% t(e$vectors)\n\n\nA matrix: 5 x 5 of type dbl\n\n\n1.000\n0.296\n0.881\n0.995\n0.545\n\n\n0.296\n1.000\n-0.022\n0.326\n0.837\n\n\n0.881\n-0.022\n1.000\n0.867\n0.130\n\n\n0.995\n0.326\n0.867\n1.000\n0.544\n\n\n0.545\n0.837\n0.130\n0.544\n1.000\n\n\n\n\n\nEsaminiamo ora gli autovalori. I primi due autovalori spiegano da soli il 96% della varianza campionaria:\n\n(e$values[1] + e$values[2]) / 5\n\n0.96035174411791\n\n\nUsando i primi due autovalori e i primi due autovettori è dunque possibile riprodurre in maniera soddisfacente la matrice R operando nel contempo una riduzione di dimensionalità dei dati.\nPer fattorizzare \\(\\textbf{R} = \\textbf{C}\\textbf{D}\\textbf{C}^{\\mathsf{T}}\\) nella forma \\(\\hat{\\boldsymbol{\\Lambda}} \\hat{\\boldsymbol{\\Lambda}}^{\\mathsf{T}}\\) iniziamo a scrivere\n\\[\n\\textbf{D}= \\textbf{D}^{1/2} \\textbf{D}^{1/2}\n\\]\ndove\n\\[\n\\textbf{D}^{1/2} =\n\\left[\n  \\begin{array}{ c c c c }\n     \\sqrt{\\theta_1} & 0 & \\dots & 0 \\\\\n     0 & \\sqrt{\\theta_2} & \\dots & 0 \\\\\n     \\dots & \\dots & & \\dots \\\\\n     0 & 0 & \\dots &  \\sqrt{\\theta_p}\n  \\end{array}\n\\right]\n\\]\nViene qui usata la notazione \\(\\theta\\) per denotare gli autovalori anziché il tradizionale \\(\\lambda\\) per evitare la confusione con la notazione \\(\\lambda_{jl}\\) usata per le saturazioni fattoriali. In questo modo, possiamo scrivere\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\textbf{R} &= \\textbf{C}\\textbf{D}\\textbf{C}^{\\mathsf{T}}\\notag\\\\\n&= \\textbf{C}\\textbf{D}^{1/2}\\textbf{D}^{1/2}\\textbf{C}^{\\mathsf{T}}\\notag\\\\\n&= (\\textbf{C}\\textbf{D}^{1/2}) (\\textbf{C}\\textbf{D}^{1/2})^{\\mathsf{T}}.\n\\end{aligned}\n\\end{equation}\n\\]\nNon possiamo però limiarci a definire \\(\\hat{\\boldsymbol{\\Lambda}}=\\textbf{C}\\textbf{D}^{1/2}\\) in quanto \\(\\textbf{C}\\textbf{D}^{1/2}\\) è di ordine \\(p \\times p\\) e non otteniamo quindi una riduzione di dimensioni. Quello che cerchiamo è una matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\) di ordine \\(p \\times m\\) con \\(m &lt; p\\). Dunque, definiamo la matrice \\(\\textbf{D}_1= \\text{diag}(\\theta_1,\n\\theta_2, \\dots, \\theta_m)\\) come la la matrice contenente gli \\(m\\) autovalori più grandi di R e \\(\\textbf{C}_1=( \\textbf{c}_1,\n\\textbf{c}_2, \\dots,  \\textbf{c}_m)\\) come la matrice contenente i rispettivi autovettori.\nMediante il metodo delle componenti principali, le saturazioni fattoriali \\(\\hat{\\boldsymbol{\\Lambda}}\\) vengono quindi stimate nel modo seguente:\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\hat{\\boldsymbol{\\Lambda}} &= \\textbf{C}_1 \\textbf{D}_1^{1/2}\\notag\\\\\n&= (\\sqrt{\\theta_1} \\textbf{c}_1, \\sqrt{\\theta_2} \\textbf{c}_2,\n\\dots, \\sqrt{\\theta_m} \\textbf{c}_m)\n\\end{aligned}\n\\end{equation}\n\\]\nPer l’esempio presente, con \\(m=2\\) e \\(p=5\\), avremo\n\\[\n\\left[\n  \\begin{array}{ c c }\n\\hat{\\lambda}_{11} & \\hat{\\lambda}_{12} \\\\\n\\hat{\\lambda}_{21} & \\hat{\\lambda}_{22} \\\\\n\\hat{\\lambda}_{31} & \\hat{\\lambda}_{32} \\\\\n\\hat{\\lambda}_{41} & \\hat{\\lambda}_{42} \\\\\n\\hat{\\lambda}_{51} & \\hat{\\lambda}_{52}\n  \\end{array}\n\\right] =\n\\left[\n  \\begin{array}{ c c }\nc_{11} & c_{12} \\\\\nc_{21} & c_{22} \\\\\nc_{31} & c_{32} \\\\\nc_{41} & c_{42} \\\\\nc_{51} & c_{52}\n  \\end{array}\n\\right]\n\\left[\n  \\begin{array}{ c c }\n\\sqrt{\\theta_1} & 0\\\\\n0 &\\sqrt{\\theta_2}\n  \\end{array}\n\\right]\n\\]\nLe saturazioni fattoriali stimate sono dunque uguali a\n\\[\n\\left[\n  \\begin{array}{ c c }\n\\sqrt{\\theta_1}c_{11} & \\sqrt{\\theta_2}c_{12} \\\\\n\\sqrt{\\theta_1}c_{21} & \\sqrt{\\theta_2}c_{22} \\\\\n\\sqrt{\\theta_1}c_{31} & \\sqrt{\\theta_2}c_{32} \\\\\n\\sqrt{\\theta_1}c_{41} & \\sqrt{\\theta_2}c_{42} \\\\\n\\sqrt{\\theta_1}c_{51} & \\sqrt{\\theta_2}c_{52}\n  \\end{array}\n\\right]\n\\]\nSvolgendo i calcoli con \\(\\textsf{R}\\) otteniamo:\n\nL &lt;- cbind(\n  e$vectors[, 1] * sqrt(e$values[1]),\n  e$vectors[, 2] * sqrt(e$values[2])\n)\n\nround(L, 3)\n\n\nA matrix: 5 x 2 of type dbl\n\n\n-0.970\n-0.231\n\n\n-0.519\n0.807\n\n\n-0.785\n-0.588\n\n\n-0.971\n-0.210\n\n\n-0.704\n0.667\n\n\n\n\n\nLa matrice di correlazione riprodotta (con le comunalità sulla diagonale principale) diventa\n\nR_hat &lt;- round(L %*% t(L), 3)\nR_hat\n\n\nA matrix: 5 x 5 of type dbl\n\n\n0.993\n0.317\n0.896\n0.990\n0.529\n\n\n0.317\n0.921\n-0.067\n0.335\n0.904\n\n\n0.896\n-0.067\n0.961\n0.885\n0.160\n\n\n0.990\n0.335\n0.885\n0.987\n0.543\n\n\n0.529\n0.904\n0.160\n0.543\n0.940\n\n\n\n\n\nLa matrice di correlazioni residue (con le specificità sulla diagonale principale) è la seguente.\n\nR - R_hat\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nK\nI\nH\nL\nJ\n\n\n\n\nK\n0.007\n-0.021\n-0.015\n0.005\n0.016\n\n\nI\n-0.021\n0.079\n0.045\n-0.009\n-0.067\n\n\nH\n-0.015\n0.045\n0.039\n-0.018\n-0.030\n\n\nL\n0.005\n-0.009\n-0.018\n0.013\n0.001\n\n\nJ\n0.016\n-0.067\n-0.030\n0.001\n0.060\n\n\n\n\n\nPossiamo ora capire il motivo del nome metodo delle componenti principali: le saturazioni fattoriali sono proporzionali agli autovettori di \\(\\textbf{R}\\). Tuttavia, l’interpretazione è diversa da quella che viene assegnata ai risultati dell’analisi delle componenti principali.\nÈ possibile condurre l’analisi fattoriale con il metodo delle componenti principali sia utilizzando la matrice \\(\\textbf{S}\\) di varianze-covarianze sia la matrice \\(\\textbf{R}\\) delle correlazioni. Tuttavia, le soluzioni ottenute usando \\(\\textbf{S}\\) o \\(\\textbf{R}\\) non sono legate da una relazione algebrica: il metodo delle componenti principali non è invariante rispetto ai cambiamenti di scala delle osservazioni. Un altro svantaggio del metodo delle componenti principali è che non fornisce un test di bontà di adattamento. Tale test può essere invece svolto quando la soluzione viene trovata con il metodo della massima verosimiglianza.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#metodo-dei-fattori-principali",
    "href": "chapters/extraction/02_estrazione.html#metodo-dei-fattori-principali",
    "title": "32  L’estrazione dei fattori",
    "section": "32.3 Metodo dei fattori principali",
    "text": "32.3 Metodo dei fattori principali\nIl metodo dei fattori principali (principal factor method, anche detto principal axis method) è uno dei metodi maggiormente usati per la stima delle saturazioni fattoriali e delle comunalità. Il metodo delle componenti principali che abbiamo usato in precedenza trascura la specificità \\(\\boldsymbol{\\Psi}\\) e si limita a fattorializzare le covarianze di S o le correlazioni di R. Il metodo dei fattori principali affronta questo problema ricorrendo ad una procedura simile al metodo delle componenti principali, nella quale però viene utilizzata una matrice ridotta di varianze-covarianze \\(\\textbf{S} - \\hat{\\boldsymbol{\\Psi}}\\) in cui una stima delle comunalità viene sostituita alle varianze presenti sulla diagonale principale della matrice S o R.\nNel caso della matrice ridotta di correlazioni \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\), per la comunalità \\(i\\)-esima \\(\\sum_{j}\\lambda_{ij}^2\\) si sceglie il quadrato del coefficiente di correlazione multipla tra \\(Y_i\\) e tutte le altre \\(p-1\\) variabili. Tale valore si può trovare nel modo seguente\n\\[\n\\hat{h}^2_i=R^2_i=1-\\frac{1}{r^{ii}},\n\\]\ndove \\(r^{ii}\\) è l’elemento diagonale \\(i\\)-esimo di \\(\\textbf{R}^{-1}\\). Nel caso di una matrice ridotta di varianze-covarianze \\(\\textbf{S} - \\hat{\\boldsymbol{\\Psi}}\\), le comunalità possono essere stimate calcolando\n\\[\n\\hat{h}_i^2=s_{ii}-\\frac{1}{r^{ii}},\n\\]\ndove \\(s_{ii}\\) è l’elemento diagonale \\(i\\)-esimo di \\(\\textbf{S}\\).\nAffinché le stime comunalità possano essere calcolate come descritto sopra, la matrice \\(\\textbf{R}\\) non deve essere singolare. Se \\(\\textbf{R}\\) è singolare, per la stima della comunalità \\(i\\)-esima, \\(\\hat{h}^2_i\\), si utilizza il valore assoluto del più elevato coefficiente di correlazione lineare tra \\(Y_i\\) e le altre variabili.\nScelta la stima della comunalità, la matrice ridotta di varianze-covarianze si ottiene sostituendo le stime delle comunalità alle varianze sulla diagonale principale della matrice \\(\\textbf{S}\\):\n\\[\n\\textbf{S} - \\hat{\\boldsymbol{\\Psi}} =\n\\left[\n  \\begin{array}{ c c c c }\n    \\hat{h}^2_1 & s_{12} & \\dots & s_{1p} \\\\\n    s_{21} & \\hat{h}^2_2 & \\dots & s_{2p} \\\\\n    \\dots & \\dots &           & \\dots\\\\\n    s_{p1} &  s_{p2} & \\dots & \\hat{h}^2_p\n  \\end{array}\n\\right]\n\\]\nIn maniera equivalente, la matrice ridotta di correlazioni si ottiene nel modo seguente:\n\\[\n\\textbf{R} - \\hat{\\boldsymbol{\\Psi}} =\n\\left[\n  \\begin{array}{ c c c c }\n    \\hat{h}^2_1 & r_{12} & \\dots & r_{1p} \\\\\n    r_{21} & \\hat{h}^2_2 & \\dots & r_{2p} \\\\\n    \\dots & \\dots &           & \\dots\\\\\n    r_{p1} &  r_{p2} & \\dots & \\hat{h}^2_p\n  \\end{array}\n\\right]\n\\]\nFacciamo ora un esempio concreto usando la matrice di correlazione dell’esempio precedente. Quale stima della comunalità \\(i\\)-esima, useremo il valore assoluto più elevato nella riga \\(i\\)-esima della matrice R. Per i dati dell’esempio, le stime delle comunalità sono dunque pari a \\(0.995\\), \\(0.837\\), \\(0.881\\), \\(0.995\\) e \\(0.837\\).\nInserendo tali valori nella diagonale principale, otteniamo la matrice ridotta delle correlazioni \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\):\n\nR1 &lt;- R\nh.hat &lt;- c(.995, .837, .881, .995, .837)\nR1[cbind(1:5,1:5)] &lt;- h.hat\nR1\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nK\nI\nH\nL\nJ\n\n\n\n\nK\n0.995\n0.296\n0.881\n0.995\n0.545\n\n\nI\n0.296\n0.837\n-0.022\n0.326\n0.837\n\n\nH\n0.881\n-0.022\n0.881\n0.867\n0.130\n\n\nL\n0.995\n0.326\n0.867\n0.995\n0.544\n\n\nJ\n0.545\n0.837\n0.130\n0.544\n0.837\n\n\n\n\n\nGli autovalori della matrice ridotta di correlazioni \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\) sono:\n\nee &lt;- eigen(R1)\nprint(round(ee$values, 3))\n\n[1]  3.202  1.394  0.029  0.000 -0.080\n\n\nLa somma degli autovalori è uguale a\n\nprint(sum(ee$values))\n\n[1] 4.545\n\n\nI primi due autovalori di \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\) sono:\n\nprint(round(ee$vectors[, 1:2], 3))\n\n      [,1]   [,2]\n[1,] 0.548 -0.177\n[2,] 0.272  0.656\n[3,] 0.431 -0.461\n[4,] 0.549 -0.159\n[5,] 0.373  0.549\n\n\nMoltiplicando tali valori per la radice quadrata dei rispettivi autovalori si ottengono le stime delle saturazioni fattoriali:\n\nround(ee$vectors[,1:2] %*% sqrt(diag(ee$values[1:2])), 3)\n\n\nA matrix: 5 x 2 of type dbl\n\n\n0.981\n-0.209\n\n\n0.487\n0.774\n\n\n0.772\n-0.544\n\n\n0.982\n-0.187\n\n\n0.667\n0.648\n\n\n\n\n\nTale risultato replica quello riportato da Rencher (2002).",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#metodo-dei-fattori-principali-iterato",
    "href": "chapters/extraction/02_estrazione.html#metodo-dei-fattori-principali-iterato",
    "title": "32  L’estrazione dei fattori",
    "section": "32.4 Metodo dei fattori principali iterato",
    "text": "32.4 Metodo dei fattori principali iterato\nSolitamente, per migliorare la stima della comunalità, la diagonale della matrice \\(\\textbf{S} - \\hat{\\boldsymbol{\\Psi}}\\) o \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\) viene ottenuta per iterazione. Dopo avere trovato \\(\\hat{\\boldsymbol{\\Lambda}}\\) a partire da \\(\\textbf{S} - \\hat{\\boldsymbol{\\Psi}}\\) o \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\) come indicato in precedenza, utilizzando le stime delle saturazioni fattoriali così ottenute possiamo stimare le comunalità nel modo seguente:\n\\[\\hat{h}^2_i = \\sum_{i=1}^m \\hat{\\lambda}_{ij}^2.\\]\nI valori di \\(\\hat{h}^2_i\\) così trovati vengono quindi sostituiti nella diagonale della matrice ridotta \\(\\textbf{S} - \\hat{\\boldsymbol{\\Psi}}\\) o \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\). A partire da questa nuova matrice, usando il metodo descritto in precedenza, possiamo ottenere una nuova stima delle saturazioni fattoriali \\(\\hat{\\boldsymbol{\\Lambda}}\\). Mediante questa nuova stima di \\(\\hat{\\boldsymbol{\\Lambda}}\\), possiamo procedere ad una nuova stima delle comunalità. Tale processo continua iterativamente sino alla convergenza. Gli autovalori e gli autovettori della versione finale di \\(\\textbf{S} - \\hat{\\boldsymbol{\\Psi}}\\) o \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\) vengono infine usati per stimare i pesi fattoriali. Il metodo dei fattori principali iterato e il metodo delle componenti principali producono risultati molto simili quando \\(m\\) assume un piccolo valore (questo si verifica quando le correlazioni sono alte) e quando \\(p\\) (il numero delle variabili) è grande.\n\n32.4.1 Casi di Heywood\nTra gli inconvenienti del metodo dei fattori principali iterato vi è il fatto che può talvolta portare a soluzioni inammissibili (quando viene fattorizzata la matrice R) caratterizzate da valori di comunalità maggiori di uno (caso di Heywood). Se \\(\\hat{h}^2_i &gt; 1\\) allora \\(\\hat{\\psi}_i &lt; 0\\) il che è chiaramente assurdo in quanto una varianza non può assumere un valore negativo. Solitamente, quando la stima di una comunalità è maggiore di uno, il processo iterativo viene interrotto e il programma riporta che non può essere trovata una soluzione.\nPer fare un esempio, usiamo la funzione fa() contenuta nel pacchetto psych. Tale funzione per trovare la soluzione dei fattori principali mediante il metodo iterativo.\n\npa &lt;- fa(R, nfactors = 2, rotate = \"none\", fm = \"pa\")\npa\n\nFactor Analysis using method =  pa\nCall: fa(r = R, nfactors = 2, rotate = \"none\", fm = \"pa\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n   PA1   PA2   h2     u2 com\nK 0.98 -0.21 1.01 -0.008 1.1\nI 0.48  0.74 0.77  0.230 1.7\nH 0.78 -0.56 0.92  0.085 1.8\nL 0.98 -0.19 0.99  0.010 1.1\nJ 0.69  0.69 0.95  0.049 2.0\n\n                       PA1  PA2\nSS loadings           3.22 1.41\nProportion Var        0.64 0.28\nCumulative Var        0.64 0.93\nProportion Explained  0.70 0.30\nCumulative Proportion 0.70 1.00\n\nMean item complexity =  1.5\nTest of the hypothesis that 2 factors are sufficient.\n\ndf null model =  10  with the objective function =  12\ndf of  the model are 1  and the objective function was  5.6 \n\nThe root mean square of the residuals (RMSR) is  0.01 \nThe df corrected root mean square of the residuals is  0.04 \n\nFit based upon off diagonal values = 1\n\n\nSi noti che, in questo caso, le unicità assumono valori negativi, il che suggerisce che la soluzione è impropria.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/02_estrazione.html#metodo-di-massima-verosimiglianza",
    "href": "chapters/extraction/02_estrazione.html#metodo-di-massima-verosimiglianza",
    "title": "32  L’estrazione dei fattori",
    "section": "32.5 Metodo di massima verosimiglianza",
    "text": "32.5 Metodo di massima verosimiglianza\nIl metodo di massima verosimiglianza è indicato quando si può assumere che le variabili manifeste seguano una distribuzione normale multivariata. In queste condizioni, il metodo produce stime dei pesi fattoriali che più verosimilmente hanno prodotto le correlazioni osservate. Gli stimatori di massima verosimiglianza sono preferibili a quelli ottenuti con altri metodi, a condizione che le premesse siano pienamente realizzate.\nLa funzione \\(F\\) che viene minimizzata dal metodo di massima verosimiglianza rappresenta una misura di “distanza” tra la matrice di covarianza osservata e quella predetta dal modello. Uguagliando a zero le derivate di \\(F\\) rispetto ai parametri del modello \\(\\boldsymbol{\\Lambda}\\) e \\(\\boldsymbol{\\Psi}\\) si ottengono le equazioni per le stime di massima verosimiglianza di \\(\\hat{\\boldsymbol{\\Lambda}}\\) e \\(\\hat{\\boldsymbol{\\Psi}}\\). Risolvendo tali equazioni rispetto alle incognite \\(\\hat{\\boldsymbol{\\Lambda}}\\) e \\(\\hat{\\boldsymbol{\\Psi}}\\) si ricavano le stime di massima verosimiglianza.\nPoiché non esiste una soluzione analitica per le equazioni che stimano i parametri \\(\\boldsymbol{\\Lambda}\\) e \\(\\boldsymbol{\\Psi}\\), si utilizzano metodi numerici iterativi per minimizzare la differenza tra la matrice di covarianze (o correlazioni) osservata e quella predetta dal modello. Tuttavia, le stime di massima verosimiglianza possono presentare problemi di convergenza e casi di Heywood, in cui le stime di comunalità sono superiori a 1. Nonostante ciò, la soluzione è indipendente dall’unità di misura delle variabili manifeste e si ottiene la stessa soluzione sia analizzando la matrice delle varianze e covarianze sia quella delle correlazioni.\nLa stima di massima verosimiglianza si ottiene usando factanal. È inoltre il metodo di stima di default di lavaan.\nConsideriamo nuovamente i dati dell’esempio precedente. La stima di massima verosimiglianza dei parametri \\(\\boldsymbol{\\Lambda}\\) e \\(\\boldsymbol{\\Psi}\\) si ottiene nel modo seguente. i\n\nfactanal(\n    covmat=R, \n    factors=2, \n    rotation=\"none\", \n    n.obs=225\n)\n\n\nCall:\nfactanal(factors = 2, covmat = R, n.obs = 225, rotation = \"none\")\n\nUniquenesses:\n    K     I     H     L     J \n0.005 0.268 0.055 0.008 0.005 \n\nLoadings:\n  Factor1 Factor2\nK  0.955  -0.289 \nI  0.528   0.673 \nH  0.720  -0.653 \nL  0.954  -0.287 \nJ  0.764   0.642 \n\n               Factor1 Factor2\nSS loadings      3.203   1.457\nProportion Var   0.641   0.291\nCumulative Var   0.641   0.932\n\nTest of the hypothesis that 2 factors are sufficient.\nThe chi square statistic is 648.09 on 1 degree of freedom.\nThe p-value is 5.81e-143 \n\n\nSi noti che il risultato è molto simile a quello ottenuto in precedenza. Si noti inoltre che le stime di massima verosimiglianza consentono un test di bontà di adattamento del modello ai dati (test chi quadrato).",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>L'estrazione dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html",
    "href": "chapters/extraction/03_numero_fattori.html",
    "title": "33  Il numero dei fattori",
    "section": "",
    "text": "33.1 Introduzione\nL’analisi fattoriale è un potente strumento statistico che ci permette di semplificare la complessità dei dati, identificando un numero ridotto di fattori latenti che spieghino le correlazioni tra un gran numero di variabili osservate. Immaginiamo di avere un questionario con sei aggettivi per misurare la personalità: loquace, assertivo, fantasioso, creativo, estroverso e intellettuale. Ci chiediamo: questi aggettivi misurano un unico tratto della personalità (ad esempio, l’apertura mentale) o più tratti distinti (come estroversione e apertura mentale)?\nDeterminare il numero ottimale di fattori latenti è un quesito fondamentale nell’analisi fattoriale. Aggiungere più fattori migliora sempre l’adattamento del modello ai dati, ma a scapito della parsimonia. Trovare il giusto equilibrio è cruciale per ottenere una soluzione interpretabile e significativa.\nL’analisi fattoriale esplorativa (EFA) è la tecnica statistica più utilizzata per affrontare questo problema. L’EFA permette di esplorare i dati e identificare le strutture latenti sottostanti. Tuttavia, la scelta del numero di fattori da estrarre rimane una decisione complessa, che richiede l’utilizzo di criteri statistici e di un’attenta valutazione teorica.\nMetodi per determinare il numero di fattori:\nLa scelta del numero di fattori ha importanti implicazioni per l’interpretazione dei risultati e per le decisioni successive. Un numero di fattori troppo basso può portare a una perdita di informazioni, mentre un numero troppo alto può rendere l’interpretazione difficile e poco parsimoniosa.\nIn definitiva, la scelta del numero di fattori è un processo iterativo che richiede una combinazione di criteri statistici e considerazioni teoriche. Non esiste una regola ferrea, ma piuttosto una serie di indicatori che, utilizzati congiuntamente, possono guidare il ricercatore verso una soluzione ottimale.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#introduzione",
    "href": "chapters/extraction/03_numero_fattori.html#introduzione",
    "title": "33  Il numero dei fattori",
    "section": "",
    "text": "Criteri basati sugli autovalori: Si basano sull’analisi degli autovalori della matrice di correlazione per identificare i fattori più importanti.\nCriteri informativi: Valutano l’adattamento del modello ai dati, penalizzando modelli troppo complessi.\nMetodi di simulazione: Confronta i risultati ottenuti con i dati reali con quelli ottenuti da dati simulati.\nMetodi esplorativi: Cercano di identificare strutture fattoriali semplici e interpretabili.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#tre-domande-sulla-dimensionalità-del-test",
    "href": "chapters/extraction/03_numero_fattori.html#tre-domande-sulla-dimensionalità-del-test",
    "title": "33  Il numero dei fattori",
    "section": "33.2 Tre domande sulla dimensionalità del test",
    "text": "33.2 Tre domande sulla dimensionalità del test\nSono presenti almeno tre questioni rilevanti riguardo la dimensionalità di un test.\n\nNumero di Dimensioni:\n\nLa prima questione riguarda il numero di dimensioni rifletto dagli item del test. Alcuni test riflettono una sola dimensione, mentre altri ne riflettono due o più. Questa questione è importante poiché ogni dimensione del test è probabile che venga valutata separatamente, necessitando ciascuna una propria analisi psicometrica.\n\nCorrelazione tra Dimensioni:\n\nLa seconda questione indaga se, in un test con più di una dimensione, queste dimensioni siano correlate tra loro. Alcuni test presentano diverse dimensioni che sono in qualche modo correlate, mentre altri hanno dimensioni essenzialmente indipendenti e non correlate. Questa questione è rilevante, in parte, perché la natura delle associazioni tra le dimensioni di un test ha implicazioni per la significatività del “punteggio totale” del test.\n\nNatura delle Dimensioni:\n\nLa terza questione si chiede quali sono le dimensioni in un test con più di una dimensione, ovvero, quali attributi psicologici sono riflessi dalle dimensioni del test? Ad esempio, nel test della personalità con sei aggettivi descritto precedentemente, la prima dimensione riflette l’attributo psicologico dell’estroversione o qualche altro attributo? L’importanza di questa questione è evidente: per valutare ed interpretare efficacemente una dimensione di un test, è necessario comprendere il significato psicologico del punteggio.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#metodi-basati-sugli-autovalori",
    "href": "chapters/extraction/03_numero_fattori.html#metodi-basati-sugli-autovalori",
    "title": "33  Il numero dei fattori",
    "section": "33.3 Metodi basati sugli autovalori",
    "text": "33.3 Metodi basati sugli autovalori\nSono stati proposti quattro criteri basati sugli autovalori per determinare il numero \\(m\\) di fattori da estrarre (Rencher, 2002).\n\nScegliere \\(m\\) tale per cui la varianza spiegata dal modello fattoriale superi una soglia predeterminata, per esempio l’80% della varianza totale, \\(tr(\\textbf{S})\\) o \\(tr(\\textbf{R})\\).\nScegliere \\(m\\) uguale al numero di autovalori aventi un valore maggiore del valore medio degli autovalori. Per R il valore medio degli autovalori è \\(1\\); per S è \\(\\sum_{j=1}^p \\theta_j/p\\).\nUsare lo scree test.\nMediante la statistica \\(\\chi^2\\), valutare l’ipotesi che \\(m\\) sia il numero corretto di fattori, \\(H_0: \\boldsymbol{\\Sigma} =  \\boldsymbol{\\Lambda}\n  \\boldsymbol{\\Lambda}^{\\mathsf{T}} +  \\boldsymbol{\\Psi}\\), dove \\(\\boldsymbol{\\Lambda}\\) è di ordine \\(p \\times m\\).\n\n\n33.3.1 Quota di varianza spiegata\nIl primo criterio si applica soprattutto al metodo delle componenti principali. La proporzione della varianza capionaria spiegata dal fattore \\(j\\)-esimo estratto da S è uguale a\n\\[\\sum_{i=i}^p \\hat{\\lambda}_{ij}^2 / tr(\\textbf{S}).\\]\nNel caso in cui i fattori vengano estratti da R avremo\n\\[\\sum_{i=i}^p \\hat{\\lambda}_{ij}^2 / p.\\]\nNel caso di fattori incorrelati, ciascun fattore contribuisce con una quota complessiva di varianza spiegata pari alla somma dei quadrati delle saturazioni fattoriali contenute nella matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\): \\(\\sum_{i=1}^p\\sum_{j=1}^m\\hat{\\lambda}_{ij}^2\\). Nel caso del metodo delle componenti principali, tale somma è anche uguale alla somma dei primi \\(m\\) autovalori, o alla somma di tutte le \\(p\\) comunalità:\n\\[\\sum_{i=1}^p\\sum_{j=1}^m\\hat{\\lambda}_{ij}^2= \\sum_{i=1}^p \\hat{h}_i^2\n= \\sum_{j=1}^m \\theta_j\\]\nSulla base di queste considerazioni, il numero \\(m\\) di fattori viene scelto in modo da spiegare una quota sufficientemente grande di S o \\(p\\).\nIl numero dei fattori può essere determinato in questo modo anche nel caso in cui l’analisi fattoriale venga eseguita con il metodo dei fattori principali (ovvero, nel caso in cui vengano usate le stime delle comunalità per generare la matrice ridotta \\(\\textbf{S} -\n\\hat{\\boldsymbol{\\Psi}}\\) o \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\)). In questo caso, però, è possibile che alcuni autovalori della matrice \\(\\textbf{S} - \\hat{\\boldsymbol{\\Psi}}\\) o \\(\\textbf{R} -\n\\hat{\\boldsymbol{\\Psi}}\\) assumano valore negativo. In tali circostanze, è possibile che la proporzione cumulativa della varianza \\(\\sum_{j=1}^m \\theta_j / \\sum_{j=1}^p \\theta_j\\) assuma un valore maggiore di \\(1.0\\) per \\(j &lt; p\\).\nLa proporzione cumulativa della varianza si riduce poi a \\(1.0\\) quando vengono considerati anche i successivi autovalori negativi. Di conseguenza, può succedere che, utilizzando la matrice \\(\\textbf{S} - \\hat{\\boldsymbol{\\Psi}}\\) o \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\), il criterio definito in base alla quota della varianza spiegata venga raggiunto per un valore \\(m\\) minore di quello che verrebbe trovato utilizzando la matrice S o R.\nNel caso del metodo dei fattori principali iterato, \\(m\\) viene specificato precedentemente a ciascuna iterazione e \\(\\sum_{i}\n\\hat{h}^2_i\\) viene ottenuto dopo ciascuna iterazione calcolando \\(\\text{tr}(\\textbf{S} - \\hat{\\boldsymbol{\\Psi}})\\). Per scegliere \\(m\\), come per il metodo delle componenti principali, possono essere usati gli autovalori di S o R.\n\n\n33.3.2 Valore medio degli autovalori\nIl calcolo del valore medio degli autovalori è una procedura euristica implementata in molti software. In una variante di tale metodo, \\(m\\) viene scelto in modo tale da uguagliare il numero degli autovalori positivi della matrice ridotta \\(\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}\\) (in tale matrice vi sono solitamente degli autovalori negativi). Tale variante ha però lo svantaggio di produrre solitamente un numero di fattori troppo grande.\n\n\n33.3.3 Scree test\nLo scree test si basa su un grafico che rappresenta gli autovalori di S o R ordinati in modo decrescente in funzione del numero dei fattori. I punti che rappresentano gli autovalori vengono collegati con una spezzata. Il valore m viene determinato in corrispondenza del fattore oltre il quale il dislivello tra fattori successivi diventa esiguo e la spezzata tende a diventare orizzontale.\n\n\n33.3.4 Parallel analysis\nLa Parallel Analysis si basa sul confronto tra gli autovalori empirici della matrice di correlazione delle variabili originali e quelli generati da un campione casuale di variabili standardizzate. In questo modo si tiene conto delle variazioni dovute agli errori di campionamento. Poiché anche in presenza di variabili incorrelate la matrice di correlazione presenta sempre autovalori maggiori di uno a causa della variabilità campionaria, il confronto tra gli autovalori empirici e quelli generati dalla Parallel Analysis permette di individuare il numero di fattori significativi. Una simulazione di Monte Carlo su una matrice di correlazione di \\(p=10\\) variabili casuali mutuamente indipendenti, ciascuna con \\(n=20\\) osservazioni, può essere utilizzata per illustrare la procedura.\n\nn &lt;- 20\nnsim &lt;- 1000\ne1 &lt;- rep(0, nsim)\nfor (i in 1:nsim) {\n  Y &lt;- cbind(\n    rnorm(n), rnorm(n), rnorm(n), rnorm(n), rnorm(n),\n    rnorm(n), rnorm(n), rnorm(n), rnorm(n), rnorm(n)\n  )\n  e &lt;- eigen(cor(Y))\n  e1[i] &lt;- e$values[1]\n}\nmax(e1)\n\n3.34528401303822\n\n\nPer i dati di questa simulazione, l’autovalore maggiore ha un valore pari a \\(3.35\\), anche se i dati sono del tutto casuali. La Parallel Analysis tiene conto di questo fatto e determina \\(m\\) confrontando gli autovalori empirici con le loro “controparti casuali.” Vanno a determinare \\(m\\) solo gli autovalori empirici che hanno un valore superiore ai corrispondenti autovalori generati da una matrice di dati dello stesso ordine composta da colonne mutualmente incorrelate. Nel caso di questa simulazione di Monte Carlo, se l’autovalore maggiore derivato da una matrice di numeri casuali ha un valore di \\(3.35\\), verranno considerati solo gli autovalori empirici che superano questo valore per determinare il numero di fattori.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#metodi-basati-sul-confronto-tra-modelli",
    "href": "chapters/extraction/03_numero_fattori.html#metodi-basati-sul-confronto-tra-modelli",
    "title": "33  Il numero dei fattori",
    "section": "33.4 Metodi basati sul confronto tra modelli",
    "text": "33.4 Metodi basati sul confronto tra modelli\nIl confronto tra modelli può essere eseguito usando varie statistiche. Una scelta popolare per stimare il numero di fattori nella EFA è il Criterio d’Informazione Bayesiano (BIC; Schwarz, 1978), introdotto come un miglioramento rispetto al Criterio d’Informazione di Akaike (AIC; Akaike, 1973). Un’alternativa è l’indice RMSEA, che può essere considerato come una stima della mancanza di adattamento che tiene in considerazione i gradi di libertà del modello (Browne e Cudeck, 1992). Un altro metodo di questo tipo è il test Minimum Average Partial (MAP), che stima le correlazioni parziali residue medie per diversi numeri di fattori e seleziona quello con il valore più basso (Velicer, 1976).\n\n33.4.1 Test del rapporto di verosimiglianze\nIn questo test si confrontano due ipotesi: l’ipotesi nulla \\(H_0\\) e l’ipotesi alternativa \\(H_1\\), per valutare la bontà di adattamento di un modello fattoriale a una matrice di covarianza delle variabili oggetto di osservazione campionaria \\(Y\\). L’ipotesi nulla postula che la struttura di interdipendenza di \\(Y\\) può essere spiegata da \\(m\\) fattori comuni, mentre l’alternativa postula che i fattori comuni non sono sufficienti per spiegare la matrice di covarianza \\(\\boldsymbol{\\Sigma}\\).\nIl test si basa sulla statistica del chi-quadrato con gradi di libertà pari a \\(\\nu = \\frac{1}{2}[(p-m)^2-(p-m)]\\), dove \\(p\\) è il numero di variabili e \\(m\\) è il numero di fattori. In pratica, si inizia con \\(m^*=1\\) e si valuta l’ipotesi \\(H_0\\) per \\(m^*\\). Se \\(H_0\\) non viene rifiutata, il procedimento si arresta. In caso contrario, si considera \\(m^*+1\\) e si ripete il test finché \\(H_0\\) viene accettata o finché si raggiunge il valore minimo di gradi di libertà pari a zero.\nIl test del rapporto di verosimiglianze è particolarmente indicato quando il numero di osservazioni è grande, ma la sua applicazione è limitata dalle dimensioni del campione. In alternativa, è possibile utilizzare gli indici AIC, BIC e RMSEA per scegliere la soluzione con il valore più piccolo di tali statistiche. Tuttavia, questi indici non forniscono un test statistico per il confronto tra modelli.\nIn pratica, si può considerare il valore \\(m\\) indicato dal test come il limite superiore del numero di fattori che sono importanti dal punto di vista pratico.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#minimizzazione-dellout-of-sample-prediction-error",
    "href": "chapters/extraction/03_numero_fattori.html#minimizzazione-dellout-of-sample-prediction-error",
    "title": "33  Il numero dei fattori",
    "section": "33.5 Minimizzazione dell’out-of-sample prediction error",
    "text": "33.5 Minimizzazione dell’out-of-sample prediction error\nRecentemente è stato proposto un nuovo metodo per stimare il numero di fattori in EFA che affronta il problema come un problema di selezione del modello Haslbeck & Bork (2022). L’obiettivo è confrontare i modelli con 1, 2, …, p fattori, dove \\(p\\) è il numero di variabili, e selezionare il modello con l’errore di previsione atteso più basso nella popolazione. Tuttavia, questo è un compito non banale perché il modello che minimizza l’errore di previsione nel campione non minimizza sempre l’errore di previsione nella popolazione.\nIntuitivamente, questo problema viene affrontato suddividendo il campione di dati in due insiemi: un set di training e un set di test. Il set di training viene utilizzato per stimare i parametri del modello, le cui previsioni vengono poi verificate utilizzando i dati di test (non utilizzati per la stima dei parametri). Questo calcolo dell’errore di previsione fuori campione viene ripetuto diverse volte, suddividendo ogni volta in modo casuale il campione negli insiemi di training e test. Tale metodo per stimare il numero di fattori è implementato nel pacchetto R fspe.\nEsempio. Per confrontare i metodi discussi per la scelta del numero \\(m\\) di fattori usiamo una matrice di correlazioni calcolata sulle sottoscale della WAIS. Le 11 sottoscale del test sono le seguenti:\n\nX1 = Information\nX2 = Comprehension\nX3 = Arithmetic\nX4 = Similarities\nX5 = Digit.span\nX6 = Vocabulary\nX7 = Digit.symbol\nX8 = Picture.completion\nX9 = Block.design\nX10 = Picture.arrangement\nX11 = Object.\n\nI dati sono stati ottenuti dal manuale della III edizione.\n\nvarnames &lt;- c(\n    \"IN\", \"CO\", \"AR\", \"SI\", \"DS\", \"VO\", \"SY\", \"PC\",\n    \"BD\", \"PA\", \"OA\", \"AG\", \"ED\"\n)\ntemp &lt;- matrix(c(\n    1, 0.67, 0.62, 0.66, 0.47, 0.81, 0.47, 0.60, 0.49, 0.51, 0.41,\n    -0.07, 0.66, .67, 1, 0.54, 0.60, 0.39, 0.72, 0.40, 0.54, 0.45,\n    0.49, 0.38, -0.08, 0.52, .62, .54, 1, 0.51, 0.51, 0.58, 0.41,\n    0.46, 0.48, 0.43, 0.37, -0.08, 0.49, .66, .60, .51, 1, 0.41,\n    0.68, 0.49, 0.56, 0.50, 0.50, 0.41, -0.19, 0.55, .47, .39, .51,\n    .41, 1, 0.45, 0.45, 0.42, 0.39, 0.42, 0.31, -0.19, 0.43,\n    .81, .72, .58, .68, .45, 1, 0.49, 0.57, 0.46, 0.52, 0.40, -0.02,\n    0.62, .47, .40, .41, .49, .45, .49, 1, 0.50, 0.50, 0.52, 0.46,\n    -0.46, 0.57, .60, .54, .46, .56, .42, .57, .50, 1, 0.61, 0.59,\n    0.51, -0.28, 0.48, .49, .45, .48, .50, .39, .46, .50, .61, 1,\n    0.54, 0.59, -0.32, 0.44, .51, .49, .43, .50, .42, .52, .52, .59,\n    .54, 1, 0.46, -0.37, 0.49, .41, .38, .37, .41, .31, .40, .46, .51,\n    .59, .46, 1, -0.28, 0.40, -.07, -.08, -.08, -.19, -.19, -.02,\n    -.46, -.28, -.32, -.37, -.28, 1, -0.29, .66, .52, .49, .55, .43,\n    .62, .57, .48, .44, .49, .40, -.29, 1\n), nrow = 13, ncol = 13, byrow = TRUE)\n\ncolnames(temp) &lt;- varnames\nrownames(temp) &lt;- varnames\n\nwais_cor &lt;- temp[1:11, 1:11]\nwais_cor\n\n\nA matrix: 11 x 11 of type dbl\n\n\n\nIN\nCO\nAR\nSI\nDS\nVO\nSY\nPC\nBD\nPA\nOA\n\n\n\n\nIN\n1.00\n0.67\n0.62\n0.66\n0.47\n0.81\n0.47\n0.60\n0.49\n0.51\n0.41\n\n\nCO\n0.67\n1.00\n0.54\n0.60\n0.39\n0.72\n0.40\n0.54\n0.45\n0.49\n0.38\n\n\nAR\n0.62\n0.54\n1.00\n0.51\n0.51\n0.58\n0.41\n0.46\n0.48\n0.43\n0.37\n\n\nSI\n0.66\n0.60\n0.51\n1.00\n0.41\n0.68\n0.49\n0.56\n0.50\n0.50\n0.41\n\n\nDS\n0.47\n0.39\n0.51\n0.41\n1.00\n0.45\n0.45\n0.42\n0.39\n0.42\n0.31\n\n\nVO\n0.81\n0.72\n0.58\n0.68\n0.45\n1.00\n0.49\n0.57\n0.46\n0.52\n0.40\n\n\nSY\n0.47\n0.40\n0.41\n0.49\n0.45\n0.49\n1.00\n0.50\n0.50\n0.52\n0.46\n\n\nPC\n0.60\n0.54\n0.46\n0.56\n0.42\n0.57\n0.50\n1.00\n0.61\n0.59\n0.51\n\n\nBD\n0.49\n0.45\n0.48\n0.50\n0.39\n0.46\n0.50\n0.61\n1.00\n0.54\n0.59\n\n\nPA\n0.51\n0.49\n0.43\n0.50\n0.42\n0.52\n0.52\n0.59\n0.54\n1.00\n0.46\n\n\nOA\n0.41\n0.38\n0.37\n0.41\n0.31\n0.40\n0.46\n0.51\n0.59\n0.46\n1.00\n\n\n\n\n\nIl primo metodo per la determinazione di \\(m\\) richiede di estrarre tanti fattori quanti sono necessari per spiegare una quota predeterminata della varianza totale. Supponiamo di porre il criterio pari all’80% della varianza totale.\n\nout &lt;- eigen(wais_cor)\nsum(out$val[1:4]) / sum(out$val)\nsum(out$val[1:5]) / sum(out$val)\n\n0.765678107076221\n\n\n0.811885250571924\n\n\nLa soluzione ottenuta in questo modo ci porterebbe a mantenere \\(m=5\\) fattori.\nIl secondo metodo suggerisce di mantenere tutti gli autovalori superiori al valore medio degli autovalori (che, nel caso di R è uguale a \\(1\\)).\n\nprint(round(out$values, 3))\n\n [1] 6.074 1.015 0.746 0.587 0.508 0.431 0.423 0.377 0.351 0.310 0.177\n\n\nNel caso presente, \\(m=2\\).\nLo scree test può essere eseguito creando il grafico seguente.\n\nn &lt;- dim(wais_cor)[1]\nscree_tb &lt;- tibble(\n    x = 1:n,\n    y = sort(eigen(wais_cor)$value, decreasing = TRUE)\n)\n\nscree_plot &lt;- scree_tb |&gt;\n  ggplot(aes(x, y)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = 1:n) +\n  ggtitle(\"Scree plot\")\n\nscree_plot\n\n\n\n\n\n\n\n\nLo scree test suggerisce la presenza di un unico fattore comune.\nLa versione della Parallel Analysis può essere eseguita con la funzione paran() contenuta nel pacchetto paran.\n\nparan(wais_cor, graph = TRUE)\n\n\nUsing eigendecomposition of correlation matrix.\nComputing: 10%  20%  30%  40%  50%  60%  70%  80%  90%  100%\n\n\nResults of Horn's Parallel Analysis for component retention\n330 iterations, using the mean estimate\n\n-------------------------------------------------- \nComponent   Adjusted    Unadjusted    Estimated \n            Eigenvalue  Eigenvalue    Bias \n-------------------------------------------------- \n1           1.647667    3.765744      2.118077\n-------------------------------------------------- \n\nAdjusted eigenvalues &gt; 1 indicate dimensions to retain.\n(1 components retained)\n\n\n\n\n\n\n\n\n\n\nLa Parallel Analysis indica una soluzione a \\(m=1\\) fattore.\nIl test inferenziale relativo al numero di fattori basato sulla statistica \\(\\chi^2\\) può essere eseguito nel modo seguente.\n\nfactanal(covmat=wais_cor, factors=4, n.obs=933)\n\n\nCall:\nfactanal(factors = 4, covmat = wais_cor, n.obs = 933)\n\nUniquenesses:\n   IN    CO    AR    SI    DS    VO    SY    PC    BD    PA    OA \n0.229 0.387 0.005 0.416 0.645 0.137 0.005 0.375 0.331 0.492 0.519 \n\nLoadings:\n   Factor1 Factor2 Factor3 Factor4\nIN 0.758   0.306   0.279   0.157  \nCO 0.672   0.312   0.229   0.107  \nAR 0.368   0.247   0.886   0.120  \nSI 0.602   0.376   0.193   0.207  \nDS 0.315   0.288   0.331   0.252  \nVO 0.851   0.242   0.208   0.192  \nSY 0.238   0.359   0.144   0.888  \nPC 0.432   0.623   0.143   0.172  \nBD 0.237   0.733   0.217   0.168  \nPA 0.367   0.539   0.150   0.245  \nOA 0.207   0.620   0.133   0.190  \n\n               Factor1 Factor2 Factor3 Factor4\nSS loadings      2.826   2.264   1.233   1.137\nProportion Var   0.257   0.206   0.112   0.103\nCumulative Var   0.257   0.463   0.575   0.678\n\nTest of the hypothesis that 4 factors are sufficient.\nThe chi square statistic is 35.4 on 17 degrees of freedom.\nThe p-value is 0.00551 \n\n\n\nfactanal(covmat = wais_cor, factors = 5, n.obs = 933)\n\n\nCall:\nfactanal(factors = 5, covmat = wais_cor, n.obs = 933)\n\nUniquenesses:\n   IN    CO    AR    SI    DS    VO    SY    PC    BD    PA    OA \n0.235 0.389 0.117 0.419 0.600 0.109 0.277 0.308 0.334 0.472 0.456 \n\nLoadings:\n   Factor1 Factor2 Factor3 Factor4 Factor5\nIN  0.745   0.264   0.301   0.192   0.118 \nCO  0.667   0.278   0.244   0.129   0.111 \nAR  0.378   0.236   0.814   0.145         \nSI  0.591   0.332   0.207   0.252   0.121 \nDS  0.288   0.208   0.366   0.341   0.155 \nVO  0.865   0.216   0.207   0.229         \nSY  0.251   0.364   0.153   0.708         \nPC  0.425   0.548   0.156   0.216   0.375 \nBD  0.246   0.708   0.230   0.201   0.107 \nPA  0.355   0.457   0.163   0.325   0.245 \nOA  0.211   0.664   0.128   0.205         \n\n               Factor1 Factor2 Factor3 Factor4 Factor5\nSS loadings      2.799   1.986   1.176   1.043   0.280\nProportion Var   0.254   0.181   0.107   0.095   0.025\nCumulative Var   0.254   0.435   0.542   0.637   0.662\n\nTest of the hypothesis that 5 factors are sufficient.\nThe chi square statistic is 12.5 on 10 degrees of freedom.\nThe p-value is 0.256 \n\n\nIl test del \\(\\chi^2\\) indica una soluzione a sei fattori.\nPer concludere, si potrebbe usare il metodo basato sulla minimizzazione dell’errore di previsione. Tuttavia, non possiamo applicare tale metodo ai dati dell’esempio in quanto sarebbe necessario disporre dei dati grezzi (la matrice di correlazioni non è sufficiente). Allo scopo di illustrare la procedura relativa al metodo basato sulla minimizzazione dell’errore di previsione useremo qui un set di dati diverso, ovvero holzinger19.\n\ndata(holzinger19)\n\nsuppressWarnings(\n    fspe_out &lt;- fspe(\n        data = holzinger19,\n        maxK = 10,\n        nfold = 10,\n        rep = 10,\n        method = \"PE\"\n    )\n)\n\n  |                                                                  |   0%\n\n\n  |------------------------------------------------------------------| 100%\n\n\n\npar(mar=c(4,4,1,1))\nplot.new()\nplot.window(xlim=c(1, 10), ylim=c(.6, .8))\naxis(1, 1:10)\naxis(2, las=2)\nabline(h=min(fspe_out$PEs), col=\"grey\")\nlines(fspe_out$PEs, lty=2)\npoints(fspe_out$PEs, pch=20, cex=1.5)\ntitle(xlab=\"Number of Factors\", ylab=\"Prediction Error\")\n\n\n\n\n\n\n\n\nPer i dati holzinger19, il metodo di {cite:t}haslbeck2022estimating produce dunque una soluzione a 4 fattori.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#la-replicabilità-delle-strutture-fattoriali",
    "href": "chapters/extraction/03_numero_fattori.html#la-replicabilità-delle-strutture-fattoriali",
    "title": "33  Il numero dei fattori",
    "section": "33.6 La replicabilità delle strutture fattoriali",
    "text": "33.6 La replicabilità delle strutture fattoriali\nUn aspetto critico dell’analisi fattoriale è la sua capacità di produrre risultati replicabili. Ciò significa che la struttura fattoriale identificata in un campione dovrebbe essere sostanzialmente simile quando replicata in un campione indipendente. Tuttavia, la cross-validazione delle strutture fattoriali spesso presenta difficoltà.\nL’Analisi Fattoriale Confirmatoria (CFA) offre una soluzione a questo problema. La CFA permette di testare una struttura fattoriale predefinita in un nuovo campione, verificando se i dati empirici supportano la struttura teorica ipotizzata.\nLa replicabilità è un indicatore importante della validità di una struttura fattoriale. Essa fornisce evidenze sulla robustezza e sulla generalizzabilità dei risultati ottenuti. Una struttura fattoriale che si replica consistentemente in diversi campioni è più probabile che rifletta una vera struttura sottostante piuttosto che caratteristiche specifiche di un singolo campione.\nCi sono diversi approcci per valutare la replicabilità:\n\nReplicazione in campioni indipendenti: Applicare la stessa analisi fattoriale a campioni diversi e confrontare i risultati.\nValidazione incrociata: Dividere un ampio campione in sottocampioni e confrontare le strutture fattoriali ottenute.\nCFA: Utilizzare i risultati di un’analisi fattoriale esplorativa come base per un modello confermatorio da testare su nuovi dati.\nMetodi di ricampionamento: Tecniche come il bootstrap per valutare la stabilità della soluzione fattoriale.\n\nIn conclusione, la replicabilità è fondamentale per stabilire la validità e l’utilità di una struttura fattoriale. La CFA, insieme ad altri metodi, rappresenta uno strumento essenziale per valutare la replicabilità e la generalizzabilità dei risultati ottenuti con l’analisi fattoriale.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#cosa-fare-con-i-fattori",
    "href": "chapters/extraction/03_numero_fattori.html#cosa-fare-con-i-fattori",
    "title": "33  Il numero dei fattori",
    "section": "33.7 Cosa fare con i fattori",
    "text": "33.7 Cosa fare con i fattori\nUna volta identificati i fattori attraverso l’analisi fattoriale, sorge la domanda su come utilizzarli efficacemente. Questo aspetto è particolarmente rilevante nel contesto dei Modelli di Equazioni Strutturali (SEM), dove i fattori assumono un significato sostanziale e possono essere impiegati in vari ruoli all’interno del modello.\n\n33.7.1 Utilizzo dei fattori nei SEM\nNei Modelli di Equazioni Strutturali, i fattori possono essere utilizzati in diversi modi:\n\nPredittori: I fattori possono fungere da variabili indipendenti per predire altri costrutti nel modello.\nMediatori: Possono essere utilizzati per spiegare il meccanismo attraverso cui una variabile influenza un’altra.\nModeratori: Possono essere impiegati per esaminare come la relazione tra due variabili cambia in funzione del livello del fattore moderatore.\nEsiti: Possono rappresentare le variabili dipendenti del modello, influenzate da altri costrutti.\n\nUn vantaggio significativo dell’utilizzo dei fattori latenti nei SEM è la loro capacità di “disattenuare” le associazioni per l’errore di misurazione. Questo significa che le relazioni stimate tra i costrutti sono più accurate, poiché l’errore di misurazione viene esplicitamente modellato e separato dalla varianza “vera” del costrutto.\n\n\n33.7.2 Sfide nell’uso dei fattori al di fuori dei SEM\nQuando si cerca di utilizzare i fattori al di fuori del contesto dei SEM, emergono alcune problematiche:\n\nSomma o media semplice: Molti ricercatori, dopo aver identificato che tre variabili saturano su un Fattore A, tendono a combinarle semplicemente sommandole o calcolandone la media. Questa pratica, tuttavia, non è accurata poiché ignora i pesi fattoriali (factor loadings) e l’errore di misurazione.\nCompositi lineari ponderati: Un approccio più sofisticato consiste nel creare un composito lineare, sommando le variabili ponderate per i loro pesi fattoriali. Questo metodo preserva le differenze nelle correlazioni tra le variabili e il fattore, ma continua a ignorare l’errore stimato. Di conseguenza, potrebbe non essere pienamente generalizzabile o significativo.\nCompositi a pesi unitari: In alcuni casi, assegnare lo stesso peso a tutte le variabili (compositi a pesi unitari) potrebbe risultare più generalizzabile rispetto ai compositi ponderati. Questo perché parte della variabilità nei pesi fattoriali potrebbe riflettere errori di campionamento piuttosto che differenze reali nell’importanza delle variabili. Si noti che il risultato numerico è identico tra la somma semplice e il composito a pesi unitari – la media semplice differisce solo per una costante (la divisione per il numero di variabili), che non altera le relazioni tra i punteggi. La differenza principale sta nel ragionamento sottostante e nel contesto metodologico: La somma/media semplice è spesso usata senza considerare l’analisi fattoriale. I compositi a pesi unitari sono una scelta consapevole basata sui risultati dell’analisi fattoriale, decidendo di trattare tutte le variabili con uguale importanza.\n\n\n\n33.7.3 Considerazioni aggiuntive\n\nInterpretabilità: I fattori latenti nei SEM offrono una rappresentazione più “pura” del costrutto sottostante, ma possono essere meno intuitivi da interpretare rispetto a punteggi compositi.\nBilanciamento tra precisione e praticità: Mentre l’uso di fattori latenti nei SEM offre maggiore precisione, in alcuni contesti (ad esempio, nella pratica clinica o nella ricerca applicata) potrebbe essere necessario un compromesso tra precisione statistica e facilità d’uso.\nStabilità cross-campione: È importante valutare la stabilità della struttura fattoriale attraverso diversi campioni prima di utilizzare i fattori in analisi successive.\nMetodi avanzati: Tecniche come la regressione con variabili latenti o l’uso di punteggi fattoriali salvati dai SEM possono offrire un compromesso tra la precisione dei modelli SEM completi e la necessità di punteggi compositi.\n\nIn conclusione, mentre i fattori offrono potenti strumenti per la modellazione di costrutti latenti, il loro utilizzo richiede una comprensione approfondita delle loro proprietà statistiche e delle implicazioni delle diverse strategie di operazionalizzazione. La scelta del metodo più appropriato dipenderà dagli obiettivi specifici della ricerca, dal contesto di applicazione e dalle caratteristiche dei dati disponibili.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#riflessioni-conclusive",
    "href": "chapters/extraction/03_numero_fattori.html#riflessioni-conclusive",
    "title": "33  Il numero dei fattori",
    "section": "33.8 Riflessioni Conclusive",
    "text": "33.8 Riflessioni Conclusive\nIn generale, la scelta del numero di fattori \\(m\\) non è sempre ovvia e rappresenta un limite dell’analisi fattoriale. Per affrontare questo problema, tradizionalmente si utilizza uno strumento come lo scree test per valutare la proporzione di varianza spiegata di ciascun item e l’interpretabilità della soluzione ottenuta dopo una rotazione adeguata. Tuttavia, poiché la scelta di \\(m\\) è soggettiva, i limiti della soluzione ottenuta sono evidenti. In alcuni casi, la scelta di \\(m\\) può essere più certa quando tutti i metodi forniscono la stessa risposta. Un’alternativa più moderna potrebbe essere l’uso di un metodo basato sulla minimizzazione dell’errore di previsione come quello descritto da Haslbeck & Bork (2022). In questo modo, si potrebbe ottenere una soluzione più affidabile e oggettiva.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/03_numero_fattori.html#session-info",
    "href": "chapters/extraction/03_numero_fattori.html#session-info",
    "title": "33  Il numero dei fattori",
    "section": "33.9 Session Info",
    "text": "33.9 Session Info\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] fspe_0.1.2        paran_1.5.3       MASS_7.3-61       viridis_0.6.5    \n [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n[13] lavaan_0.6-19     psych_2.4.6.26    scales_1.3.0      markdown_1.13    \n[17] knitr_1.49        lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n[21] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n[25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.17.1    jsonlite_1.8.9       magrittr_2.0.3      \n  [4] TH.data_1.1-2        estimability_1.5.1   farver_2.1.2        \n  [7] nloptr_2.1.1         rmarkdown_2.29       vctrs_0.6.5         \n [10] Cairo_1.6-2          minqa_1.2.8          base64enc_0.1-3     \n [13] rstatix_0.7.2        htmltools_0.5.8.1    broom_1.0.7         \n [16] Formula_1.2-5        htmlwidgets_1.6.4    plyr_1.8.9          \n [19] sandwich_3.1-1       emmeans_1.10.5       zoo_1.8-12          \n [22] uuid_1.2-1           igraph_2.1.1         mime_0.12           \n [25] lifecycle_1.0.4      pkgconfig_2.0.3      Matrix_1.7-1        \n [28] R6_2.5.1             fastmap_1.2.0        shiny_1.9.1         \n [31] digest_0.6.37        OpenMx_2.21.13       fdrtool_1.2.18      \n [34] colorspace_2.1-1     rprojroot_2.0.4      Hmisc_5.2-0         \n [37] labeling_0.4.3       fansi_1.0.6          timechange_0.3.0    \n [40] abind_1.4-8          compiler_4.4.2       withr_3.0.2         \n [43] glasso_1.11          htmlTable_2.4.3      backports_1.5.0     \n [46] carData_3.0-5        ggsignif_0.6.4       GPArotation_2024.3-1\n [49] corpcor_1.6.10       gtools_3.9.5         tools_4.4.2         \n [52] pbivnorm_0.6.0       foreign_0.8-87       zip_2.3.1           \n [55] httpuv_1.6.15        nnet_7.3-19          glue_1.8.0          \n [58] quadprog_1.5-8       promises_1.3.0       nlme_3.1-166        \n [61] lisrelToR_0.3        grid_4.4.2           pbdZMQ_0.3-13       \n [64] checkmate_2.3.2      cluster_2.1.6        reshape2_1.4.4      \n [67] generics_0.1.3       gtable_0.3.6         tzdb_0.4.0          \n [70] data.table_1.16.2    hms_1.1.3            car_3.1-3           \n [73] utf8_1.2.4           sem_3.1-16           pillar_1.9.0        \n [76] IRdisplay_1.1        rockchalk_1.8.157    later_1.3.2         \n [79] splines_4.4.2        cherryblossom_0.1.0  lattice_0.22-6      \n [82] survival_3.7-0       kutils_1.73          tidyselect_1.2.1    \n [85] miniUI_0.1.1.1       pbapply_1.7-2        airports_0.1.0      \n [88] stats4_4.4.2         xfun_0.49            qgraph_1.9.8        \n [91] arm_1.14-4           stringi_1.8.4        pacman_0.5.1        \n [94] boot_1.3-31          evaluate_1.0.1       codetools_0.2-20    \n [97] mi_1.1               cli_3.6.3            RcppParallel_5.1.9  \n[100] IRkernel_1.3.2       rpart_4.1.23         xtable_1.8-4        \n[103] repr_1.1.7           munsell_0.5.1        Rcpp_1.0.13-1       \n[106] coda_0.19-4.1        png_0.1-8            XML_3.99-0.17       \n[109] parallel_4.4.2       usdata_0.3.1         jpeg_0.1-10         \n[112] lme4_1.1-35.5        mvtnorm_1.3-2        openxlsx_4.2.7.1    \n[115] crayon_1.5.3         openintro_2.5.0      rlang_1.1.4         \n[118] multcomp_1.4-26      mnormt_2.1.1        \n\n\n\n\n\n\nHaslbeck, J., & Bork, R. van. (2022). Estimating the number of factors in exploratory factor analysis via out-of-sample prediction errors. Psychological Methods.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Il numero dei fattori</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html",
    "href": "chapters/extraction/04_rotazione.html",
    "title": "34  La rotazione fattoriale",
    "section": "",
    "text": "34.1 Indeterminatezza della soluzione fattoriale\nLa necessità di effettuare la rotazione deriva dal fatto che la matrice delle saturazioni non ha un’unica soluzione. Attraverso trasformazioni matematiche, è possibile ottenere infinite matrici dello stesso ordine. Questo fenomeno è noto come indeterminatezza della soluzione fattoriale.\nLa matrice delle saturazioni fattoriali \\(\\boldsymbol{\\Lambda}\\) non è univocamente definita poiché non esiste una soluzione unica per determinare le saturazioni fattoriali. Una matrice di correlazioni \\(\\boldsymbol{R}\\) può produrre diverse soluzioni fattoriali, ovvero matrici con lo stesso numero di fattori comuni ma con diverse configurazioni di saturazioni fattoriali, o matrici di saturazioni fattoriali corrispondenti a un diverso numero di fattori comuni.\nEsempio. Siano \\(\\boldsymbol{\\Lambda}_1\\) e \\(\\boldsymbol{\\Lambda}_2\\) due matrici aventi lo stesso numero di righe e colonne, ma contenenti saturazioni fattoriali diverse. \\(\\boldsymbol{\\Lambda}_1\\) è definita dai valori seguenti\nl1 &lt;- matrix(\n  c(\n    0.766,  -0.232,\n    0.670,  -0.203,\n    0.574,  -0.174,\n    0.454,   0.533,\n    0.389,   0.457,\n    0.324,   0.381\n  ),\n  byrow = TRUE, ncol = 2\n)\nmentre per \\(\\boldsymbol{\\Lambda}_2\\) abbiamo\nl2 &lt;- matrix(\n  c(\n    0.783,  0.163,\n    0.685,  0.143,\n    0.587,  0.123,\n    0.143,  0.685,\n    0.123,  0.587,\n    0.102,  0.489\n  ),\n  byrow = TRUE, ncol = 2\n)\nEsaminiamo la matrice delle correlazioni riprodotte dalle due matrici di pesi fattoriali (con le comunalità sulla diagonale di \\(\\boldsymbol{R}\\)):\nl1 %*% t(l1) |&gt; round(2)\n\n\nA matrix: 6 x 6 of type dbl\n\n\n0.64\n0.56\n0.48\n0.22\n0.19\n0.16\n\n\n0.56\n0.49\n0.42\n0.20\n0.17\n0.14\n\n\n0.48\n0.42\n0.36\n0.17\n0.14\n0.12\n\n\n0.22\n0.20\n0.17\n0.49\n0.42\n0.35\n\n\n0.19\n0.17\n0.14\n0.42\n0.36\n0.30\n\n\n0.16\n0.14\n0.12\n0.35\n0.30\n0.25\nl2 %*% t(l2) |&gt; round(2)\n\n\nA matrix: 6 x 6 of type dbl\n\n\n0.64\n0.56\n0.48\n0.22\n0.19\n0.16\n\n\n0.56\n0.49\n0.42\n0.20\n0.17\n0.14\n\n\n0.48\n0.42\n0.36\n0.17\n0.14\n0.12\n\n\n0.22\n0.20\n0.17\n0.49\n0.42\n0.35\n\n\n0.19\n0.17\n0.14\n0.42\n0.36\n0.30\n\n\n0.16\n0.14\n0.12\n0.35\n0.30\n0.25\nCome si vede, viene ottenuto lo stesso risultato utilizzando matrici \\(\\boldsymbol{\\Lambda}\\) con lo stesso numero \\(m\\) di colonne ma saturazioni fattoriali diverse.\nSi consideri ora il caso di matrici \\(\\boldsymbol{\\Lambda}\\) corrispondenti a soluzioni fattoriali con un diverso numero di fattori comuni. Siano \\(\\boldsymbol{\\Lambda}_1\\) e \\(\\boldsymbol{\\Lambda}_2\\) due matrici aventi lo stesso numero di righe ma un numero diverso di colonne:\nl1 &lt;- matrix(\n  c(\n    0.9,\n    0.7,\n    0.5,\n    0.3\n  ),\n  byrow = TRUE, ncol = 1\n)\n\nl2 &lt;- matrix(\n  c(\n    0.78, 0.45,\n    0.61, 0.35,\n    0.43, 0.25,\n    0.25, 0.15\n  ),\n  byrow = TRUE, ncol = 2\n)\nSi noti che la stessa matrice di correlazioni riprodotte (con le comunalità sulla diagonale principale) viene generata dalle saturazioni fattoriali corrispondenti ad un numero diverso di fattori comuni:\nl1 %*% t(l1) |&gt; round(2)\n\n\nA matrix: 4 x 4 of type dbl\n\n\n0.81\n0.63\n0.45\n0.27\n\n\n0.63\n0.49\n0.35\n0.21\n\n\n0.45\n0.35\n0.25\n0.15\n\n\n0.27\n0.21\n0.15\n0.09\nl2 %*% t(l2) |&gt; round(2)\n\n\nA matrix: 4 x 4 of type dbl\n\n\n0.81\n0.63\n0.45\n0.26\n\n\n0.63\n0.49\n0.35\n0.20\n\n\n0.45\n0.35\n0.25\n0.14\n\n\n0.26\n0.20\n0.14\n0.08",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#parsimonia-e-semplicità",
    "href": "chapters/extraction/04_rotazione.html#parsimonia-e-semplicità",
    "title": "34  La rotazione fattoriale",
    "section": "34.2 Parsimonia e semplicità",
    "text": "34.2 Parsimonia e semplicità\nPer ottenere risultati affidabili dall’analisi fattoriale, si affronta il problema dell’indeterminazione fattoriale scegliendo la soluzione che soddisfa due criteri fondamentali: il criterio della parsimonia e il criterio della semplicità.\nIl criterio della parsimonia richiede di scegliere il modello con il minor numero di fattori comuni che può spiegare la covarianza tra le variabili. In pratica, se ci sono due soluzioni fattoriali con un diverso numero di fattori che riproducono allo stesso modo la matrice di covarianza o di correlazione, si sceglie quella con il minor numero di fattori.\nIn caso invece ci siano diverse soluzioni fattoriali con lo stesso numero m di fattori, il criterio della semplicità guida nella scelta della trasformazione più appropriata della matrice di saturazioni fattoriali \\(\\boldsymbol{\\Lambda}\\). Questa trasformazione, nota come rotazione, cerca di rendere i fattori più interpretabili. Ci sono due tipi di rotazione: ortogonale e obliqua.\nLa rotazione ortogonale assume che i fattori siano incorrelati, mentre la rotazione obliqua consente correlazioni tra i fattori. L’obiettivo della rotazione è di trovare una soluzione che renda i fattori più facilmente interpretabili e, quindi, in grado di spiegare meglio i dati.\n\n34.2.1 Il Criterio della Struttura Semplice nell’Analisi Fattoriale\nL’analisi fattoriale impiega la rotazione degli assi fattoriali per ottenere una “struttura semplice” nella matrice delle saturazioni fattoriali. Questo criterio, proposto originariamente da Thurstone nel 1947, mira a realizzare una matrice caratterizzata da un numero limitato di saturazioni (o carichi fattoriali) significative e diverse da zero, minimizzando al contempo la presenza di variabili influenzate da più di un fattore.\nPer raggiungere una struttura semplice, Thurstone ha delineato specifiche condizioni che la matrice fattoriale ruotata deve soddisfare:\n\nOgni variabile deve presentare saturazioni nulle con la maggior parte dei fattori, escludendo uno o pochi con cui mostra saturazioni significative.\nPer ciascun fattore, devono esistere almeno \\(m\\) saturazioni nulle, dove \\(m\\) è il numero totale di fattori comuni.\n\nL’obiettivo della rotazione è quindi massimizzare il numero di saturazioni nulle o quasi nulle, facilitando l’interpretazione dei fattori. Analizzando la matrice ruotata, è possibile identificare le variabili che sono fortemente associate a specifici fattori e valutare l’intensità di tali associazioni.\nUn fattore si interpreta efficacemente quando i suoi carichi sono elevati e positivi su un gruppo ristretto di variabili; ciò suggerisce che il fattore rappresenta un tratto o una caratteristica comune a tali variabili. Tuttavia, l’interpretazione diventa più complessa quando le variabili presentano saturazioni significative con più di un fattore, poiché indica la presenza di sovrapposizioni nelle influenze fattoriali.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#rotazione-nello-spazio-geometrico",
    "href": "chapters/extraction/04_rotazione.html#rotazione-nello-spazio-geometrico",
    "title": "34  La rotazione fattoriale",
    "section": "34.3 Rotazione nello Spazio Geometrico",
    "text": "34.3 Rotazione nello Spazio Geometrico\n\n34.3.1 Rotazione Ortogonale\nCome precedentemente osservato, la matrice delle saturazioni fattoriali non è unica, implicando l’esistenza di multiple soluzioni equivalenti per determinare i pesi fattoriali. La rotazione ortogonale è un tipo di trasformazione lineare applicata ai pesi fattoriali per produrre una nuova matrice di saturazioni fattoriali che rispetti criteri specifici di struttura semplice. Questo processo ha lo scopo di rendere i dati più facilmente interpretabili.\nGeometricamente parlando, la rotazione ortogonale è simile a una rotazione rigida degli assi in uno spazio cartesiano che rappresenta i pesi fattoriali. Tale rotazione conserva le distanze tra i punti (che rappresentano le saturazioni fattoriali) ma modifica la loro posizione relativa rispetto ai fattori. Di conseguenza, si ottiene una configurazione dei pesi fattoriali che è più semplice da interpretare.\nLe tecniche di rotazione ortogonale sono tipicamente implementate attraverso metodi come la massima verosimiglianza o l’analisi dei componenti principali, con l’obiettivo di massimizzare il numero di saturazioni nulle o quasi nulle nella matrice delle saturazioni risultante. Questo processo aiuta a chiarire quale variabile è influenzata maggiormente da quali fattori, facilitando l’interpretazione dei risultati dell’analisi fattoriale.\n\n\n34.3.2 Vincoli alla Rotazione dei Fattori\nIl problema della non identificabilità della matrice dei pesi fattoriali, denotata come \\(\\hat{\\boldsymbol{\\Lambda}}\\), indica l’esistenza di molteplici matrici equivalenti che possono produrre identiche correlazioni tra le variabili di un modello. Per affrontare questa questione, è essenziale imporre vincoli sulla rotazione dei fattori. Uno dei criteri fondamentali nella scelta del tipo di rotazione è l’ottenimento di una matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\) semplificata, i cui elementi si avvicinano il più possibile ai valori 0 e 1. Questo facilita l’interpretazione dei fattori come combinazioni lineari delle variabili.\nLe rotazioni ortogonali, utili in presenza di fattori non correlati, mantengono inalterate le comunalità, poiché conservano le distanze geometriche tra i punti rappresentati dai pesi fattoriali. In questo caso, le comunalità sono calcolate come la somma dei quadrati dei pesi fattoriali. Al contrario, le rotazioni non ortogonali modificano la quota di varianza spiegata da ciascun fattore, calcolata dalla somma dei quadrati dei pesi fattoriali divisa per la traccia della matrice di correlazione.\nEsistono vari algoritmi per eseguire la rotazione ortogonale dei fattori, tra cui il metodo grafico, il metodo Quartimax e il metodo Varimax. Ciascuno di questi metodi ha specifiche applicazioni e impatti sulla struttura della matrice risultante, facilitando così l’interpretazione dei dati analizzati.\n\n\n34.3.3 Metodo Grafico per la Rotazione dei Fattori\nQuando si dispone di soli $ m=2 $ fattori, il sistema di coordinate bidimensionale è utilizzato per rappresentare geometricamente i fattori. La visualizzazione grafica delle saturazioni fattoriali permette di determinare visivamente la rotazione più appropriata. Ogni riga della matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\) rappresenta un paio di pesi fattoriali, \\(\\hat{\\lambda}_{i1}, \\hat{\\lambda}_{i2}\\), con \\(i=1, \\dots, p\\), che corrispondono alle coordinate di \\(p\\) punti (equivalenti al numero di variabili manifeste). Per ottimizzare la rappresentazione, gli assi vengono ruotati di un angolo \\(\\phi\\) per avvicinarli il più possibile alla disposizione dei punti sul grafico. Le nuove coordinate \\((\\hat{\\lambda}_{i1}^*, \\hat{\\lambda}_{i2}^*)\\) sono calcolate mediante la trasformazione \\(\\hat{\\boldsymbol{\\Lambda}}^* = \\hat{\\boldsymbol{\\Lambda}} \\textbf{T}\\), dove\n\\[\n\\textbf{T} =\n\\begin{bmatrix}\n\\cos{\\phi} & -\\sin{\\phi}\\\\\n\\sin{\\phi} & \\cos{\\phi}\n\\end{bmatrix}\n\\]\nè una matrice ortogonale \\(2 \\times 2\\).\nEsempio: Consideriamo un caso studiato da Brown, Williams e Barlow (1984), analizzato in Rencher (2002). Ad una ragazza di dodici anni è stato chiesto di valutare sette suoi conoscenti su cinque attributi: gentilezza, intelligenza, felicità, simpatia e giustizia. Per queste variabili, la matrice di correlazione \\(R\\) è stata analizzata per estrarre due fattori mediante il metodo delle componenti principali, senza rotazione iniziale:\n\nR &lt;- matrix(\n  c(\n    1.00, .296, .881, .995, .545,\n    .296, 1.000, -.022, .326, .837,\n    .881, -.022, 1.000, .867, .130,\n    .995, .326, .867, 1.000, .544,\n    .545, .837, .130, .544, 1.00\n  ),\n  ncol = 5, byrow = TRUE, dimnames = list(\n    c(\"K\", \"I\", \"H\", \"L\", \"J\"), c(\"K\", \"I\", \"H\", \"L\", \"J\")\n  )\n)\n\nprint(R)\n\n      K      I      H     L     J\nK 1.000  0.296  0.881 0.995 0.545\nI 0.296  1.000 -0.022 0.326 0.837\nH 0.881 -0.022  1.000 0.867 0.130\nL 0.995  0.326  0.867 1.000 0.544\nJ 0.545  0.837  0.130 0.544 1.000\n\n\nDalla matrice \\(R\\), estraiamo due fattori. Si osserva che i fattori risultano difficili da interpretare: il primo fattore mostra alte saturazioni positive su tutte le variabili manifeste, mentre il secondo fattore si caratterizza per alte saturazioni positive su una variabile e negative sulle altre.\n\nf.pc &lt;- principal(R, 2, rotate = FALSE) \nf.pc\n\nPrincipal Components Analysis\nCall: principal(r = R, nfactors = 2, rotate = FALSE)\nStandardized loadings (pattern matrix) based upon correlation matrix\n   PC1   PC2   h2     u2 com\nK 0.97 -0.23 0.99 0.0067 1.1\nI 0.52  0.81 0.92 0.0792 1.7\nH 0.78 -0.59 0.96 0.0391 1.9\nL 0.97 -0.21 0.99 0.0135 1.1\nJ 0.70  0.67 0.94 0.0597 2.0\n\n                       PC1  PC2\nSS loadings           3.26 1.54\nProportion Var        0.65 0.31\nCumulative Var        0.65 0.96\nProportion Explained  0.68 0.32\nCumulative Proportion 0.68 1.00\n\nMean item complexity =  1.6\nTest of the hypothesis that 2 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.03 \n\nFit based upon off diagonal values = 1\n\n\nIn un grafico delle saturazioni fattoriali, i punti rappresentano le cinque coppie di saturazioni (una per ciascun fattore):\n\nplot(\n  f.pc$load[, 1], f.pc$load[, 2],\n  bty = \"n\", xaxt = \"n\",\n  xlab = \"Primo Fattore\", ylab = \"Secondo Fattore\",\n  ylim = c(-.6, 1), xlim = c(0, 1), pch = 19, asp = 1\n)\naxis(1, pos = c(0, 0))\nabline(0, 0)\n\n\n\n\n\n\n\n\nRencher (2002) suggerisce che una rotazione ortogonale di \\(-35^\\circ\\) avvicinerebbe efficacemente gli assi ai punti nel diagramma di dispersione. Per verificarlo, si può disegnare i nuovi assi nel grafico dopo una rotazione di \\(-35^\\circ\\).\n\nplot(\n  f.pc$load[, 1], f.pc$load[, 2],\n  bty = \"n\", xaxt = \"n\",\n  xlab = \"Primo Fattore\", ylab = \"Secondo Fattore\",\n  ylim = c(-.6, 1), xlim = c(0, 1), pch = 19, asp = 1\n)\naxis(1, pos = c(0, 0))\nabline(0, 0)\n\nar &lt;- matrix(c(\n  0, 0,\n  0, 1,\n  0, 0,\n  1, 0\n), ncol = 2, byrow = TRUE)\n\nangle &lt;- 35\nrad &lt;- angle * pi / 180\nT &lt;- matrix(c(\n  cos(rad), -sin(rad),\n  sin(rad),  cos(rad)\n), ncol = 2, byrow = TRUE)\n\nround(ar %*% T, 3)\n\narrows(0, 0, 0.574, 0.819, lwd = 2)\narrows(0, 0, 0.819, -0.574, lwd = 2)\n\n\nA matrix: 4 x 2 of type dbl\n\n\n0.000\n0.000\n\n\n0.574\n0.819\n\n\n0.000\n0.000\n\n\n0.819\n-0.574\n\n\n\n\n\n\n\n\n\n\n\n\nNella figura, le due frecce rappresentano gli assi ruotati. La rotazione di \\(-35^{\\circ}\\) ha effettivamente avvicinato gli assi ai punti del diagramma. Se usiamo dunque il valore \\(\\phi = -35^{\\circ}\\) nella matrice di rotazione, possiamo calcolare le saturazioni fattoriali della soluzione ruotata \\(\\hat{\\boldsymbol{\\Lambda}}^* = \\hat{\\boldsymbol{\\Lambda}} \\textbf{T}\\).\nLe saturazioni fattoriali ruotate corrispondono alla proiezione ortogonale dei punti sugli assi ruotati:\n\nangle &lt;- -35\nrad &lt;- angle * pi / 180\nT &lt;- matrix(c(\n  cos(rad), -sin(rad),\n  sin(rad),  cos(rad)\n), ncol = 2, byrow = TRUE)\nround(f.pc$load %*% T, 3)\n\n\nA matrix: 5 x 2 of type dbl\n\n\nK\n0.927\n0.367\n\n\nI\n-0.037\n0.959\n\n\nH\n0.980\n-0.031\n\n\nL\n0.916\n0.385\n\n\nJ\n0.194\n0.950\n\n\n\n\n\nLa soluzione ottenuta in questo modo riproduce quanto riportato da {cite:t}rencher10methods.\n\n\n34.3.4 Medodi di rotazione ortogonale\nUn tipo di rotazione ortogonale spesso utilizzata è la rotazione Varimax (Kaiser, 1958). La matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\) è semplificata in modo tale che le varianze dei quadrati degli elementi \\(\\lambda_{ij}\\) appartenenti a colonne diverse di \\(\\hat{\\boldsymbol{\\Lambda}}\\) siano massime. Se le saturazioni fattoriali in una colonna di \\(\\hat{\\boldsymbol{\\Lambda}}\\) sono simili tra loro, la varianza sarà prossima a zero. Tale varianza è tanto più grande quanto più i quadrati degli elementi \\(\\lambda_{ij}\\) assumono valori prossimi a \\(0\\) e \\(1\\). Amplificando le correlazioni più alte e riducendo quelle più basse, la rotazione Varimax agevola l’interpretazione di ciascun fattore.\nUsando la funzione factanal() del modulo R base, la rotazione Varimax può essere applicata alla soluzione ottenuta mediante il metodo di massima verosimiglianza. Usando le funzioni principal() e factor.pa() disponibili nel pacchetto psych, la rotazione Varimax può essere applicata alle soluzioni ottenute mediante il metodo delle componenti principali e il metodo del fattore principale.\nAd esempio, usando il metodo delle componenti principali otteniamo:\n\nf_pc &lt;- principal(R, 2, n.obs = 7, rotate = \"varimax\")\nf_pc\n\nPrincipal Components Analysis\nCall: principal(r = R, nfactors = 2, rotate = \"varimax\", n.obs = 7)\nStandardized loadings (pattern matrix) based upon correlation matrix\n   RC1   RC2   h2     u2 com\nK 0.95  0.30 0.99 0.0067 1.2\nI 0.03  0.96 0.92 0.0792 1.0\nH 0.97 -0.10 0.96 0.0391 1.0\nL 0.94  0.32 0.99 0.0135 1.2\nJ 0.26  0.93 0.94 0.0597 1.2\n\n                       RC1  RC2\nSS loadings           2.81 1.99\nProportion Var        0.56 0.40\nCumulative Var        0.56 0.96\nProportion Explained  0.58 0.42\nCumulative Proportion 0.58 1.00\n\nMean item complexity =  1.1\nTest of the hypothesis that 2 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.03 \n with the empirical chi square  0.12  with prob &lt;  0.73 \n\nFit based upon off diagonal values = 1\n\n\nUn altro metodo di rotazione ortogonale è il metodo Quartimax (Neuhaus e Wringley, 1954), il quale opera una semplificazione della matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\) massimizzando le covarianze tra i quadrati degli elementi \\(\\lambda_{ij}\\) appartenenti a righe diverse, subordinatamente alla condizione che la varianza delle righe rimanga inalterata.\n\n\n34.3.5 Metodi di Rotazione Obliqua\nIl termine “rotazione obliqua” può sembrare inappropriato, in quanto la rotazione implica generalmente una trasformazione ortogonale che preserva le distanze. Tuttavia, come evidenziato da {cite:t}rencher10methods, un’espressione più corretta potrebbe essere “trasformazione obliqua”. Nonostante ciò, l’uso comune ha consolidato il termine “rotazione obliqua”.\nNel contesto della rotazione obliqua, gli assi della soluzione ruotata non sono costretti a rimanere ortogonali tra loro, permettendo così un allineamento più diretto agli agglomerati di punti nello spazio delle saturazioni fattoriali. Questo tipo di trasformazione facilita l’interpretazione dei fattori in presenza di correlazioni tra di essi.\nEsistono diversi approcci analitici per realizzare una rotazione obliqua. Ad esempio, il metodo Direct Oblimin, sviluppato da Jennrich e Sampson nel 1966, utilizza il seguente criterio:\n\\[\n\\sum_{ij} \\left(\\sum_v \\lambda_i^2 \\lambda_j^2 - w \\frac{1}{p} \\sum_v \\lambda_i^2 \\sum_v \\lambda_j^2\\right)\n\\]\nQui, \\(\\sum_{ij}\\) rappresenta la somma su tutte le coppie di fattori \\(ij\\). Il processo prevede una minimizzazione, al contrario della massimizzazione tipica delle rotazioni ortogonali, riflettendo la ricerca di una soluzione che minimizzi la correlazione ridondante tra i fattori, mantenendo al contempo chiarezza interpretativa.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#matrice-dei-pesi-fattoriali-e-matrice-di-struttura",
    "href": "chapters/extraction/04_rotazione.html#matrice-dei-pesi-fattoriali-e-matrice-di-struttura",
    "title": "34  La rotazione fattoriale",
    "section": "34.4 Matrice dei Pesi Fattoriali e Matrice di Struttura",
    "text": "34.4 Matrice dei Pesi Fattoriali e Matrice di Struttura\n\n34.4.1 Rotazione Ortogonale\nNel contesto della rotazione ortogonale, i fattori rimangono incorrelati tra loro. Consideriamo il caso di due fattori latenti non correlati (\\(\\xi_1\\) e \\(\\xi_2\\)) e quattro variabili manifeste (\\(y_1, y_2, y_3, y_4\\)). I coefficienti \\(\\lambda_{11}, \\lambda_{12}, \\lambda_{13}, \\lambda_{14}\\) rappresentano le saturazioni fattoriali delle variabili nel primo fattore, mentre \\(\\lambda_{21}, \\lambda_{22}, \\lambda_{23}, \\lambda_{24}\\) sono quelle nel secondo fattore. In un modello di percorso, la correlazione tra due variabili è calcolata come la somma di tutti i percorsi validi che le collegano. Se i fattori comuni sono incorrelati, esiste un solo percorso valido che collega ciascuna variabile manifesta a ciascun fattore comune secondo le regole di Wright. Pertanto, le correlazioni tra variabili manifeste e fattori comuni sono direttamente uguali alle saturazioni fattoriali. Queste saturazioni possono essere interpretate come i pesi beta di un modello di regressione multipla, indicando il contributo specifico di ciascun fattore comune nella varianza spiegata degli item (Tabachnick & Fidell, 2001).\n\n\n\n\n\n\nFigura 34.1: Rotazione ortogonale.\n\n\n\n\n\n34.4.2 Rotazione Obliqua\nNel caso della rotazione obliqua, i fattori comuni risultano correlati tra loro, rendendo la soluzione fattoriale più complessa. Pertanto, la matrice delle saturazioni fattoriali non riflette più direttamente le correlazioni tra variabili e fattori. Un modello di percorso in questa configurazione include almeno due percorsi validi che collegano ciascuna variabile manifesta a ciascun fattore comune. È necessario distinguere tra tre matrici diverse:\n\nMatrice Pattern (\\(\\hat{\\boldsymbol{\\Lambda}}\\)): Conosciuta anche come matrice dei modelli, questa matrice rappresenta i coefficienti di regressione parziali delle variabili sulle dimensioni fattoriali, escludendo l’influenza degli altri fattori.\nMatrice di Struttura: Rappresenta le correlazioni complessive tra le variabili manifeste e i fattori, considerando sia gli effetti diretti che quelli indiretti dei fattori correlati.\nMatrice di Intercorrelazione Fattoriale (\\(\\hat{\\boldsymbol{\\Phi}}\\)): Indica le correlazioni tra i fattori stessi.\n\nIn un modello di percorso con rotazione obliqua, gli assi che rappresentano i fattori non sono ortogonali, il che significa che i fattori sono correlati. Le variabili manifeste sono quindi collegate ai fattori attraverso percorsi che includono effetti diretti e indiretti. Ad esempio, per la variabile \\(y_1\\) e il fattore \\(\\xi_1\\), i percorsi includono una freccia causale \\(\\lambda_{11}\\) per l’effetto diretto e un percorso indiretto rappresentato dal prodotto \\(\\lambda_{21}\\phi_{12}\\). L’analisi dei percorsi dimostra che la correlazione tra \\(\\xi_1\\) e \\(y_1\\) è la somma dei valori numerici di questi percorsi validi, ovvero \\(\\lambda_{11} + \\lambda_{21} \\phi_{12}\\).\n\n\n\n\n\n\nFigura 34.2: Rotazione obliqua.\n\n\n\nPer illustrare la rotazione obliqua, utilizziamo i dati discussi da {cite:t}rencher10methods. Si consideri la matrice di correlazione presentata qui sotto.\n\nR &lt;- matrix(\n  c(\n    1.00,  0.735, 0.711, 0.704,\n    0.735, 1.00,  0.693, 0.709,\n    0.711, 0.693, 1.00,  0.839,\n    0.704, 0.709, 0.839, 1.00\n  ),\n  ncol = 4,\n  byrow = TRUE\n)\nR\n\n\nA matrix: 4 x 4 of type dbl\n\n\n1.000\n0.735\n0.711\n0.704\n\n\n0.735\n1.000\n0.693\n0.709\n\n\n0.711\n0.693\n1.000\n0.839\n\n\n0.704\n0.709\n0.839\n1.000\n\n\n\n\n\nIniziamo calcolando la soluzione a due fattori mediante il metodo delle componenti principali e una rotazione Varimax (ovvero, ortogonale). Otteniamo le seguenti saturazioni fattoriali.\n\nf1_pc &lt;- principal(R, 2, rotate = \"varimax\") \nf1_pc\n\nPrincipal Components Analysis\nCall: principal(r = R, nfactors = 2, rotate = \"varimax\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n   RC1  RC2   h2    u2 com\n1 0.50 0.78 0.86 0.140 1.7\n2 0.47 0.81 0.88 0.124 1.6\n3 0.90 0.33 0.92 0.078 1.3\n4 0.89 0.35 0.92 0.083 1.3\n\n                       RC1  RC2\nSS loadings           2.08 1.50\nProportion Var        0.52 0.37\nCumulative Var        0.52 0.89\nProportion Explained  0.58 0.42\nCumulative Proportion 0.58 1.00\n\nMean item complexity =  1.5\nTest of the hypothesis that 2 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.06 \n\nFit based upon off diagonal values = 0.99\n\n\nSi noti che i due fattori non sono molto distinti. Consideriamo dunque la soluzione prodotta da una rotazione obliqua. Usiamo qui l’algoritmo Oblimin.\n\npr_oblimin &lt;- principal(R, 2, rotate = \"oblimin\")\n\nLa matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\) delle saturazioni fattoriali si ricava come indicato di seguito.\n\ncbind(pr_oblimin$load[, 1], pr_oblimin$load[, 2])\n\n\nA matrix: 4 x 2 of type dbl\n\n\n0.0321\n0.9019\n\n\n-0.0254\n0.9556\n\n\n0.9686\n-0.0110\n\n\n0.9473\n0.0133\n\n\n\n\n\nLa matrice \\(\\hat{\\boldsymbol{\\Phi}}\\) di inter-correlazione fattoriale è la seguente.\n\npr_oblimin$Phi\n\n\nA matrix: 2 x 2 of type dbl\n\n\n\nTC1\nTC2\n\n\n\n\nTC1\n1.000\n0.787\n\n\nTC2\n0.787\n1.000\n\n\n\n\n\nLa matrice di struttura, che riporta le correlazioni tra indicatori e fattori comuni, si ottiene pre-moltiplicando la matrice \\(\\boldsymbol{\\Lambda}\\) delle saturazioni fattoriali alla matrice \\(\\boldsymbol{\\Phi}\\) di inter-correlazione fattoriale.\n\\[\n\\text{matrice di struttura} = \\boldsymbol{\\Lambda}\\boldsymbol{\\Phi}.\n\\]\nPer esempio, la correlazione tra la prima variabile manifesta e il primo fattore si ottiene nel modo seguente.\n\npr_oblimin$load[1, 1] + pr_oblimin$load[1, 2] * pr_oblimin$Phi[2, 1]\n\nTC1: 0.741813471502872\n\n\nL’intera matrice di struttura si può trovare eseguendo la moltiplicazione \\(\\boldsymbol{\\Lambda}\\boldsymbol{\\Phi}\\).\n\npr_oblimin$load %*% pr_oblimin$Phi %&gt;% \n  round(3)\n\n\nA matrix: 4 x 2 of type dbl\n\n\nTC1\nTC2\n\n\n\n\n0.742\n0.927\n\n\n0.727\n0.936\n\n\n0.960\n0.751\n\n\n0.958\n0.759",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#esempio-con-semtools",
    "href": "chapters/extraction/04_rotazione.html#esempio-con-semtools",
    "title": "34  La rotazione fattoriale",
    "section": "34.5 Esempio con semTools",
    "text": "34.5 Esempio con semTools\nPresento qui un esempio di uso di vari metodi di estrazione fattoriale. Tra tali metodi, la rotazione obliqua Geomin è molto popolare ed è il default di M-Plus.\nIniziamo a caricare il pacchetto semTools.\n\nsuppressPackageStartupMessages(library(\"semTools\")) \n\nEseguiamo l’analisi fattoriale esplorativa del classico set di dati di Holzinger e Swineford (1939) il quale è costituito dai punteggi dei test di abilità mentale di bambini di seconda e terza media di due scuole diverse (Pasteur e Grant-White). Nel set di dati originale (disponibile nel pacchetto MBESS), sono forniti i punteggi di 26 test. Tuttavia, un sottoinsieme più piccolo con 9 variabili è più ampiamente utilizzato in letteratura. Questi sono i dati qui usati.\nNel presente esempio, verrà eseguita l’analisi fattoriale esplorativa con l’estrazione di tre fattori. Il metodo di estrazione è mlr:\n\nmaximum likelihood estimation with robust (Huber-White) standard errors and a scaled test statistic that is (asymptotically) equal to the Yuan-Bentler test statistic. For both complete and incomplete data.\n\nLa soluzione iniziale non è ruotata.\n\nunrotated &lt;- efaUnrotate(\n    HolzingerSwineford1939, \n    nf = 3, \n    varList = paste0(\"x\", 1:9), \n    estimator = \"mlr\"\n)\nout &lt;- summary(unrotated)\nprint(out)\n\nlavaan 0.6-19 ended normally after 217 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        36\n\n  Number of observations                           301\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                22.897      23.864\n  Degrees of freedom                                12          12\n  P-value (Chi-square)                           0.029       0.021\n  Scaling correction factor                                  0.959\n    Yuan-Bentler correction (Mplus variant)                       \n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  factor1 =~                                          \n    x1      (l1_1)    0.653    0.083    7.909    0.000\n    x2      (l2_1)    0.353    0.079    4.481    0.000\n    x3      (l3_1)    0.415    0.086    4.832    0.000\n    x4      (l4_1)    0.926    0.067   13.762    0.000\n    x5      (l5_1)    1.014    0.067   15.176    0.000\n    x6      (l6_1)    0.868    0.062   13.887    0.000\n    x7      (l7_1)    0.283    0.091    3.113    0.002\n    x8      (l8_1)    0.340    0.083    4.095    0.000\n    x9      (l9_1)    0.460    0.078    5.881    0.000\n  factor2 =~                                          \n    x1      (l1_2)    0.349    0.124    2.815    0.005\n    x2      (l2_2)    0.242    0.159    1.523    0.128\n    x3      (l3_2)    0.497    0.132    3.767    0.000\n    x4      (l4_2)   -0.337    0.067   -5.058    0.000\n    x5      (l5_2)   -0.461    0.077   -6.009    0.000\n    x6      (l6_2)   -0.280    0.057   -4.908    0.000\n    x7      (l7_2)    0.372    0.188    1.976    0.048\n    x8      (l8_2)    0.510    0.133    3.831    0.000\n    x9      (l9_2)    0.489    0.066    7.416    0.000\n  factor3 =~                                          \n    x1      (l1_3)   -0.338    0.103   -3.275    0.001\n    x2      (l2_3)   -0.405    0.092   -4.401    0.000\n    x3      (l3_3)   -0.404    0.120   -3.355    0.001\n    x4      (l4_3)    0.049    0.098    0.503    0.615\n    x5      (l5_3)    0.122    0.105    1.154    0.248\n    x6      (l6_3)   -0.000    0.076   -0.003    0.998\n    x7      (l7_3)    0.609    0.125    4.863    0.000\n    x8      (l8_3)    0.409    0.143    2.853    0.004\n    x9      (l9_3)    0.112    0.123    0.915    0.360\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  factor1 ~~                                          \n    factor2           0.000                           \n    factor3           0.000                           \n  factor2 ~~                                          \n    factor3           0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    factor1           1.000                           \n    factor2           1.000                           \n    factor3           1.000                           \n   .x1                0.696    0.113    6.184    0.000\n   .x2                1.035    0.106    9.803    0.000\n   .x3                0.692    0.097    7.132    0.000\n   .x4                0.377    0.053    7.170    0.000\n   .x5                0.403    0.064    6.303    0.000\n   .x6                0.365    0.046    7.984    0.000\n   .x7                0.594    0.148    4.014    0.000\n   .x8                0.479    0.099    4.842    0.000\n   .x9                0.551    0.065    8.518    0.000\n\nConstraints:\n                                               |Slack|\n    0-(1_2*1_1+2_2*2_1+3_2*3_1+4_2*4_1+5_2*5_    0.000\n    0-(1_3*1_1+2_3*2_1+3_3*3_1+4_3*4_1+5_3*5_    0.000\n    0-(1_3*1_2+2_3*2_2+3_3*3_2+4_3*4_2+5_3*5_    0.000\n\n\n\nSi noti che, in assenza di rotazione, è impossibile assegnare un significato ai fattori comuni.\n\n34.5.1 Orthogonal varimax\nUtilizziamo ora la rotazione ortogonale Varimax.\n\nout_varimax &lt;- orthRotate(\n    unrotated, \n    method = \"varimax\"\n)\nout &lt;- summary(out_varimax, sort = FALSE, suppress = 0.3)\nprint(out)\n\nStandardized Rotated Factor Loadings\n\n\n   factor1 factor2 factor3\nx1  0.320*  0.607*        \nx2          0.481*        \nx3          0.662*        \nx4  0.838*                \nx5  0.867*                \nx6  0.815*                \nx7                  0.695*\nx8                  0.704*\nx9          0.409*  0.511*\n\nFactor Correlation\n        factor1 factor2 factor3\nfactor1       1       0       0\nfactor2       0       1       0\nfactor3       0       0       1\n\nMethod of rotation: varimax \n\nTest Statistics for Standardized Rotated Factor Loadings\n\n\n       lhs op rhs std.loading    se      z     p ci.lower ci.upper\n1  factor1 =~  x1       0.320 0.055  5.799 0.000    0.212    0.428\n2  factor1 =~  x2       0.135 0.063  2.151 0.031    0.012    0.259\n3  factor1 =~  x3       0.080 0.049  1.622 0.105   -0.017    0.176\n4  factor1 =~  x4       0.838 0.028 30.193 0.000    0.784    0.892\n5  factor1 =~  x5       0.867 0.024 36.189 0.000    0.820    0.914\n6  factor1 =~  x6       0.815 0.024 33.939 0.000    0.768    0.862\n7  factor1 =~  x7       0.102 0.049  2.058 0.040    0.005    0.199\n8  factor1 =~  x8       0.078 0.048  1.617 0.106   -0.016    0.172\n9  factor1 =~  x9       0.170 0.053  3.222 0.001    0.067    0.273\n10 factor2 =~  x1       0.607 0.075  8.138 0.000    0.461    0.753\n11 factor2 =~  x2       0.481 0.067  7.184 0.000    0.350    0.612\n12 factor2 =~  x3       0.662 0.058 11.499 0.000    0.549    0.775\n13 factor2 =~  x4       0.113 0.043  2.615 0.009    0.028    0.198\n14 factor2 =~  x5       0.032 0.040  0.802 0.422   -0.047    0.111\n15 factor2 =~  x6       0.162 0.042  3.855 0.000    0.079    0.244\n16 factor2 =~  x7      -0.062 0.047 -1.341 0.180   -0.154    0.029\n17 factor2 =~  x8       0.174 0.082  2.117 0.034    0.013    0.336\n18 factor2 =~  x9       0.409 0.079  5.173 0.000    0.254    0.564\n19 factor3 =~  x1       0.130 0.066  1.978 0.048    0.001    0.259\n20 factor3 =~  x2      -0.041 0.071 -0.578 0.563   -0.179    0.098\n21 factor3 =~  x3       0.113 0.049  2.324 0.020    0.018    0.209\n22 factor3 =~  x4       0.077 0.040  1.916 0.055   -0.002    0.155\n23 factor3 =~  x5       0.070 0.042  1.669 0.095   -0.012    0.153\n24 factor3 =~  x6       0.066 0.038  1.715 0.086   -0.009    0.141\n25 factor3 =~  x7       0.695 0.092  7.591 0.000    0.516    0.875\n26 factor3 =~  x8       0.704 0.083  8.520 0.000    0.542    0.865\n27 factor3 =~  x9       0.511 0.065  7.885 0.000    0.384    0.638\n       lhs op rhs std.loading    se      z     p ci.lower ci.upper\n1  factor1 =~  x1       0.320 0.055  5.799 0.000    0.212    0.428\n2  factor1 =~  x2       0.135 0.063  2.151 0.031    0.012    0.259\n3  factor1 =~  x3       0.080 0.049  1.622 0.105   -0.017    0.176\n4  factor1 =~  x4       0.838 0.028 30.193 0.000    0.784    0.892\n5  factor1 =~  x5       0.867 0.024 36.189 0.000    0.820    0.914\n6  factor1 =~  x6       0.815 0.024 33.939 0.000    0.768    0.862\n7  factor1 =~  x7       0.102 0.049  2.058 0.040    0.005    0.199\n8  factor1 =~  x8       0.078 0.048  1.617 0.106   -0.016    0.172\n9  factor1 =~  x9       0.170 0.053  3.222 0.001    0.067    0.273\n10 factor2 =~  x1       0.607 0.075  8.138 0.000    0.461    0.753\n11 factor2 =~  x2       0.481 0.067  7.184 0.000    0.350    0.612\n12 factor2 =~  x3       0.662 0.058 11.499 0.000    0.549    0.775\n13 factor2 =~  x4       0.113 0.043  2.615 0.009    0.028    0.198\n14 factor2 =~  x5       0.032 0.040  0.802 0.422   -0.047    0.111\n15 factor2 =~  x6       0.162 0.042  3.855 0.000    0.079    0.244\n16 factor2 =~  x7      -0.062 0.047 -1.341 0.180   -0.154    0.029\n17 factor2 =~  x8       0.174 0.082  2.117 0.034    0.013    0.336\n18 factor2 =~  x9       0.409 0.079  5.173 0.000    0.254    0.564\n19 factor3 =~  x1       0.130 0.066  1.978 0.048    0.001    0.259\n20 factor3 =~  x2      -0.041 0.071 -0.578 0.563   -0.179    0.098\n21 factor3 =~  x3       0.113 0.049  2.324 0.020    0.018    0.209\n22 factor3 =~  x4       0.077 0.040  1.916 0.055   -0.002    0.155\n23 factor3 =~  x5       0.070 0.042  1.669 0.095   -0.012    0.153\n24 factor3 =~  x6       0.066 0.038  1.715 0.086   -0.009    0.141\n25 factor3 =~  x7       0.695 0.092  7.591 0.000    0.516    0.875\n26 factor3 =~  x8       0.704 0.083  8.520 0.000    0.542    0.865\n27 factor3 =~  x9       0.511 0.065  7.885 0.000    0.384    0.638\n\n\n\n\n34.5.2 Orthogonal Quartimin\nUn metodo alternativo per la rotazione ortogonale è Quartimin.\n\nout_quartimin &lt;- orthRotate(\n    unrotated, \n    method = \"quartimin\"\n)\nout &lt;- summary(out_quartimin, sort = FALSE, suppress = 0.3)\nprint(out)\n\nStandardized Rotated Factor Loadings\n\n\n   factor1 factor2 factor3\nx1  0.353*  0.590*        \nx2          0.474*        \nx3          0.657*        \nx4  0.844*                \nx5  0.869*                \nx6  0.823*                \nx7                  0.692*\nx8                  0.702*\nx9          0.397*  0.508*\n\nFactor Correlation\n        factor1 factor2 factor3\nfactor1       1       0       0\nfactor2       0       1       0\nfactor3       0       0       1\n\nMethod of rotation: Quartimin \n\nTest Statistics for Standardized Rotated Factor Loadings\n\n\n       lhs op rhs std.loading    se      z     p ci.lower ci.upper\n1  factor1 =~  x1       0.353 0.062  5.720 0.000    0.232    0.473\n2  factor1 =~  x2       0.158 0.066  2.375 0.018    0.028    0.288\n3  factor1 =~  x3       0.115 0.057  2.015 0.044    0.003    0.226\n4  factor1 =~  x4       0.844 0.027 30.814 0.000    0.790    0.898\n5  factor1 =~  x5       0.869 0.023 37.224 0.000    0.823    0.914\n6  factor1 =~  x6       0.823 0.024 35.026 0.000    0.777    0.869\n7  factor1 =~  x7       0.116 0.054  2.161 0.031    0.011    0.222\n8  factor1 =~  x8       0.104 0.054  1.914 0.056   -0.003    0.210\n9  factor1 =~  x9       0.202 0.059  3.403 0.001    0.086    0.319\n10 factor2 =~  x1       0.590 0.078  7.523 0.000    0.436    0.743\n11 factor2 =~  x2       0.474 0.068  6.935 0.000    0.340    0.608\n12 factor2 =~  x3       0.657 0.059 11.204 0.000    0.542    0.771\n13 factor2 =~  x4       0.072 0.041  1.748 0.080   -0.009    0.152\n14 factor2 =~  x5      -0.010 0.039 -0.263 0.793   -0.087    0.066\n15 factor2 =~  x6       0.122 0.040  3.008 0.003    0.042    0.201\n16 factor2 =~  x7      -0.071 0.048 -1.465 0.143   -0.166    0.024\n17 factor2 =~  x8       0.167 0.090  1.849 0.064   -0.010    0.343\n18 factor2 =~  x9       0.397 0.087  4.581 0.000    0.227    0.567\n19 factor3 =~  x1       0.124 0.071  1.740 0.082   -0.016    0.264\n20 factor3 =~  x2      -0.042 0.074 -0.574 0.566   -0.187    0.102\n21 factor3 =~  x3       0.114 0.054  2.093 0.036    0.007    0.221\n22 factor3 =~  x4       0.056 0.037  1.503 0.133   -0.017    0.128\n23 factor3 =~  x5       0.048 0.038  1.263 0.207   -0.027    0.123\n24 factor3 =~  x6       0.046 0.034  1.347 0.178   -0.021    0.112\n25 factor3 =~  x7       0.692 0.093  7.477 0.000    0.511    0.874\n26 factor3 =~  x8       0.702 0.084  8.331 0.000    0.537    0.867\n27 factor3 =~  x9       0.508 0.070  7.298 0.000    0.371    0.644\n       lhs op rhs std.loading    se      z     p ci.lower ci.upper\n1  factor1 =~  x1       0.353 0.062  5.720 0.000    0.232    0.473\n2  factor1 =~  x2       0.158 0.066  2.375 0.018    0.028    0.288\n3  factor1 =~  x3       0.115 0.057  2.015 0.044    0.003    0.226\n4  factor1 =~  x4       0.844 0.027 30.814 0.000    0.790    0.898\n5  factor1 =~  x5       0.869 0.023 37.224 0.000    0.823    0.914\n6  factor1 =~  x6       0.823 0.024 35.026 0.000    0.777    0.869\n7  factor1 =~  x7       0.116 0.054  2.161 0.031    0.011    0.222\n8  factor1 =~  x8       0.104 0.054  1.914 0.056   -0.003    0.210\n9  factor1 =~  x9       0.202 0.059  3.403 0.001    0.086    0.319\n10 factor2 =~  x1       0.590 0.078  7.523 0.000    0.436    0.743\n11 factor2 =~  x2       0.474 0.068  6.935 0.000    0.340    0.608\n12 factor2 =~  x3       0.657 0.059 11.204 0.000    0.542    0.771\n13 factor2 =~  x4       0.072 0.041  1.748 0.080   -0.009    0.152\n14 factor2 =~  x5      -0.010 0.039 -0.263 0.793   -0.087    0.066\n15 factor2 =~  x6       0.122 0.040  3.008 0.003    0.042    0.201\n16 factor2 =~  x7      -0.071 0.048 -1.465 0.143   -0.166    0.024\n17 factor2 =~  x8       0.167 0.090  1.849 0.064   -0.010    0.343\n18 factor2 =~  x9       0.397 0.087  4.581 0.000    0.227    0.567\n19 factor3 =~  x1       0.124 0.071  1.740 0.082   -0.016    0.264\n20 factor3 =~  x2      -0.042 0.074 -0.574 0.566   -0.187    0.102\n21 factor3 =~  x3       0.114 0.054  2.093 0.036    0.007    0.221\n22 factor3 =~  x4       0.056 0.037  1.503 0.133   -0.017    0.128\n23 factor3 =~  x5       0.048 0.038  1.263 0.207   -0.027    0.123\n24 factor3 =~  x6       0.046 0.034  1.347 0.178   -0.021    0.112\n25 factor3 =~  x7       0.692 0.093  7.477 0.000    0.511    0.874\n26 factor3 =~  x8       0.702 0.084  8.331 0.000    0.537    0.867\n27 factor3 =~  x9       0.508 0.070  7.298 0.000    0.371    0.644\n\n\n\n\n34.5.3 Oblique Quartimin\nL’algoritmo Quartimin può anche essere usato per una soluzione obliqua.\n\nout_oblq &lt;- oblqRotate(\n    unrotated, \n    method = \"quartimin\"\n)\nout &lt;- summary(out_oblq, sort = FALSE, suppress = 0.3)\nprint(out)\n\nStandardized Rotated Factor Loadings\n\n\n   factor1 factor2 factor3\nx1          0.602*        \nx2          0.505*        \nx3          0.689*        \nx4  0.840*                \nx5  0.888*                \nx6  0.808*                \nx7                  0.723*\nx8                  0.702*\nx9          0.366*  0.463*\n\nFactor Correlation\n        factor1 factor2 factor3\nfactor1   1.000   0.326   0.216\nfactor2   0.326   1.000   0.270\nfactor3   0.216   0.270   1.000\n\nMethod of rotation: Quartimin \n\nTest Statistics for Standardized Rotated Factor Loadings\n\n\n       lhs op rhs std.loading    se      z     p ci.lower ci.upper\n1  factor1 =~  x1       0.191 0.064  2.965 0.003    0.065    0.317\n2  factor1 =~  x2       0.044 0.066  0.665 0.506   -0.085    0.172\n3  factor1 =~  x3      -0.070 0.034 -2.031 0.042   -0.137   -0.002\n4  factor1 =~  x4       0.840 0.033 25.622 0.000    0.776    0.905\n5  factor1 =~  x5       0.888 0.027 32.583 0.000    0.835    0.942\n6  factor1 =~  x6       0.808 0.028 28.441 0.000    0.752    0.863\n7  factor1 =~  x7       0.044 0.037  1.179 0.238   -0.029    0.116\n8  factor1 =~  x8      -0.033 0.036 -0.916 0.360   -0.103    0.037\n9  factor1 =~  x9       0.035 0.048  0.728 0.467   -0.059    0.129\n10 factor2 =~  x1       0.602 0.086  7.003 0.000    0.434    0.771\n11 factor2 =~  x2       0.505 0.071  7.163 0.000    0.367    0.644\n12 factor2 =~  x3       0.689 0.056 12.344 0.000    0.580    0.799\n13 factor2 =~  x4       0.022 0.045  0.483 0.629   -0.067    0.110\n14 factor2 =~  x5      -0.067 0.036 -1.890 0.059   -0.137    0.002\n15 factor2 =~  x6       0.078 0.041  1.887 0.059   -0.003    0.158\n16 factor2 =~  x7      -0.152 0.037 -4.059 0.000   -0.225   -0.078\n17 factor2 =~  x8       0.104 0.109  0.960 0.337   -0.109    0.317\n18 factor2 =~  x9       0.366 0.097  3.780 0.000    0.176    0.556\n19 factor3 =~  x1       0.031 0.062  0.500 0.617   -0.090    0.152\n20 factor3 =~  x2      -0.117 0.066 -1.776 0.076   -0.245    0.012\n21 factor3 =~  x3       0.023 0.039  0.587 0.557   -0.054    0.100\n22 factor3 =~  x4       0.005 0.042  0.128 0.898   -0.076    0.087\n23 factor3 =~  x5       0.008 0.035  0.216 0.829   -0.061    0.076\n24 factor3 =~  x6      -0.011 0.030 -0.362 0.717   -0.070    0.048\n25 factor3 =~  x7       0.723 0.087  8.328 0.000    0.553    0.893\n26 factor3 =~  x8       0.702 0.098  7.137 0.000    0.509    0.894\n27 factor3 =~  x9       0.463 0.075  6.211 0.000    0.317    0.609\n       lhs op rhs std.loading    se      z     p ci.lower ci.upper\n1  factor1 =~  x1       0.191 0.064  2.965 0.003    0.065    0.317\n2  factor1 =~  x2       0.044 0.066  0.665 0.506   -0.085    0.172\n3  factor1 =~  x3      -0.070 0.034 -2.031 0.042   -0.137   -0.002\n4  factor1 =~  x4       0.840 0.033 25.622 0.000    0.776    0.905\n5  factor1 =~  x5       0.888 0.027 32.583 0.000    0.835    0.942\n6  factor1 =~  x6       0.808 0.028 28.441 0.000    0.752    0.863\n7  factor1 =~  x7       0.044 0.037  1.179 0.238   -0.029    0.116\n8  factor1 =~  x8      -0.033 0.036 -0.916 0.360   -0.103    0.037\n9  factor1 =~  x9       0.035 0.048  0.728 0.467   -0.059    0.129\n10 factor2 =~  x1       0.602 0.086  7.003 0.000    0.434    0.771\n11 factor2 =~  x2       0.505 0.071  7.163 0.000    0.367    0.644\n12 factor2 =~  x3       0.689 0.056 12.344 0.000    0.580    0.799\n13 factor2 =~  x4       0.022 0.045  0.483 0.629   -0.067    0.110\n14 factor2 =~  x5      -0.067 0.036 -1.890 0.059   -0.137    0.002\n15 factor2 =~  x6       0.078 0.041  1.887 0.059   -0.003    0.158\n16 factor2 =~  x7      -0.152 0.037 -4.059 0.000   -0.225   -0.078\n17 factor2 =~  x8       0.104 0.109  0.960 0.337   -0.109    0.317\n18 factor2 =~  x9       0.366 0.097  3.780 0.000    0.176    0.556\n19 factor3 =~  x1       0.031 0.062  0.500 0.617   -0.090    0.152\n20 factor3 =~  x2      -0.117 0.066 -1.776 0.076   -0.245    0.012\n21 factor3 =~  x3       0.023 0.039  0.587 0.557   -0.054    0.100\n22 factor3 =~  x4       0.005 0.042  0.128 0.898   -0.076    0.087\n23 factor3 =~  x5       0.008 0.035  0.216 0.829   -0.061    0.076\n24 factor3 =~  x6      -0.011 0.030 -0.362 0.717   -0.070    0.048\n25 factor3 =~  x7       0.723 0.087  8.328 0.000    0.553    0.893\n26 factor3 =~  x8       0.702 0.098  7.137 0.000    0.509    0.894\n27 factor3 =~  x9       0.463 0.075  6.211 0.000    0.317    0.609\n\n\n\n\n34.5.4 Orthogonal Geomin\nConsideriamo ora la rotazione Geomin. L’algoritmo Geomin fornisce un metodo di rotazione che riduce al minimo la media geometrica delle saturazioni fattoriali innalzate al quadrato. Qui è usato per ottenere una soluzione ortogonale.\n\nout_geomin_orh &lt;- orthRotate(\n    unrotated, \n    method = \"geomin\"\n)\nout &lt;- summary(out_geomin_orh, sort = FALSE, suppress = 0.3)\nprint(out)\n\nStandardized Rotated Factor Loadings\n\n\n   factor1 factor2 factor3\nx1  0.315*         -0.621*\nx2                 -0.474*\nx3                 -0.671*\nx4  0.838*                \nx5  0.867*                \nx6  0.814*                \nx7          0.696*        \nx8          0.677*        \nx9          0.456* -0.468*\n\nFactor Correlation\n        factor1 factor2 factor3\nfactor1       1       0       0\nfactor2       0       1       0\nfactor3       0       0       1\n\nMethod of rotation: Geomin \n\nTest Statistics for Standardized Rotated Factor Loadings\n\n\n       lhs op rhs std.loading    se       z     p ci.lower ci.upper\n1  factor1 =~  x1       0.315 0.134   2.352 0.019    0.053    0.578\n2  factor1 =~  x2       0.130 0.118   1.102 0.271   -0.101    0.360\n3  factor1 =~  x3       0.074 0.123   0.599 0.549   -0.168    0.316\n4  factor1 =~  x4       0.838 0.036  23.402 0.000    0.767    0.908\n5  factor1 =~  x5       0.867 0.025  35.190 0.000    0.819    0.915\n6  factor1 =~  x6       0.814 0.040  20.427 0.000    0.736    0.892\n7  factor1 =~  x7       0.112 0.068   1.645 0.100   -0.021    0.245\n8  factor1 =~  x8       0.085 0.063   1.341 0.180   -0.039    0.209\n9  factor1 =~  x9       0.172 0.095   1.803 0.071   -0.015    0.359\n10 factor2 =~  x1       0.053 0.061   0.868 0.386   -0.067    0.173\n11 factor2 =~  x2      -0.099 0.070  -1.409 0.159   -0.237    0.039\n12 factor2 =~  x3       0.033 0.048   0.698 0.485   -0.060    0.127\n13 factor2 =~  x4       0.051 0.046   1.115 0.265   -0.039    0.140\n14 factor2 =~  x5       0.054 0.061   0.878 0.380   -0.066    0.173\n15 factor2 =~  x6       0.035 0.037   0.924 0.355   -0.039    0.108\n16 factor2 =~  x7       0.696 0.090   7.702 0.000    0.519    0.874\n17 factor2 =~  x8       0.677 0.088   7.660 0.000    0.504    0.850\n18 factor2 =~  x9       0.456 0.073   6.261 0.000    0.314    0.599\n19 factor3 =~  x1      -0.621 0.101  -6.148 0.000   -0.818   -0.423\n20 factor3 =~  x2      -0.474 0.074  -6.414 0.000   -0.619   -0.329\n21 factor3 =~  x3      -0.671 0.058 -11.658 0.000   -0.784   -0.558\n22 factor3 =~  x4      -0.129 0.161  -0.800 0.424   -0.445    0.187\n23 factor3 =~  x5      -0.048 0.174  -0.277 0.782   -0.389    0.293\n24 factor3 =~  x6      -0.176 0.164  -1.071 0.284   -0.497    0.146\n25 factor3 =~  x7      -0.021 0.059  -0.354 0.724   -0.137    0.095\n26 factor3 =~  x8      -0.257 0.094  -2.720 0.007   -0.442   -0.072\n27 factor3 =~  x9      -0.468 0.105  -4.472 0.000   -0.673   -0.263\n       lhs op rhs std.loading    se       z     p ci.lower ci.upper\n1  factor1 =~  x1       0.315 0.134   2.352 0.019    0.053    0.578\n2  factor1 =~  x2       0.130 0.118   1.102 0.271   -0.101    0.360\n3  factor1 =~  x3       0.074 0.123   0.599 0.549   -0.168    0.316\n4  factor1 =~  x4       0.838 0.036  23.402 0.000    0.767    0.908\n5  factor1 =~  x5       0.867 0.025  35.190 0.000    0.819    0.915\n6  factor1 =~  x6       0.814 0.040  20.427 0.000    0.736    0.892\n7  factor1 =~  x7       0.112 0.068   1.645 0.100   -0.021    0.245\n8  factor1 =~  x8       0.085 0.063   1.341 0.180   -0.039    0.209\n9  factor1 =~  x9       0.172 0.095   1.803 0.071   -0.015    0.359\n10 factor2 =~  x1       0.053 0.061   0.868 0.386   -0.067    0.173\n11 factor2 =~  x2      -0.099 0.070  -1.409 0.159   -0.237    0.039\n12 factor2 =~  x3       0.033 0.048   0.698 0.485   -0.060    0.127\n13 factor2 =~  x4       0.051 0.046   1.115 0.265   -0.039    0.140\n14 factor2 =~  x5       0.054 0.061   0.878 0.380   -0.066    0.173\n15 factor2 =~  x6       0.035 0.037   0.924 0.355   -0.039    0.108\n16 factor2 =~  x7       0.696 0.090   7.702 0.000    0.519    0.874\n17 factor2 =~  x8       0.677 0.088   7.660 0.000    0.504    0.850\n18 factor2 =~  x9       0.456 0.073   6.261 0.000    0.314    0.599\n19 factor3 =~  x1      -0.621 0.101  -6.148 0.000   -0.818   -0.423\n20 factor3 =~  x2      -0.474 0.074  -6.414 0.000   -0.619   -0.329\n21 factor3 =~  x3      -0.671 0.058 -11.658 0.000   -0.784   -0.558\n22 factor3 =~  x4      -0.129 0.161  -0.800 0.424   -0.445    0.187\n23 factor3 =~  x5      -0.048 0.174  -0.277 0.782   -0.389    0.293\n24 factor3 =~  x6      -0.176 0.164  -1.071 0.284   -0.497    0.146\n25 factor3 =~  x7      -0.021 0.059  -0.354 0.724   -0.137    0.095\n26 factor3 =~  x8      -0.257 0.094  -2.720 0.007   -0.442   -0.072\n27 factor3 =~  x9      -0.468 0.105  -4.472 0.000   -0.673   -0.263\n\n\n\n\n34.5.5 Oblique Geomin\nLa rotazione Geomin può anche essere usata per ottenere una soluzione obliqua.\n\nout_geomin_obl &lt;- oblqRotate(\n    unrotated, \n    method = \"geomin\"\n)\nout &lt;- summary(out_geomin_obl, sort = FALSE, suppress = 0.3)\nprint(out)\n\nStandardized Rotated Factor Loadings\n\n\n   factor1 factor2 factor3\nx1                 -0.604*\nx2                 -0.507*\nx3                 -0.691*\nx4  0.839*                \nx5  0.887*                \nx6  0.806*                \nx7          0.726*        \nx8          0.703*        \nx9          0.463* -0.368*\n\nFactor Correlation\n        factor1 factor2 factor3\nfactor1   1.000   0.230  -0.327\nfactor2   0.230   1.000  -0.278\nfactor3  -0.327  -0.278   1.000\n\nMethod of rotation: Geomin \n\nTest Statistics for Standardized Rotated Factor Loadings\n\n\n       lhs op rhs std.loading    se       z     p ci.lower ci.upper\n1  factor1 =~  x1       0.188 0.070   2.670 0.008    0.050    0.326\n2  factor1 =~  x2       0.044 0.054   0.806 0.420   -0.063    0.150\n3  factor1 =~  x3      -0.073 0.049  -1.467 0.142   -0.170    0.024\n4  factor1 =~  x4       0.839 0.032  26.467 0.000    0.777    0.901\n5  factor1 =~  x5       0.887 0.029  30.077 0.000    0.829    0.945\n6  factor1 =~  x6       0.806 0.030  26.717 0.000    0.747    0.865\n7  factor1 =~  x7       0.031 0.034   0.915 0.360   -0.036    0.099\n8  factor1 =~  x8      -0.045 0.048  -0.948 0.343   -0.139    0.048\n9  factor1 =~  x9       0.025 0.034   0.747 0.455   -0.041    0.091\n10 factor2 =~  x1       0.029 0.051   0.567 0.571   -0.071    0.129\n11 factor2 =~  x2      -0.119 0.072  -1.664 0.096   -0.260    0.021\n12 factor2 =~  x3       0.020 0.037   0.538 0.591   -0.053    0.092\n13 factor2 =~  x4       0.007 0.043   0.174 0.862   -0.076    0.091\n14 factor2 =~  x5       0.010 0.036   0.285 0.775   -0.060    0.080\n15 factor2 =~  x6      -0.009 0.030  -0.309 0.758   -0.068    0.049\n16 factor2 =~  x7       0.726 0.072  10.065 0.000    0.585    0.868\n17 factor2 =~  x8       0.703 0.118   5.963 0.000    0.472    0.934\n18 factor2 =~  x9       0.463 0.080   5.810 0.000    0.307    0.619\n19 factor3 =~  x1      -0.604 0.081  -7.438 0.000   -0.763   -0.445\n20 factor3 =~  x2      -0.507 0.073  -6.983 0.000   -0.649   -0.364\n21 factor3 =~  x3      -0.691 0.061 -11.373 0.000   -0.810   -0.572\n22 factor3 =~  x4      -0.024 0.034  -0.702 0.482   -0.091    0.043\n23 factor3 =~  x5       0.065 0.045   1.459 0.144   -0.022    0.153\n24 factor3 =~  x6      -0.080 0.048  -1.679 0.093   -0.173    0.013\n25 factor3 =~  x7       0.150 0.107   1.403 0.161   -0.060    0.360\n26 factor3 =~  x8      -0.106 0.164  -0.645 0.519   -0.428    0.216\n27 factor3 =~  x9      -0.368 0.133  -2.770 0.006   -0.629   -0.108\n       lhs op rhs std.loading    se       z     p ci.lower ci.upper\n1  factor1 =~  x1       0.188 0.070   2.670 0.008    0.050    0.326\n2  factor1 =~  x2       0.044 0.054   0.806 0.420   -0.063    0.150\n3  factor1 =~  x3      -0.073 0.049  -1.467 0.142   -0.170    0.024\n4  factor1 =~  x4       0.839 0.032  26.467 0.000    0.777    0.901\n5  factor1 =~  x5       0.887 0.029  30.077 0.000    0.829    0.945\n6  factor1 =~  x6       0.806 0.030  26.717 0.000    0.747    0.865\n7  factor1 =~  x7       0.031 0.034   0.915 0.360   -0.036    0.099\n8  factor1 =~  x8      -0.045 0.048  -0.948 0.343   -0.139    0.048\n9  factor1 =~  x9       0.025 0.034   0.747 0.455   -0.041    0.091\n10 factor2 =~  x1       0.029 0.051   0.567 0.571   -0.071    0.129\n11 factor2 =~  x2      -0.119 0.072  -1.664 0.096   -0.260    0.021\n12 factor2 =~  x3       0.020 0.037   0.538 0.591   -0.053    0.092\n13 factor2 =~  x4       0.007 0.043   0.174 0.862   -0.076    0.091\n14 factor2 =~  x5       0.010 0.036   0.285 0.775   -0.060    0.080\n15 factor2 =~  x6      -0.009 0.030  -0.309 0.758   -0.068    0.049\n16 factor2 =~  x7       0.726 0.072  10.065 0.000    0.585    0.868\n17 factor2 =~  x8       0.703 0.118   5.963 0.000    0.472    0.934\n18 factor2 =~  x9       0.463 0.080   5.810 0.000    0.307    0.619\n19 factor3 =~  x1      -0.604 0.081  -7.438 0.000   -0.763   -0.445\n20 factor3 =~  x2      -0.507 0.073  -6.983 0.000   -0.649   -0.364\n21 factor3 =~  x3      -0.691 0.061 -11.373 0.000   -0.810   -0.572\n22 factor3 =~  x4      -0.024 0.034  -0.702 0.482   -0.091    0.043\n23 factor3 =~  x5       0.065 0.045   1.459 0.144   -0.022    0.153\n24 factor3 =~  x6      -0.080 0.048  -1.679 0.093   -0.173    0.013\n25 factor3 =~  x7       0.150 0.107   1.403 0.161   -0.060    0.360\n26 factor3 =~  x8      -0.106 0.164  -0.645 0.519   -0.428    0.216\n27 factor3 =~  x9      -0.368 0.133  -2.770 0.006   -0.629   -0.108",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#interpretazione-dei-fattori-latenti-nellanalisi-fattoriale",
    "href": "chapters/extraction/04_rotazione.html#interpretazione-dei-fattori-latenti-nellanalisi-fattoriale",
    "title": "34  La rotazione fattoriale",
    "section": "34.6 Interpretazione dei fattori latenti nell’analisi fattoriale",
    "text": "34.6 Interpretazione dei fattori latenti nell’analisi fattoriale\nNell’interpretare i fattori comuni latenti in un’analisi fattoriale, è cruciale scegliere tra la matrice pattern e la matrice struttura. Entrambe sono utili per l’interpretazione, ma forniscono informazioni diverse:\n\nMatrice pattern:\n\nMostra le saturazioni fattoriali dirette.\nIndica gli effetti diretti dei fattori latenti sulle variabili manifeste.\nRivela quanto ciascun fattore contribuisce direttamente alla varianza di una variabile osservata.\nFondamentale per comprendere il significato psicologico dei fattori.\n\nMatrice struttura:\n\nRappresenta le correlazioni tra fattori latenti e variabili osservate.\nInclude sia gli effetti diretti che quelli indiretti.\nDescrive la covariazione complessiva tra fattori e variabili manifeste.\nNon distingue tra relazioni dirette e indirette.\n\n\nUn fattore identificato nell’analisi fattoriale rappresenta una variabile latente univariata, ovvero una dimensione sottostante che cattura l’essenza di un fenomeno psicologico. L’interpretazione del fattore emerge dall’intersezione dei significati delle variabili che vi saturano.\nNel caso di rotazioni oblique, dove i fattori sono correlati, è essenziale interpretare ogni fattore come una dimensione psicologica distinta. Ad esempio, l’etichetta assegnata al fattore \\(F_1\\) deve essere concettualmente separata dal fenomeno rappresentato dal fattore \\(F_2\\), nonostante la loro correlazione.\nAdottando questa strategia interpretativa, la matrice pattern diventa lo strumento principale per l’interpretazione. I suoi coefficienti riflettono gli effetti diretti dei fattori latenti sulle variabili manifeste, indicando un’influenza “causale” del fattore su tali variabili. La matrice struttura, invece, descrive le correlazioni complessive tra variabili manifeste e fattori, includendo sia relazioni dirette che indirette.\nIn conclusione, per un’interpretazione più accurata e teoricamente solida dei fattori latenti, è preferibile basarsi sulla matrice pattern, che fornisce informazioni specifiche sugli effetti diretti tra fattori comuni e variabili manifeste.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/04_rotazione.html#session-info",
    "href": "chapters/extraction/04_rotazione.html#session-info",
    "title": "34  La rotazione fattoriale",
    "section": "34.7 Session Info",
    "text": "34.7 Session Info\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] GPArotation_2024.3-1 MASS_7.3-61          viridis_0.6.5       \n [4] viridisLite_0.4.2    ggpubr_0.6.0         ggExtra_0.10.1      \n [7] gridExtra_2.3        patchwork_1.3.0      bayesplot_1.11.1    \n[10] semTools_0.5-6       semPlot_1.1.6        lavaan_0.6-19       \n[13] psych_2.4.6.26       scales_1.3.0         markdown_1.13       \n[16] knitr_1.49           lubridate_1.9.3      forcats_1.0.0       \n[19] stringr_1.5.1        dplyr_1.1.4          purrr_1.0.2         \n[22] readr_2.1.5          tidyr_1.3.1          tibble_3.2.1        \n[25] ggplot2_3.5.1        tidyverse_2.0.0      here_1.0.1          \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.17.1   jsonlite_1.8.9      magrittr_2.0.3     \n  [4] TH.data_1.1-2       estimability_1.5.1  farver_2.1.2       \n  [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n [10] Cairo_1.6-2         minqa_1.2.8         base64enc_0.1-3    \n [13] rstatix_0.7.2       htmltools_0.5.8.1   broom_1.0.7        \n [16] Formula_1.2-5       htmlwidgets_1.6.4   plyr_1.8.9         \n [19] sandwich_3.1-1      emmeans_1.10.5      zoo_1.8-12         \n [22] uuid_1.2-1          igraph_2.1.1        mime_0.12          \n [25] lifecycle_1.0.4     pkgconfig_2.0.3     Matrix_1.7-1       \n [28] R6_2.5.1            fastmap_1.2.0       shiny_1.9.1        \n [31] numDeriv_2016.8-1.1 digest_0.6.37       OpenMx_2.21.13     \n [34] fdrtool_1.2.18      colorspace_2.1-1    rprojroot_2.0.4    \n [37] Hmisc_5.2-0         fansi_1.0.6         timechange_0.3.0   \n [40] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n [43] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n [46] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n [49] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n [52] foreign_0.8-87      zip_2.3.1           httpuv_1.6.15      \n [55] nnet_7.3-19         glue_1.8.0          quadprog_1.5-8     \n [58] promises_1.3.0      nlme_3.1-166        lisrelToR_0.3      \n [61] grid_4.4.2          pbdZMQ_0.3-13       checkmate_2.3.2    \n [64] cluster_2.1.6       reshape2_1.4.4      generics_0.1.3     \n [67] gtable_0.3.6        tzdb_0.4.0          data.table_1.16.2  \n [70] hms_1.1.3           car_3.1-3           utf8_1.2.4         \n [73] sem_3.1-16          pillar_1.9.0        IRdisplay_1.1      \n [76] rockchalk_1.8.157   later_1.3.2         splines_4.4.2      \n [79] cherryblossom_0.1.0 lattice_0.22-6      survival_3.7-0     \n [82] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n [85] pbapply_1.7-2       airports_0.1.0      stats4_4.4.2       \n [88] xfun_0.49           qgraph_1.9.8        arm_1.14-4         \n [91] stringi_1.8.4       pacman_0.5.1        boot_1.3-31        \n [94] evaluate_1.0.1      codetools_0.2-20    mi_1.1             \n [97] cli_3.6.3           RcppParallel_5.1.9  IRkernel_1.3.2     \n[100] rpart_4.1.23        xtable_1.8-4        repr_1.1.7         \n[103] munsell_0.5.1       Rcpp_1.0.13-1       coda_0.19-4.1      \n[106] png_0.1-8           XML_3.99-0.17       parallel_4.4.2     \n[109] usdata_0.3.1        jpeg_0.1-10         lme4_1.1-35.5      \n[112] mvtnorm_1.3-2       openxlsx_4.2.7.1    crayon_1.5.3       \n[115] openintro_2.5.0     rlang_1.1.4         multcomp_1.4-26    \n[118] mnormt_2.1.1       \n\n\n\n\n\n\nRencher, A. (2002). Methods of multivariate analysis. 2002. Wiley Publications.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>La rotazione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/05_val_soluzione.html",
    "href": "chapters/extraction/05_val_soluzione.html",
    "title": "35  Valutare e rifinire la soluzione fattoriale",
    "section": "",
    "text": "35.1 Valutazione della matrice pattern\nLa maggior parte di strumenti usati nell’assessment psicologico e neuropsicologico non valuta una singola dimensione psicologica, ma piuttosto misura molteplici aspetti di un costrutto. Di conseguenza, l’analisi fattoriale produce solitamente una soluzione a più fattori. Idealmente, dopo la rotazione, ciascun item saturerà fortemente su un singolo fattore e debolmente sugli altri. In realtà, anche dopo la rotazione degli assi fattoriali, spesso si presentano item che saturano debolmente su tutti i fattori, oppure item che saturano fortemente su più di un fattore.\nUno dei primi passi da compiere per rifinire la soluzione fattoriale è quello di valutare la matrice struttura e intervenire utilizzando il criterio della “struttura semplice”, per poi valutare gli effetti delle azioni intraprese (es., eliminazione di alcuni item) nella matrice pattern. Ricordiamo che la matrice struttura contiene le correlazioni tra item e fattori, mentre la matrice pattern contiene le saturazioni fattoriali.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Valutare e rifinire la soluzione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/05_val_soluzione.html#valutazione-della-matrice-pattern",
    "href": "chapters/extraction/05_val_soluzione.html#valutazione-della-matrice-pattern",
    "title": "35  Valutare e rifinire la soluzione fattoriale",
    "section": "",
    "text": "35.1.1 Item con basse saturazioni su tutti i fattori\nPrima di procedere con l’analisi fattoriale è auspicabile esaminare la matrice di correlazioni tra gli item ed eliminare quegli item che sono insufficientemente correlati con gli altri item della matrice. Tuttavia, anche dopo questo screening iniziale, è possibile che vi siano item caratterizzati da saturazioni basse su tutti i fattori. Dal punto di vista pratico, si considerano “basse” le saturazioni il cui valore assoluto è minore di 0.30 (Hair et al., 1995). Hair e collaboratori suggeriscono due soluzioni nel caso di item con saturazioni basse su tutti i fattori:\n\neliminare gli item con basse saturazioni,\nvalutare le comunalità degli item problematici e il contributo specifico che forniscono allo strumento.\n\nSe un item ha una bassa comunalità, o se il contributo di un item nei confronti del significato generale dello strumento è di poca importanza, allora l’item dovrebbe essere eliminato. Dopo l’eliminazione degli item critici, si procede calcolando una nuova soluzione fattoriale e si esaminano i risultati ottenuti.\nSe vi sono degli item con basse saturazioni su tutti i fattori che però contribuiscono in maniera importante a determinare il significato della scala nel suo complesso, allora questi item dovrebbero essere mantenuti. Alle volte, per tali item è possibile creare delle sottoscale separate dalle altre.\n\n\n35.1.2 Item con saturazioni evevate su più di un fattore\nÈ comune anche il caso opposto, ovvero quello nel quale ci sono item che saturano su fattori multipli (con saturazioni fattoriali \\(&gt;\\) .30), specialmente nel caso di soluzioni fattoriali ottenutie dopo una rotazione obliqua. Kline (2000) suggerisce di eliminare tali item in quanto rendono difficile da interpretare il significato della scala che così si ottiene. Hair e collaboratori (1995) ritengono invece che tali item dovrebbero essere mantenuti, dato possono chiarire il significato dei fattori che la scala identifica.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Valutare e rifinire la soluzione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/05_val_soluzione.html#valutazione-dellattendibilità",
    "href": "chapters/extraction/05_val_soluzione.html#valutazione-dellattendibilità",
    "title": "35  Valutare e rifinire la soluzione fattoriale",
    "section": "35.2 Valutazione dell’attendibilità",
    "text": "35.2 Valutazione dell’attendibilità\nAll’interno del problema della costruzione di uno strumento vengono esaminati tre aspetti dell’attendibilità: la consistenza interna, la stabilità e l’equivalenza.\n\n35.2.1 Consistenza interna\n\n35.2.1.1 La procedura split-half\nLa consistenza interna misura il grado di coerenza tra gli item che costituiscono lo strumento o le sottoscale dello strumento. Se tutti gli item che costituiscono uno strumento o una sua sottoscala misurano la stessa cosa, allora saranno fortemente associati tra loro.\nÈ possibile misurare la consistenza interna con il metodo dello split-half, ovvero mediante la correlazione di Pearson tra i punteggi ottenuti utilizzando ciascuna delle due metà degli item dello strumento. Usando un software, è meglio trovare la media delle correlazioni inter-item ricavabili a partire da tutte le possibili divisioni a metà dell’insieme di item che costituiscono lo strumento. La correlazione trovata in questo modo viene poi corretta utilizzando la formula “profetica” di Spearman-Brown per tenere in considerazione il fatto che l’attendibilità è stata calcolata utilizzando soltanto metà degli item dello strumento.\nSi noti che la formula di Spearman-Brown è basata sull’assunzione che le due metà dello strumento siano parallele, ovvero che abbiano identici punteggi veri e uguali varianze d’errore (questa assunzione comporta la conseguenza per cui le due metà degli item devono producono punteggi aventi la stessa media e la stessa varianza). Se queste assunzioni molto stringenti non vengono soddisfatte, allora la procedura descritta sopra conduce ad una sovrastima dell’attendibilità quale consisenza interna della scala.\n\n\n35.2.1.2 L’analisi della varianza\nSe tutti gli item di uno strumento o di una sottoscala sono espressione dello stesso costrutto, allora ci dobbiamo aspettare che anche le medie dei punteggi sugli item siano uguali. Come è stato detto sopra, questa è infatti una delle assunzioni delle forme strettamente parallele di un test. È dunque possibile verificare questa assunzione mediante un’ANOVA che sottopone a test l’ipotesi nulla dell’uguaglianza delle medie di gruppi. Nel caso degli item di un test, dato che ciascun soggetto completa tutti gli item che costituiscono lo strumento, è appropriato usare un’ANOVA per misure ripetute che, nella sua declinazione più moderna, corrisponde ad un modello multi-livello (mixed-effect model).\n\n\n35.2.1.3 L’indice \\(\\alpha\\) di Cronbach\nL’indice \\(\\alpha\\) di Cronbach è comunque la misura più utilizzata per valutare l’attendibilità quale consistenza interna di uno strumento. L’\\(\\alpha\\) di Cronbach è stato interpretato come la proporzione di varianza della scala che può essere attribuita al fattore comune (DeVellis, 1991). Può anche essere interpretato come la correlazione stimata tra i punteggi della scala e un’altro strumento della stessa lunghezza tratto dall’universo degli item possibili che costituiscono il dominio del costrutto (Kline, 1986). La radice quadrata del coefficiente \\(\\alpha\\) di Cronbach rappresenta la correlazione stimata tra i punteggi ottenuti tramite lo strumento e i punteggi veri (Nunnally & Bernstein, 1994).\nIn precedenza abbiamo descritto una serie di limiti del coefficiente \\(\\alpha\\) di Cronbach. In generale, molti ricercatori suggeriscono di usare al suo posto l’indice \\(\\omega\\) di McDonald.\n\n\n\n35.2.2 Stabilità temporale\nLa stabilità temporale viene valutata attraverso la procedura di test-retest. La correlazione tra le misure ottenute in due momenti negli stessi rispondenti ci fornisce l’attendibilità di test-retest.\nKline (2000) ha messo in evidenza come l’attendibilità di test-retest sia influenzata da molteplici fattori, tra cui le caratteristiche del campione, la maturità dei rispondenti, i cambiamenti nello stato emozionale, le differenze nelle condizioni di somministrazione del test, la possibilità di ricordare le risposte date in precedenza, la difficoltà degli item, la grandezza del campione e le caratteristiche del costrutto (ad esempio, stato vs. tratto).\nParticolare attenzione deve essere rivolta all’intervallo temporale usato nella procedura di test-retest. Se il periodo di tempo che intercorre tra le due somministrazioni è troppo corto, i risultati possono risultare distorti a causa del fatto che i soggetti si ricordano le risposte date in precedenza. Questo può condurre ad una sovrastima dell’attendibilità test-retest (Pedhazur & Schmelkin, 1991). Un intervallo temporale troppo lungo tra le due somministrazioni ha invece come limite il fatto che, in questo caso, vi è un’alta possibilità che intervengano dei cambiamenti nei rispondenti rispetto al costrutto in esame. Alla luce di queste considerazioni è stato suggerito di utilizzare un intervallo temporale abbastanza breve, ovvero di una o due settimane (Nunnally & Bernstein, 1994; Pedhazur & Schmelkin, 1991). Se è necessario valutare la stabilità temporale nel corso di un lungo arco temporale, Nunnally e Bernstein (1994) suggeriscono di utilizzare un intervallo di sei mesi o maggiore.\n\n\n35.2.3 Equivalenza\nPer cercare di evitare i problemi associati all’attendibilità quale stabilità temporale, alcuni autori si sono posti il problema di esaminare la correlazione tra forme parallele (o equivalenti) dello strumento. La correlazione tra forme parallele di uno strumento va sotto il nome di coefficiente di equivalenza e fornisce una misura alternativa dell’attendibilità dello strumento (Burns & Grove, 2001; Pedhazur & Schmelkin, 1991; Polit & Hungler, 1999).\nNunnally e Bernstein (1994) suggeriscono di confrontare i risultati ottenuti con la somministrazione delle forme parallele lo stesso giorno con quelli ottenuti nel caso di un intervallo temporale di due settimane. Kline (2000) ritiene che l’attendibilità tra due forme parallele debba essere di almeno 0.9 perché, per valori inferiori, sarebbe difficile sostenere che le forme sono veramente parallele.\nÈ tuttavia molto oneroso predisporre due forme parallele di uno strumento. Per questa ragione, il coefficiente di equivalenza viene raramente usato.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Valutare e rifinire la soluzione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/05_val_soluzione.html#selezione-di-un-sottoinsieme-di-item",
    "href": "chapters/extraction/05_val_soluzione.html#selezione-di-un-sottoinsieme-di-item",
    "title": "35  Valutare e rifinire la soluzione fattoriale",
    "section": "35.3 Selezione di un sottoinsieme di item",
    "text": "35.3 Selezione di un sottoinsieme di item\nTipicamente, la costruzione di un test viene realizzata somministrando un grande numero di item per poi selezionare gli item “migliori” che andranno a fare parte del test vero e proprio. Si supponga di somministrare inizialmente \\(m\\) item, quando si desidera che il test finale sia costituito da \\(p &lt; m\\) item. Un modo di affrontare questo problema potrebbe essere quello di calcolare l’attendibilità del test (coefficiente \\(\\omega\\)) per tutti i possibili sottoinsiemi di \\(p\\) item, così da individuare il sottoinsieme migliore. Questo modo di procedere, però, è problematico perché richiede la valutazione di un elevatissimo numero di possibilità. Per esempio, da un insieme iniziale neanche troppo numeroso di 100 item, il numero di sottoinsiemi di 20 item è uguale a\n\\[\n\\binom{100}{20} = 5.36 \\times 10^{20}.\n\\]\nÈ dunque necessario trovare metodi alternativi che evitino una tale esplosione combinatoria. A questo fine, ovvero per procedere alla selezione del sottoinsieme dei “migliori” item, {cite:t}mcdonald2013test suggerisce di calcolare la quantità di informazione di ciascun item. La quantità di informazione di un item è definita come rapporto tra segnale/rumore, in relazione alla scomposizione della varianza dell’item:\n\\[\n\\frac{\\lambda_i^2}{\\psi_{ii}}.\n\\]\n{cite:t}mcdonald2013test mostra come l’omissione di uno o più item produce sempre una riduzione dell’attendibilità del test (ovvero, una riduzione nel valore del coefficiente \\(\\omega\\)). Tuttavia, tale riduzione è tanto più piccola quanto più piccola è la quantità di informazione degli item omessi. Il processo di selezione degli item può dunque essere guidato da un semplice principio: si selezionano gli item aventi la quantità di informazione maggiore. Ovvero, in altre parole, si rimuovono gli item aventi la quantità di informazione più bassa.\nEsempio. Per fare un esempio, consideriamo nuovamente la matrice di varianze e di covarianze della scala SWLS.\n\nvarnames &lt;- c(\"Y1\", \"Y2\", \"Y3\", \"Y4\", \"Y5\")\nSWLS &lt;- matrix(c(\n  2.565, 1.424, 1.481, 1.328, 1.529,\n  1.424, 2.493, 1.267, 1.051, 1.308,\n  1.481, 1.267, 2.462, 1.093, 1.360,\n  1.328, 1.051, 1.093, 2.769, 1.128,\n  1.529, 1.308, 1.360, 1.128, 3.355\n),\nncol = 5, byrow = TRUE,\ndimnames = list(varnames, varnames)\n)\nSWLS\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nY1\nY2\nY3\nY4\nY5\n\n\n\n\nY1\n2.565\n1.424\n1.481\n1.328\n1.529\n\n\nY2\n1.424\n2.493\n1.267\n1.051\n1.308\n\n\nY3\n1.481\n1.267\n2.462\n1.093\n1.360\n\n\nY4\n1.328\n1.051\n1.093\n2.769\n1.128\n\n\nY5\n1.529\n1.308\n1.360\n1.128\n3.355\n\n\n\n\n\nUtilizzando la funzione cfa() contenuta nel pacchetto lavaan, il modello ad un fattore viene definito nel modo seguente.\n\nmod_1 &lt;- \"\n  F =~ Y1 + Y2 + Y3 + Y4 + Y5\n\"\n\nOtteniamo così una stima dei pesi fattoriali e delle unicità.\n\nfit &lt;- lavaan::cfa(\n  mod_1,\n  sample.cov = SWLS,\n  sample.nobs = 215,\n  std.lv = TRUE\n)\n\nCalcoliamo la quantità di informazione fornita da ciascun item. Iniziamo a estrarre dall’oggetto fit la matrice delle saturazioni fattoriali.\n\nlambda &lt;- inspect(fit, what=\"std\")$lambda\nlambda\n\n\nA lavaan.matrix: 5 x 1 of type dbl\n\n\n\nF\n\n\n\n\nY1\n0.8166956\n\n\nY2\n0.6941397\n\n\nY3\n0.7257827\n\n\nY4\n0.5905795\n\n\nY5\n0.6429117\n\n\n\n\n\nEstraiamo da fit le specificità.\n\ntheta &lt;- diag(inspect(fit, what=\"std\")$theta)\ntheta\n\nY10.333008280668232Y20.518170080229262Y30.473239478247548Y40.651215859909304Y50.586664570302436\n\n\nPossiamo ora calcolare quantità di informazione degli item facendo il rapporto tra ciascuna saturazione fattoriale innalzata al quadrato e la corrispondente specificità.\n\nfor (i in 1:5) {\n  print(lambda[i]^2 / theta[i])\n}\n\n      Y1 \n2.002928 \n       Y2 \n0.9298683 \n      Y3 \n1.113095 \n       Y4 \n0.5355891 \n       Y5 \n0.7045515 \n\n\nIl risultato ottenuto indica che il quarto item è il meno informativo e che il quinto item è il secondo meno informativo. Se un solo item deve essere eliminato, dunque, elimineremo il quarto item. Se devono essere eliminati due item, andranno eliminati il quarto e il quinto item.\n\n35.3.1 Attendibilità e numero di item\nDi quanto cambia l’attendibilità di uno strumento se viene variato il numero di item? Una risposta a questa domanda può essere fornita dalla formula profetica di Spearman-Brown. Supponiamo che nella formula di Spearman-Brown,\n\\[\n\\begin{equation}\n  \\rho_p = \\frac{p \\rho_1}{(p-1)\\rho_1 + 1},\n\\end{equation}\n\\](eq-spearman-brown)\n\\(\\rho_1\\) rappresenti l’attendibilità di un test costituito da un certo numero di item. Se poniamo \\(p=2\\), la {eq}eq-spearman-brown ci fornisce una stima dell’attendibilità che si otterrebbe raddoppiando il numero di item nel test. Valori di \\(p\\) minori di \\(1\\), invece, vengono usati per predire la diminuizione dell’attendibilità conseguente ad una diminuzione nel numero degli item del test.\nRicordiamo però che le predizioni della formula di Spearman-Brown sono accurate solo se la forma allungata o accorciata del test è parallela rispetto al test considerato. Per esempio, se ad un test con un coefficiente di attendibilità molto alto vengono aggiunti item aventi una bassa attendibilità, allora l’attendibilità del test allungato sarà minore di quella predetta dalla formula di Spearman-Brown.\nAnche se la formula di Spearman-Brown ha un ruolo centrale nella teoria classica dei test, si tenga conto che non rappresenta l’unico strumento che può essere utilizzato per valutare la relazione tra attendibilità e numero degli item del test. La quantità detta informazione dell’item (item information), formulata dai modelli IRT, consente di predire i cambiamenti nella qualità della misura a seguito dell’aggiunta o della cancellazione di un sottoinsieme di item.\nEsempio. Si consideri la scala SWLS. Chiediamoci come varia l’attendibilità della scala se il numero di item aumenta da 5 a 20. Poniamo che l’attendibilità della scala SWLS costituita da 5 item sia uguale a 0.824. Applicando la formula di Spearman-Brown otteniamo la stima seguente.\n\n(4 * 0.824) / ((4 - 1) * 0.824 + 1)\n\n0.949308755760369\n\n\nEsempio. Possiamo giungere al risultato precedente in un altro modo. Supponiamo che i 15 item aggiuntivi abbiano le stesse saturazioni fattoriali medie (\\(\\bar{\\lambda}\\)) e le stesse varianze specifiche medie (\\(\\bar{\\psi}\\)) rispetto agli item originali. Mediante gli item di cui disponiamo, stimiamo l’attendibilità di un “item medio” nel modo seguente\n\\[\n\\rho_1 = \\frac{\\bar{\\lambda}^2}{\\bar{\\lambda}^2 + \\bar{\\psi}},\n\\]\novvero otteniamo la stima di 0.48:\n\nrho_1 &lt;- mean(lambda)^2 / (mean(lambda)^2 + mean(theta)) \nrho_1\n\n0.484512352433458\n\n\nL’attendibilità predetta di un test costituito da 20 item sarà dunque uguale a\n\n(20 * rho_1) / ((20 - 1) * rho_1 + 1) \n\n0.949490393106468\n\n\nil che replica il risultato ottenuto precedentemente.\nEsempio. Un altro modo ancora per ottenere lo stesso risultato è quello di utilizzare un modello mono-fattoriale per item paralleli.\n\nmod_2 &lt;- \"\n  F =~ a*Y1 + a*Y2 + a*Y3 + a*Y4 + a*Y5\n  Y1 ~~ b*Y1\n  Y2 ~~ b*Y2\n  Y3 ~~ b*Y3\n  Y4 ~~ b*Y4\n  Y5 ~~ b*Y5\n\"\n\nAdattiamo il modello ai dati.\n\nfit2 &lt;- lavaan::cfa(\n  mod_2,\n  sample.cov = SWLS,\n  sample.nobs = 215,\n  std.lv = TRUE\n)\n\nEstraiamo dall’oggetto fit2 le saturazioni fattoriali.\n\nlambda &lt;- inspect(fit2, what=\"std\")$lambda\nlambda\n\n\nA lavaan.matrix: 5 x 1 of type dbl\n\n\n\nF\n\n\n\n\nY1\n0.6893938\n\n\nY2\n0.6893938\n\n\nY3\n0.6893938\n\n\nY4\n0.6893938\n\n\nY5\n0.6893938\n\n\n\n\n\nEstraiamo da fit2 le specificità.\n\ntheta &lt;- diag(inspect(fit2, what=\"std\")$theta)\ntheta\n\nY10.524736147775294Y20.524736147775294Y30.524736147775294Y40.524736147775294Y50.524736147775294\n\n\nCalcoliamo l’attendibilità dell’item “medio” usando \\(\\lambda\\) e \\(\\psi\\) (chiamato theta da lavaan).\n\nrho_1 &lt;- lambda[1]^2 / (lambda[1]^2 + theta[2])\nrho_1 \n\nY2: 0.475263852224706\n\n\nPosso ora applicare la formula di Spearman-Brown.\n\n(20 * rho_1) / ((20 - 1) * rho_1 + 1) \n\nY2: 0.94768340402785\n\n\nIl risultato è praticamente identico a quelli trovati in precedenza.\n\n\n35.3.2 Numero di item e affidabilità\nLa formula di Spearman-Brown può anche essere riarrangiata in maniera tale da consentirci di predire il numero degli item necessari per raggiungere un determinato livello di affidabilità:\n\\[\n\\begin{equation}\np = \\frac{\\rho_p (1-\\rho_1)}{\\rho_1(1-\\rho_p)},\n\\end{equation}\n\\](eq-s-b-inv)\ndove \\(\\rho_1\\) è l’attendibilità stimata di un “item medio,” \\(\\rho_p\\) è il livello desiderato di attendibilità del test allungato e \\(p\\) è il numero di item del test allungato.\nEsempio. L’attendibilità della scala SWLS costituita da 5 item è \\(\\omega = 0.824\\). Quanti item devono essere aggiunti se si vuole raggiungere un livello di attendibilità pari a \\(0.95\\)?\nPonendo \\(\\rho_p = 0.95\\) e \\(\\rho_1= 0.479\\), in base alla {eq}eq-s-b-inv si ottiene che\n\n(.95 * (1 - rho_1)) / (rho_1 * (1 - .95))\n\nY2: 20.9777932006845\n\n\nil test dovrà essere costituito da 21 item.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Valutare e rifinire la soluzione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/05_val_soluzione.html#analisi-degli-item",
    "href": "chapters/extraction/05_val_soluzione.html#analisi-degli-item",
    "title": "35  Valutare e rifinire la soluzione fattoriale",
    "section": "35.4 Analisi degli item",
    "text": "35.4 Analisi degli item\nL’analisi degli item svolge un ruolo importante nello sviluppo e nella revisione dei test psicometrici. L’analisi degli item esamina le risposte fornite ai singoli item del questionario allo scopo di valutare la qualità degli item e del questionario nel suo complesso. Sotto al rubrica di analisi degli item possiamo raggruppare le procedure che possono essere utilizzate per descrivere la difficoltà degli item, le relazioni tra coppie di item, il punteggio totale del test, le relazioni tra gli item e il punteggio totale del test. Tali analisi statistiche vengono usate per la selezione degli item al fine di costruire un questionario omogeneo, attendibile e dotato di validità predittiva.\nLa selezione degli item di un test, però, non può essere svolta in maniera automatica usando soltanto criteri statistici quali quelli elencati sopra. La selezione degli item, invece, deve anche tenere includere considerazioni di ordine teorico basate sulla centralità degli item rispetto alla definizione del costrutto e considerazioni relative agli scopi della misurazione e al modo in cui l’item è stato formulato e costruito. Se alcuni aspetti di un costrutto non vengono rappresentanti da item che soddisfano i criteri statistici descritti sopra, o se c’è un numero insufficiente di item per produrre uno strumento attendibile, allora alcuni item dovranno essere riscritti. Nella riformulazione degli item, risultano utili le intuizioni che si sono guadagnate dalle analisi statistiche degli item che si sono dovuti scartare.\n\n35.4.1 Difficoltà degli item\nUna statistica comune da calcolare durante l’analisi degli item è la proporzione di esaminandi che rispondono correttamente ad ogni item. Questa è nota come difficoltà dell’item, p. La proporzione \\(p_j\\) di partecipanti che rispondono correttamente all’item \\(j\\)-esimo, o proporzione di partecipanti che si dichiarano in accordo con l’affermazione espressa dall’item, se il test non è di prestazione, fornisce una stima del livello di difficoltà \\(\\pi_j\\) dell’item.\nIn realtà, \\(p_j\\) dovrebbe essere chiamato “facilità dell’item” in quanto assume il suo valore maggiore (ovvero \\(1\\)) quando tutti i rispondenti rispondono correttamente all’item e il suo valore minimo (ovvero \\(0\\)) quando le risposte sono tutte sbagliate. Questo valore non va confuso con la difficoltà dell’item nella teoria della risposta agli item o con il valore-\\(p\\) dei test di ipotesi frequentisti.\nI valori \\(p_j\\) giocano un ruolo importante nelle procedure di selezione degli item. La difficoltà degli item deve essere interpretata in riferimento alla probabilità di indovinare la risposta corretta. Si suppone, infatti, che i rispondenti tirino ad indovinare quando non conoscono la risposta alla domanda di un questionario. Nel caso di item dicotomici, per esempio, ci possiamo aspettare un valore \\(p_j\\) pari a \\(0.50\\) sulla base del caso soltanto; nel caso di item a risposta multipla con quattro opzioni di scelta, invece, \\(p_j\\) assume un valore pari a \\(0.25\\) quando i rispondenti tirano ad indovinare.\nSe il test è composto per la maggior parte da item “facili”, allora il test non sarà in grado di discriminare tra rispondenti con diversi livelli di abilità, in quanto quasi tutti i rispondenti saranno in grado di fornire una risposta corretta alla maggioranza degli item. Lo stesso si può dire per un test composto da item “difficili”. Se il test è composto unicamente da item di difficoltà media, non potrà differenziare i rispondenti che hanno un grado di abilità media da quelli con abilità superiori alla media, dato che non ci sono item “difficili”, e neppure da quelli con abilità inferiori alla media, dato che non ci sono item “facili”.\nIn generale, dunque, è buona pratica costruire test composti da item che coprano tutti i livelli di difficoltà. La scelta che viene usualmente fatta è quella di una dispersione moderata e simmetrica del livello di difficoltà attorno ad un valore leggermente superiore al valore che sta a metà tra il livello del caso (\\(1.0\\) diviso per il numero di alternative) e il punteggio pieno (\\(1.0\\)).\nPer item che presentano cinque alternative di risposta, ad esempio, il livello del caso è pari a \\(1.0 / 5 = 0.20\\). Il livello ottimale di difficoltà è uguale a\n\\[\n0.20 + (1.0 - 0.20) / 2 = 0.60.\n\\]\nPer item dicotomici, il livello del caso è \\(1.0 / 2 = 0.50\\) e il livello ottimale di difficoltà è uguale a\n\\[\n0.50 + (1.00 - 0.50) / 2 = 0.75.\n\\]\nIn generale, item con livelli di difficoltà superiore a \\(0.90\\) o inferiore a \\(0.20\\) dovrebbero essere utilizzati con cautela.\nEsempio. Riporto qui sotto le proporzioni di risposte corrette (usando la correzione per il guessing) di 192 studenti di Psicometria nel primo parziale dell’AA 2021/2022. Il test aveva 16 item con 5 alternative di risposta ciascuno. Dunque la difficoltà media ottimale è pari a 0.6.\n\nitem_par_1 &lt;- c(\n  0.54255319, 0.76063830, 0.64361702, 0.65957447, 0.67021277, 0.12234043,\n  0.14361702, 0.18085106, 0.76063830, 0.82978723, 0.81914894, 0.84042553,\n  0.07978723, 0.07978723, 0.76063830, 0.79255319\n)\n\nNel compito, la difficoltà media è risultata essere un po’ inferiore.\n\nmean(item_par_1) %&gt;% \n  round(2)\n\n0.54\n\n\nLa distribuzione dei livelli di difficoltà degli item suggerisce che forse alcuni item “difficili” si sarebbero potuti sostituire con item di difficoltà media.\n\nplot(density(item_par_1))\n\n\n\n\n\n\n\n\n:::\nUn altro esempio riguarda il data set SAPA del pacchetto hemp. Per questi dati possiamo utilizzare la funzione colMeans per calcolare la difficoltà degli item. Poiché abbiamo dei partecipanti che hanno risposte mancanti su alcuni item, dobbiamo passare l’argomento na.rm = TRUE per ignorare i dati mancanti. In caso contrario, la funzione colMeans restituirebbe NA per gli item che hanno almeno un valore mancante. Per rendere più leggibili i valori di difficoltà degli item, arrotondiamo a tre decimali utilizzando la funzione round.\n\nitem_diff &lt;- colMeans(SAPA, na.rm = TRUE)\nround(item_diff, 3)\n\nreason.40.64reason.160.698reason.170.697reason.190.615letter.70.6letter.330.571letter.340.613letter.580.444matrix.450.526matrix.460.55matrix.470.614matrix.550.374rotate.30.194rotate.40.213rotate.60.299rotate.80.185\n\n\nL’output mostra che gli item reason.16 e reason.17 ottengono i livelli di difficoltà più alti, mentre rotate.8 ha il livello di difficoltà più basso. Circa il 70% degli studenti è stato in grado di rispondere correttamente a reason.16 e reason.17, mentre solo il 19% ha risposto correttamente a rotate.8.\n\n\n35.4.2 Correzione per guessing\nAlle volte i valori \\(p_j\\) sono calcolati introducendo una correzione per le risposte fornite casualmente dai soggetti (guessing). Si consideri un test a scelta multipla composto da item aventi ciascuno \\(C\\) alternative di risposta ed una sola risposta corretta. Si supponga che un rispondente risponda correttamente a \\(R\\) item e risponda in maniera sbagliata a \\(W\\) item.\nLa correzione per guessing si ottiene applicando una formula basata sul seguente ragionamento. Se assumiamo che un rispondente si limita a tirare ad indovinare allora, ogni \\(C\\) risposte, ci aspettiamo 1 risposta giusta e \\(C-1\\) risposte sbagliate. Per calcolare il punteggio totale del test in modo da eliminare il numero di risposte corrette ottenute tirando ad indovinare è necessario sottrarre 1 punto per ogni \\(C-1\\) item a cui è stata fornita una risposta corretta. Questo ragionamento conduce alla seguente formula:\n\\[\n\\begin{equation}\nFS = R - \\frac{W}{C - 1},\n\\end{equation}\n\\](eq-guessing)\ncon \\(R\\) = # risposte corrette, \\(W\\) = # risposte sbagliate, \\(C\\) = # alternative di risposta. Per esempio, se \\(C=5\\), allora è necessario sottrarre un punto ogni 4, il che è proprio quello che fa la {eq}eq-guessing.\nLa {eq}eq-guessing produce un punteggio totale corretto per il guessing identico a quello che si otterrebbe assegnando 1 punto a ciascuna risposta corretta e assegnando \\(- \\frac{1}{C-1}\\) punti alle risposte sbagliate; le risposte non date non vengono considerate.\nLa correzione per guessing rappresenta il tentativo di scomporre il numero totale di risposte corrette in due componenti: le risposte corrette dovute alle conoscenze del soggetto, le risposte che risultano corrette come effetto del caso. La stessa formula può anche essere utilizzata per calcolare la difficoltà degli item corretta per il guessing (come è stato fatto nell’esempio del parziale di Psicometria).\n\n\n35.4.3 Discriminatività\nLa discriminatività è una misura di quanto ogni item è in grado di distinguere i soggetti con elevati livelli nel costrutto da quelli con un livello basso. L’indice di discriminatività \\(D\\) per i test di prestazione massima si trova nel modo seguente. Dopo avere calcolato il punteggio totale al test, si dividono i soggetti in due gruppi: soggetti con basso punteggio e soggetti con alto punteggio. Una volta definiti i due gruppi, l’indice di discriminatività \\(D\\) sarà dato da:\n\\[D = P(\\text{alto}) - P(\\text{basso}),\\]\ndove \\(P(\\text{alto}\\) è la proporzione di soggetti che ha risposto correttamente all’item nel gruppo con punteggi alti e \\(P(\\text{basso}\\) è la proporzione di soggetti che ha risposto correttamente all’item nel gruppo con punteggi bassi. Il valore di \\(D\\) può variare da -1 a +1. Nella tabella seguente sono fornite le linee guida per l’interpretazione di questo indice (Ebel, 1965).\n\n\n\nValore di \\(D\\)\nCommento\n\n\n\n\n\\(D \\geq 0.40\\)\nOttima, nessuna revisione\n\n\n\\(0.30 \\leq D &lt; 0.40\\)\nBuona, revisioni minime\n\n\n\\(0.20 \\leq D &lt; 0.30\\)\nSufficiente, revisioni parziali\n\n\n\\(D &lt; 0.20\\)\nInsufficiente, riformulazione o eliminazione\n\n\n\nLa discriminatività degli item di tipo Likert viene valutata con la medesima procedura degli item dei testi di prestazione massima, anche se cambiano le procedure statistiche da utilizzare. Si può dividere la distribuzione dei punteggi totali (o punteggi medi) in quartili e confrontare il punteggio medio o mediano del quartile superiore con quello del quartile inferiore, oppure, se il test è orientato al criterio e lo scopo è selezionare gli item che discriminano meglio due gruppi precostituiti di soggetto, eseguire i medesimi confronti tra il gruppo target (ad esempio, pazienti) e quello “di controllo” (per esempio, popolazione generale).\nÈ consigliabile valutare la dimensione dell’effetto, ad esempio attraverso l’indice \\(d\\) di Cohen. La dimensione dell’effetto dovrebbe essere almeno moderata (\\(d &gt; |0.50|\\)).\nEsempio. Per il primo parziale di Psicometria AA 2021/2022, l’indice \\(d\\) di Cohen calcolato sulla proporzione di risposte corrette per il gruppo di studenti con i punteggi più bassi (primo quartile) e il gruppo di studenti con i punteggi più alti (ultimo quartile) è stato di 4.76, 95% CI [4.0, 5.51]. L’indice complessivo di discriminatività sembra dunque adeguato. Sarebbe però necessario calcolare questo indice item per item.\n\n\n35.4.4 Potere discriminante dell’item e analisi fattoriale\nUn’altra statistica ampiamente utilizzata nell’analisi degli item è il potere discriminante degli item, che si riferisce alla capacità dell’item nel distinguere gli esaminandi con una alta abilità da quelli con una bassa abilità. Sebbene esistano molti modi per calcolare la discriminazione degli item, la forma più comune è la correlazione punto-biseriale tra le risposte degli esaminandi all’item e il loro punteggio totale nel test. Valori grandi e positivi indicano una forte relazione tra il rispondere correttamente all’item e avere un punteggio alto nel test, mentre valori vicini allo zero indicano nessuna relazione e valori negativi indicano che il rispondere correttamente all’item è associato a un punteggio complessivo del test più basso. Valori vicini allo zero o negativi suggeriscono che l’item potrebbe non funzionare correttamente. Alcune delle ragioni per ottenere una discriminazione degli item bassa o negativa potrebbero essere l’utilizzo di una chiave di risposta errata per l’item o l’assenza di risposte corrette. Indipendentemente dalla causa, gli item con correlazioni punto-biseriale basse o negative devono essere modificati, se il test/strumento è in fase di revisione, o rimossi dal test e dal punteggio.\nPer calcolare il potere discriminante dell’item per i dati SAPA, prima calcoliamo il punteggio totale del test utilizzando la funzione rowSums insieme all’opzione na.rm = TRUE e lo salviamo come total_score. Successivamente, correlaziamo gli item in SAPA con il punteggio totale del test utilizzando la funzione cor. Specificamente, usiamo l’argomento use = \"pairwise.complete.obs\" nella funzione cor a causa della presenza di risposte mancanti. Infine, salviamo la matrice di correlazione come item_discr e la stampiamo.\n\ntotal_score &lt;- rowSums(SAPA, na.rm = TRUE)\nitem_discr &lt;- cor(SAPA, total_score, use = \"pairwise.complete.obs\")\nround(item_discr, 2)\n\n\nA matrix: 16 x 1 of type dbl\n\n\nreason.4\n0.59\n\n\nreason.16\n0.53\n\n\nreason.17\n0.59\n\n\nreason.19\n0.56\n\n\nletter.7\n0.58\n\n\nletter.33\n0.56\n\n\nletter.34\n0.59\n\n\nletter.58\n0.58\n\n\nmatrix.45\n0.51\n\n\nmatrix.46\n0.51\n\n\nmatrix.47\n0.55\n\n\nmatrix.55\n0.45\n\n\nrotate.3\n0.51\n\n\nrotate.4\n0.56\n\n\nrotate.6\n0.55\n\n\nrotate.8\n0.48\n\n\n\n\n\nI risultati mostrano che tutti gli item del test SAPA sono moderatamente e positivamente correlati con il punteggio totale del test. Questo indica che tutti gli item funzionano correttamente e non fornisce informazioni salienti su quali item rimuovere o modificare.\nUn altro modo per calcolare il potere discriminante degli item consiste nel dividere i candidati in due gruppi (ad esempio, 1 = alto rendimento e 0 = basso rendimento) in base ai loro punteggi totali nel test e correlare questa variabile di raggruppamento con le risposte agli item. Questo è noto come indice di discriminazione degli item. Un’opzione per creare gruppi di alto e basso rendimento è selezionare il 25% più alto e il 25% più basso dei candidati in base ai loro punteggi totali nel test. Va notato che la decisione di utilizzare il 25% è arbitraria. Potremmo utilizzare un altro valore (ad esempio, il 10% o il 20%) per definire i gruppi di alto e basso rendimento. Dopo aver definito il punto di cut-off per i gruppi, calcoliamo la proporzione di candidati che hanno risposto correttamente all’elemento nei gruppi di alto e basso rendimento.\nNell’esempio seguente, calcoliamo l’indice di discriminazione dell’elemento reason.4 nel set di dati SAPA utilizzando la funzione idi del pacchetto hemp. Per specificare i gruppi di alto e basso rendimento, utilizziamo il valore perc_cut = .25 nella funzione idi.\n\nidi(SAPA, SAPA$reason.4, perc_cut = .25)\n\nUpper 25%0.805135951661631Lower 25%0.194864048338369\n\n\nAbbiamo scoperto che l’81% dei candidati nel gruppo di alto rendimento ha risposto correttamente all’item reason.4, mentre solo il 19% dei candidati nel gruppo di basso rendimento ha risposto correttamente. Questo suggerisce che l’item era più facile per i candidati di alto rendimento e più difficile per quelli di basso rendimento. Pertanto, possiamo dire che questo particolare item risulta utile per differenziare i due gruppi, ma non necessariamente all’interno di ciascun gruppo.\nSecondo McDondald (1999), la nozione di potere discriminante dell’item può essere trattata in maniera più precisa nell’ambito del modello monofattoriale. Se l’insieme di item a disposizione non è eccessivamente grande (200 o meno), infatti, è possibile procedere alla selezione degli item migliori tramite l’analisi fattoriale – ovvero, scegliendo gli item con le saturazioni maggiori.\n\n\n35.4.5 Punteggio sull’item e punteggio totale\nIl grado di associazione tra il punteggio sull’item e il punteggio totale viene considerato dalla teoria classica dei test come un indice che descrive il potere discriminante dell’item. Se il test fornisce una misura attendibile di un unico attributo, e se un item è fortemente associato al punteggio del test, allora l’item sarà in grado di distinguere tra rispondenti che ottengono un punteggio basso nel test e rispondenti che ottengono un punteggio alto nel test.\nNel caso di una forte associazione positiva tra il punteggio sull’item e il punteggio totale, la probabilità di risposta corretta sull’item è alta per rispondenti che ottengono un punteggio totale alto, e bassa per i rispondenti che ottengono un punteggio totale basso. Nel caso di una debole associazione tra il punteggio sull’item e il punteggio totale, invece, la probabilità di risposta corretta all’item non è predittiva del punteggio totale. Gli item con un basso potere discriminante dovrebbero dunque essere rimossi dal reattivo.\nÈ necessario distinguere i casi in cui gli item sono dicotomici dal caso di item continui. Nel caso di item dicotomici e di un test unidimensionale, il potere discriminante viene calcolato mediante la correlazione biseriale o punto-biseriale.\n\n\n35.4.6 Relazioni tra coppie di item\nLe relazioni tra coppie di item sono importanti sia per la costruzione sia per la validazione dei test psicometrici. La teoria classica dei test definisce l’attendibilità di un test (o di un item) come il rapporto tra la varianza del punteggio vero e la varianza del punteggio osservato. Il coefficiente di attendibilità può però essere calcolato anche trovando la correlazione tra due forme parallele di un test (o tra due item). Inoltre, è possibile interpretare la correlazione tra due forme parallele di un test (o tra due item) come il quadrato del coefficiente di correlazione tra i punteggi osservati e i punteggi veri di un test (o di un item).\nMolti indici sono disponibili per misurare il grado di associazione tra item. Per item quantitativi, possiamo usare la correlazione di Pearson o la covarianza. Per item qualitativi politomici ordinali, usiamo la correlazione policorica. Per item ordinali dicotomici, usiamo la correlazione tetracorica. Per item dicotomici usiamo, ad esempio, l’indice \\(\\phi\\).\n\n\n35.4.7 Ridondanza\nNel processo di raffinamento del test occorre anche tenere conto degli item ridondanti, ossia degli item che sono troppo associati tra loro. La ridondanza può essere valutata con indici statistici quali la correlazione: se due o più item hanno tra loro una correlazione maggiore di \\(|0.70|\\) viene mantenuto nell’item pool solo uno di essi, dato che gli altri item forniscono la stessa informazione.\n\n\n35.4.8 Massimizzazione della varianza del punteggio totale\nUno dei criteri che possono essere utilizzati per la selezionare degli item che andranno a costituire la versione finale di un test è la massimizzazione della varianza del punteggio totale. Più in particolare, si vuole massimizzare il rapporto tra la varianza del punteggio totale e la somma delle varianze dei punteggi dei \\(p\\) item. Dato che il coefficiente \\(\\alpha\\) di Cronbach ha la seguente forma:\n\\[\\alpha = \\frac{p}{p-1}\\left[1- \\frac{\\sum \\sigma^2_{Y_i}}{\\sigma^2_T} \\right],\\]\nla scelta di massimizzare il rapporto definito in precedenza avrà anche la conseguenza di massimizzare \\(\\alpha\\).\n{cite:t}mcdonald2013test fa notare che una procedura di selezione degli item basata sul principio della massimizzazione di \\(\\alpha\\) ha però dei limiti. In primo luogo, tale procedura è appropriata solo quando l’insieme di item è troppo grande per selezionare gli item in base all’esame delle saturazioni fattoriali ottenute applicando il modello mono-fattoriale. In secondo luogo, {cite:t}mcdonald2013test nota che la procedura di selezione basata sulla massimizzazione di \\(\\alpha\\) è adeguata solo nel caso di una struttura mono-fattoriale. La selezione degli item basata sulla massimizzazione di \\(\\alpha\\) deve dunque essere accompagnata da considerazione relative al contenuto e alla struttura del costrutto.\n\n\n35.4.9 Indice di affidabilità dell’item\nOltre agli indici di difficoltà e discriminazione degli item, un’altra statistica utile per l’analisi degli item è l’indice di affidabilità dell’item. L’indice di affidabilità dell’item (IRI) è definito come:\n\\[\nIRI = S_i \\cdot r_{i,tt},\n\\]\ndove \\(S_i\\) è la deviazione standard dell’item \\(i\\) e \\(r_{i,tt}\\) è la correlazione tra l’item \\(i\\) e il punteggio totale del test. L’IRI può teoricamente variare tra -0.5 e 0.5, con valori grandi e positivi indicativi di alta affidabilità.\nDi seguito calcoliamo l’IRI per tutti gli item nel set di dati SAPA. Possiamo farlo utilizzando la funzione iri in hemp.\n\niri(SAPA)\n\n\nA matrix: 16 x 1 of type dbl\n\n\nreason.4\n0.2820989\n\n\nreason.16\n0.2451971\n\n\nreason.17\n0.2692675\n\n\nreason.19\n0.2717135\n\n\nletter.7\n0.2865325\n\n\nletter.33\n0.2757209\n\n\nletter.34\n0.2897118\n\n\nletter.58\n0.2863221\n\n\nmatrix.45\n0.2544930\n\n\nmatrix.46\n0.2562540\n\n\nmatrix.47\n0.2668171\n\n\nmatrix.55\n0.2161230\n\n\nrotate.3\n0.2016459\n\n\nrotate.4\n0.2276081\n\n\nrotate.6\n0.2539219\n\n\nrotate.8\n0.1867207\n\n\n\n\n\nI risultati restituiti dalla funzione iri mostrano che l’IRI varia da circa 0.19 a 0.29 per il set di dati SAPA. Tutti questi sono valori ragionevoli per l’IRI (ovvero nessuno è negativo o vicino allo zero).\n\n\n35.4.10 Indice di validità dell’item\nQuando invece del punteggio totale del test viene utilizzato un criterio esterno, questo indice è noto come indice di validità dell’item (IVI). L’IVI può variare anche tra -0.5 e 0.5, con valori elevati (in valore assoluto) che indicano una validità maggiore. Valori negativi elevati indicano una maggiore validità quando ci si aspetta che gli elementi siano correlati in modo negativo con il criterio.\nNell’esempio seguente, utilizziamo la funzione ivi in hemp con “reason.17” come criterio esterno e “reason.4” come elemento di interesse e troviamo che l’IVI è 0.19.\n\nivi(item = SAPA$reason.4, crit = SAPA$reason.17)\n\n0.190321881267833\n\n\n\n\n35.4.11 Distrattori\nUn altro aspetto importante degli elementi che deve essere analizzato sono le opzioni di risposta. Nel contesto dei test a scelta multipla, le opzioni di risposta alternative (cioè sbagliate) vengono definite “distrattori”. I distrattori svolgono un ruolo importante in un elemento a scelta multipla. Per garantire elementi a scelta multipla di alta qualità, è cruciale includere distrattori plausibili e ben funzionanti che siano più probabili di attirare i candidati con conoscenze parziali. I distrattori non plausibili potrebbero dover essere riscritti o sostituiti con un distrattore migliore. La qualità dei distrattori viene tipicamente valutata attraverso l’analisi dei distrattori. L’analisi dei distrattori viene spesso condotta osservando la proporzione di candidati che scelgono un distrattore particolare.\nPer illustrare l’analisi dei distrattori, utilizziamo gli item del data set multiplechoice in hemp. Si tratta di un ipotetico test a scelta multipla composto da 27 item somministrati a 496 candidati. Le quattro opzioni di risposta sono codificate come 1, 2, 3 e 4 nel data set. Utilizziamo la funzione distract in hemp per calcolare la proporzione di candidati che selezionano ciascun distrattore.\n\ndistractors &lt;- distract(multiplechoice)\nhead(distractors)\n\n\nA matrix: 6 x 4 of type dbl\n\n\n\n1\n2\n3\n4\n\n\n\n\nitem1\n0.044\n0.058\n0.052\n0.845\n\n\nitem2\n0.109\n0.069\n0.792\n0.030\n\n\nitem3\n0.188\n0.562\n0.058\n0.192\n\n\nitem4\n0.034\n0.125\n0.742\n0.099\n\n\nitem5\n0.351\n0.254\n0.042\n0.353\n\n\nitem6\n0.081\n0.198\n0.558\n0.163\n\n\n\n\n\nNella tabella sopra, vediamo che molti item avevano distrattori selezionati circa il 5% delle volte o meno. Questi distrattori potrebbero essere candidati per una revisione in quanto sono stati approvati ad un livello così basso da suggerire che la maggior parte degli esaminandi non li ha considerati come opzioni plausibili. Per l’item 1, i distrattori funzionavano tutti più o meno allo stesso modo (ovvero circa il 5% delle volte ogniuno è stato approvato), suggerendo che funzionavano tutti bene rispetto l’uno all’altro, ma che l’item era troppo facile (la risposta corretta era l’opzione 4, selezionata dall’84.5% degli esaminandi). Al contrario, l’item 5 era un item più difficile, con la risposta corretta che ancora una volta era l’opzione 4. Le opzioni 1 e 2 erano molto probabilmente fraintendimenti, mentre l’opzione 3 potrebbe essere rivista o potenzialmente eliminata da questo item a causa del basso tasso di approvazione (solo il 4.2%). Dato l’approvazione molto alta dell’opzione 1 (35.1%), è molto probabile che anche questa opzione fosse corretta. Per ottenere una visione più completa del funzionamento dell’item, sarebbe consigliabile calcolare l’indice di discriminazione specifico per quell’item. Questo ci permetterebbe di ottenere ulteriori informazioni sulla capacità dell’item di distinguere tra candidati di alto e basso livello.",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Valutare e rifinire la soluzione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/extraction/05_val_soluzione.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/extraction/05_val_soluzione.html#informazioni-sullambiente-di-sviluppo",
    "title": "35  Valutare e rifinire la soluzione fattoriale",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()",
    "crumbs": [
      "Costruzione",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Valutare e rifinire la soluzione fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html",
    "href": "chapters/cfa/01_cfa.html",
    "title": "36  Analisti Fattoriale Confermativa",
    "section": "",
    "text": "36.1 Introduzione\nIn questo capitolo esamineremo la CFA per l’analisi dei modelli di misurazione con fattori comuni e indicatori continui. A differenza dell’analisi fattoriale esplorativa (EFA), nella CFA vengono analizzati modelli di misurazione vincolati. Ciò significa che il ricercatore specifica (1) il numero esatto di fattori; (2) il pattern dei carichi fattoriali, ossia la corrispondenza specifica tra i fattori e gli indicatori; e (3) la presenza di errori correlati, se presenti.\nLa seconda caratteristica menzionata sopra implica che un indicatore satura solo sui fattori specificati dal ricercatore, e tutte le saturazioni incrociate di quell’indicatore su altri fattori sono fissate a zero. Sebbene sia possibile specificare un numero esatto di fattori nella EFA, la tecnica analizza modelli di misurazione non restrittivi, in cui ciascun indicatore satura su tutti i fattori (ossia tutte le saturazioni incrociate sono liberamente stimate).\nUn’altra differenza è che i modelli EFA con più fattori sono identificati solo dopo aver specificato un metodo di rotazione dei fattori, come obliqua (i fattori possono covariare) oppure ortogonale (i fattori sono non correlati). Poiché la CFA richiede un modello identificato, non c’è una fase di rotazione e di solito è permesso ai fattori di covariare.\nNell’ambito dei requisiti per l’identificazione, è possibile stimare errori correlati nella CFA, ma è più difficile ottenere questo risultato nella EFA. Pertanto, la tecnica della CFA supporta meglio l’analisi delle strutture di covarianza degli errori rispetto alla EFA.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#limitazioni-dellapproccio-fattoriale",
    "href": "chapters/cfa/01_cfa.html#limitazioni-dellapproccio-fattoriale",
    "title": "36  Analisti Fattoriale Confermativa",
    "section": "36.2 Limitazioni dell’approccio fattoriale",
    "text": "36.2 Limitazioni dell’approccio fattoriale\nL’approccio classico dell’analisi fattoriale (EFA più rotazione fattoriale) ha rivelato avere diversi limiti. Nella ricerca iniziale, dibattiti teorici importanti, come il numero di fattori dell’intelligenza o della personalità, erano basati sui risultati di diverse rotazioni fattoriali. Questi dibattiti si sono rivelati essere semplici speculazioni, poiché conclusioni diverse potevano essere supportate a seconda dell’interpretazione dei dati. Per esempio, il dibattito tra Eysenck e Cattell sul numero di fattori della personalità (due o sedici) dipendeva dall’uso di rotazioni ortogonali o oblique sugli stessi dati.\nNella seconda metà del XX secolo, c’era una generale insoddisfazione verso l’analisi fattoriale a causa della sua apparente capacità di adattarsi a quasi qualsiasi soluzione. Furono raccomandati criteri rigorosi per il suo uso, come la necessità di grandi campioni, che spesso rendevano l’analisi impraticabile a quei tempi. Inoltre, furono introdotti vincoli relativi alle ipotesi del modello e al requisito che le variabili nella matrice di correlazione avessero varianze equivalenti, creando problemi pratici significativi, specialmente con dati binari spesso usati nei test psicometrici.\nSolo con l’introduzione di metodi psicometrici moderni, come l’analisi fattoriale confermativa (CFA) discussa in questo capitolo e la Teoria di Risposta all’Item (discussa in una sezione successiva), questi problemi sono stati risolti.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#efa-vs.-cfa-confronto-tra-analisi-fattoriale-esplorativa-e-confermativa",
    "href": "chapters/cfa/01_cfa.html#efa-vs.-cfa-confronto-tra-analisi-fattoriale-esplorativa-e-confermativa",
    "title": "36  Analisti Fattoriale Confermativa",
    "section": "36.3 EFA vs. CFA: Confronto tra Analisi Fattoriale Esplorativa e Confermativa",
    "text": "36.3 EFA vs. CFA: Confronto tra Analisi Fattoriale Esplorativa e Confermativa\n\n36.3.1 Fondamenti Comuni e Differenze\nSia l’Analisi Fattoriale Esplorativa (EFA) che quella Confermativa (CFA) si basano sul modello dei fattori comuni. Entrambe le tecniche presuppongono che la varianza degli indicatori osservati possa essere suddivisa in varianza comune e varianza unica. La varianza comune è quella condivisa tra gli indicatori e sottende le covarianze osservate, mentre la varianza unica comprende sia la varianza specifica delle variabili che l’errore di misurazione. I fattori estratti, detti fattori comuni, rappresentano le variabili latenti costruite da questa varianza comune.\nNell’EFA, la struttura dei fattori è indeterminata e viene esplorata senza ipotesi a priori riguardo al numero o alla natura dei fattori. L’EFA è quindi particolarmente utile nelle fasi iniziali di ricerca, quando la teoria è poco sviluppata o si sospetta la presenza di fattori inaspettati.\nAl contrario, la CFA si basa su un modello di fattori predefinito, che specifica a priori quali indicatori sono associati a ciascun fattore, rendendola idonea per confermare teorie esistenti o per validare strutture fattoriali precedentemente esplorate. In CFA, i carichi incrociati (indicatori che caricano su più di un fattore) sono generalmente vincolati a zero, stabilendo una relazione diretta e specifica tra fattori e indicatori.\n\n\n36.3.2 Indeterminatezza Fattoriale\nUn problema ricorrente in entrambe le tecniche è l’indeterminatezza fattoriale, dove i fattori comuni non possono essere definiti in modo univoco dai loro indicatori a causa della natura approssimativa delle stime. Questo si manifesta sia in EFA, con l’indeterminatezza della rotazione, che in CFA, dove l’analisi potrebbe non replicarsi in nuovi campioni a causa dell’uso dei medesimi dati per verificare il modello.\n\n\n36.3.3 Indeterminatezza dei Punteggi Fattoriali\nUn’altra complicazione è l’indeterminatezza dei punteggi fattoriali, che si verifica quando esistono infinite soluzioni valide per i punteggi fattoriali a partire dagli indicatori. Questo comporta che diversi metodi possono produrre ordinamenti differenti dei casi, un problema noto come indeterminatezza dei punteggi fattoriali (Grice, 2001).\n\n\n36.3.4 Rotazione e Specificazione del Modello\nLa EFA può presentare ambiguità a causa dell’infinita quantità di configurazioni dei carichi fattoriali che potrebbero adattarsi ai dati, un fenomeno meno pronunciato nella CFA dove la specifica del modello è più rigida.\n\n\n36.3.5 Applicazioni Pratiche\nL’EFA è spesso preferita in nuovi ambiti di ricerca, dove i fattori potrebbero non essere ben definiti, mentre la CFA è utilizzata per confermare le strutture fattoriali in studi di validazione o in seguito a revisioni di test esistenti.\n\n\n36.3.6 Problemi con l’Uso Combinato di EFA e CFA\nSi noti che l’applicazione della CFA immediatamente dopo la EFA nello stesso campione può essere problematica. Talvolta, l’uso congiunto non verifica né conferma i risultati dell’EFA. La restrittività dei modelli CFA, con i carichi incrociati impostati a zero, può portare a risultati che non sono coerenti con i dati analizzati nell’EFA.\n\n\n36.3.7 Nuove Approcci Intermedi\nMetodi come l’Analisi Strutturale Esplorativa (ESEM) offrono un approccio ibrido che combina la flessibilità dell’EFA con alcuni degli aspetti confirmatori della CFA. Questo permette una maggiore precisione nel testare l’adattamento del modello, pur mantenendo la capacità di esplorare nuove strutture fattoriali.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#raccomandazioni-per-la-selezione-degli-indicatori-nellanalisi-fattoriale",
    "href": "chapters/cfa/01_cfa.html#raccomandazioni-per-la-selezione-degli-indicatori-nellanalisi-fattoriale",
    "title": "36  Analisti Fattoriale Confermativa",
    "section": "36.4 Raccomandazioni per la Selezione degli Indicatori nell’Analisi Fattoriale",
    "text": "36.4 Raccomandazioni per la Selezione degli Indicatori nell’Analisi Fattoriale\nLa selezione accurata degli indicatori è cruciale per il successo dell’analisi fattoriale, sia essa Esplorativa (EFA) o Confermativa (CFA). Le linee guida suggerite da Fabrigar e Wegener (2012) e Little et al. (1999), come riassunto in Kline (2023), enfatizzano i seguenti punti chiave:\n\nDefinizione dei Concetti Teorici: È essenziale articolare i concetti teorici in modo dettagliato per delineare chiaramente ogni dominio di interesse. Ad esempio, se lo studio riguarda le dimensioni dell’ansia, è importante riferirsi a letteratura teorica ed empirica che discute vari aspetti come ansia di stato, ansia di tratto e ansia sociale.\nScelta degli Indicatori: Gli indicatori selezionati dovrebbero coprire adeguatamente i domini d’interesse senza affidarsi esclusivamente allo stesso metodo di misurazione, come i questionari di autovalutazione, per ridurre la varianza dovuta a metodi comuni. L’impiego di modelli CFA specializzati può aiutare a stimare questi effetti del metodo.\nGuida Teorica o Empirica Forte: Se esiste una solida base teorica o empirica, gli indicatori omogenei sono preferibili poiché forniscono stime più precise e meno distorte, specialmente in analisi di tipo più confermativo.\nAnalisi di Indicatori Meno Omogenei: Se la guida teorica è debole, può essere vantaggioso esaminare un insieme di indicatori meno omogenei che coprono un’ampia gamma del dominio d’interesse. Ciò evita di basarsi su approssimazioni che potrebbero non riflettere pienamente i concetti chiave.\nUso di Indicatori di Qualità Psicometrica Inferiore: Anche gli indicatori con minore qualità psicometrica possono essere utili se coprono ampiamente il costrutto, generano punteggi che riflettono ampie differenze individuali e sono analizzati attraverso metodi più confermativi.\nProblemi Tecnici: L’analisi potrebbe incontrare problemi come i casi Heywood o la mancata convergenza se alcuni fattori hanno un numero insufficiente di indicatori, specialmente in campioni piccoli. Un numero sicuro minimo di indicatori per ogni fattore previsto è di circa 3-5. Tuttavia, in alcuni casi, potrebbe essere vantaggioso utilizzare meno indicatori per fattore se questi sono psicometricamente solidi.\n\nHayduk e Littvay (2012) hanno sottolineato che non è sempre preferibile avere più indicatori per fattore; in certi contesti, un singolo indicatore ben scelto può essere sufficiente. Se gli indicatori sono altamente ridondanti, non aggiungono informazioni significative. L’idea di una “regola d’oro” di 3-5 indicatori per fattore è una guida generale, ma la scelta dovrebbe essere basata sulle ipotesi specifiche di ricerca piuttosto che su una regola arbitraria.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#fondamenti-dei-modelli-di-base-nella-cfa",
    "href": "chapters/cfa/01_cfa.html#fondamenti-dei-modelli-di-base-nella-cfa",
    "title": "36  Analisti Fattoriale Confermativa",
    "section": "36.5 Fondamenti dei Modelli di Base nella CFA",
    "text": "36.5 Fondamenti dei Modelli di Base nella CFA\nI modelli di base nella Confermative Factor Analysis (CFA) con più fattori sono caratterizzati da specifiche fondamentali che garantiscono una misurazione precisa delle variabili latenti. Ecco una sintesi delle caratteristiche principali di tali modelli:\n\nRelazione tra Indicatori e Fattori: Ogni indicatore è una variabile continua influenzata da due principali componenti: un fattore comune, che rappresenta la variabile latente che l’indicatore è inteso a misurare, e la varianza unica. Quest’ultima include sia l’errore di misurazione casuale sia la varianza specifica non spiegata dal fattore, entrambi rappresentati dal termine di errore.\nIndipendenza dei Termini di Errore: I termini di errore sono assunti come indipendenti l’uno dall’altro e dai fattori. Ciò implica l’assenza di confondenti non misurati per qualsiasi coppia di indicatori e l’indipendenza delle cause omesse dai fattori.\nLinearità e Covarianza: Le relazioni all’interno del modello sono lineari e i fattori possono covariare, il che significa che non esistono effetti causali diretti tra i fattori.\n\nQueste caratteristiche definiscono la misurazione unidimensionale, sottolineando che ciascun indicatore è pensato per misurare una sola dimensione e non condivide varianza con altri indicatori una volta controllati i fattori comuni. Tuttavia, è anche possibile specificare modelli CFA multidimensionali, dove alcuni indicatori possono caricare su più di un fattore o dove coppie di termini di errore possono essere correlati.\nInoltre, esistono metodi specializzati per analizzare relazioni non lineari tra fattori e indicatori continui, o tra i fattori stessi, come descritto da Amemiya e Yalcin (2001). Le relazioni tra indicatori categorici e fattori sono intrinsecamente non lineari, e questi scenari sono trattati nel CFA categorico.\nUn esempio di modello CFA di base con due fattori e sei indicatori viene presentato di seguito, dove tutti i carichi incrociati sono fissati a zero, Figura 36.1. Per esempio, il fattore B non ha un effetto causale diretto sull’indicatore X1, il quale è misurato da un altro fattore (A). Tuttavia, ciò non significa che X1 e il fattore B siano completamente scorrelati. La struttura del modello permette a X1 di covariare con B poiché B è correlato con A, che è una causa di X1 (l’altra causa è E1, il termine di errore di X1). In modo simile, si prevede che gli indicatori X1 e X4 covarino poiché sono influenzati dai fattori A e B, rispettivamente, i quali sono correlati.\nLe costanti di scala, indicate come (1) nel modello, definiscono le metriche per le variabili non misurate, inclusi i fattori comuni e i termini di errore degli indicatori, stabilendo così una base uniforme per la misurazione nel modello CFA.\n\n\n\n\n\n\nFigura 36.1: Modello di analisi fattoriali confermativa con due fattori comuni e sei indicatori. (Figura tratta da Kline (2023))",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#scalatura-dei-fattori-e-inclusione-delle-covariate-nei-modelli-cfa-di-base",
    "href": "chapters/cfa/01_cfa.html#scalatura-dei-fattori-e-inclusione-delle-covariate-nei-modelli-cfa-di-base",
    "title": "36  Analisti Fattoriale Confermativa",
    "section": "36.6 Scalatura dei Fattori e Inclusione delle Covariate nei Modelli CFA di Base",
    "text": "36.6 Scalatura dei Fattori e Inclusione delle Covariate nei Modelli CFA di Base\nNella rappresentazione di base dei modelli Confermative Factor Analysis (CFA), la scalatura dei fattori viene spesso eseguita utilizzando il metodo della variabile di riferimento, conosciuto anche come metodo della variabile marker o approccio di identificazione del carico di riferimento (Newsom, 2015). In questo approccio, un vincolo di Unit Loading Identification (ULI) è applicato al carico di un indicatore per ciascun fattore. Per esempio, nel modello illustrato, il carico di \\(X1\\) sul fattore \\(A\\) è fissato a 1.0, stabilendo così la varianza del fattore \\(A\\) sulla base della varianza comune dell’indicatore \\(X1\\), che funge da variabile di riferimento per \\(A\\). Analogamente, la varianza del fattore \\(B\\) è calibrata utilizzando \\(X4\\) come variabile marker.\nQuando più indicatori per lo stesso fattore presentano precisione equivalente e nessuno di essi è considerato particolarmente rappresentativo del concetto sottostante, la scelta dell’indicatore come variabile di riferimento diventa generalmente arbitraria. Questa selezione non influisce solitamente sull’adattamento globale del modello, sulla soluzione standardizzata, o sulle stime delle varianze di errore dell’indicatore nelle soluzioni non standardizzate. Le saturazioni fisse a 1.0 per le variabili di riferimento rimangono invariate nelle soluzioni non standardizzate e non sono soggette a test di significatività, poiché sono considerate costanti.\nUn potenziale svantaggio di questo metodo è l’assenza di test di significatività per le saturazioni fisse, il che può essere limitante se si desidera valutare la significatività di tutte le saturazioni. Metodi alternativi per scalare i fattori, che non richiedono la selezione di variabili di riferimento, saranno discussi nelle sezioni successive.\nNei modelli CFA di base, tutti i fattori sono considerati variabili esogene, il che significa che sono liberi di variare e covariare indipendentemente l’uno dall’altro. Tuttavia, è possibile includere variabili esterne, dette covariate, che si presume possano influenzare i fattori comuni. Ad esempio, l’età dei partecipanti potrebbe essere vista come una covariata che influisce sui fattori comuni in un modello CFA.\nL’inclusione di covariate trasforma i fattori comuni da variabili esogene a endogene, implicando che non sono più completamente liberi di variare in modo indipendente, ma possono essere direttamente influenzati dalle covariate. Questo richiede l’aggiunta di termini di disturbo nei fattori comuni per rappresentare l’effetto diretto delle covariate su di essi, integrando così l’effetto delle variabili esterne nel modello CFA.\n\n36.6.1 Parametri del Modello nella CFA\nIn un modello CFA con indicatori continui, quando le medie delle variabili non sono considerate, i parametri liberi includono varianze, covarianze di variabili esogene e gli effetti diretti (carichi) sulle variabili endogene. Ad esempio, analizzando il modello di base illustrato in figura, i parametri liberi possono essere suddivisi come segue:\n\nVarianze: Comprendono le varianze di due fattori e sei termini di errore associati agli indicatori, per un totale di otto varianze.\nCovarianza: È presente una covarianza tra i due fattori.\nEffetti Diretti (Carichi): Quattro carichi fattoriali rappresentano gli effetti diretti dei fattori sugli indicatori, specificamente per gli indicatori X2, X3, X5 e X6. Questi carichi non sono fissati, a differenza di quelli usati per scalare i fattori.\n\nSommando questi parametri, il totale dei parametri liberi nel modello è 13. Con \\(v = 6\\) variabili osservate, il numero totale di osservazioni statisticamente indipendenti, calcolato come \\(6(7)/2\\), è 21. Di conseguenza, i gradi di libertà per il modello presentato sono calcolati sottraendo i parametri liberi dalle osservazioni indipendenti, risultando in \\(21 - 13 = 8\\) gradi di libertà.\n\n36.6.1.1 Requisiti di Identificazione: Necessari ma Non Sufficienti per i Modelli di CFA\nPer assicurare che un modello di Confermative Factor Analysis (CFA) sia correttamente specificato e possa essere utilizzato per trarre conclusioni valide, è fondamentale soddisfare alcuni requisiti di identificazione essenziali. Questi requisiti sono necessari ma non sempre sufficienti; cioè, la loro soddisfazione non garantisce automaticamente l’identificazione completa del modello.\n\nGradi di Libertà (dfM) Maggiori o Uguali a Zero:\n\nCalcolo: I gradi di libertà di un modello CFA si determinano sottraendo il numero di parametri liberi (come varianze, covarianze, e carichi fattoriali) dal numero totale di osservazioni indipendenti disponibili, solitamente le varianze e covarianze degli indicatori.\nSignificato: Avere gradi di libertà positivi indica che ci sono sufficienti dati per stimare i parametri del modello e verificare il suo adattamento. Un modello con zero gradi di libertà è “saturato” e si adatterà perfettamente ai dati, ma non fornirà validazione ulteriore.\nImportanza: Mantenere dfM ≥ 0 è cruciale per evitare la sottospecificazione del modello, che potrebbe condurre a stime inaccurate e conclusioni fuorvianti.\n\nScalatura Corretta di Ogni Variabile Non Misurata:\n\nNecessità: È essenziale scalare ogni variabile latente, come i fattori, per definirne l’unità di misura. Senza una scalatura adeguata, parametri come i carichi fattoriali rimarrebbero indeterminati.\nMetodi: La scalatura può essere effettuata in vari modi, come fissando il carico di un indicatore per fattore a 1.0 (metodo della variabile di riferimento), fissando la varianza del fattore a un valore preciso, tipicamente 1.0 (metodo della standardizzazione della varianza), o applicando vincoli alle stime delle saturazioni fattoriali (metodo di codifica degli effetti).\n\n\nIn sintesi, pur essendo fondamentale soddisfare questi requisiti per stabilire una base identificabile e interpretabile per un modello CFA, l’identificazione completa del modello può richiedere considerazioni aggiuntive legate alla struttura specifica e alle ipotesi teoriche che sottendono al modello.\n\n\n\n\n\n\nFigura 36.2: Scalatura dei fattori nel metodo della variabile di riferimento con vincoli di identificazione del carico unitario (ULI) (a), metodo di standardizzazione della variabile con vincoli di identificazione della varianza unitaria (UVI) (b) e metodo di codifica degli effetti con vincoli di identificazione della codifica degli effetti (ECI) (a + b + c)/3 = (d + e + f)/3 = 1.0 (c). (Figura tratta da Kline (2023))\n\n\n\n\n\n36.6.1.2 Requisiti Sufficienti Aggiuntivi per l’Identificazione nei Modelli CFA\nOltre ai criteri base, esistono requisiti addizionali che favoriscono l’identificazione adeguata nei modelli di Confermative Factor Analysis (CFA):\n\nRegola dei Tre Indicatori per i Modelli a Singolo Fattore: Affinché un modello CFA con un solo fattore sia pienamente identificabile, è necessario che disponga di almeno tre indicatori. Questo è dovuto al fatto che con solamente due indicatori non si dispone di sufficiente informazione per separare accuratamente la varianza del fattore e i suoi carichi specifici dagli errori di misurazione. Un modello con esattamente tre indicatori ha zero gradi di libertà, il che significa che si adatterà perfettamente ai dati ma non permetterà ulteriori test o validazioni. Per garantire gradi di libertà positivi (dfM &gt; 0) e consentire un’efficace validazione del modello, è consigliabile utilizzare almeno quattro indicatori.\nRegola dei Due Indicatori per i Modelli con Più Fattori: Nei modelli CFA che coinvolgono più di un fattore, è essenziale che ogni fattore sia rappresentato da almeno due indicatori. Questa disposizione aiuta a definire chiaramente ogni fattore e a distinguerlo dagli altri fattori presenti nel modello. Tuttavia, i modelli che si limitano a due indicatori per fattore possono presentare problematiche, specialmente in campioni di dimensioni ridotte, poiché possono emergere instabilità nelle stime e complessità nell’interpretazione dei risultati.\n\nQuesti requisiti aggiuntivi sono fondamentali non solo per garantire che un modello CFA sia teoricamente valido (attraverso una corretta scalatura e definizione delle variabili latenti), ma anche per assicurare la sua utilità pratica, fornendo sufficienti gradi di libertà per consentire validazioni affidabili e interpretazioni significative del modello.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#oltre-i-requisiti-minimi-di-identificazione",
    "href": "chapters/cfa/01_cfa.html#oltre-i-requisiti-minimi-di-identificazione",
    "title": "36  Analisti Fattoriale Confermativa",
    "section": "36.7 Oltre i Requisiti Minimi di Identificazione",
    "text": "36.7 Oltre i Requisiti Minimi di Identificazione\nNel contesto della Confermative Factor Analysis (CFA), i requisiti di identificazione sono considerati necessari ma non sufficienti. Questo significa che, anche se il loro soddisfacimento è cruciale per una corretta stima dei parametri del modello, essi non garantiscono da soli che il modello sia il migliore possibile o pienamente identificato in termini di struttura e fondamenti teorici. Questa distinzione sottolinea l’importanza di andare oltre i criteri minimi per esplorare l’adeguatezza complessiva e la validità del modello all’interno del suo contesto teorico e applicativo.\n\n36.7.1 Perché sono Necessari\n\nGradi di Libertà (dfM) Maggiori o Uguali a Zero: Avere gradi di libertà non negativi è essenziale per assicurare che ci siano abbastanza dati per stimare i parametri del modello. Se i gradi di libertà sono negativi, indica che ci sono troppi parametri da stimare rispetto alle informazioni disponibili, il che rende il modello inidentificabile.\nScalatura Corretta di Ogni Variabile Non Misurata: La scalatura delle variabili latenti consente di stabilire un’unità di misura chiara, rendendo possibile l’interpretazione dei parametri come i carichi fattoriali. Senza una scalatura appropriata, i parametri del modello rimarrebbero indeterminati e potrebbero condurre a conclusioni ambigue.\n\n\n\n36.7.2 Perché Non Sono Sufficienti\nNonostante la soddisfazione di questi requisiti renda il modello tecnicamente identificabile e stima i parametri, ci sono altre considerazioni che possono influenzare l’adeguatezza del modello:\n\nAdeguamento del Modello: Anche se un modello ha gradi di libertà positivi e le variabili sono correttamente scalate, potrebbe non adattarsi bene ai dati. L’adeguatezza del modello è valutata attraverso statistiche di fit come il Chi-quadrato, RMSEA, CFI, e altri. Un modello può soddisfare i requisiti di identificazione ma avere un cattivo fit.\nValidità Teorica: Un modello può essere tecnicamente corretto ma non catturare accuratamente le relazioni teoriche tra le variabili. La costruzione del modello deve essere guidata da una solida base teorica che giustifica le relazioni tra i fattori e gli indicatori.\n\n\n\n36.7.3 Esempio Pratico\nImmaginiamo un modello CFA per misurare due concetti psicologici, come l’ansia e la depressione, con tre indicatori per ciascun fattore. Anche se il modello potrebbe avere gradi di libertà sufficienti e ogni fattore è correttamente scalato con un indicatore con carico fissato a 1.0, potrebbero sorgere problemi:\n\nCross-loadings: Gli indicatori per l’ansia potrebbero anche avere carichi significativi sulla depressione, il che non è catturato nel modello perché ogni indicatore è supposto misurare un solo fattore. Questo problema di validità del modello non è rilevato dai semplici criteri di identificazione.\nAdattamento del Modello: Il modello potrebbe mostrare un cattivo adattamento ai dati, suggerendo che la struttura ipotizzata dei fattori e degli indicatori non riflette accuratamente le relazioni tra le variabili osservate.\n\nIn conclusione, mentre i requisiti di identificazione sono critici per la fattibilità tecnica di un modello CFA, non garantiscono di per sé che il modello sia il migliore possibile o che rifletta accuratamente le dinamiche sottostanti. Ulteriori analisi e valutazioni sono necessarie per assicurare che il modello sia sia identificabile che valido.\n\n36.7.3.1 Altri Metodi per la Scalatura dei Fattori nei Modelli di CFA\nLa scalatura dei fattori è fondamentale per garantire una corretta identificazione e interpretazione dei fattori in un modello di Confermative Factor Analysis (CFA). Oltre al comune metodo della variabile di riferimento, esistono altri due approcci principali:\n\nMetodo di Standardizzazione della Varianza (Variance Standardization Method):\n\nDescrizione: Questo metodo fissa la varianza di ciascun fattore a 1.0, un approccio noto come unit variance identification (UVI).\nImplicazioni: La standardizzazione dei fattori implica che le loro varianze non sono stimate come parametri liberi. Invece, le covarianze tra i fattori sono liberamente stimate e interpretate come correlazioni di Pearson.\nCarichi degli Indicatori: Tutti i carichi degli indicatori sono considerati parametri liberi, il che permette di testarne la significatività statistica attraverso i loro errori standard.\nVantaggi e Limitazioni: Il principale vantaggio di questo metodo è la sua semplicità e l’assenza di necessità di selezionare una variabile di riferimento. Tuttavia, è generalmente più adatto per modellare fattori esogeni.\n\nMetodo di Codifica degli Effetti (Effects Coding Method):\n\nFunzionamento: A differenza dei metodi precedenti, questo non richiede la selezione di una variabile di riferimento e non implica la standardizzazione dei fattori.\nVincolo di Codifica degli Effetti (ECI): Si impone che la media dei carichi fattoriali per gli indicatori di un dato fattore sia uguale a 1.0. Questo obbliga il software SEM a trovare una combinazione ottimale di carichi che, in media, risultino in 1.0.\nStima della Varianza del Fattore: La varianza del fattore viene stimata come la varianza comune media calcolata attraverso tutti gli indicatori, considerando il loro contributo individuale alla misurazione del fattore.\nVantaggi: Questo metodo consente che tutti gli indicatori contribuiscano equamente alla scalatura del loro fattore comune. È particolarmente utile in studi longitudinali o quando si confrontano gruppi diversi, dato che le varianze dei fattori possono fornire informazioni preziose.\n\n\nOgni metodo di scalatura presenta vantaggi specifici e limitazioni, che devono essere considerati in base agli obiettivi della ricerca e alle caratteristiche del modello CFA:\n\nIl Metodo di Standardizzazione della Varianza offre una soluzione semplice e diretta, ma potrebbe non essere sempre il più appropriato, specialmente in contesti dove i fattori sono endogeni.\nIl Metodo di Codifica degli Effetti è vantaggioso per stabilire una scalatura equilibrata e stabile dei fattori, utile soprattutto in studi comparativi o longitudinali.\n\nLa scelta del metodo di scalatura dovrebbe essere guidata dalle necessità specifiche della ricerca, dalla struttura dei dati e dalle ipotesi teoriche del modello di CFA utilizzato.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#esempio-di-cfa-per-un-modello-di-abilità-cognitive-la-kaufman-assessment-battery-for-children",
    "href": "chapters/cfa/01_cfa.html#esempio-di-cfa-per-un-modello-di-abilità-cognitive-la-kaufman-assessment-battery-for-children",
    "title": "36  Analisti Fattoriale Confermativa",
    "section": "36.8 Esempio di CFA per un Modello di Abilità Cognitive: La Kaufman Assessment Battery for Children",
    "text": "36.8 Esempio di CFA per un Modello di Abilità Cognitive: La Kaufman Assessment Battery for Children\nLa Kaufman Assessment Battery for Children (KABC-I) è un test di valutazione delle abilità cognitive, somministrato individualmente a bambini dagli 2 anni e mezzo ai 12 anni e mezzo (Kaufman & Kaufman, 1983). Questo strumento è stato progettato per misurare due distinti fattori cognitivi attraverso otto indicatori.\nI primi tre compiti del test sono orientati all’elaborazione sequenziale e richiedono ai bambini di ricordare e ripetere stimoli uditivi (come nel Richiamo Numerico e nell’Ordine delle Parole) o visivi (come nei Movimenti della Mano) in un ordine specifico. Questi compiti sono pensati per riflettere la capacità di memoria a breve termine e di sequenziamento delle informazioni.\nGli altri cinque compiti, che includono la Chiusura Gestaltica e la Serie Fotografica, sono ritenuti misurare un tipo di ragionamento più olistico e meno sequenziale, associato all’elaborazione simultanea. Questi compiti valutano la capacità di integrare e sintetizzare le informazioni visuo-spaziali in un contesto più ampio, spesso indipendentemente dall’ordine in cui le informazioni sono presentate.\nKeith (1985) ha proposto delle denominazioni alternative per i fattori misurati dalla KABC-I, suggerendo i termini “memoria a breve termine” per sostituire “elaborazione sequenziale” e “ragionamento visuo-spaziale” per “elaborazione simultanea”. Queste etichette alternative riflettono una prospettiva leggermente diversa sui tipi di competenze cognitive che i due fattori intendono misurare.\nQuesto modello di CFA, utilizzando i compiti della KABC-I come indicatori, fornisce una struttura utile per comprendere come diversi tipi di elaborazione cognitiva possano essere categorizzati e valutati nei contesti educativi e diagnostici.\n\n# input the correlations in lower diagnonal form\nkabcLower.cor &lt;- \"\n 1.00\n .39 1.00\n .35  .67 1.00\n .21  .11  .16 1.00\n .32  .27  .29  .38 1.00\n .40  .29  .28  .30  .47 1.00\n .39  .32  .30  .31  .42  .41 1.00\n .39  .29  .37  .42  .58  .51  .42 1.00 \"\n\n# name the variables and convert to full correlation matrix\n# hm, hand movements; nr, number recall; wo, word order; gc, gestalt closure;\n# tr, triangles; sm, spatial memory; ma, matrix analogies; ps, photo series\nkabc.cor &lt;- lavaan::getCov(kabcLower.cor, names = c(\n    \"hm\", \"nr\", \"wo\",\n    \"gc\", \"tr\", \"sm\", \"ma\", \"ps\"\n))\n# display correlations\nkabc.cor\n\n\nA matrix: 8 x 8 of type dbl\n\n\n\nhm\nnr\nwo\ngc\ntr\nsm\nma\nps\n\n\n\n\nhm\n1.00\n0.39\n0.35\n0.21\n0.32\n0.40\n0.39\n0.39\n\n\nnr\n0.39\n1.00\n0.67\n0.11\n0.27\n0.29\n0.32\n0.29\n\n\nwo\n0.35\n0.67\n1.00\n0.16\n0.29\n0.28\n0.30\n0.37\n\n\ngc\n0.21\n0.11\n0.16\n1.00\n0.38\n0.30\n0.31\n0.42\n\n\ntr\n0.32\n0.27\n0.29\n0.38\n1.00\n0.47\n0.42\n0.58\n\n\nsm\n0.40\n0.29\n0.28\n0.30\n0.47\n1.00\n0.41\n0.51\n\n\nma\n0.39\n0.32\n0.30\n0.31\n0.42\n0.41\n1.00\n0.42\n\n\nps\n0.39\n0.29\n0.37\n0.42\n0.58\n0.51\n0.42\n1.00\n\n\n\n\n\n\n# add the standard deviations and convert to covariances\nkabc.cov &lt;- lavaan::cor2cov(kabc.cor, sds = c(3.40, 2.40, 2.90, 2.70, 2.70, 4.20, 2.80, 3.00))\n\n# display covariances\nkabc.cov\n\n\nA matrix: 8 x 8 of type dbl\n\n\n\nhm\nnr\nwo\ngc\ntr\nsm\nma\nps\n\n\n\n\nhm\n11.5600\n3.1824\n3.4510\n1.9278\n2.9376\n5.7120\n3.7128\n3.978\n\n\nnr\n3.1824\n5.7600\n4.6632\n0.7128\n1.7496\n2.9232\n2.1504\n2.088\n\n\nwo\n3.4510\n4.6632\n8.4100\n1.2528\n2.2707\n3.4104\n2.4360\n3.219\n\n\ngc\n1.9278\n0.7128\n1.2528\n7.2900\n2.7702\n3.4020\n2.3436\n3.402\n\n\ntr\n2.9376\n1.7496\n2.2707\n2.7702\n7.2900\n5.3298\n3.1752\n4.698\n\n\nsm\n5.7120\n2.9232\n3.4104\n3.4020\n5.3298\n17.6400\n4.8216\n6.426\n\n\nma\n3.7128\n2.1504\n2.4360\n2.3436\n3.1752\n4.8216\n7.8400\n3.528\n\n\nps\n3.9780\n2.0880\n3.2190\n3.4020\n4.6980\n6.4260\n3.5280\n9.000\n\n\n\n\n\n\n\n\n\n\n\nFigura 36.3: Modello CFA per la Kaufman Assessment Battery for Children. (Figura tratta da Kline (2023))\n\n\n\n\n36.8.0.1 Modello a Fattore Singolo\nNell’ambito della CFA, se il modello bersaglio ha due o più fattori, spesso il primo modello analizzato è un modello a fattore singolo. Se non si può rigettare un modello a fattore singolo, non ha molto senso valutare modelli con più fattori.\nSpecifichiamo il modello ad un fattore comune nella sintassi di lavaan.\n\n# single factor (general ability)\n# indicator hm automatically specified as reference variable\nkabc1.model &lt;- \"\n    General =~ hm + nr + wo + gc + tr + sm + ma + ps \n\"\n\nAdattiamo il modello ai dati.\n\n# variances calculated with N in the denominator, not N - 1\nkabc1 &lt;- lavaan::sem(\n    kabc1.model, \n    sample.cov = kabc.cov, \n    sample.nobs = 200\n)\n\nGeneriamo un modello di percorso.\n\nsemPlot::semPaths(kabc1,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\nEsaminiamo la soluzione non standardizzata.\n\nlavaan::parameterEstimates(kabc1) \n\n\nA lavaan.data.frame: 17 x 9\n\n\nlhs\nop\nrhs\nest\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nGeneral\n=~\nhm\n1.0000000\n0.0000000\nNA\nNA\n1.0000000\n1.0000000\n\n\nGeneral\n=~\nnr\n0.6358147\n0.1113808\n5.708474\n1.139933e-08\n0.4175122\n0.8541171\n\n\nGeneral\n=~\nwo\n0.8052592\n0.1362548\n5.909952\n3.422078e-09\n0.5382048\n1.0723137\n\n\nGeneral\n=~\ngc\n0.6587497\n0.1228832\n5.360780\n8.286325e-08\n0.4179031\n0.8995963\n\n\nGeneral\n=~\ntr\n0.9631365\n0.1379106\n6.983774\n2.873479e-12\n0.6928367\n1.2334363\n\n\nGeneral\n=~\nsm\n1.4325685\n0.2107960\n6.795994\n1.075673e-11\n1.0194159\n1.8457211\n\n\nGeneral\n=~\nma\n0.8827509\n0.1366769\n6.458669\n1.056277e-10\n0.6148691\n1.1506327\n\n\nGeneral\n=~\nps\n1.1663077\n0.1592443\n7.324014\n2.406964e-13\n0.8541946\n1.4784209\n\n\nhm\n~~\nhm\n7.8123223\n0.8633074\n9.049294\n0.000000e+00\n6.1202709\n9.5043737\n\n\nnr\n~~\nnr\n4.2395307\n0.4561738\n9.293675\n0.000000e+00\n3.3454465\n5.1336148\n\n\nwo\n~~\nwo\n5.9752809\n0.6498204\n9.195280\n0.000000e+00\n4.7016563\n7.2489055\n\n\ngc\n~~\ngc\n5.6523162\n0.5992591\n9.432174\n0.000000e+00\n4.4777900\n6.8268424\n\n\ntr\n~~\ntr\n3.8306998\n0.4679368\n8.186362\n2.220446e-16\n2.9135606\n4.7478390\n\n\nsm\n~~\nsm\n9.9792360\n1.1791148\n8.463329\n0.000000e+00\n7.6682135\n12.2902584\n\n\nma\n~~\nma\n4.9254754\n0.5583124\n8.822078\n0.000000e+00\n3.8312032\n6.0197475\n\n\nps\n~~\nps\n3.9357558\n0.5311293\n7.410165\n1.261213e-13\n2.8947616\n4.9767501\n\n\nGeneral\n~~\nGeneral\n3.6898743\n0.9206831\n4.007757\n6.129807e-05\n1.8853686\n5.4943800\n\n\n\n\n\nLa saturazione non standardizzata per il compito “Movimenti della Mano” è stato fissato automaticamente a 1.0 per scalare il singolo fattore comune. Le istruzioni seguenti consentono di estrarre dall’output di sem() solo le informazioni relative alle saturazioni fattoriali.\n\nparameterEstimates(kabc1, standardized = TRUE) %&gt;%\n    dplyr::filter(op == \"=~\") %&gt;%\n    dplyr::select(\n        \"Latent Factor\" = lhs,\n        Indicator = rhs,\n        B = est,\n        SE = se,\n        Z = z,\n        \"p-value\" = pvalue,\n        Beta = std.all\n    ) %&gt;%\n    knitr::kable(\n        digits = 3, booktabs = TRUE, format = \"markdown\",\n        caption = \"Factor Loadings\"\n    )\n\n\n\nTable: Factor Loadings\n\n|Latent Factor |Indicator |     B|    SE|     Z| p-value|  Beta|\n|:-------------|:---------|-----:|-----:|-----:|-------:|-----:|\n|General       |hm        | 1.000| 0.000|    NA|      NA| 0.566|\n|General       |nr        | 0.636| 0.111| 5.708|       0| 0.510|\n|General       |wo        | 0.805| 0.136| 5.910|       0| 0.535|\n|General       |gc        | 0.659| 0.123| 5.361|       0| 0.470|\n|General       |tr        | 0.963| 0.138| 6.984|       0| 0.687|\n|General       |sm        | 1.433| 0.211| 6.796|       0| 0.657|\n|General       |ma        | 0.883| 0.137| 6.459|       0| 0.607|\n|General       |ps        | 1.166| 0.159| 7.324|       0| 0.749|\n\n\nEsaminiamo le misure di bontà di adattamento.\n\nfitMeasures(kabc1, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) \n\nchisq105.426636833896df20cfi0.818371182415794tli0.745719655382111rmsea0.146139177527705srmr0.0836549258603996\n\n\nTroviamo i residui grezzi, ovvero la differenza tra la matrice di covarianza osservata e quella predetta dal modello.\n\nlavaan::residuals(kabc1, type = \"raw\") \n\n\n    $type\n        'raw'\n    $cov\n        \n\nA lavaan.matrix.symmetric: 8 x 8 of type dbl\n\n\n\nhm\nnr\nwo\ngc\ntr\nsm\nma\nps\n\n\n\n\nhm\n3.391968e-06\n8.204118e-01\n4.624396e-01\n-5.125426e-01\n-6.309405e-01\n3.974422e-01\n4.369960e-01\n-3.454190e-01\n\n\nnr\n8.204118e-01\n-3.007158e-07\n2.750684e+00\n-8.362410e-01\n-5.187395e-01\n-4.523309e-01\n6.864707e-02\n-6.586868e-01\n\n\nwo\n4.624396e-01\n2.750684e+00\n-1.997207e-06\n-7.108105e-01\n-6.024261e-01\n-8.632506e-01\n-1.991026e-01\n-2.625515e-01\n\n\ngc\n-5.125426e-01\n-8.362410e-01\n-7.108105e-01\n8.574682e-06\n4.152498e-01\n-9.715946e-02\n1.861762e-01\n5.500416e-01\n\n\ntr\n-6.309405e-01\n-5.187395e-01\n-6.024261e-01\n4.152498e-01\n5.252583e-06\n2.120137e-01\n2.215741e-02\n5.296243e-01\n\n\nsm\n3.974422e-01\n-4.523309e-01\n-8.632506e-01\n-9.715946e-02\n2.120137e-01\n9.851239e-06\n1.312725e-01\n2.287698e-01\n\n\nma\n4.369960e-01\n6.864707e-02\n-1.991026e-01\n1.861762e-01\n2.215741e-02\n1.312725e-01\n-6.972920e-06\n-2.885842e-01\n\n\nps\n-3.454190e-01\n-6.586868e-01\n-2.625515e-01\n5.500416e-01\n5.296243e-01\n2.287698e-01\n-2.885842e-01\n4.996915e-06\n\n\n\n\n\n\n\n\nSpecificando type = \"cor.bollen\" o type = \"cor\" otteniamo la differenza tra la matrice di correlazione osservata e quella predetta dal modello.\n\nlavaan::residuals(kabc1, type = \"cor.bollen\") \n\n\n    $type\n        'cor.bollen'\n    $cov\n        \n\nA lavaan.matrix.symmetric: 8 x 8 of type dbl\n\n\n\nhm\nnr\nwo\ngc\ntr\nsm\nma\nps\n\n\n\n\nhm\n1.110223e-16\n0.10104586\n0.04713624\n-5.611329e-02\n-6.907548e-02\n2.797178e-02\n4.613372e-02\n-3.403496e-02\n\n\nnr\n1.010459e-01\n0.00000000\n0.39719932\n-1.296982e-01\n-8.045479e-02\n-4.509968e-02\n1.026682e-02\n-9.194410e-02\n\n\nwo\n4.713624e-02\n0.39719932\n0.00000000\n-9.123670e-02\n-7.732491e-02\n-7.123064e-02\n-2.464305e-02\n-3.033004e-02\n\n\ngc\n-5.611329e-02\n-0.12969816\n-0.09123670\n-2.220446e-16\n5.724749e-02\n-8.611177e-03\n2.475019e-02\n6.824730e-02\n\n\ntr\n-6.907548e-02\n-0.08045479\n-0.07732491\n5.724749e-02\n-4.440892e-16\n1.878976e-02\n2.945638e-03\n6.571396e-02\n\n\nsm\n2.797178e-02\n-0.04509968\n-0.07123064\n-8.611177e-03\n1.878976e-02\n1.110223e-16\n1.121879e-02\n1.824730e-02\n\n\nma\n4.613372e-02\n0.01026682\n-0.02464305\n2.475019e-02\n2.945638e-03\n1.121879e-02\n-2.220446e-16\n-3.452782e-02\n\n\nps\n-3.403496e-02\n-0.09194410\n-0.03033004\n6.824730e-02\n6.571396e-02\n1.824730e-02\n-3.452782e-02\n1.110223e-16\n\n\n\n\n\n\n\n\nIn alternativa, possiamo ottenere i residui standardizzati alla maniera di Mplus (standardized.mplus), che vengono calcolati utilizzando la seguente formula:\n\\[\n    \\text{Residuo Standardizzato} = \\frac{\\text{Cov. Osservata} - \\text{Cov. Stimata}}{\\sqrt{\\text{Var. dell'Errore per X} \\times \\text{Var. dell'Errore per Y}}},\n\\]\ndove: - La covarianza osservata è il valore della covarianza tra due variabili nel set di dati. - La covarianza stimata è la covarianza tra le stesse due variabili, come previsto dal modello SEM. - La varianza dell’errore per la variabile X e Y sono le varianze degli errori per le due variabili in questione.\nI residui standardizzati misurano quanto la relazione osservata tra due variabili si discosta da quella prevista dal modello, in unità standardizzate. Un valore vicino a zero indica che il modello si adatta bene ai dati per quella specifica relazione. Valori più grandi in valore assoluto suggeriscono un cattivo adattamento in quella specifica parte del modello.\n\nlavaan::residuals(kabc1, type = \"standardized.mplus\") \n\n\n    $type\n        'standardized.mplus'\n    $cov\n        \n\nA lavaan.matrix.symmetric: 8 x 8 of type dbl\n\n\n\nhm\nnr\nwo\ngc\ntr\nsm\nma\nps\n\n\n\n\nhm\n3.391968e-06\n2.062139e+00\n1.025807e+00\n-1.230613e+00\n-2.200057e+00\n7.231232e-01\n1.086382e+00\n-1.240744e+00\n\n\nnr\n2.062139e+00\n-3.007158e-07\n6.217924e+00\n-2.726883e+00\n-2.364180e+00\n-1.188246e+00\n2.365039e-01\n-3.421534e+00\n\n\nwo\n1.025807e+00\n6.217924e+00\n-1.997207e-06\n-1.952162e+00\n-2.354605e+00\n-1.995031e+00\n-6.007547e-01\n-1.037442e+00\n\n\ngc\n-1.230613e+00\n-2.726883e+00\n-1.952162e+00\n8.574682e-06\n1.378539e+00\n-2.099879e-01\n5.442300e-01\n1.833012e+00\n\n\ntr\n-2.200057e+00\n-2.364180e+00\n-2.354605e+00\n1.378539e+00\n5.252583e-06\n5.955481e-01\n8.866991e-02\n2.178031e+00\n\n\nsm\n7.231232e-01\n-1.188246e+00\n-1.995031e+00\n-2.099879e-01\n5.955481e-01\n9.851239e-06\n3.134075e-01\n6.749792e-01\n\n\nma\n1.086382e+00\n2.365039e-01\n-6.007547e-01\n5.442300e-01\n8.866991e-02\n3.134075e-01\n-6.972920e-06\n-1.375249e+00\n\n\nps\n-1.240744e+00\n-3.421534e+00\n-1.037442e+00\n1.833012e+00\n2.178031e+00\n6.749792e-01\n-1.375249e+00\n4.996915e-06\n\n\n\n\n\n\n\n\nIl modello a fattore singolo mostra un rapporto elevato chi-quadro/df. Inoltre, i residui per questa analisi indicano che l’adattamento locale è scadente. Pertanto, il modello a fattore singolo per la KABC-I è rigettato.\n\n\n36.8.0.2 Modello a Due Fattori\nIn una seconda analisi, adattiamo ai dati il modello a due fattori rappresentato nella {numref}kline-14-3-fig.\n\nkabc2_model &lt;- \"\n    Sequent =~ hm + nr + wo\n    Simultan =~ gc + tr + sm + ma + ps \n\"\n\n\nkabc2 &lt;- lavaan::sem(kabc2_model, sample.cov = kabc.cov, sample.nobs = 200)\n\n\nsemPlot::semPaths(kabc2,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\n\nlavaan::parameterEstimates(kabc2)\n\n\nA lavaan.data.frame: 19 x 9\n\n\nlhs\nop\nrhs\nest\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nSequent\n=~\nhm\n1.000000\n0.0000000\nNA\nNA\n1.0000000\n1.000000\n\n\nSequent\n=~\nnr\n1.146800\n0.1808640\n6.340674\n2.287626e-10\n0.7923127\n1.501287\n\n\nSequent\n=~\nwo\n1.387710\n0.2188783\n6.340100\n2.296165e-10\n0.9587167\n1.816704\n\n\nSimultan\n=~\ngc\n1.000000\n0.0000000\nNA\nNA\n1.0000000\n1.000000\n\n\nSimultan\n=~\ntr\n1.444543\n0.2274092\n6.352174\n2.122924e-10\n0.9988292\n1.890257\n\n\nSimultan\n=~\nsm\n2.029267\n0.3347784\n6.061522\n1.348394e-09\n1.3731130\n2.685420\n\n\nSimultan\n=~\nma\n1.212293\n0.2120626\n5.716676\n1.086279e-08\n0.7966581\n1.627928\n\n\nSimultan\n=~\nps\n1.727179\n0.2648674\n6.520921\n6.987722e-11\n1.2080488\n2.246310\n\n\nhm\n~~\nhm\n8.663885\n0.9379533\n9.237011\n0.000000e+00\n6.8255305\n10.502240\n\n\nnr\n~~\nnr\n1.998406\n0.4136628\n4.831003\n1.358468e-06\n1.1876420\n2.809170\n\n\nwo\n~~\nwo\n2.902105\n0.6044697\n4.801076\n1.578156e-06\n1.7173660\n4.086844\n\n\ngc\n~~\ngc\n5.419079\n0.5851519\n9.260979\n0.000000e+00\n4.2722025\n6.565956\n\n\ntr\n~~\ntr\n3.425503\n0.4580211\n7.478920\n7.482903e-14\n2.5277979\n4.323208\n\n\nsm\n~~\nsm\n9.997500\n1.2016198\n8.320019\n0.000000e+00\n7.6423681\n12.352631\n\n\nma\n~~\nma\n5.104718\n0.5776174\n8.837541\n0.000000e+00\n3.9726083\n6.236827\n\n\nps\n~~\nps\n3.482437\n0.5372437\n6.482043\n9.048895e-11\n2.4294583\n4.535415\n\n\nSequent\n~~\nSequent\n2.838307\n0.8375575\n3.388791\n7.020154e-04\n1.1967246\n4.479889\n\n\nSimultan\n~~\nSimultan\n1.834493\n0.5302994\n3.459354\n5.414734e-04\n0.7951254\n2.873861\n\n\nSequent\n~~\nSimultan\n1.270820\n0.3243863\n3.917614\n8.942973e-05\n0.6350349\n1.906606\n\n\n\n\n\n\nstandardizedSolution(kabc2)\n\n\nA lavaan.data.frame: 19 x 9\n\n\nlhs\nop\nrhs\nest.std\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nSequent\n=~\nhm\n0.4967517\n0.06185190\n8.031309\n8.881784e-16\n0.3755242\n0.6179792\n\n\nSequent\n=~\nnr\n0.8070386\n0.04626958\n17.442098\n0.000000e+00\n0.7163519\n0.8977253\n\n\nSequent\n=~\nwo\n0.8082004\n0.04624070\n17.478118\n0.000000e+00\n0.7175703\n0.8988305\n\n\nSimultan\n=~\ngc\n0.5029005\n0.06088027\n8.260485\n2.220446e-16\n0.3835774\n0.6222236\n\n\nSimultan\n=~\ntr\n0.7264627\n0.04412957\n16.462040\n0.000000e+00\n0.6399703\n0.8129550\n\n\nSimultan\n=~\nsm\n0.6560490\n0.04959951\n13.226925\n0.000000e+00\n0.5588358\n0.7532623\n\n\nSimultan\n=~\nma\n0.5878905\n0.05485948\n10.716298\n0.000000e+00\n0.4803679\n0.6954131\n\n\nSimultan\n=~\nps\n0.7817406\n0.04012341\n19.483401\n0.000000e+00\n0.7031001\n0.8603810\n\n\nhm\n~~\nhm\n0.7532377\n0.06145007\n12.257719\n0.000000e+00\n0.6327978\n0.8736777\n\n\nnr\n~~\nnr\n0.3486887\n0.07468267\n4.668937\n3.027615e-06\n0.2023134\n0.4950641\n\n\nwo\n~~\nwo\n0.3468121\n0.07474351\n4.640030\n3.483594e-06\n0.2003175\n0.4933067\n\n\ngc\n~~\ngc\n0.7470911\n0.06123343\n12.200705\n0.000000e+00\n0.6270758\n0.8671064\n\n\ntr\n~~\ntr\n0.4722520\n0.06411696\n7.365476\n1.765255e-13\n0.3465850\n0.5979189\n\n\nsm\n~~\nsm\n0.5695997\n0.06507943\n8.752377\n0.000000e+00\n0.4420463\n0.6971530\n\n\nma\n~~\nma\n0.6543848\n0.06450273\n10.145071\n0.000000e+00\n0.5279617\n0.7808078\n\n\nps\n~~\nps\n0.3888817\n0.06273220\n6.199076\n5.679559e-10\n0.2659288\n0.5118345\n\n\nSequent\n~~\nSequent\n1.0000000\n0.00000000\nNA\nNA\n1.0000000\n1.0000000\n\n\nSimultan\n~~\nSimultan\n1.0000000\n0.00000000\nNA\nNA\n1.0000000\n1.0000000\n\n\nSequent\n~~\nSimultan\n0.5569247\n0.06673231\n8.345654\n0.000000e+00\n0.4261318\n0.6877176\n\n\n\n\n\n\nfitMeasures(kabc2, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) \n\nchisq38.3247560709631df19cfi0.958912902048374tli0.939450592492341rmsea0.0713124261537753srmr0.0721641433704015\n\n\nUn modello di analisi fattoriale confermativa (CFA) che utilizza un singolo fattore può essere visto come un caso specifico o un “sottoinsieme” di modelli CFA più complessi con due o più fattori che impiegano gli stessi indicatori e lo stesso schema di covarianza degli errori, se presente. Questa struttura gerarchica tra i modelli a singolo fattore e quelli multifattoriali implica che i ricercatori possono applicare il test del chi-quadro per confrontare direttamente l’adattamento di un modello CFA a singolo fattore con quello di modelli CFA a più fattori. In pratica, ciò permette di valutare se l’introduzione di fattori aggiuntivi migliora significativamente l’adattamento del modello ai dati rispetto a un modello più semplice a singolo fattore. Questo tipo di analisi è fondamentale per determinare la complessità ottimale del modello in base alla struttura sottostante dei dati. Sebbene questo argomento verrà approfondito successivamente, è importante anticipare qui l’utilizzo del test del rapporto di verosimiglianza, che consente di confrontare i modelli in maniera quantitativa.\n\nlavTestLRT(kabc1, kabc2)\n\n\nA anova: 2 x 8\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nRMSEA\nDf diff\nPr(&gt;Chisq)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nkabc2\n19\n7592.082\n7648.153\n38.32476\nNA\nNA\nNA\nNA\n\n\nkabc1\n20\n7657.183\n7709.956\n105.42664\n67.10188\n0.5748995\n1\n2.578323e-16\n\n\n\n\n\nI risultati del test indicano che l’adattamento del modello con due fattori è statisticamente migliore rispetto a quello del modello a fattore singolo (il modello ad un fattore ha un valore \\(\\chi^2\\) superiore di 67.1 punti, con un grado di libertà).\nAnche se il test del rapporto tra verosimiglianze favorisce il modello a due fattori, possiamo notare che l’esame dei residui mostra un problema con l’indicatore hm.\n\nlavaan::residuals(kabc2, type = \"standardized.mplus\") \n\n\n    $type\n        'standardized.mplus'\n    $cov\n        \n\nA lavaan.matrix.symmetric: 8 x 8 of type dbl\n\n\n\nhm\nnr\nwo\ngc\ntr\nsm\nma\nps\n\n\n\n\nhm\n7.780153e-06\n-5.910875e-01\n-3.790464e+00\n1.1263137\n2.046028e+00\n3.463851e+00\n3.50524450\n2.990517e+00\n\n\nnr\n-5.910875e-01\n-3.514443e-06\n1.539009e+00\n-2.3285301\n-1.557830e+00\n-1.118616e-01\n1.12918789\n-2.001593e+00\n\n\nwo\n-3.790464e+00\n1.539009e+00\n3.866527e-06\n-1.3151837\n-1.000758e+00\n-3.546391e-01\n0.72661943\n5.240882e-01\n\n\ngc\n1.126314e+00\n-2.328530e+00\n-1.315184e+00\nNA\n4.285284e-01\n-7.843301e-01\n0.32314851\n9.101754e-01\n\n\ntr\n2.046028e+00\n-1.557830e+00\n-1.000758e+00\n0.4285284\n1.458742e-06\n-2.667037e-01\n-0.24457751\n6.767396e-01\n\n\nsm\n3.463851e+00\n-1.118616e-01\n-3.546391e-01\n-0.7843301\n-2.667037e-01\n-4.748427e-07\n0.66419233\n-1.437615e-01\n\n\nma\n3.505245e+00\n1.129188e+00\n7.266194e-01\n0.3231485\n-2.445775e-01\n6.641923e-01\n0.00832857\n-1.977762e+00\n\n\nps\n2.990517e+00\n-2.001593e+00\n5.240882e-01\n0.9101754\n6.767396e-01\n-1.437615e-01\n-1.97776188\n-2.784206e-06\n\n\n\n\n\n\n\n\nPer affrontare questo problema, calcoliamo i modification indices che ci dicono quale parametro del modello ha l’effetto maggiore sulla misura di fit complessivo.\n\nmodindices(kabc2, sort = TRUE, maximum.number = 5)\n\n\nA lavaan.data.frame: 5 x 8\n\n\n\nlhs\nop\nrhs\nmi\nepc\nsepc.lv\nsepc.all\nsepc.nox\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n25\nSimultan\n=~\nhm\n20.097078\n1.0539461\n1.4275011\n0.4209070\n0.4209070\n\n\n35\nnr\n~~\nwo\n20.097058\n4.7406831\n4.7406831\n1.9685321\n1.9685321\n\n\n26\nSimultan\n=~\nnr\n7.013048\n-0.5104555\n-0.6913786\n-0.2887972\n-0.2887972\n\n\n29\nhm\n~~\nwo\n7.012988\n-1.7458372\n-1.7458372\n-0.3481696\n-0.3481696\n\n\n32\nhm\n~~\nsm\n4.847027\n1.6094583\n1.6094583\n0.1729329\n0.1729329\n\n\n\n\n\nI risultati degli indici di modifica (MI) indicano che il misfit del modello è principalmente attribuibile alla fissazione a zero del carico tra l’indicatore hm e il fattore comune Simulan, nonché alla fissazione a zero della covarianza tra le componenti residue di nr e wo. Per migliorare l’adattamento del modello, si propone quindi di modificare questi aspetti, iniziando con il primo, ovvero riconsiderando il carico di hm sul fattore Simulan.\n\nkabc3_model &lt;- \"\n    Sequent =~ hm + nr + wo\n    Simultan =~ hm + gc + tr + sm + ma + ps\n\"\n\n\nkabc3 &lt;- lavaan::sem(kabc3_model, sample.cov = kabc.cov, sample.nobs = 200)\n\n\nlavaan::parameterEstimates(kabc3) \n\n\nA lavaan.data.frame: 20 x 9\n\n\nlhs\nop\nrhs\nest\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nSequent\n=~\nhm\n1.0000000\n0.0000000\nNA\nNA\n1.0000000\n1.0000000\n\n\nSequent\n=~\nnr\n2.2852332\n0.7771491\n2.940534\n3.276473e-03\n0.7620489\n3.8084174\n\n\nSequent\n=~\nwo\n2.7668158\n0.9413238\n2.939282\n3.289738e-03\n0.9218551\n4.6117764\n\n\nSimultan\n=~\nhm\n1.0000000\n0.0000000\nNA\nNA\n1.0000000\n1.0000000\n\n\nSimultan\n=~\ngc\n1.0142725\n0.2548896\n3.979261\n6.912968e-05\n0.5146980\n1.5138470\n\n\nSimultan\n=~\ntr\n1.4565607\n0.3290001\n4.427235\n9.544855e-06\n0.8117324\n2.1013889\n\n\nSimultan\n=~\nsm\n2.1034685\n0.4831066\n4.354046\n1.336477e-05\n1.1565969\n3.0503401\n\n\nSimultan\n=~\nma\n1.2587811\n0.2976830\n4.228595\n2.351548e-05\n0.6753331\n1.8422292\n\n\nSimultan\n=~\nps\n1.7520898\n0.3905795\n4.485872\n7.261622e-06\n0.9865680\n2.5176116\n\n\nhm\n~~\nhm\n7.8507750\n0.8449706\n9.291181\n0.000000e+00\n6.1946630\n9.5068869\n\n\nnr\n~~\nnr\n1.8989188\n0.4874537\n3.895588\n9.796073e-05\n0.9435271\n2.8543105\n\n\nwo\n~~\nwo\n2.7502708\n0.7132588\n3.855923\n1.152939e-04\n1.3523092\n4.1482323\n\n\ngc\n~~\ngc\n5.4435171\n0.5854830\n9.297482\n0.000000e+00\n4.2959916\n6.5910426\n\n\ntr\n~~\ntr\n3.5207608\n0.4571204\n7.702043\n1.332268e-14\n2.6248213\n4.4167002\n\n\nsm\n~~\nsm\n9.7669919\n1.1785787\n8.287093\n2.220446e-16\n7.4570200\n12.0769637\n\n\nma\n~~\nma\n5.0129076\n0.5686560\n8.815361\n0.000000e+00\n3.8983624\n6.1274528\n\n\nps\n~~\nps\n3.5538129\n0.5289941\n6.718057\n1.841638e-11\n2.5170035\n4.5906224\n\n\nSequent\n~~\nSequent\n0.7338316\n0.4895799\n1.498901\n1.338994e-01\n-0.2257274\n1.6933905\n\n\nSimultan\n~~\nSimultan\n1.7594443\n0.7603419\n2.314017\n2.066678e-02\n0.2692015\n3.2496871\n\n\nSequent\n~~\nSimultan\n0.5790716\n0.1780863\n3.251635\n1.147432e-03\n0.2300289\n0.9281142\n\n\n\n\n\nIl modello così modificato fornisce un buon adattamento ai dati.\n\nfitMeasures(kabc3, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) \n\nchisq18.1076447310257df18cfi0.999771132448379tli0.99964398380859rmsea0.00546820941202999srmr0.0352903027571906\n\n\n\nlavaan::residuals(kabc3, type = \"standardized.mplus\") \n\n\n    $type\n        'standardized.mplus'\n    $cov\n        \n\nA lavaan.matrix.symmetric: 8 x 8 of type dbl\n\n\n\nhm\nnr\nwo\ngc\ntr\nsm\nma\nps\n\n\n\n\nhm\n6.023887e-06\n1.165198e+00\n-1.636709e+00\n-1.065997e+00\n-1.709965e+00\n1.325031e+00\n1.729549e+00\n-0.511565953\n\n\nnr\n1.165198e+00\n-4.958870e-07\n-2.989195e-07\n-1.919329e+00\n-7.634603e-01\n2.866983e-01\n1.427665e+00\n-1.059404206\n\n\nwo\n-1.636709e+00\n-2.989195e-07\n7.382537e-07\n-9.387584e-01\n-2.473958e-01\n4.443780e-02\n1.029014e+00\n1.284912040\n\n\ngc\n-1.065997e+00\n-1.919329e+00\n-9.387584e-01\n6.890725e-06\n6.026587e-01\n-8.665013e-01\n2.578925e-01\n1.034770669\n\n\ntr\n-1.709965e+00\n-7.634603e-01\n-2.473958e-01\n6.026587e-01\n6.776975e-06\n-3.044029e-01\n-2.984495e-01\n1.087983988\n\n\nsm\n1.325031e+00\n2.866983e-01\n4.443780e-02\n-8.665013e-01\n-3.044029e-01\n6.851530e-06\n3.381911e-01\n-0.360500264\n\n\nma\n1.729549e+00\n1.427665e+00\n1.029014e+00\n2.578925e-01\n-2.984495e-01\n3.381911e-01\n2.356012e-07\n-2.180947375\n\n\nps\n-5.115660e-01\n-1.059404e+00\n1.284912e+00\n1.034771e+00\n1.087984e+00\n-3.605003e-01\n-2.180947e+00\n0.008196077\n\n\n\n\n\n\n\n\nEseguiamo il confronto tra questo terzo modello e il secondo.\n\nlavTestLRT(kabc2, kabc3)\n\n\nA anova: 2 x 8\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nRMSEA\nDf diff\nPr(&gt;Chisq)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nkabc3\n18\n7573.864\n7633.234\n18.10764\nNA\nNA\nNA\nNA\n\n\nkabc2\n19\n7592.082\n7648.153\n38.32476\n20.21711\n0.3099767\n1\n6.913179e-06\n\n\n\n\n\nIl test del rapporto tra verosimiglianze favorisce il modello nel quale hm satura su entrambi i fattori comuni.\n\nstandardizedSolution(kabc3)\n\n\nA lavaan.data.frame: 20 x 9\n\n\nlhs\nop\nrhs\nest.std\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nSequent\n=~\nhm\n0.2525852\n0.08220770\n3.072524\n2.122564e-03\n0.09146104\n0.4137093\n\n\nSequent\n=~\nnr\n0.8177224\n0.05332037\n15.336023\n0.000000e+00\n0.71321642\n0.9222284\n\n\nSequent\n=~\nwo\n0.8193490\n0.05332364\n15.365586\n0.000000e+00\n0.71483657\n0.9238614\n\n\nSimultan\n=~\nhm\n0.3911086\n0.07920465\n4.937950\n7.894795e-07\n0.23587038\n0.5463469\n\n\nSimultan\n=~\ngc\n0.4995366\n0.06083478\n8.211366\n2.220446e-16\n0.38030263\n0.6187706\n\n\nSimultan\n=~\ntr\n0.7173667\n0.04430392\n16.191948\n0.000000e+00\n0.63053266\n0.8042008\n\n\nSimultan\n=~\nsm\n0.6659828\n0.04841875\n13.754648\n0.000000e+00\n0.57108381\n0.7608818\n\n\nSimultan\n=~\nma\n0.5978172\n0.05378964\n11.113985\n0.000000e+00\n0.49239147\n0.7032430\n\n\nSimultan\n=~\nps\n0.7766255\n0.03976264\n19.531536\n0.000000e+00\n0.69869211\n0.8545588\n\n\nhm\n~~\nhm\n0.6825459\n0.06079286\n11.227403\n0.000000e+00\n0.56339406\n0.8016977\n\n\nnr\n~~\nnr\n0.3313300\n0.08720253\n3.799546\n1.449613e-04\n0.16041622\n0.5022438\n\n\nwo\n~~\nwo\n0.3286672\n0.08738134\n3.761298\n1.690341e-04\n0.15740296\n0.4999315\n\n\ngc\n~~\ngc\n0.7504632\n0.06077839\n12.347532\n0.000000e+00\n0.63133972\n0.8695867\n\n\ntr\n~~\ntr\n0.4853850\n0.06356431\n7.636123\n2.242651e-14\n0.36080119\n0.6099687\n\n\nsm\n~~\nsm\n0.5564669\n0.06449211\n8.628450\n0.000000e+00\n0.43006469\n0.6828691\n\n\nma\n~~\nma\n0.6426146\n0.06431274\n9.992026\n0.000000e+00\n0.51656392\n0.7686652\n\n\nps\n~~\nps\n0.3968529\n0.06176136\n6.425586\n1.313629e-10\n0.27580286\n0.5179029\n\n\nSequent\n~~\nSequent\n1.0000000\n0.00000000\nNA\nNA\n1.00000000\n1.0000000\n\n\nSimultan\n~~\nSimultan\n1.0000000\n0.00000000\nNA\nNA\n1.00000000\n1.0000000\n\n\nSequent\n~~\nSimultan\n0.5096198\n0.07025684\n7.253668\n4.056755e-13\n0.37191892\n0.6473207\n\n\n\n\n\n\nsemPlot::semPaths(kabc3,\n    what = \"col\", whatLabels = \"std\", style = \"mx\",\n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\nNei modelli precedenti, abbiamo adottato un metodo di scalatura dei fattori comuni che fissava la saturazione fattoriale di uno degli indicatori per ciascun fattore comune a 1.0 come riferimento. Ora, esploreremo un diverso approccio di scalatura che prevede la standardizzazione della varianza delle variabili latenti.\nPer attuare questa procedura nel software lavaan, è necessario modificare la configurazione predefinita in cui la saturazione fattoriale del primo indicatore di ogni fattore comune è fissata a 1.0. Per fare ciò, useremo la sintassi NA* per indicare che la saturazione fattoriale del primo indicatore deve essere stimata. Questo si realizza inserendo NA* nell’istruzione che definisce la relazione tra le variabili latenti e gli indicatori (espresso tramite =~). Inoltre, è fondamentale specificare che la varianza delle variabili latenti sia fissata a 1.0, il che si attua mediante la sintassi 1* nell’istruzione che stabilisce la varianza di ciascun fattore comune (~~).\n\nkabc3alt_model &lt;- \"\n    Sequent =~ NA*hm + nr + wo\n    Simultan =~ NA*hm + gc + tr + sm + ma + ps\n\n    Sequent ~~ 1*Sequent\n    Simultan ~~ 1*Simultan\n\"\n\nAdattiamo il modello così parametrizzato ai dati.\n\nkabc3alt &lt;- lavaan::sem(\n    kabc3alt_model, sample.cov = kabc.cov, sample.nobs = 200, std.lv = TRUE\n)\n\nEsaminiamo la soluzione non standardizzata.\n\nsemPlot::semPaths(kabc3alt,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\nEsaminiamo la soluzione stanardizzata.\n\nloadings &lt;- standardizedSolution(kabc3alt)\nloadings\n\n\nA lavaan.data.frame: 20 x 9\n\n\nlhs\nop\nrhs\nest.std\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nSequent\n=~\nhm\n0.2525843\n0.08220772\n3.072513\n2.122649e-03\n0.09146008\n0.4137084\n\n\nSequent\n=~\nnr\n0.8177223\n0.05332042\n15.336006\n0.000000e+00\n0.71321623\n0.9222284\n\n\nSequent\n=~\nwo\n0.8193488\n0.05332369\n15.365569\n0.000000e+00\n0.71483633\n0.9238614\n\n\nSimultan\n=~\nhm\n0.3911092\n0.07920463\n4.937959\n7.894445e-07\n0.23587098\n0.5463474\n\n\nSimultan\n=~\ngc\n0.4995384\n0.06083464\n8.211414\n2.220446e-16\n0.38030467\n0.6187721\n\n\nSimultan\n=~\ntr\n0.7173674\n0.04430383\n16.191995\n0.000000e+00\n0.63053351\n0.8042013\n\n\nSimultan\n=~\nsm\n0.6659822\n0.04841876\n13.754631\n0.000000e+00\n0.57108320\n0.7608813\n\n\nSimultan\n=~\nma\n0.5978175\n0.05378959\n11.114002\n0.000000e+00\n0.49239189\n0.7032432\n\n\nSimultan\n=~\nps\n0.7766261\n0.03976255\n19.531596\n0.000000e+00\n0.69869292\n0.8545593\n\n\nSequent\n~~\nSequent\n1.0000000\n0.00000000\nNA\nNA\n1.00000000\n1.0000000\n\n\nSimultan\n~~\nSimultan\n1.0000000\n0.00000000\nNA\nNA\n1.00000000\n1.0000000\n\n\nhm\n~~\nhm\n0.6825461\n0.06079284\n11.227409\n0.000000e+00\n0.56339433\n0.8016979\n\n\nnr\n~~\nnr\n0.3313302\n0.08720260\n3.799545\n1.449621e-04\n0.16041623\n0.5022441\n\n\nwo\n~~\nwo\n0.3286675\n0.08738141\n3.761297\n1.690344e-04\n0.15740304\n0.4999319\n\n\ngc\n~~\ngc\n0.7504614\n0.06077847\n12.347487\n0.000000e+00\n0.63133780\n0.8695850\n\n\ntr\n~~\ntr\n0.4853840\n0.06356425\n7.636116\n2.242651e-14\n0.36080034\n0.6099676\n\n\nsm\n~~\nsm\n0.5564677\n0.06449207\n8.628466\n0.000000e+00\n0.43006551\n0.6828698\n\n\nma\n~~\nma\n0.6426142\n0.06431272\n9.992024\n0.000000e+00\n0.51656358\n0.7686648\n\n\nps\n~~\nps\n0.3968519\n0.06176127\n6.425579\n1.313689e-10\n0.27580204\n0.5179018\n\n\nSequent\n~~\nSimultan\n0.5096198\n0.07025682\n7.253670\n4.056755e-13\n0.37191898\n0.6473207\n\n\n\n\n\n\nrelevant_loadings &lt;- loadings[loadings$op == \"=~\", c(\"lhs\", \"rhs\", \"est.std\")]\nrelevant_loadings\n\n\nA lavaan.data.frame: 9 x 3\n\n\n\nlhs\nrhs\nest.std\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n\n\n\n\n1\nSequent\nhm\n0.2525843\n\n\n2\nSequent\nnr\n0.8177223\n\n\n3\nSequent\nwo\n0.8193488\n\n\n4\nSimultan\nhm\n0.3911092\n\n\n5\nSimultan\ngc\n0.4995384\n\n\n6\nSimultan\ntr\n0.7173674\n\n\n7\nSimultan\nsm\n0.6659822\n\n\n8\nSimultan\nma\n0.5978175\n\n\n9\nSimultan\nps\n0.7766261\n\n\n\n\n\nIdealmente, per sostenere l’ipotesi di validità convergente, un fattore dovrebbe spiegare almeno il 50% della varianza in ciascuno dei suoi indicatori continui, come sostengono Bagozzi e Yi (2012). Ciò implica che, per essere considerato adeguatamente rappresentativo del costrutto che intende misurare, tutti gli indicatori di un fattore dovrebbero mostrare che la maggior parte della loro varianza è spiegata dal fattore stesso. Un modo meno rigoroso ma ancora informativo per valutare la validità convergente è attraverso l’uso della Varianza Media Estratta (AVE), calcolata come la media dei quadrati dei carichi fattoriali standardizzati di tutti gli indicatori associati a un particolare fattore. Un valore AVE superiore a 0.50 indica che, in media, il fattore spiega più della metà della varianza degli indicatori rispetto alla varianza residua attribuibile agli errori di misurazione, come indicato da Hair et al. (2022).\nNell’ambito di un modello a due fattori, i risultati ottenuti dall’esempio in esame evidenziano alcune criticità in relazione al criterio più stringente: il modello non riesce a spiegare una variazione significativa (R^2 &gt; 0.50) per quattro dei otto indicatori, ossia la metà di essi. Tuttavia, se consideriamo l’AVE, i risultati migliorano leggermente per il fattore sequenziale, che spiega in media circa il 52% della varianza dei suoi tre indicatori (AVE = 0.517).\nNella pratica analitica reale, valori di R^2 inferiori a 0.50 sono spesso considerati accettabili. Comrey e Lee (1992) hanno proposto una scala di valutazione gradiente in cui un R^2 superiore a 0.50 è classificato come eccellente, mentre valori approssimativamente pari a 0.40, 0.30, 0.20 e 0.10 sono considerati molto buoni, buoni, sufficienti e scarsi, rispettivamente. Secondo queste linee guida più flessibili, i risultati per gli indicatori del modello CFA a due fattori della KABC-I sono classificati come “eccellenti” (R^2 &gt; 0.50) per tre degli otto indicatori, nessuno è giudicato “scarso” (R^2 circa 0.10), e i rimanenti cinque indicatori presentano valori intermedi. È essenziale sottolineare che queste linee guida non dovrebbero essere applicate in modo indiscriminato in tutti i contesti di CFA o con tutti i tipi di indicatori. Gli indicatori continui, come i punteggi totali nell’esempio citato, tendono a mostrare carichi fattoriali più elevati rispetto agli indicatori ordinali, come quelli basati su scale di risposta tipo Likert.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#considerazioni-finali",
    "href": "chapters/cfa/01_cfa.html#considerazioni-finali",
    "title": "36  Analisti Fattoriale Confermativa",
    "section": "36.9 Considerazioni Finali",
    "text": "36.9 Considerazioni Finali\nL’analisi fattoriale confermativa (CFA) rappresenta uno strumento cruciale nell’ambito delle ricerche psicologiche e sociali, in quanto consente di esaminare modelli di misurazione riflessiva. In questi modelli, i fattori comuni agiscono come proxy per le variabili teoriche. La CFA richiede che il ricercatore definisca preventivamente aspetti critici del modello, come il numero di fattori, l’assegnazione degli indicatori ai fattori e gli schemi di covarianza degli errori.\nNei modelli CFA base, ciascun indicatore continuo è associato a un unico fattore e si presume che gli errori siano indipendenti, formando così una struttura unidimensionale. L’analisi di modelli con più fattori permette di verificare le ipotesi di validità convergente e discriminante.\nÈ anche possibile esplorare modelli CFA che includono covarianze di errore o indicatori correlati a più fattori. Tuttavia, gestire tali modelli è più complesso, specialmente in termini di identificazione del modello. Problemi tecnici come la non convergenza delle soluzioni o risultati inammissibili sono più comuni nei campioni di dimensioni ridotte o quando i fattori sono definiti da soli due indicatori. L’aggiustamento del modello può diventare una sfida, considerata l’ampia varietà di modifiche potenziali.\nUn’altra questione critica è rappresentata dai modelli CFA equivalenti, i quali possono produrre risultati simili nonostante le loro differenze strutturali. Per affrontare queste sfide efficacemente, è essenziale fondare l’analisi più su basi teoriche che su meri calcoli statistici. L’efficacia della CFA, quindi, dipende notevolmente dal contesto teorico e dalla competenza metodologica del ricercatore, essendo cruciale un’approfondita comprensione del dominio di studio per guidare l’analisi.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/01_cfa.html#session-info",
    "href": "chapters/cfa/01_cfa.html#session-info",
    "title": "36  Analisti Fattoriale Confermativa",
    "section": "36.10 Session Info",
    "text": "36.10 Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] nortest_1.0-4     MASS_7.3-61       ggokabeito_0.1.0 \n [4] viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [7] ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0  \n[10] bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n[13] lavaan_0.6-19     psych_2.4.6.26    scales_1.3.0     \n[16] markdown_1.13     knitr_1.48        lubridate_1.9.3  \n[19] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n[22] purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n[25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n[28] here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.16.0  jsonlite_1.8.9     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5.1 farver_2.1.2      \n  [7] nloptr_2.1.1       rmarkdown_2.28     vctrs_0.6.5       \n [10] Cairo_1.6-2        minqa_1.2.8        base64enc_0.1-3   \n [13] rstatix_0.7.2      htmltools_0.5.8.1  broom_1.0.7       \n [16] Formula_1.2-5      htmlwidgets_1.6.4  plyr_1.8.9        \n [19] sandwich_3.1-1     emmeans_1.10.4     zoo_1.8-12        \n [22] uuid_1.2-1         igraph_2.0.3       mime_0.12         \n [25] lifecycle_1.0.4    pkgconfig_2.0.3    Matrix_1.7-0      \n [28] R6_2.5.1           fastmap_1.2.0      shiny_1.9.1       \n [31] digest_0.6.37      OpenMx_2.21.12     fdrtool_1.2.18    \n [34] colorspace_2.1-1   rprojroot_2.0.4    Hmisc_5.1-3       \n [37] fansi_1.0.6        timechange_0.3.0   abind_1.4-8       \n [40] compiler_4.4.1     withr_3.0.1        glasso_1.11       \n [43] htmlTable_2.4.3    backports_1.5.0    carData_3.0-5     \n [46] ggsignif_0.6.4     corpcor_1.6.10     gtools_3.9.5      \n [49] tools_4.4.1        pbivnorm_0.6.0     foreign_0.8-87    \n [52] zip_2.3.1          httpuv_1.6.15      nnet_7.3-19       \n [55] glue_1.8.0         quadprog_1.5-8     promises_1.3.0    \n [58] nlme_3.1-166       lisrelToR_0.3      grid_4.4.1        \n [61] pbdZMQ_0.3-13      checkmate_2.3.2    cluster_2.1.6     \n [64] reshape2_1.4.4     generics_0.1.3     gtable_0.3.5      \n [67] tzdb_0.4.0         data.table_1.16.0  hms_1.1.3         \n [70] car_3.1-3          utf8_1.2.4         sem_3.1-16        \n [73] pillar_1.9.0       IRdisplay_1.1      rockchalk_1.8.157 \n [76] later_1.3.2        splines_4.4.1      lattice_0.22-6    \n [79] survival_3.7-0     kutils_1.73        tidyselect_1.2.1  \n [82] miniUI_0.1.1.1     pbapply_1.7-2      stats4_4.4.1      \n [85] xfun_0.48          qgraph_1.9.8       arm_1.14-4        \n [88] stringi_1.8.4      pacman_0.5.1       boot_1.3-31       \n [91] evaluate_1.0.0     codetools_0.2-20   mi_1.1            \n [94] cli_3.6.3          RcppParallel_5.1.9 IRkernel_1.3.2    \n [97] rpart_4.1.23       xtable_1.8-4       repr_1.1.7        \n[100] munsell_0.5.1      Rcpp_1.0.13        coda_0.19-4.1     \n[103] png_0.1-8          XML_3.99-0.17      parallel_4.4.1    \n[106] jpeg_0.1-10        lme4_1.1-35.5      mvtnorm_1.3-1     \n[109] openxlsx_4.2.7.1   crayon_1.5.3       rlang_1.1.4       \n[112] multcomp_1.4-26    mnormt_2.1.1      \n\n\n\n\n\n\nKline, R. B. (2023). Principles and practice of structural equation modeling. Guilford publications.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Analisti Fattoriale Confermativa</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/02_meanstructure.html",
    "href": "chapters/cfa/02_meanstructure.html",
    "title": "37  La struttura delle medie",
    "section": "",
    "text": "37.1 Introduzione\nNei modelli di equazioni strutturali (SEM), simili all’analisi fattoriale, esaminiamo principalmente le relazioni di covarianza tra le variabili. Una caratteristica distintiva dei modelli SEM rispetto all’analisi fattoriale tradizionale è la possibilità di includere le medie sia delle variabili osservate che di quelle latenti. Questo è particolarmente utile in modelli come quelli di analisi fattoriale confermativa (CFA) longitudinale, dove le ipotesi si concentrano sulle medie dei costrutti analizzati.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>La struttura delle medie</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/02_meanstructure.html#interpretazione-delle-intercette-nei-modelli-sem",
    "href": "chapters/cfa/02_meanstructure.html#interpretazione-delle-intercette-nei-modelli-sem",
    "title": "37  La struttura delle medie",
    "section": "37.2 Interpretazione delle Intercette nei Modelli SEM",
    "text": "37.2 Interpretazione delle Intercette nei Modelli SEM\nIn un modello SEM, l’intercetta di una variabile indicatore (denotata con \\(\\tau\\)) indica la media stimata di quella variabile. Il valore di \\(\\tau\\) rappresenta il valore atteso dell’indicatore quando il fattore latente a cui è associato è zero. La relazione generale per un indicatore \\(y\\) in un modello SEM è data dalla formula:\n\\[\ny = \\tau + \\lambda \\cdot \\text{fattore latente} + \\varepsilon,\n\\]\ndove:\n\n\\(y\\) è il punteggio osservato dell’indicatore.\n\\(\\lambda\\) rappresenta il carico fattoriale, che indica quanto fortemente l’indicatore è influenzato dal fattore latente.\n\\(\\tau\\) è l’intercetta, cioè la media stimata dell’indicatore.\n\\(\\varepsilon\\) è l’errore di misura associato all’indicatore.\n\n\n37.2.1 Struttura delle Medie nel Modello CFA\nNel contesto di un modello CFA, la struttura delle medie è descritta dalla formula:\n\\[ \\text{media(variabile latente)} = \\Lambda \\mu_{\\text{lat}} + \\tau, \\]\nqui:\n\n\\(\\Lambda\\) è la matrice dei carichi fattoriali.\n\\(\\mu_{\\text{lat}}\\) è il vettore che rappresenta le medie dei costrutti latenti.\n\\(\\tau\\) è il vettore delle intercette degli indicatori.\n\n\n\n37.2.2 Utilizzo delle Medie nel Software lavaan\nNel software lavaan, utilizzato per l’analisi SEM, è possibile stimare le intercette inserendo l’opzione meanstructure = TRUE nella sintassi del modello. Questo comando permette di includere automaticamente una costante “1” in tutte le equazioni del modello, facilitando così il calcolo delle intercette per le variabili endogene. È necessario fornire i dati originali o una matrice di covarianza, insieme alle medie di tutte le variabili interessate.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>La struttura delle medie</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/02_meanstructure.html#un-esempio-pratico",
    "href": "chapters/cfa/02_meanstructure.html#un-esempio-pratico",
    "title": "37  La struttura delle medie",
    "section": "37.3 Un Esempio Pratico",
    "text": "37.3 Un Esempio Pratico\nUtilizziamo il dataset HolzingerSwineford1939 per costruire un modello di misurazione con tre costrutti latenti (visual, textual, speed), ciascuno definito da tre indicatori (x1, x2, x3, ecc.).\n\ndata(HolzingerSwineford1939)\nglimpse(HolzingerSwineford1939)\n\nRows: 301\nColumns: 15\n$ id     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, ~\n$ sex    &lt;int&gt; 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, ~\n$ ageyr  &lt;int&gt; 13, 13, 13, 13, 12, 14, 12, 12, 13, 12, 12, 12, 12, 12, 12, 12,~\n$ agemo  &lt;int&gt; 1, 7, 1, 2, 2, 1, 1, 2, 0, 5, 2, 11, 7, 8, 6, 1, 11, 5, 8, 3, 1~\n$ school &lt;fct&gt; Pasteur, Pasteur, Pasteur, Pasteur, Pasteur, Pasteur, Pasteur, ~\n$ grade  &lt;int&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ~\n$ x1     &lt;dbl&gt; 3.333333, 5.333333, 4.500000, 5.333333, 4.833333, 5.333333, 2.8~\n$ x2     &lt;dbl&gt; 7.75, 5.25, 5.25, 7.75, 4.75, 5.00, 6.00, 6.25, 5.75, 5.25, 5.7~\n$ x3     &lt;dbl&gt; 0.375, 2.125, 1.875, 3.000, 0.875, 2.250, 1.000, 1.875, 1.500, ~\n$ x4     &lt;dbl&gt; 2.333333, 1.666667, 1.000000, 2.666667, 2.666667, 1.000000, 3.3~\n$ x5     &lt;dbl&gt; 5.75, 3.00, 1.75, 4.50, 4.00, 3.00, 6.00, 4.25, 5.75, 5.00, 3.5~\n$ x6     &lt;dbl&gt; 1.2857143, 1.2857143, 0.4285714, 2.4285714, 2.5714286, 0.857142~\n$ x7     &lt;dbl&gt; 3.391304, 3.782609, 3.260870, 3.000000, 3.695652, 4.347826, 4.6~\n$ x8     &lt;dbl&gt; 5.75, 6.25, 3.90, 5.30, 6.30, 6.65, 6.20, 5.15, 4.65, 4.55, 5.7~\n$ x9     &lt;dbl&gt; 6.361111, 7.916667, 4.416667, 4.861111, 5.916667, 7.500000, 4.8~\n\n\n\nhs_model &lt;- \"\n    visual =~ NA*x1 + x2 + x3\n    textual =~ NA*x4 + x5 + x6\n    speed =~ NA*x7 + x8 + x9\n\n    visual ~~ 1*visual\n    textual ~~ 1 * textual\n    speed ~~ 1 * speed\n\"\n\nUtilizziamo l’argomento meanstructure = TRUE per richiedere la stima delle intercette degli indicatori \\(\\tau\\).\n\nOgni costrutto latente è definito in relazione ai suoi indicatori, dove le intercette degli indicatori (\\(\\tau\\)) non sono fissate a priori, ma stimate dal modello.\nLe varianze dei costrutti latenti sono fissate a 1, mentre le loro medie sono fissate a 0 (come evidenziato dall’output, righe 34-36).\nNel caso presente, poiché le medie dei costrutti latenti sono fissate a zero, la media predetta per gli indicatori corrisponde alle intercette stimate.\n\n\nfit &lt;- cfa(hs_model,\n    data = HolzingerSwineford1939,\n    meanstructure = TRUE\n)\n\n\nparameterEstimates(fit) |&gt;\n    print()\n\n       lhs op     rhs   est    se      z pvalue ci.lower ci.upper\n1   visual =~      x1 0.900 0.081 11.128      0    0.741    1.058\n2   visual =~      x2 0.498 0.077  6.429      0    0.346    0.650\n3   visual =~      x3 0.656 0.074  8.817      0    0.510    0.802\n4  textual =~      x4 0.990 0.057 17.474      0    0.879    1.101\n5  textual =~      x5 1.102 0.063 17.576      0    0.979    1.224\n6  textual =~      x6 0.917 0.054 17.082      0    0.811    1.022\n7    speed =~      x7 0.619 0.070  8.903      0    0.483    0.756\n8    speed =~      x8 0.731 0.066 11.090      0    0.602    0.860\n9    speed =~      x9 0.670 0.065 10.305      0    0.543    0.797\n10  visual ~~  visual 1.000 0.000     NA     NA    1.000    1.000\n11 textual ~~ textual 1.000 0.000     NA     NA    1.000    1.000\n12   speed ~~   speed 1.000 0.000     NA     NA    1.000    1.000\n13      x1 ~~      x1 0.549 0.114  4.833      0    0.326    0.772\n14      x2 ~~      x2 1.134 0.102 11.146      0    0.934    1.333\n15      x3 ~~      x3 0.844 0.091  9.317      0    0.667    1.022\n16      x4 ~~      x4 0.371 0.048  7.779      0    0.278    0.465\n17      x5 ~~      x5 0.446 0.058  7.642      0    0.332    0.561\n18      x6 ~~      x6 0.356 0.043  8.277      0    0.272    0.441\n19      x7 ~~      x7 0.799 0.081  9.823      0    0.640    0.959\n20      x8 ~~      x8 0.488 0.074  6.573      0    0.342    0.633\n21      x9 ~~      x9 0.566 0.071  8.003      0    0.427    0.705\n22  visual ~~ textual 0.459 0.064  7.189      0    0.334    0.584\n23  visual ~~   speed 0.471 0.073  6.461      0    0.328    0.613\n24 textual ~~   speed 0.283 0.069  4.117      0    0.148    0.418\n25      x1 ~1         4.936 0.067 73.473      0    4.804    5.067\n26      x2 ~1         6.088 0.068 89.855      0    5.955    6.221\n27      x3 ~1         2.250 0.065 34.579      0    2.123    2.378\n28      x4 ~1         3.061 0.067 45.694      0    2.930    3.192\n29      x5 ~1         4.341 0.074 58.452      0    4.195    4.486\n30      x6 ~1         2.186 0.063 34.667      0    2.062    2.309\n31      x7 ~1         4.186 0.063 66.766      0    4.063    4.309\n32      x8 ~1         5.527 0.058 94.854      0    5.413    5.641\n33      x9 ~1         5.374 0.058 92.546      0    5.260    5.488\n34  visual ~1         0.000 0.000     NA     NA    0.000    0.000\n35 textual ~1         0.000 0.000     NA     NA    0.000    0.000\n36   speed ~1         0.000 0.000     NA     NA    0.000    0.000\n\n\n\n37.3.1 Interpretazione delle Medie Stimate\nLa media dei punteggi osservati per gli indicatori (x1, x2, x3, ecc.) viene calcolata attraverso le intercette stimate dal modello. È fondamentale distinguere tra la media empirica, calcolata direttamente dai dati, e la media predetta dal modello. La media predetta degli indicatori in un modello dove la media dei costrutti latenti è fissata a zero è influenzata esclusivamente dalle loro intercette.\n\n\n37.3.2 Calcolo delle Medie Osservate e Predette in R\nConsideriamo gli indicatori x1, x2, x3. Per calcolare la media osservata di questi indicatori, usiamo le loro intercette stimate.\n\nintercepts &lt;- params$est[params$op == \"~1\"][1:9] # Intercette degli indicatori (τ)\n\nQuesto ci fornisce le intercette degli indicatori:\n\nintercepts |&gt; print()\n\n[1] 4.935770 6.088040 2.250415 3.060908 4.340532 2.185572 4.185902 5.527076\n[9] 5.374123\n\n\nPer ottenere la media osservata dei punteggi di x1, x2, x3, calcoliamo la media aritmetica delle loro intercette:\n\n mean_observed_scores &lt;- mean(intercepts[1:3])\n print(mean_observed_scores)\n\n[1] 4.424742\n\n\nQuesto valore rappresenta la media osservata calcolata come la media aritmetica delle intercette di x1, x2, x3. Nel contesto del nostro modello CFA, dove la media dei costrutti latenti è fissata a zero, la media predetta degli indicatori corrisponde alla media osservata:\n\nmean((HolzingerSwineford1939$x1 + HolzingerSwineford1939$x2 + HolzingerSwineford1939$x3) / 3) \n\n4.42474160196013\n\n\n\nmean_predicted_scores &lt;- mean_observed_scores\nprint(mean_predicted_scores)\n\n[1] 4.424742\n\n\n\n\n37.3.3 Medie di Costrutti Latenti Non Zero\nIn situazioni in cui le medie dei costrutti latenti non sono fissate a zero, la media predetta degli indicatori è influenzata sia dalle intercette sia dai carichi fattoriali. Per esempio, se la media del costrutto latente fosse diversa da zero, l’equazione per calcolare la media di un indicatore (come x1) includerebbe il contributo del costrutto latente:\n\\[\n\\text{media predetta}(x1) = \\mu_{\\text{latente}} \\cdot \\lambda_{x1} + \\tau_{x1},\n\\]\ndove:\n\n\\(\\mu_{\\text{latente}}\\) è la media stimata del costrutto latente.\n\\(\\lambda_{x1}\\) è il carico dell’indicatore x1.\n\\(\\tau_{x1}\\) è l’intercetta stimata dell’indicatore x1.\n\nEsaminiamo un esempio nel quale le medie dei fattori latenti non sono fissate a zero. Per ottenere questo risultato è necessario identificare il modello introducendo due vincoli:\n\nl’intercetta degli indicatori è fissata a zero;\nuna delle intercette delle variabili latenti è fissata a zero.\n\n\nhs_model &lt;- \"\n    visual =~ NA*x1 + x2 + x3\n    textual =~ NA*x4 + x5 + x6\n    speed =~ NA*x7 + x8 + x9\n\n    visual ~~ 1*visual\n    textual ~~ 1*textual\n    speed ~~ 1*speed\n\n    x1 ~ 0*1 # Setting the intercepts of the manifest\n    x4 ~ 0*1 # variables to zero\n    x7 ~ 0*1\n\n    visual ~ 0*1 # Setting the mean of visual to zero\n    textual ~ 1 # freely estimating the mean of textual\n    speed ~ 1 # freely estimating the mean of speed\n\"\n\n\n# Fit del modello con la struttura delle medie\nfit &lt;- cfa(hs_model, data = HolzingerSwineford1939, meanstructure = TRUE)\n\n\nparams &lt;- parameterEstimates(fit)\nprint(params)\n\n       lhs op     rhs    est    se       z pvalue ci.lower ci.upper\n1   visual =~      x1  4.983 0.212  23.538  0.000    4.568    5.398\n2   visual =~      x2  1.702 0.093  18.248  0.000    1.519    1.885\n3   visual =~      x3  2.243 0.106  21.099  0.000    2.035    2.451\n4  textual =~      x4  1.783 0.081  21.945  0.000    1.624    1.942\n5  textual =~      x5  1.985 0.090  22.007  0.000    1.808    2.162\n6  textual =~      x6  1.652 0.076  21.702  0.000    1.502    1.801\n7    speed =~      x7  1.136 0.072  15.882  0.000    0.996    1.277\n8    speed =~      x8  1.341 0.070  19.127  0.000    1.204    1.478\n9    speed =~      x9  1.229 0.068  17.979  0.000    1.095    1.363\n10  visual ~~  visual  1.000 0.000      NA     NA    1.000    1.000\n11 textual ~~ textual  1.000 0.000      NA     NA    1.000    1.000\n12   speed ~~   speed  1.000 0.000      NA     NA    1.000    1.000\n13      x1 ~1          0.000 0.000      NA     NA    0.000    0.000\n14      x4 ~1          0.000 0.000      NA     NA    0.000    0.000\n15      x7 ~1          0.000 0.000      NA     NA    0.000    0.000\n16  visual ~1          0.000 0.000      NA     NA    0.000    0.000\n17 textual ~1          0.885 0.054  16.405  0.000    0.779    0.990\n18   speed ~1          2.845 0.187  15.207  0.000    2.478    3.211\n19      x1 ~~      x1  0.890 0.257   3.457  0.001    0.385    1.394\n20      x2 ~~      x2  1.134 0.100  11.368  0.000    0.938    1.329\n21      x3 ~~      x3  0.844 0.087   9.719  0.000    0.674    1.015\n22      x4 ~~      x4  0.371 0.045   8.239  0.000    0.283    0.459\n23      x5 ~~      x5  0.446 0.055   8.116  0.000    0.338    0.554\n24      x6 ~~      x6  0.356 0.041   8.679  0.000    0.276    0.437\n25      x7 ~~      x7  0.799 0.077  10.411  0.000    0.649    0.950\n26      x8 ~~      x8  0.488 0.062   7.915  0.000    0.367    0.608\n27      x9 ~~      x9  0.566 0.062   9.132  0.000    0.445    0.688\n28  visual ~~ textual  0.870 0.016  53.410  0.000    0.838    0.902\n29  visual ~~   speed  0.877 0.019  47.025  0.000    0.840    0.913\n30 textual ~~   speed  0.783 0.027  28.490  0.000    0.729    0.837\n31      x2 ~1          4.460 0.064  69.660  0.000    4.335    4.586\n32      x3 ~1          0.106 0.058   1.814  0.070   -0.008    0.220\n33      x5 ~1          0.934 0.075  12.485  0.000    0.787    1.080\n34      x6 ~1         -0.649 0.064 -10.085  0.000   -0.775   -0.523\n35      x8 ~1          0.588 0.236   2.496  0.013    0.126    1.050\n36      x9 ~1          0.847 0.226   3.742  0.000    0.403    1.291\n\n\nIn sintesi, la media predetta degli indicatori in un modello SEM può variare a seconda della configurazione delle medie dei costrutti latenti e del contributo dei carichi fattoriali.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>La struttura delle medie</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/02_meanstructure.html#session-info",
    "href": "chapters/cfa/02_meanstructure.html#session-info",
    "title": "37  La struttura delle medie",
    "section": "37.4 Session Info",
    "text": "37.4 Session Info\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [5] ggExtra_0.10.1    bayesplot_1.11.1  gridExtra_2.3     patchwork_1.2.0  \n [9] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-17     psych_2.4.3      \n[13] scales_1.3.0      markdown_1.12     knitr_1.45        lubridate_1.9.3  \n[17] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[21] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.0    \n[25] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5   nloptr_2.0.3      \n  [7] rmarkdown_2.26     vctrs_0.6.5        minqa_1.2.6       \n [10] base64enc_0.1-3    rstatix_0.7.2      htmltools_0.5.7   \n [13] broom_1.0.5        Formula_1.2-5      htmlwidgets_1.6.4 \n [16] plyr_1.8.9         sandwich_3.1-0     emmeans_1.10.0    \n [19] zoo_1.8-12         uuid_1.2-0         igraph_2.0.2      \n [22] mime_0.12          lifecycle_1.0.4    pkgconfig_2.0.3   \n [25] Matrix_1.6-5       R6_2.5.1           fastmap_1.1.1     \n [28] shiny_1.8.0        digest_0.6.35      OpenMx_2.21.11    \n [31] fdrtool_1.2.17     colorspace_2.1-0   rprojroot_2.0.4   \n [34] Hmisc_5.1-1        fansi_1.0.6        timechange_0.3.0  \n [37] abind_1.4-5        compiler_4.3.3     withr_3.0.0       \n [40] glasso_1.11        htmlTable_2.4.2    backports_1.4.1   \n [43] carData_3.0-5      ggsignif_0.6.4     MASS_7.3-60.0.1   \n [46] corpcor_1.6.10     gtools_3.9.5       tools_4.3.3       \n [49] pbivnorm_0.6.0     foreign_0.8-86     zip_2.3.1         \n [52] httpuv_1.6.14      nnet_7.3-19        glue_1.7.0        \n [55] quadprog_1.5-8     nlme_3.1-164       promises_1.2.1    \n [58] lisrelToR_0.3      grid_4.3.3         pbdZMQ_0.3-11     \n [61] checkmate_2.3.1    cluster_2.1.6      reshape2_1.4.4    \n [64] generics_0.1.3     gtable_0.3.4       tzdb_0.4.0        \n [67] data.table_1.15.2  hms_1.1.3          car_3.1-2         \n [70] utf8_1.2.4         sem_3.1-15         pillar_1.9.0      \n [73] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [76] splines_4.3.3      lattice_0.22-5     survival_3.5-8    \n [79] kutils_1.73        tidyselect_1.2.0   miniUI_0.1.1.1    \n [82] pbapply_1.7-2      stats4_4.3.3       xfun_0.42         \n [85] qgraph_1.9.8       arm_1.13-1         stringi_1.8.3     \n [88] boot_1.3-29        evaluate_0.23      codetools_0.2-19  \n [91] mi_1.1             cli_3.6.2          RcppParallel_5.1.7\n [94] IRkernel_1.3.2     rpart_4.1.23       xtable_1.8-4      \n [97] repr_1.1.6         munsell_0.5.0      Rcpp_1.0.12       \n[100] coda_0.19-4.1      png_0.1-8          XML_3.99-0.16.1   \n[103] parallel_4.3.3     ellipsis_0.3.2     jpeg_0.1-10       \n[106] lme4_1.1-35.1      mvtnorm_1.2-4      openxlsx_4.2.5.2  \n[109] crayon_1.5.2       rlang_1.1.3        multcomp_1.4-25   \n[112] mnormt_2.1.1",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>La struttura delle medie</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/03_cat_data.html",
    "href": "chapters/cfa/03_cat_data.html",
    "title": "38  Dati non gaussiani e categoriali",
    "section": "",
    "text": "38.1 Introduzione\nNel materiale precedente di questa dispensa è stato discusso l’utilizzo dello stimatore di massima verosimiglianza (ML), comunemente adottato nei modelli di Analisi Fattoriale Confermativa (CFA) e Structural Equation Modeling (SEM) presenti nella letteratura di ricerca applicata. Tuttavia, l’uso dello stimatore ML è appropriato esclusivamente per dati multivariati normali, ovvero quando la distribuzione congiunta delle variabili continue è normalmente distribuita. In presenza di dati continui che presentano una forte deviazione dalla normalità, come asimmetria o curtosi elevate, o quando gli indicatori non sono a livello di scala intervallare (per esempio, dati binari, politomici o ordinali), è consigliabile adottare stimatori alternativi al ML.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Dati non gaussiani e categoriali</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/03_cat_data.html#dati-non-gaussiani-e-stimatori-alternativi",
    "href": "chapters/cfa/03_cat_data.html#dati-non-gaussiani-e-stimatori-alternativi",
    "title": "38  Dati non gaussiani e categoriali",
    "section": "38.2 Dati non Gaussiani e Stimatori Alternativi",
    "text": "38.2 Dati non Gaussiani e Stimatori Alternativi\nNonostante la stima di massima verosimiglianza (ML) rimanga robusta a piccole deviazioni dalla normalità, situazioni di marcata non normalità richiedono l’adozione di stimatori alternativi per preservare l’affidabilità statistica. L’uso del ML in tali condizioni può portare a:\n\nSovrastima della statistica chi-quadrato (\\(\\chi^2\\)) del modello;\nSottostima degli indici di bontà di adattamento, come il Tucker-Lewis Index (TLI) e il Comparative Fit Index (CFI);\nSottostima degli errori standard delle stime dei parametri.\n\nQuesti problemi si accentuano in campioni di dimensioni ridotte. Per mitigare tali effetti, si raccomanda l’uso dei seguenti stimatori:\n\nGLS (Generalized Least Squares):\n\nUso: Adatto per dati completi senza valori mancanti.\nFunzione di Discrepanza: La funzione di discrepanza del GLS misura quanto la matrice di covarianza stimata dal modello (\\(\\Sigma(\\theta)\\)) si differenzia dalla matrice di covarianza osservata (\\(S\\)). La formula \\(F_{\\text{GLS}}(S, \\Sigma(\\theta)) = \\frac{1}{2} \\text{traccia}(S - \\Sigma(\\theta))^2\\) utilizza la traccia (la somma degli elementi sulla diagonale principale della matrice) per quantificare questa differenza.\nInterpretazione: Un valore più basso della funzione di discrepanza indica un migliore adattamento del modello ai dati.\n\nWLS (Weighted Least Squares):\n\nUso: Conosciuto come stimatore Asintoticamente Libero da Distribuzione (ADF), utile per dati complessi.\nFunzione di Discrepanza: \\(F_{\\text{ADF}}(S, \\Sigma(\\theta)) = \\text{vecs}(S - \\Sigma(\\theta))'W\\text{vecs}(S - \\Sigma(\\theta))\\). Qui, vecs() trasforma la matrice di covarianza in un vettore (prendendo solo la parte inferiore della matrice), e W è una matrice di pesi che dà diversa importanza ai vari elementi nel calcolo della discrepanza.\nInterpretazione: Un valore più basso indica che il modello si adatta meglio ai dati, tenendo conto della ponderazione specifica di W.\n\nDWLS (Diagonally Weighted Least Squares):\n\nUso: Una versione semplificata di WLS.\nFunzione di Discrepanza: \\(F_{\\text{DWLS}}(S, \\Sigma(\\theta)) = \\text{vecs}(S - \\Sigma(\\theta))'D\\text{vecs}(S - \\Sigma(\\theta))\\), dove D è una matrice di pesi diagonale.\nInterpretazione: Simile a WLS, ma semplifica i calcoli usando solo una matrice di pesi diagonale, che considera solo gli elementi sulla diagonale della matrice di covarianza.\n\nULS (Unweighted Least Squares):\n\nUso: Considerato un caso speciale di WLS.\nFunzione di Discrepanza: \\(F_{\\text{ULS}}(S, \\Sigma(\\theta)) = \\text{vecs}(S - \\Sigma(\\theta))'\\text{vecs}(S - \\Sigma(\\theta))\\). Qui, si utilizza una matrice di identità come peso, il che significa che tutti gli elementi hanno lo stesso peso nel calcolo della discrepanza.\nInterpretazione: Un approccio più diretto rispetto a WLS, che non pondera gli elementi in modo diverso. Un valore più basso indica un migliore adattamento del modello.\n\n\nIn sintesi, questi stimatori vengono utilizzati per valutare quanto bene un modello SEM si adatti ai dati. Differiscono nel modo in cui trattano le discrepanze tra i dati osservati e quelli stimati dal modello, e ciascuno ha specifiche situazioni in cui risulta più appropriato.\n\n38.2.1 ML Robusto: Adattamento in Presenza di Non Normalità\nOltre ai quattro metodi di stima già menzionati (GLS, WLS, DWLS, ULS), un altro stimatore importante nel contesto del Structural Equation Modeling (SEM) è il ML Robusto (Robust Maximum Likelihood). Il ML Robusto è una variante della stima di massima verosimiglianza tradizionale, progettata per migliorare l’affidabilità statistica quando i dati deviano significativamente dalla normalità. Questo stimatore: - Corregge la Sovrastima di \\(\\chi^2\\): Offre una correzione alla sovrastima della statistica chi-quadrato tipica del ML tradizionale. - Errore Standard Affidabile: Fornisce stime più accurate degli errori standard, cruciali in presenza di non normalità. - Migliora Indici di Bontà di Adattamento: Offre valutazioni più precise di indici come TLI e CFI.\nIn conclusione, l’adozione di stimatori come il ML Robusto o il WLS si rivela essenziale per garantire l’integrità delle analisi SEM in presenza di dati non normali, specialmente quando le dimensioni del campione sono limitate o i dati presentano caratteristiche complesse.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Dati non gaussiani e categoriali</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/03_cat_data.html#un-esempio-concreto",
    "href": "chapters/cfa/03_cat_data.html#un-esempio-concreto",
    "title": "38  Dati non gaussiani e categoriali",
    "section": "38.3 Un Esempio Concreto",
    "text": "38.3 Un Esempio Concreto\nEsaminiamo qui un esempio discusso da Brown (2015).\n\nIn questo esempio useremo i dati artificiali presentati da Brown (2015) nelle tabelle 9.5 – 9.7.\n\n\nd &lt;- readRDS(here::here(\"data\", \"brown_table_9_5_data.RDS\"))\nhead(d)\n\n\nA data.frame: 6 x 5\n\n\n\nx1\nx2\nx3\nx4\nx5\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n\n\n4\n4\n2\n2\n1\n1\n\n\n5\n1\n0\n1\n6\n0\n\n\n6\n0\n0\n0\n0\n0\n\n\n\n\n\nLe statistiche descrittive di questo campione di dati mostrano valori eccessivi di asimmetria e di curtosi.\n\npsych::describe(d)\n\n\nA psych: 5 x 13\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nx1\n1\n870\n1.4701149\n2.172832\n0\n1.0086207\n0\n0\n8\n8\n1.506406\n1.252591\n0.07366591\n\n\nx2\n2\n870\n0.8229885\n1.601474\n0\n0.4152299\n0\n0\n8\n8\n2.398394\n5.670143\n0.05429505\n\n\nx3\n3\n870\n1.2655172\n2.070024\n0\n0.7772989\n0\n0\n8\n8\n1.797942\n2.343203\n0.07018040\n\n\nx4\n4\n870\n1.0264368\n1.928047\n0\n0.5359195\n0\n0\n8\n8\n2.157445\n3.977564\n0.06536693\n\n\nx5\n5\n870\n0.6068966\n1.519175\n0\n0.1839080\n0\n0\n8\n8\n3.103965\n9.373781\n0.05150485\n\n\n\n\n\nDefiniamo un modello ad un fattore e, seguendo {cite:t}brown2015confirmatory, aggiungiamo una correlazione residua tra gli indicatori X1 e X3:\n\nmodel &lt;- '\n  f1 =~ x1 + x2 + x3 + x4 + x5\n  x1 ~~ x3 \n'\n\nProcediamo alla stima dei parametri utilizzando uno stimatore di ML robusto. La sintassi lavaan è la seguente:\n\nfit &lt;- cfa(model, data = d, mimic = \"MPLUS\", estimator = \"MLM\")\n\nPer esaminare la soluzione ottenuta ci focalizziamo sulla statistica \\(\\chi^2\\) – si consideri la soluzione robusta fornita nell’output.\n\nout &lt;- summary(fit)\nprint(out)\n\nlavaan 0.6.17 ended normally after 28 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        16\n\n  Number of observations                           870\n\nModel Test User Model:\n                                               Standard      Scaled\n  Test Statistic                                 25.913      10.356\n  Degrees of freedom                                  4           4\n  P-value (Chi-square)                            0.000       0.035\n  Scaling correction factor                                   2.502\n    Satorra-Bentler correction (Mplus variant)                     \n\nParameter Estimates:\n\n  Standard errors                           Robust.sem\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  f1 =~                                               \n    x1                1.000                           \n    x2                0.703    0.062   11.338    0.000\n    x3                1.068    0.044   24.304    0.000\n    x4                0.918    0.063   14.638    0.000\n    x5                0.748    0.055   13.582    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .x1 ~~                                               \n   .x3                0.655    0.143    4.579    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                1.470    0.074   19.968    0.000\n   .x2                0.823    0.054   15.166    0.000\n   .x3                1.266    0.070   18.043    0.000\n   .x4                1.026    0.065   15.712    0.000\n   .x5                0.607    0.051   11.790    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                2.040    0.228    8.952    0.000\n   .x2                1.241    0.124   10.019    0.000\n   .x3                1.227    0.169    7.255    0.000\n   .x4                1.458    0.177    8.233    0.000\n   .x5                0.807    0.100    8.063    0.000\n    f1                2.675    0.289    9.273    0.000\n\n\n\nPer fare un confronto, adattiamo lo stesso modello ai dati usando lo stimatore di ML.\n\nfit2 &lt;- cfa(model, data = d)\n\nNotiamo come il valore della statistica \\(\\chi^2\\) ora ottenuto sia molto maggiore di quello trovato in precedenza.\n\nout &lt;- summary(fit2)\nprint(out)\n\nlavaan 0.6.17 ended normally after 28 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        11\n\n  Number of observations                           870\n\nModel Test User Model:\n                                                      \n  Test statistic                                25.913\n  Degrees of freedom                                 4\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  f1 =~                                               \n    x1                1.000                           \n    x2                0.703    0.035   20.133    0.000\n    x3                1.068    0.034   31.730    0.000\n    x4                0.918    0.042   21.775    0.000\n    x5                0.748    0.033   22.416    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .x1 ~~                                               \n   .x3                0.655    0.091    7.213    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                2.040    0.128   15.897    0.000\n   .x2                1.241    0.070   17.671    0.000\n   .x3                1.227    0.095   12.942    0.000\n   .x4                1.458    0.090   16.135    0.000\n   .x5                0.807    0.053   15.119    0.000\n    f1                2.675    0.220   12.154    0.000\n\n\n\n\n38.3.1 Dati Categoriali\nNella discussione precedente, abbiamo esaminato il modello CFA presupponendo che i dati fossero continui e normalmente distribuiti in maniera multivariata. Tuttavia, abbiamo anche trattato la stima robusta per dati non normalmente distribuiti. Ora, è fondamentale riconoscere che molti dei dati utilizzati nelle analisi fattoriali confermative (CFA) o SEM provengono da questionari e scale di tipo Likert, che producono dati categoriali, inclusi formati binari, ordinali e nominali. Questi dati sono di natura ordinale e non sono continui.\nL’uso del metodo di massima verosimiglianza (ML) ordinario non è raccomandato quando si analizzano dati con almeno un indicatore categoriale. Trattare tali variabili come se fossero continue può portare a varie conseguenze indesiderate, tra cui:\n\nStime Attenuate delle Relazioni: Le relazioni tra gli indicatori possono risultare attenuate, specialmente se influenzate da effetti di pavimento o soffitto.\nEmergenza di “Pseudo-Fattori”: La possibilità di identificare falsi fattori, che non rappresentano veri costrutti ma sono piuttosto artefatti del metodo statistico utilizzato.\nDistorsione degli Indici di Bontà di Adattamento e delle Stime degli Errori Standard: Questi indici, che valutano la qualità dell’adattamento del modello, possono essere distorti, così come le stime degli errori standard.\nStime Errate dei Parametri: I parametri del modello potrebbero essere stimati in modo inaccurato.\n\nPer mitigare questi problemi, esistono stimatori specifici per i dati categoriali, tra cui:\n\nWLS (Weighted Least Squares): Adatto per dati categoriali, considera il peso specifico di ciascuna osservazione.\nWLSMV (Weighted Least Squares Mean and Variance Adjusted): Una versione modificata di WLS che si adatta meglio alle peculiarità dei dati categoriali.\nULS (Unweighted Least Squares): Questo stimatore non prevede ponderazioni e può essere utile per dati categoriali senza presupporre pesi specifici.\n\nNelle sezioni seguenti, approfondiremo l’approccio CFA per dati categoriali, evidenziando le specificità e le migliori pratiche per gestire questo tipo di dati nelle analisi CFA. Questo ci permetterà di effettuare inferenze più accurate, preservando l’integrità e la validità delle conclusioni derivanti dalle analisi.\n\n\n38.3.2 Un esempio concreto\nNell’esempio discusso da {cite:t}brown2015confirmatory, i ricercatori desiderano verificare un modello uni-fattoriale di dipendenza da alcol in un campione di 750 pazienti ambulatoriali. Gli indicatori di alcolismo sono item binari che riflettono la presenza/assenza di sei criteri diagnostici per l’alcolismo (0 = criterio non soddisfatto, 1 = criterio soddisfatto). I dati sono i seguenti:\n\nd1 &lt;- readRDS(here::here(\"data\", \"brown_table_9_9_data.RDS\"))\nhead(d1)\n\n\nA data.frame: 6 x 6\n\n\n\ny1\ny2\ny3\ny4\ny5\ny6\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n1\n1\n1\n1\n1\n1\n\n\n2\n1\n1\n1\n1\n1\n1\n\n\n3\n1\n1\n1\n1\n1\n0\n\n\n4\n1\n1\n1\n1\n1\n1\n\n\n5\n0\n0\n0\n0\n0\n0\n\n\n6\n1\n1\n0\n1\n1\n1\n\n\n\n\n\nÈ possibile evidenziare la natura ordinale dei dati esaminando le tabelle bivariate che mostrano la frequenza di combinazioni specifiche tra due variabili.\n\nxtabs(~ y1 + y2, d1)\n\n   y2\ny1    0   1\n  0 103  65\n  1 156 426\n\n\n\nxtabs(~ y3 + y4, d1)\n\n   y4\ny3    0   1\n  0  41  39\n  1 119 551\n\n\n\nxtabs(~ y5 + y6, d1)\n\n   y6\ny5    0   1\n  0  95 168\n  1  60 427\n\n\nNelle tabelle precedenti, si osserva una maggiore frequenza di casi in cui entrambe le variabili assumono il valore 1, rispetto ai casi in cui entrambe sono 0 o in cui una è 1 e l’altra è 0. Questo suggerisce l’esistenza di una relazione ordinale tra le coppie di variabili nel dataset.\n\n\n38.3.3 Il Modello Basato sulle Soglie per Risposte Categoriali Ordinate\nIl modello basato sulle soglie per risposte categoriali ordinate si basa sull’idea che ogni risposta di una variabile categoriale possa essere vista come il risultato di una variabile continua non osservata, che è normalmente distribuita. Questa variabile nascosta, chiamata variabile latente, rappresenta la tendenza di una persona a rispondere in un determinato modo. Le risposte che vediamo, classificate in categorie, sono in realtà approssimazioni di questa variabile latente.\nImmaginiamo di utilizzare un questionario dove le risposte sono su una scala Likert a 7 punti. Questo crea una variabile categoriale con sette categorie ordinate. Se denotiamo con I un particolare item del questionario e con I* la sua corrispondente variabile latente non osservabile, possiamo descrivere il loro legame attraverso le seguenti equazioni, che mappano la variabile latente alle risposte osservabili:\n\\[\n\\begin{align*}\nI &= 1 \\quad \\text{se} \\quad -\\infty &lt; I^* \\leq t_1 \\\\\nI &= 2 \\quad \\text{se} \\quad t_1 &lt; I^* \\leq t_2 \\\\\nI &= 3 \\quad \\text{se} \\quad t_2 &lt; I^* \\leq t_3 \\\\\nI &= 4 \\quad \\text{se} \\quad t_3 &lt; I^* \\leq t_4 \\\\\nI &= 5 \\quad \\text{se} \\quad t_4 &lt; I^* \\leq t_5 \\\\\nI &= 6 \\quad \\text{se} \\quad t_5 &lt; I^* \\leq t_6 \\\\\nI &= 7 \\quad \\text{se} \\quad t_6 &lt; I^* &lt; \\infty\n\\end{align*}\n\\]\nIn queste equazioni, $ t_i $ (con i da 1 a 6) rappresenta le soglie che dividono l’intero spettro della variabile latente in sette categorie. Le soglie sono disposte in modo che $ -&lt; t_1 &lt; t_2 &lt; t_3 &lt; t_4 &lt; t_5 &lt; t_6 &lt; $. È importante notare che il numero di soglie è sempre uno in meno rispetto al numero di categorie, un po’ come il numero di variabili dummy usate nell’analisi di regressione per codificare una variabile categoriale.\nQuesto processo di categorizzazione può essere visualizzato come segue: si immagini una curva normale che rappresenta la distribuzione della variabile latente I. Le sei linee verticali nella figura rappresentano le soglie $ t_1 $ a $ t_6 $. Le risposte possibili vanno da I = 1 a I = 7, e la categoria specifica (I) dipende dall’intervallo, definito dalle soglie, in cui il valore di I si trova.\n\n# Definire le soglie\nthresholds &lt;- c(-3, -2, -1, 0, 1, 2, 3)\n\n# Creare un dataframe per la curva normale\nx_values &lt;- seq(-4, 4, length.out = 300)\ny_values &lt;- dnorm(x_values)\ncurve_data &lt;- data.frame(x = x_values, y = y_values)\n\n# Creare il plot\nggplot(curve_data, aes(x = x, y = y)) +\n    geom_line() +\n    geom_vline(xintercept = thresholds, col = \"red\") +\n    scale_y_continuous(breaks = NULL) +\n    scale_x_continuous(breaks = thresholds, labels = c(\"t1\", \"t2\", \"t3\", \"t4\", \"t5\", \"t6\", \"t7\")) +\n    labs(\n        title = \"Categorization of Latent Continuous Variable to Categorical Variable\",\n        x = \"Latent Continuous Variable I*\",\n        y = \"\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\nLa conversione della variabile latente $ I^* $ in dati su una scala Likert comporta inevitabilmente degli errori di misurazione e campionamento. Come evidenziato da O’Brien (1985), questo processo di categorizzazione introduce due tipi principali di errore:\n\nErrore di categorizzazione: Questo errore deriva dalla segmentazione di una scala continua in una scala categoriale, dove la variabile latente viene divisa in categorie distinte.\nErrore di trasformazione: Questo errore emerge quando le categorie hanno larghezze disuguali, influenzando la fedeltà della rappresentazione delle misure originali della variabile latente.\n\nDi conseguenza, è fondamentale che le soglie siano stimate contemporaneamente agli altri parametri nel modello di equazioni strutturali per garantire che tali errori siano minimizzati e che l’analisi rifletta accuratamente la realtà sottostante.\n\n\n38.3.4 Modellazione di Variabili Categoriali nei Modelli CFA\nNell’ambito dei modelli CFA, le variabili categoriali ordinate vengono spesso modellate collegandole a una variabile latente sottostante, denominata $ I^* $. Questa variabile latente rappresenta una sorta di “propensione nascosta” che influisce sulle risposte osservate nelle variabili categoriali.\nPer esemplificare, consideriamo il seguente modello che esprime la variabile latente $ I^* $ attraverso una serie di predittori (x1, x2, …, xp), ognuno dei quali contribuisce all’esito con un effetto quantificato dai coefficienti $ _1, _2, …, _P $:\n\\[\nI^*_i = \\beta_0 + \\beta_1 x_{1i} + \\dots + \\beta_P x_{Pi} + e_i.\n\\]\nIn questa equazione: - $ I^*_i $ indica la propensione latente per l’osservatore $ i $. - $ _0 $ è un termine costante che agisce come intercetta. - $ _1, , _P $ sono i coefficienti che misurano l’impatto di ciascun predittore sulla propensione latente. - $ e_i $ è il termine di errore che rappresenta le variazioni non spiegate dai predittori.\nQuando la variabile categoriale $ I $ funge da indicatore di un fattore latente $ $ in un modello fattoriale confermativo, la formulazione dell’equazione si semplifica a:\n\\[\nI^*_i = \\beta_0 + \\beta_1 \\xi_i + e_i.\n\\]\nIn questa configurazione, $ _1 $ rappresenta il carico fattoriale, indicando quanto fortemente il fattore latente $ $ influisce sulla variabile latente $ I^* $. Questo schema è analogo a quello usato per modellare indicatori di misurazione continui nei modelli SEM.\nQuesto approccio riflette l’idea che le risposte categoriali osservabili possono essere considerate come manifestazioni esterne di una propensione interna latente. Per la stima di tali modelli, il metodo dei minimi quadrati ponderati (WLS) è generalmente appropriato. Tuttavia, è importante tenere presente che la modellazione di risposte categoriali ordinate può richiedere considerazioni aggiuntive per gestire adeguatamente la loro natura ordinale, dettagli che verranno approfonditi nelle sezioni seguenti.\n\n\n38.3.5 Adattamento del Modello con lmer\nSpecifichiamo il modello nel modo seguente:\n\nmodel1 &lt;- '\n  etoh =~ y1 + y2 + y3 + y4 + y5 + y6\n'\n\nNell’analizzare dati ottenuti da scale ordinali, il software lavaan impiega un metodo specializzato per gestire la natura particolare dei dati categoriali. Questo approccio utilizza lo stimatore WLSMV (Weighted Least Squares Mean and Variance Adjusted). La stima dei parametri avviene tramite il metodo dei minimi quadrati ponderati diagonalmente (DWLS), che si concentra sulle componenti diagonali della matrice di peso. Questa specificità rende lo stimatore WLSMV particolarmente adatto per analizzare dati non normali.\nUna caratteristica importante dello stimatore WLSMV è la sua capacità di calcolare errori standard robusti. Questi sono determinati attraverso un metodo che mantiene l’affidabilità delle stime anche quando i dati non soddisfano le tradizionali assunzioni di normalità. Inoltre, le statistiche di test prodotte da WLSMV sono adeguatamente corrette per tenere conto delle variazioni nella media e nella varianza dei dati. Questo tipo di correzione è cruciale per garantire l’accuratezza e la validità delle statistiche di test, specialmente quando la distribuzione dei dati devia dalla normalità.\nIn conclusione, lavaan offre un approccio avanzato per la modellazione di dati categoriali utilizzando lo stimatore WLSMV, che è ottimizzato per rispondere alle esigenze specifiche di questi tipi di dati. Questo si traduce in stime più precise e statistiche di test affidabili, rendendo lavaan uno strumento molto appropriato per l’analisi di dati categoriali complessi.\n\nfit1 &lt;- cfa(\n  model1, \n  data = d1, \n  ordered = names(d1), \n  estimator = \"WLSMVS\", \n  mimic = \"mplus\"\n)\n\nEsaminiamo la soluzione ottenuta:\n\nout = summary(fit1, fit.measures = TRUE)\nprint(out)\n\nlavaan 0.6.17 ended normally after 16 iterations\n\n  Estimator                                       DWLS\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n\n  Number of observations                           750\n\nModel Test User Model:\n                                                  Standard      Scaled\n  Test Statistic                                     5.651       9.540\n  Degrees of freedom                                     9           9\n  P-value (Chi-square)                               0.774       0.389\n  Scaling correction factor                                      0.592\n    mean and variance adjusted correction (WLSMV)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1155.845     694.433\n  Degrees of freedom                                15           9\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.664\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000       0.999\n  Tucker-Lewis Index (TLI)                       1.005       0.999\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000       0.009\n  90 Percent confidence interval - lower         0.000       0.000\n  90 Percent confidence interval - upper         0.028       0.051\n  P-value H_0: RMSEA &lt;= 0.050                    0.999       0.944\n  P-value H_0: RMSEA &gt;= 0.080                    0.000       0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.031       0.031\n\nParameter Estimates:\n\n  Parameterization                               Delta\n  Standard errors                           Robust.sem\n  Information                                 Expected\n  Information saturated (h1) model        Unstructured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  etoh =~                                             \n    y1                1.000                           \n    y2                0.822    0.072   11.392    0.000\n    y3                0.653    0.092    7.097    0.000\n    y4                1.031    0.075   13.703    0.000\n    y5                1.002    0.072   13.861    0.000\n    y6                0.759    0.076   10.011    0.000\n\nThresholds:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    y1|t1            -0.759    0.051  -14.890    0.000\n    y2|t1            -0.398    0.047   -8.437    0.000\n    y3|t1            -1.244    0.061  -20.278    0.000\n    y4|t1            -0.795    0.051  -15.436    0.000\n    y5|t1            -0.384    0.047   -8.148    0.000\n    y6|t1            -0.818    0.052  -15.775    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .y1                0.399                           \n   .y2                0.594                           \n   .y3                0.744                           \n   .y4                0.361                           \n   .y5                0.397                           \n   .y6                0.653                           \n    etoh              0.601    0.063    9.596    0.000\n\n\n\nSi presti particolare attenzione alla seguente porzione dell’output:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    y1|t1            -0.759    0.051  -14.890    0.000\n    y2|t1            -0.398    0.047   -8.437    0.000\n    y3|t1            -1.244    0.061  -20.278    0.000\n    y4|t1            -0.795    0.051  -15.436    0.000\n    y5|t1            -0.384    0.047   -8.148    0.000\n    y6|t1            -0.818    0.052  -15.775    0.000\nIn questa porzione dell’output di lavaan sono presentati i risultati per le “soglie” (thresholds) relative alle variabili categoriali ordinate utilizzate nel modello SEM. Ecco una spiegazione dettagliata:\n\nThresholds (Soglie):\n\nOgni soglia rappresenta un punto di cutoff lungo la variabile continua latente (indicata in precedenza come I*), che determina le categorie della variabile categoriale osservata.\nNell’output, y1|t1, y2|t1, ecc., rappresentano soglie per le variabili rispettive (y1, y2, …, y6). Il termine “t1” si riferisce alla prima soglia per ciascuna di queste variabili.\n\nEstimate (Stima):\n\nQuesti valori indicano la posizione della soglia sulla scala della variabile continua latente. Per esempio, la soglia per y1 è a -0.759. Questo significa che la divisione tra le prime due categorie di y1 si verifica a -0.759 sulla scala della variabile latente.\n\nStd.Err (Errore Standard):\n\nL’errore standard della stima di ogni soglia. Ad esempio, per y1, l’errore standard è 0.051. Questo offre un’idea della variabilità o incertezza nella stima della soglia.\n\nz-value:\n\nIl valore z indica il rapporto tra la stima della soglia e il suo errore standard. Un valore z elevato suggerisce che la stima della soglia è significativamente diversa da zero (ovvero, la soglia è ben definita). Per esempio, per y1, il valore z è -14.890, che è statisticamente significativo.\n\nP(&gt;|z|):\n\nIl p-value associato al valore z. Un p-value basso (ad esempio, 0.000) indica che la stima della soglia è statisticamente significativa. Questo significa che possiamo essere abbastanza sicuri che la posizione della soglia sulla variabile latente sia accurata e non dovuta al caso.\n\n\nIn sintesi, queste soglie consentono di trasformare la variabile latente continua in una variabile categoriale osservata nel modello. La stima di queste soglie e la loro significatività statistica sono cruciali per comprendere come la variabile latente si traduce nelle categorie osservate.\nConfrontiamo ora la soluzione ottenuta con lo stimatore WLSMVS con quella ottenuta mediante lo stimatore ML.\n\nfit2 &lt;- cfa(\n  model1, \n  data = d1\n)\n\n\nout &lt;- summary(fit2, fit.measures = TRUE)\nprint(out)\n\nlavaan 0.6.17 ended normally after 35 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n\n  Number of observations                           750\n\nModel Test User Model:\n                                                      \n  Test statistic                                14.182\n  Degrees of freedom                                 9\n  P-value (Chi-square)                           0.116\n\nModel Test Baseline Model:\n\n  Test statistic                               614.305\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.991\n  Tucker-Lewis Index (TLI)                       0.986\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2087.600\n  Loglikelihood unrestricted model (H1)      -2080.508\n                                                      \n  Akaike (AIC)                                4199.199\n  Bayesian (BIC)                              4254.640\n  Sample-size adjusted Bayesian (SABIC)       4216.535\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.028\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.054\n  P-value H_0: RMSEA &lt;= 0.050                    0.914\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.021\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  etoh =~                                             \n    y1                1.000                           \n    y2                0.934    0.093   10.057    0.000\n    y3                0.390    0.055    7.038    0.000\n    y4                1.008    0.087   11.541    0.000\n    y5                1.158    0.101   11.468    0.000\n    y6                0.700    0.077    9.142    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .y1                0.109    0.007   14.692    0.000\n   .y2                0.169    0.010   16.781    0.000\n   .y3                0.085    0.005   18.483    0.000\n   .y4                0.102    0.007   14.285    0.000\n   .y5                0.140    0.010   14.506    0.000\n   .y6                0.132    0.008   17.514    0.000\n    etoh              0.065    0.009    7.664    0.000\n\n\n\nSi noti che la soluzione ottenuta mediante lo stimatore WLSMVS produce indici di bontà di adattamento migliori e errori standard dei parametri più piccoli.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Dati non gaussiani e categoriali</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/03_cat_data.html#riflessioni-conclusive",
    "href": "chapters/cfa/03_cat_data.html#riflessioni-conclusive",
    "title": "38  Dati non gaussiani e categoriali",
    "section": "38.4 Riflessioni Conclusive",
    "text": "38.4 Riflessioni Conclusive\nIn questo capitolo, abbiamo esplorato la modellazione CFA con dati non normalmente distribuiti. È essenziale riconoscere che, nella pratica analitica, incontrare dati non normalmente distribuiti dovrebbe essere considerato normale. Di conseguenza, si raccomanda l’utilizzo della massima verosimiglianza robusta (ML robusta) ogni volta che sorgono dubbi sulla normalità dei dati.\nCi sono alcune considerazioni importanti da tenere presente:\n\nStabilità delle stime di parametro: Anche se le versioni robuste di ML forniscono errori standard robusti e statistiche di test adattate, le stime dei parametri ottenute rimangono quelle della stima ML originale.\nRobustezza limitata: Gli aggiustamenti robusti compensano la violazione della normalità, ma non coprono la presenza di valori anomali, che richiedono un’analisi separata.\nLimitazioni degli aggiustamenti: Gli aggiustamenti robusti non trattano violazioni delle specifiche del modello, che è un altro argomento di discussione nella letteratura CFA e SEM.\n\nAbbiamo anche discusso l’uso dello stimatore WLSMV per dati categoriali, evidenziando come esso fornisca una stima dell’errore standard più precisa rispetto all’MLE standard e all’MLE robusta.\nVa notato che WLSMV è un metodo generale per dati categoriali nella CFA, ampiamente implementato in software come MPlus. In lavaan, l’uso di WLSMV può essere attivato semplicemente con lavaan(..., estimator = \"WLSMV\"), equivalente a lavaan(..., estimator = \"DWLS\", se = \"robust.sem\", test = \"scaled.shifted\").\nOltre al WLSMV, lavaan offre anche lo stimatore sperimentale di massima verosimiglianza marginale (MML), che, pur essendo preciso, può essere lento e più suscettibile a problemi di convergenza a causa della complessità dell’integrazione numerica. Un altro stimatore è l’ADF (estimator = “WLS”), che non assume specifiche distributive sui dati, ma richiede una dimensione campionaria molto grande (N &gt; 5000) per considerare affidabili le stime dei parametri, gli errori standard e le statistiche di test.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Dati non gaussiani e categoriali</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/03_cat_data.html#session-info",
    "href": "chapters/cfa/03_cat_data.html#session-info",
    "title": "38  Dati non gaussiani e categoriali",
    "section": "38.5 Session Info",
    "text": "38.5 Session Info\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [5] ggExtra_0.10.1    bayesplot_1.11.1  gridExtra_2.3     patchwork_1.2.0  \n [9] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-17     psych_2.4.3      \n[13] scales_1.3.0      markdown_1.12     knitr_1.45        lubridate_1.9.3  \n[17] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[21] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.0    \n[25] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5   farver_2.1.1      \n  [7] nloptr_2.0.3       rmarkdown_2.26     vctrs_0.6.5       \n [10] minqa_1.2.6        base64enc_0.1-3    rstatix_0.7.2     \n [13] htmltools_0.5.7    broom_1.0.5        Formula_1.2-5     \n [16] htmlwidgets_1.6.4  plyr_1.8.9         sandwich_3.1-0    \n [19] emmeans_1.10.0     zoo_1.8-12         uuid_1.2-0        \n [22] igraph_2.0.2       mime_0.12          lifecycle_1.0.4   \n [25] pkgconfig_2.0.3    Matrix_1.6-5       R6_2.5.1          \n [28] fastmap_1.1.1      shiny_1.8.0        digest_0.6.35     \n [31] OpenMx_2.21.11     fdrtool_1.2.17     colorspace_2.1-0  \n [34] rprojroot_2.0.4    Hmisc_5.1-1        fansi_1.0.6       \n [37] timechange_0.3.0   abind_1.4-5        compiler_4.3.3    \n [40] withr_3.0.0        glasso_1.11        htmlTable_2.4.2   \n [43] backports_1.4.1    carData_3.0-5      ggsignif_0.6.4    \n [46] MASS_7.3-60.0.1    corpcor_1.6.10     gtools_3.9.5      \n [49] tools_4.3.3        pbivnorm_0.6.0     foreign_0.8-86    \n [52] zip_2.3.1          httpuv_1.6.14      nnet_7.3-19       \n [55] glue_1.7.0         quadprog_1.5-8     nlme_3.1-164      \n [58] promises_1.2.1     lisrelToR_0.3      grid_4.3.3        \n [61] pbdZMQ_0.3-11      checkmate_2.3.1    cluster_2.1.6     \n [64] reshape2_1.4.4     generics_0.1.3     gtable_0.3.4      \n [67] tzdb_0.4.0         data.table_1.15.2  hms_1.1.3         \n [70] car_3.1-2          utf8_1.2.4         sem_3.1-15        \n [73] pillar_1.9.0       IRdisplay_1.1      rockchalk_1.8.157 \n [76] later_1.3.2        splines_4.3.3      lattice_0.22-5    \n [79] survival_3.5-8     kutils_1.73        tidyselect_1.2.0  \n [82] miniUI_0.1.1.1     pbapply_1.7-2      stats4_4.3.3      \n [85] xfun_0.42          qgraph_1.9.8       arm_1.13-1        \n [88] stringi_1.8.3      boot_1.3-29        evaluate_0.23     \n [91] codetools_0.2-19   mi_1.1             cli_3.6.2         \n [94] RcppParallel_5.1.7 IRkernel_1.3.2     rpart_4.1.23      \n [97] xtable_1.8-4       repr_1.1.6         munsell_0.5.0     \n[100] Rcpp_1.0.12        coda_0.19-4.1      png_0.1-8         \n[103] XML_3.99-0.16.1    parallel_4.3.3     ellipsis_0.3.2    \n[106] jpeg_0.1-10        lme4_1.1-35.1      mvtnorm_1.2-4     \n[109] openxlsx_4.2.5.2   crayon_1.5.2       rlang_1.1.3       \n[112] multcomp_1.4-25    mnormt_2.1.1      \n\n\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Dati non gaussiani e categoriali</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/04_mmm.html",
    "href": "chapters/cfa/04_mmm.html",
    "title": "39  CFA per matrici multi-tratto multi-metodo",
    "section": "",
    "text": "39.1 Introduzione\nLa validità descrive quanto accuratamente un metodo di misurazione riesce a quantificare ciò che è inteso misurare. Esistono diverse categorie di validità, ognuna delle quali si verifica attraverso metodi specifici. Una suddivisione convenzionale delle diverse tipologie di validità, che non riflette necessariamente gli sviluppi più recenti in questo campo, può essere descritta come segue (per ulteriori dettagli si rimanda al capitolo dedicato alla validità nella presente dispena):",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>CFA per matrici multi-tratto multi-metodo</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/04_mmm.html#introduzione",
    "href": "chapters/cfa/04_mmm.html#introduzione",
    "title": "39  CFA per matrici multi-tratto multi-metodo",
    "section": "",
    "text": "Validità di facciata: Valuta se gli item di un test appaiono appropriati e ragionevoli rispetto al costrutto che si intende misurare, sia agli occhi di chi partecipa al test sia di chi lo utilizza. Questo tipo di validità è basato sulla percezione esteriore della misura e si valuta tramite i giudizi di esperti sulla plausibilità delle misure.\nValidità di contenuto: Una misura possiede validità di contenuto quando i suoi indicatori rappresentano in modo esaustivo e accurato l’area di contenuto da misurare. Anche questa validità si basa sui giudizi di esperti.\nValidità di costrutto: Corrisponde alla definizione generale di validità e si riferisce alla capacità di uno strumento di misurare il costrutto che intende misurare. La validità di costrutto si verifica attraverso la correttezza con cui gli indicatori misurano i costrutti teorici di interesse e si convalida attraverso l’analisi delle relazioni tra il costrutto misurato e altri costrutti correlati, secondo modelli teorici specifici.\nValidità di criterio: Indica la capacità di uno strumento di fare previsioni accurate su un criterio esterno, valutando quanto bene la misura predice questo criterio.\nValidità concorrente: Si determina osservando quanto uno strumento di misurazione correla con altri strumenti considerati validi per misurare lo stesso attributo. Una forte correlazione è generalmente vista come una conferma della validità.\nValidità convergente: Si verifica confrontando e correlando i punteggi ottenuti con la misura da validare con quelli ottenuti da un altro costrutto teoricamente relazionato. La verifica di questa validità dipende dall’esistenza di misure valide per costrutti correlati.\nValidità discriminante: È l’opposto della validità convergente e si verifica quando la misura in esame non mostra correlazioni significative con le misure di costrutti teoricamente distinti.\n\n\n39.1.1 MTMM e CFA\nLa Matrice Multi-Tratto Multi-Metodo (MTMM) è un approccio utilizzato per valutare la validità di costrutto, esaminando la correlazione tra diversi costrutti misurati sia con gli stessi metodi sia con metodi differenti. La validità di costrutto è considerata alta quando la misura di un costrutto è indipendente dal metodo di misurazione utilizzato.\n\n\n39.1.2 Un esempio concreto\nNell’esempio discusso da {cite:t}brown2015confirmatory, il ricercatore desidera esaminare la validità del costrutto dei disturbi di personalità del Cluster A del DSM-IV, che sono pattern persistenti di sintomi caratterizzati da comportamenti strani o eccentrici (American Psychiatric Association, 1994). Il cluster A comprende tre costrutti di disturbo della personalità:\n\nparanoico (un pattern duraturo di sfiducia e sospetto tale che le motivazioni degli altri sono interpretate come malevole);\nschizoide (un pattern duraturo di distacco dalle relazioni sociali e una gamma ristretta di espressioni emotive);\nschizotipico (un pattern duraturo di disagio acuto nelle relazioni sociali, distorsioni cognitive e percettive ed eccentricità comportamentali).\n\nIn un campione di 500 pazienti, ciascuno di questi tre tratti è misurato mediante tre metodi di valutazione:\n\nun inventario di autovalutazione dei disturbi di personalità;\nvalutazioni dimensionali da un colloquio clinico strutturato sui disturbi della personalità;\nvalutazioni osservazionali effettuate da psicologi.\n\nI dati sono contenuti in una matrice 3 (T) × 3 (M), organizzata in modo tale che le correlazioni tra i diversi tratti (disturbi della personalità: paranoico, schizotipico, schizoide) siano annidate all’interno di ciascun metodo (tipo di valutazione: inventario, colloquio clinico, valutazioni degli osservatori).\nI dati sono riportati qui sotto.\n\nsds &lt;- c(3.61,  3.66,  3.59,  2.94,  3.03,  2.85,  2.22,  2.42,  2.04)\n\ncors &lt;- '\n  1.000 \n  0.290  1.000 \n  0.372  0.478  1.000 \n  0.587  0.238  0.209  1.000 \n  0.201  0.586  0.126  0.213  1.000 \n  0.218  0.281  0.681  0.195  0.096  1.000 \n  0.557  0.228  0.195  0.664  0.242  0.232  1.000 \n  0.196  0.644  0.146  0.261  0.641  0.248  0.383  1.000 \n  0.219  0.241  0.676  0.290  0.168  0.749  0.361  0.342  1.000'\n\ncovs &lt;- getCov(\n  cors, \n  sds = sds, \n  names = c(\"pari\", \"szti\", \"szdi\", \"parc\", \"sztc\", \"szdc\", \"paro\", \"szto\", \"szdo\")\n  )\n\nLa Matrice Multi-Tratto Multi-Metodo (MTMM) si organizza in due tipi di blocchi di coefficienti:\n\nBlocchi di mono-metodo: contengono le correlazioni tra indicatori che provengono dallo stesso metodo di misurazione. Questi blocchi esaminano come diversi indicatori del medesimo tratto si correlano tra loro quando misurati tramite lo stesso strumento.\nBlocchi di etero-metodo: includono le correlazioni tra indicatori misurati mediante metodi diversi. Particolarmente significativa è la “diagonale di validità” all’interno di questi blocchi, dove le correlazioni rappresentano stime di validità convergente. In altre parole, misure diverse di costrutti teoricamente simili dovrebbero mostrare forti correlazioni.\n\nNell’analisi MTMM, una forte correlazione tra metodi che misurano lo stesso tratto evidenzia la validità convergente. Per esempio, potrebbe risultare che diverse misure della personalità schizotipica mostrino correlazioni elevate, con coefficienti ( r ) che variano da 0.676 a 0.749. Al contrario, elementi al di fuori della diagonale nei blocchi di etero-metodo rivelano la validità discriminante, dove le misure di costrutti teoricamente distinti non dovrebbero essere altamente correlate. Questa validità è confermata quando tali correlazioni sono significativamente più basse rispetto a quelle della diagonale di validità, ad esempio, coefficienti che variano da 0.126 a 0.290.\nInoltre, è possibile rilevare gli effetti del metodo esaminando gli elementi al di fuori della diagonale nei blocchi di mono-metodo. Qui, la varianza nelle correlazioni tra diversi tratti misurati con lo stesso metodo, rispetto alle correlazioni tra gli stessi tratti misurati con metodi diversi, riflette l’entità degli effetti del metodo. Ad esempio, le valutazioni dell’osservatore dei tratti della personalità paranoica e schizotipica potrebbero essere più correlate (r = 0.383) rispetto alle loro misure con metodi diversi (ad esempio, la correlazione tra le misure di personalità paranoide e schizotipica, con l’uso rispettivamente dell’inventario e della valutazione dell’osservatore, è di 0.196).\nLa validità del costrutto è supportata quando i dati indicano alta validità convergente e discriminante con effetti del metodo trascurabili.\nIl modello CFA per analizzare la matrice MTMM può includere correlazioni residue tra le specificità di ciascun metodo, supponendo che ogni fattore comune (come paranoid, schizotypal, schizoid) sia identificato da item misurati con metodi diversi e che le specificità di ciascun metodo siano correlate tra loro.\n{cite:t}brown2015confirmatory mostra come sia possibile analizzare la matrice MTMM con un modello CFA nel quale si ipotizza che vi siano correlazioni residue tra le specificità di ciascun metodo. Il modello è dunque formulato nel modo seguente: ogni fattore comune (paranoid, schizotypal, schizoid) è identificato dagli item corrispondenti definiti da metodi diversi; le specificità di ciascun metodo, inoltre, sono correlate tra loro.\n\nmodel &lt;- '\n  paranoid    =~ pari + parc + paro\n  schizotypal =~ szti + sztc + szto\n  schizoid    =~ szdi + szdc + szdo\n  pari ~~ szti + szdi\n  szti ~~ szdi\n  parc ~~ sztc + szdc\n  sztc ~~ szdc\n  paro ~~ szto + szdo\n  szto ~~ szdo\n'  \n\nAdattiamo il modello ai dati.\n\nfit &lt;- cfa(\n  model, \n  sample.cov = covs, \n  sample.nobs = 500, \n  std.lv = TRUE\n)\n\nEsaminiamo la soluzione ottenuta.\n\nsummary(fit, fit.measures = TRUE, standardized = TRUE) |&gt;\n    print()\n\nlavaan 0.6.17 ended normally after 59 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        30\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                                14.371\n  Degrees of freedom                                15\n  P-value (Chi-square)                           0.498\n\nModel Test Baseline Model:\n\n  Test statistic                              2503.656\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.001\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -9879.996\n  Loglikelihood unrestricted model (H1)      -9872.811\n                                                      \n  Akaike (AIC)                               19819.992\n  Bayesian (BIC)                             19946.430\n  Sample-size adjusted Bayesian (SABIC)      19851.209\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.041\n  P-value H_0: RMSEA &lt;= 0.050                    0.989\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.025\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  paranoid =~                                                           \n    pari              2.588    0.145   17.833    0.000    2.588    0.712\n    parc              2.472    0.121   20.350    0.000    2.472    0.841\n    paro              1.747    0.088   19.946    0.000    1.747    0.788\n  schizotypal =~                                                        \n    szti              2.950    0.132   22.367    0.000    2.950    0.788\n    sztc              2.348    0.123   19.047    0.000    2.348    0.768\n    szto              2.047    0.089   22.905    0.000    2.047    0.843\n  schizoid =~                                                           \n    szdi              2.713    0.120   22.526    0.000    2.713    0.769\n    szdc              2.438    0.107   22.826    0.000    2.438    0.860\n    szdo              1.782    0.073   24.323    0.000    1.782    0.872\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .pari ~~                                                               \n   .szti              1.274    0.338    3.774    0.000    1.274    0.217\n   .szdi              2.537    0.329    7.703    0.000    2.537    0.441\n .szti ~~                                                               \n   .szdi              3.872    0.342   11.329    0.000    3.872    0.746\n .parc ~~                                                               \n   .sztc             -0.335    0.210   -1.597    0.110   -0.335   -0.107\n   .szdc             -0.608    0.176   -3.461    0.001   -0.608   -0.265\n .sztc ~~                                                               \n   .szdc             -0.933    0.188   -4.967    0.000   -0.933   -0.330\n .paro ~~                                                               \n   .szto              0.737    0.118    6.240    0.000    0.737    0.413\n   .szdo              0.505    0.096    5.274    0.000    0.505    0.368\n .szto ~~                                                               \n   .szdo              0.625    0.102    6.158    0.000    0.625    0.478\n  paranoid ~~                                                           \n    schizotypal       0.381    0.046    8.341    0.000    0.381    0.381\n    schizoid          0.359    0.046    7.856    0.000    0.359    0.359\n  schizotypal ~~                                                        \n    schizoid          0.310    0.047    6.666    0.000    0.310    0.310\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .pari              6.514    0.513   12.695    0.000    6.514    0.493\n   .parc              2.529    0.334    7.562    0.000    2.529    0.293\n   .paro              1.867    0.179   10.434    0.000    1.867    0.380\n   .szti              5.309    0.460   11.529    0.000    5.309    0.379\n   .sztc              3.846    0.330   11.654    0.000    3.846    0.411\n   .szto              1.704    0.175    9.742    0.000    1.704    0.289\n   .szdi              5.080    0.386   13.158    0.000    5.080    0.408\n   .szdc              2.085    0.230    9.047    0.000    2.085    0.260\n   .szdo              1.005    0.107    9.351    0.000    1.005    0.240\n    paranoid          1.000                               1.000    1.000\n    schizotypal       1.000                               1.000    1.000\n    schizoid          1.000                               1.000    1.000\n\n\n\n\neffectsize::interpret(fit) |&gt;\n    print()\n\n    Name      Value Threshold Interpretation\n1    GFI 0.99376810      0.95   satisfactory\n2   AGFI 0.98130431      0.90   satisfactory\n3    NFI 0.99425997      0.90   satisfactory\n4   NNFI 1.00061169      0.90   satisfactory\n5    CFI 1.00000000      0.90   satisfactory\n6  RMSEA 0.00000000      0.05   satisfactory\n7   SRMR 0.02482894      0.08   satisfactory\n8    RFI 0.98622392      0.90   satisfactory\n9   PNFI 0.41427499      0.50           poor\n10   IFI 1.00025272      0.90   satisfactory\n\n\nPer i dati considerati da {cite:t}brown2015confirmatory, l’adattamento del modello MTMM è eccellente. Ciò fornisce forti evidenze di validità di costrutto per i fattori Paranoico, Schizoide e Schizotipico che sono stati ipotizzati.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>CFA per matrici multi-tratto multi-metodo</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/04_mmm.html#session-info",
    "href": "chapters/cfa/04_mmm.html#session-info",
    "title": "39  CFA per matrici multi-tratto multi-metodo",
    "section": "39.2 Session Info",
    "text": "39.2 Session Info\n\nsessionInfo()\n\nR version 4.3.3 (2024-02-29)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [5] ggExtra_0.10.1    bayesplot_1.11.1  gridExtra_2.3     patchwork_1.2.0  \n [9] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-17     psych_2.4.3      \n[13] scales_1.3.0      markdown_1.12     knitr_1.45        lubridate_1.9.3  \n[17] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[21] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.0    \n[25] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0  jsonlite_1.8.8     datawizard_0.9.1  \n  [4] magrittr_2.0.3     TH.data_1.1-2      estimability_1.5  \n  [7] nloptr_2.0.3       rmarkdown_2.26     vctrs_0.6.5       \n [10] minqa_1.2.6        effectsize_0.8.7   base64enc_0.1-3   \n [13] rstatix_0.7.2      htmltools_0.5.7    broom_1.0.5       \n [16] Formula_1.2-5      htmlwidgets_1.6.4  plyr_1.8.9        \n [19] sandwich_3.1-0     emmeans_1.10.0     zoo_1.8-12        \n [22] uuid_1.2-0         igraph_2.0.2       mime_0.12         \n [25] lifecycle_1.0.4    pkgconfig_2.0.3    Matrix_1.6-5      \n [28] R6_2.5.1           fastmap_1.1.1      shiny_1.8.0       \n [31] digest_0.6.35      OpenMx_2.21.11     fdrtool_1.2.17    \n [34] colorspace_2.1-0   rprojroot_2.0.4    Hmisc_5.1-1       \n [37] fansi_1.0.6        timechange_0.3.0   abind_1.4-5       \n [40] compiler_4.3.3     withr_3.0.0        glasso_1.11       \n [43] htmlTable_2.4.2    backports_1.4.1    carData_3.0-5     \n [46] performance_0.11.0 ggsignif_0.6.4     MASS_7.3-60.0.1   \n [49] corpcor_1.6.10     gtools_3.9.5       tools_4.3.3       \n [52] pbivnorm_0.6.0     foreign_0.8-86     zip_2.3.1         \n [55] httpuv_1.6.14      nnet_7.3-19        glue_1.7.0        \n [58] quadprog_1.5-8     nlme_3.1-164       promises_1.2.1    \n [61] lisrelToR_0.3      grid_4.3.3         pbdZMQ_0.3-11     \n [64] checkmate_2.3.1    cluster_2.1.6      reshape2_1.4.4    \n [67] generics_0.1.3     gtable_0.3.4       tzdb_0.4.0        \n [70] data.table_1.15.2  hms_1.1.3          car_3.1-2         \n [73] utf8_1.2.4         sem_3.1-15         pillar_1.9.0      \n [76] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [79] splines_4.3.3      lattice_0.22-5     survival_3.5-8    \n [82] kutils_1.73        tidyselect_1.2.0   miniUI_0.1.1.1    \n [85] pbapply_1.7-2      stats4_4.3.3       xfun_0.42         \n [88] qgraph_1.9.8       arm_1.13-1         stringi_1.8.3     \n [91] boot_1.3-29        evaluate_0.23      codetools_0.2-19  \n [94] mi_1.1             cli_3.6.2          RcppParallel_5.1.7\n [97] IRkernel_1.3.2     rpart_4.1.23       parameters_0.21.6 \n[100] xtable_1.8-4       repr_1.1.6         munsell_0.5.0     \n[103] Rcpp_1.0.12        coda_0.19-4.1      png_0.1-8         \n[106] XML_3.99-0.16.1    parallel_4.3.3     ellipsis_0.3.2    \n[109] bayestestR_0.13.2  jpeg_0.1-10        lme4_1.1-35.1     \n[112] mvtnorm_1.2-4      insight_0.19.10    openxlsx_4.2.5.2  \n[115] crayon_1.5.2       rlang_1.1.3        multcomp_1.4-25   \n[118] mnormt_2.1.1",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>CFA per matrici multi-tratto multi-metodo</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/05_bifactor.html",
    "href": "chapters/cfa/05_bifactor.html",
    "title": "40  Modello bifattoriale",
    "section": "",
    "text": "40.1 Introduzione\nNumerose misure psicologiche sono progettate per valutare individui su un singolo costrutto. Tuttavia, caratteristiche psicologiche complesse come depressione e ansia spesso si manifestano in modi vari. Di conseguenza, è consigliabile includere item che coprono diverse aree tematiche per assicurare una validità di contenuto adeguata. Pertanto, molte scale di valutazione comunemente usate producono dati che si prestano a interpretazioni valide sia attraverso un modello unidimensionale, con un forte fattore generale, sia tramite un modello multidimensionale, che comprende due o più fattori correlati.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Modello bifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/05_bifactor.html#struttura-fattoriale",
    "href": "chapters/cfa/05_bifactor.html#struttura-fattoriale",
    "title": "40  Modello bifattoriale",
    "section": "40.2 Struttura Fattoriale",
    "text": "40.2 Struttura Fattoriale\nRecenti studi hanno evidenziato che, di fronte a misure che generano dati multidimensionali a causa di una diversificata struttura di contenuto, l’adozione di un modello di misurazione bifattoriale può essere particolarmente efficace per rappresentare la struttura sottostante. Questo modello suggerisce che le correlazioni tra gli item di un test possono essere spiegate attraverso due tipi di fattori: (a) un fattore generale che riflette la varianza condivisa tra tutti gli item, e (b) una serie di fattori di gruppo che catturano la varianza specifica non spiegata dal fattore generale e che è comune tra item simili in termini di contenuto. Generalmente, si ritiene che il fattore generale e i fattori di gruppo siano indipendenti.\nIl fattore generale rappresenta il costrutto principale che lo strumento si propone di misurare, mentre i fattori di gruppo individuano costrutti più specifici legati a sottodomini. I modelli bifattoriali sono utilizzati per diverse finalità importanti:\n\nAnalizzare la distribuzione della varianza quando si presume che uno strumento misuri sia varianza generale sia specifica di gruppo.\nGestire la multidimensionalità in modo che la misura risulti “essenzialmente unidimensionale”, pur presentando dimensioni secondarie.\nVerificare la presenza di un fattore generale sufficientemente robusto da giustificare l’uso di un modello di misurazione unidimensionale.\nDeterminare l’adeguatezza di un punteggio complessivo e valutare l’utilità di analizzare le sottoscale specifiche.\n\nQuesti approcci permettono una comprensione più profonda e una valutazione più accurata della struttura sottostante dei dati psicologici, offrendo agli specialisti gli strumenti per interpretare con maggiore precisione i risultati dei test psicologici.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Modello bifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/05_bifactor.html#un-esempio-pratico",
    "href": "chapters/cfa/05_bifactor.html#un-esempio-pratico",
    "title": "40  Modello bifattoriale",
    "section": "40.3 Un esempio pratico",
    "text": "40.3 Un esempio pratico\nConsideriamo i dati SRS_data forniti dal pacchetto BifactorIndicesCalculator. Il dataset contiene 500 risposte al test SRS-22r sulla qualità della vita legata alla scoliosi, composta da 20 item. La sottoscala “Function” è composta dagli item 5, 9, 12, 15 e 18. La sottoscala “Pain” è composta dagli item 1, 2, 8, 11 e 17. La sottoscala “SelfImage” è composta dagli item 4, 6, 10, 14 e 19. La sottoscala “MentalHealth” è composta dagli item 3, 7, 13, 16 e 20.\nIniziamo esaminando le statistiche descrittive a livello di item e le correlazioni tra gli item.\n\ndescribe(SRS_data)\n\n\nA psych: 20 x 13\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nSRS_1\n1\n500\n3.69\n1.065\n4\n3.78\n1.48\n1\n5\n4\n-0.5509\n-0.4413\n0.0476\n\n\nSRS_2\n2\n500\n3.81\n1.030\n4\n3.92\n1.48\n1\n5\n4\n-0.7148\n-0.0679\n0.0460\n\n\nSRS_3\n3\n500\n3.90\n1.090\n4\n4.04\n1.48\n1\n5\n4\n-0.8323\n-0.0397\n0.0487\n\n\nSRS_4\n4\n500\n3.23\n1.280\n3\n3.29\n1.48\n1\n5\n4\n-0.1059\n-1.0236\n0.0572\n\n\nSRS_5\n5\n500\n4.20\n0.894\n4\n4.30\n1.48\n2\n5\n3\n-0.7453\n-0.5385\n0.0400\n\n\nSRS_6\n6\n500\n3.91\n0.872\n4\n3.95\n1.48\n1\n5\n4\n-0.3767\n-0.3859\n0.0390\n\n\nSRS_7\n7\n500\n4.24\n1.011\n5\n4.40\n0.00\n1\n5\n4\n-1.2842\n1.0807\n0.0452\n\n\nSRS_8\n8\n500\n3.80\n1.132\n4\n3.91\n1.48\n1\n5\n4\n-0.5140\n-0.6993\n0.0506\n\n\nSRS_9\n9\n500\n4.41\n1.004\n5\n4.63\n0.00\n1\n5\n4\n-1.7706\n2.4517\n0.0449\n\n\nSRS_10\n10\n500\n3.70\n0.855\n4\n3.71\n1.48\n1\n5\n4\n-0.2302\n-0.2097\n0.0382\n\n\nSRS_11\n11\n500\n4.50\n0.839\n5\n4.69\n0.00\n1\n5\n4\n-2.0302\n4.0862\n0.0375\n\n\nSRS_12\n12\n500\n4.26\n1.028\n5\n4.44\n0.00\n1\n5\n4\n-1.3462\n1.1192\n0.0460\n\n\nSRS_13\n13\n500\n3.85\n0.927\n4\n3.94\n1.48\n1\n5\n4\n-0.6457\n0.0497\n0.0414\n\n\nSRS_14\n14\n500\n4.72\n0.676\n5\n4.89\n0.00\n1\n5\n4\n-2.8155\n8.2069\n0.0303\n\n\nSRS_15\n15\n500\n4.81\n0.629\n5\n4.99\n0.00\n1\n5\n4\n-4.1179\n18.0401\n0.0281\n\n\nSRS_16\n16\n500\n4.24\n0.960\n5\n4.40\n0.00\n1\n5\n4\n-1.2300\n0.9875\n0.0429\n\n\nSRS_17\n17\n500\n4.74\n0.860\n5\n4.99\n0.00\n1\n5\n4\n-3.4840\n11.2987\n0.0385\n\n\nSRS_18\n18\n500\n2.92\n0.923\n3\n2.92\n0.00\n1\n5\n4\n0.0515\n0.8364\n0.0413\n\n\nSRS_19\n19\n500\n3.60\n1.107\n4\n3.69\n1.48\n1\n5\n4\n-0.5442\n-0.2818\n0.0495\n\n\nSRS_20\n20\n500\n3.99\n0.900\n4\n4.09\n1.48\n1\n5\n4\n-0.8992\n0.7480\n0.0402\n\n\n\n\n\n\nround(cor(SRS_data, use = \"pairwise.complete.obs\"), 2)\n\n\nA matrix: 20 x 20 of type dbl\n\n\n\nSRS_1\nSRS_2\nSRS_3\nSRS_4\nSRS_5\nSRS_6\nSRS_7\nSRS_8\nSRS_9\nSRS_10\nSRS_11\nSRS_12\nSRS_13\nSRS_14\nSRS_15\nSRS_16\nSRS_17\nSRS_18\nSRS_19\nSRS_20\n\n\n\n\nSRS_1\n1.00\n0.88\n0.43\n0.36\n0.36\n0.39\n0.34\n0.70\n0.31\n0.37\n0.46\n0.59\n0.37\n0.36\n0.19\n0.40\n0.38\n0.30\n0.27\n0.33\n\n\nSRS_2\n0.88\n1.00\n0.40\n0.38\n0.35\n0.42\n0.35\n0.70\n0.34\n0.40\n0.49\n0.58\n0.35\n0.38\n0.20\n0.37\n0.39\n0.29\n0.31\n0.32\n\n\nSRS_3\n0.43\n0.40\n1.00\n0.32\n0.33\n0.39\n0.50\n0.46\n0.30\n0.32\n0.24\n0.46\n0.55\n0.34\n0.19\n0.55\n0.25\n0.25\n0.28\n0.41\n\n\nSRS_4\n0.36\n0.38\n0.32\n1.00\n0.23\n0.43\n0.32\n0.33\n0.19\n0.43\n0.20\n0.30\n0.30\n0.20\n0.23\n0.32\n0.10\n0.21\n0.54\n0.32\n\n\nSRS_5\n0.36\n0.35\n0.33\n0.23\n1.00\n0.33\n0.39\n0.33\n0.47\n0.34\n0.23\n0.52\n0.31\n0.29\n0.20\n0.42\n0.28\n0.31\n0.25\n0.40\n\n\nSRS_6\n0.39\n0.42\n0.39\n0.43\n0.33\n1.00\n0.48\n0.37\n0.25\n0.64\n0.28\n0.37\n0.41\n0.38\n0.22\n0.47\n0.22\n0.31\n0.62\n0.41\n\n\nSRS_7\n0.34\n0.35\n0.50\n0.32\n0.39\n0.48\n1.00\n0.40\n0.24\n0.37\n0.22\n0.42\n0.53\n0.44\n0.23\n0.78\n0.19\n0.39\n0.39\n0.56\n\n\nSRS_8\n0.70\n0.70\n0.46\n0.33\n0.33\n0.37\n0.40\n1.00\n0.30\n0.37\n0.36\n0.52\n0.40\n0.28\n0.14\n0.42\n0.31\n0.32\n0.28\n0.34\n\n\nSRS_9\n0.31\n0.34\n0.30\n0.19\n0.47\n0.25\n0.24\n0.30\n1.00\n0.36\n0.26\n0.49\n0.32\n0.29\n0.22\n0.32\n0.35\n0.27\n0.20\n0.31\n\n\nSRS_10\n0.37\n0.40\n0.32\n0.43\n0.34\n0.64\n0.37\n0.37\n0.36\n1.00\n0.26\n0.37\n0.30\n0.34\n0.22\n0.39\n0.19\n0.28\n0.54\n0.35\n\n\nSRS_11\n0.46\n0.49\n0.24\n0.20\n0.23\n0.28\n0.22\n0.36\n0.26\n0.26\n1.00\n0.42\n0.19\n0.33\n0.20\n0.23\n0.39\n0.18\n0.24\n0.19\n\n\nSRS_12\n0.59\n0.58\n0.46\n0.30\n0.52\n0.37\n0.42\n0.52\n0.49\n0.37\n0.42\n1.00\n0.43\n0.46\n0.23\n0.46\n0.44\n0.33\n0.32\n0.43\n\n\nSRS_13\n0.37\n0.35\n0.55\n0.30\n0.31\n0.41\n0.53\n0.40\n0.32\n0.30\n0.19\n0.43\n1.00\n0.35\n0.17\n0.57\n0.21\n0.30\n0.35\n0.60\n\n\nSRS_14\n0.36\n0.38\n0.34\n0.20\n0.29\n0.38\n0.44\n0.28\n0.29\n0.34\n0.33\n0.46\n0.35\n1.00\n0.33\n0.46\n0.38\n0.30\n0.32\n0.34\n\n\nSRS_15\n0.19\n0.20\n0.19\n0.23\n0.20\n0.22\n0.23\n0.14\n0.22\n0.22\n0.20\n0.23\n0.17\n0.33\n1.00\n0.27\n0.20\n0.17\n0.29\n0.24\n\n\nSRS_16\n0.40\n0.37\n0.55\n0.32\n0.42\n0.47\n0.78\n0.42\n0.32\n0.39\n0.23\n0.46\n0.57\n0.46\n0.27\n1.00\n0.19\n0.38\n0.36\n0.59\n\n\nSRS_17\n0.38\n0.39\n0.25\n0.10\n0.28\n0.22\n0.19\n0.31\n0.35\n0.19\n0.39\n0.44\n0.21\n0.38\n0.20\n0.19\n1.00\n0.25\n0.15\n0.16\n\n\nSRS_18\n0.30\n0.29\n0.25\n0.21\n0.31\n0.31\n0.39\n0.32\n0.27\n0.28\n0.18\n0.33\n0.30\n0.30\n0.17\n0.38\n0.25\n1.00\n0.25\n0.31\n\n\nSRS_19\n0.27\n0.31\n0.28\n0.54\n0.25\n0.62\n0.39\n0.28\n0.20\n0.54\n0.24\n0.32\n0.35\n0.32\n0.29\n0.36\n0.15\n0.25\n1.00\n0.41\n\n\nSRS_20\n0.33\n0.32\n0.41\n0.32\n0.40\n0.41\n0.56\n0.34\n0.31\n0.35\n0.19\n0.43\n0.60\n0.34\n0.24\n0.59\n0.16\n0.31\n0.41\n1.00\n\n\n\n\n\n\nSRS_UnidimensionalModel &lt;-\n\"\n    SRS =~ SRS_1 + SRS_2 + SRS_3 + SRS_4 + SRS_5 +\n    SRS_6 + SRS_7 + SRS_8 + SRS_9 + SRS_10 +\n    SRS_11 + SRS_12 + SRS_13 + SRS_14 + SRS_15 +\n    SRS_16 + SRS_17 + SRS_18 + SRS_19 + SRS_20\n\"\n\nSRS_Unidimensional &lt;- lavaan::cfa(SRS_UnidimensionalModel,\n    SRS_data,\n    ordered = paste0(\"SRS_\", 1:20),\n    orthogonal = TRUE\n)\n\n\nsemPaths(\n    SRS_Unidimensional, \n    intercepts = FALSE\n)\n\n\n\n\n\n\n\n\nEsaminiamo la bontà di adattamento.\n\nfit.subset &lt;- c(\n    \"chisq.scaled\", \"df\", \"pvalue.scaled\",\n    \"rmsea.scaled\", \"rmsea.pvalue.scale\",\n    \"rmsea.ci.lower.scaled\", \"rmsea.ci.upper.scaled\",\n    \"cfi\", \"tli\", \"srmr\"\n)\n\n\nfitmeasures(SRS_Unidimensional, fit.subset) |&gt; print()\n\n         chisq.scaled                    df         pvalue.scaled \n             2087.431               170.000                 0.000 \n         rmsea.scaled rmsea.ci.lower.scaled rmsea.ci.upper.scaled \n                0.150                 0.145                 0.156 \n                  cfi                   tli                  srmr \n                0.961                 0.956                 0.119 \n\n\n\nSRS_BifactorModel &lt;-\n\"\n    SRS =~ SRS_1 + SRS_2 + SRS_3 + SRS_4 + SRS_5 + \n           SRS_6 + SRS_7 + SRS_8 + SRS_9 + SRS_10 +\n           SRS_11 + SRS_12 + SRS_13 + SRS_14 + SRS_15 +\n           SRS_16 + SRS_17 + SRS_18 + SRS_19 + SRS_20\n    Function =~ SRS_5 + SRS_9 + SRS_12 + SRS_15 + SRS_18\n    Pain =~ SRS_1 + SRS_2 + SRS_8 + SRS_11 + SRS_17\n    SelfImage =~ SRS_4 + SRS_6 + SRS_10 + SRS_14 + SRS_19\n    MentalHealth =~ SRS_3 + SRS_7 + SRS_13 + SRS_16 + SRS_20\n\"\n\nSRS_bifactor &lt;- lavaan::cfa(SRS_BifactorModel,\n    SRS_data,\n    ordered = paste0(\"SRS_\", 1:20),\n    orthogonal = TRUE\n)\n\n\nsemPaths(\n    SRS_bifactor,\n    intercepts = FALSE\n)\n\n\n\n\n\n\n\n\nEsaminiamo la bontà di adattamento.\n\nfitmeasures(SRS_bifactor, fit.subset) |&gt; print()\n\n         chisq.scaled                    df         pvalue.scaled \n              468.648               150.000                 0.000 \n         rmsea.scaled rmsea.ci.lower.scaled rmsea.ci.upper.scaled \n                0.065                 0.059                 0.072 \n                  cfi                   tli                  srmr \n                0.997                 0.996                 0.055 \n\n\nConfrontiamo i due modelli.\n\nlavTestLRT(SRS_Unidimensional, SRS_bifactor) |&gt; print()\n\n\nScaled Chi-Squared Difference Test (method = \"satorra.2000\")\n\nlavaan-&gt;lavTestLRT():  \n   lavaan NOTE: The \"Chisq\" column contains standard test statistics, not \n   the robust test that should be reported per model. A robust difference \n   test is a function of two standard (not robust) statistics.\n                    Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq)    \nSRS_bifactor       150           309                                  \nSRS_Unidimensional 170          1965       1007      20     &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nConsideriamo ora gli indici specifici per un modello bifattoriale.\n\nbifactorIndices(SRS_bifactor, UniLambda = SRS_Unidimensional) |&gt; print()\n\n$ModelLevelIndices\n   ECV.SRS        PUC  Omega.SRS OmegaH.SRS       ARPB \n     0.673      0.789      0.942      0.834      0.121 \n\n$FactorLevelIndices\n             ECV_SS ECV_SG ECV_GS Omega OmegaH     H    FD\nSRS           0.673 0.6728  0.673 0.942 0.8338 0.943 0.952\nFunction      0.197 0.0415  0.803 0.799 0.0912 0.403 0.710\nPain          0.412 0.1115  0.588 0.882 0.3427 0.696 0.921\nSelfImage     0.328 0.0818  0.672 0.850 0.2025 0.591 0.830\nMentalHealth  0.342 0.0923  0.658 0.892 0.2930 0.615 0.854\n\n$ItemLevelIndices\n        IECV RelParBias\nSRS_1  0.510     0.3534\nSRS_2  0.498     0.3675\nSRS_3  0.798     0.0366\nSRS_4  0.612     0.0522\nSRS_5  0.635     0.0226\nSRS_6  0.637     0.0881\nSRS_7  0.632     0.1789\nSRS_8  0.682     0.1034\nSRS_9  0.588     0.0192\nSRS_10 0.649     0.0642\nSRS_11 0.633     0.1217\nSRS_12 0.938     0.1001\nSRS_13 0.624     0.1179\nSRS_14 0.999     0.0990\nSRS_15 1.000     0.1011\nSRS_16 0.601     0.1898\nSRS_17 0.750     0.0392\nSRS_18 0.999     0.0933\nSRS_19 0.458     0.1664\nSRS_20 0.691     0.1048\n\n\n\nI ModelLevelIndices possono essere spiegati nel modo seguente:\n\nECV.SRS: Questo indica la proporzione di varianza spiegata dal fattore generale nel modello bifattoriale. ECV sta per “Explained Common Variance” (Varianza Comune Spiegata). Un valore più alto indica che una maggiore parte della varianza totale nei dati è spiegata dal fattore generale.\nPUC: Questo è l’acronimo di “Percentage of Uniqueness in Common” (Percentuale di Unicità nel Comune). Indica quanto della varianza unica (cioè quella non spiegata dal fattore generale) è presente nei fattori di gruppo. Un valore basso indica che i fattori di gruppo spiegano una maggiore parte della varianza unica nei dati.\nOmega.SRS: Questo indice rappresenta il coefficiente di affidabilità del fattore generale del modello bifattoriale. Indica quanto sia affidabile il fattore generale nel catturare la varianza comune tra tutti gli item del test. Un valore più alto indica maggiore affidabilità.\nOmegaH.SRS: Questo indice rappresenta il coefficiente di affidabilità dei fattori di gruppo nel modello bifattoriale. Indica quanto sia affidabile l’insieme dei fattori di gruppo nel catturare la varianza condivisa tra gli item del gruppo specifico. Anche qui, un valore più alto indica maggiore affidabilità.\nARPB: Questo sta per “Average Reproducibility of Parameter Estimates” (Riproducibilità Media delle Stime dei Parametri). Rappresenta la riproducibilità media delle stime dei parametri del modello bifattoriale. In sostanza, valuta quanto le stime dei parametri del modello sono affidabili e riproducibili.\n\nLa sezione dell’output FactorLevelIndices riguarda gli indici a livello di fattore del modello bifattoriale.\n\nECV_SS, ECV_SG, ECV_GS: Questi rappresentano rispettivamente la proporzione di varianza spiegata dal Fattore Generale (GG), dal Fattore Specifico (SS) e dall’Interazione tra Fattore Generale e Fattore Specifico (GS) per ciascun fattore. Indicano quanto ciascun tipo di varianza contribuisce alla spiegazione della varianza totale nell’insieme dei dati del fattore.\nOmega: Questo indice rappresenta il coefficiente di affidabilità dell’estratto del Fattore Generale per ciascun fattore. Indica quanto sia affidabile il Fattore Generale nel catturare la varianza comune tra gli item di quel particolare fattore. Un valore più alto indica maggiore affidabilità.\nOmegaH: Questo indice rappresenta il coefficiente di affidabilità dell’estratto del Fattore Specifico per ciascun fattore. Indica quanto sia affidabile l’insieme dei Fattori Specifici nel catturare la varianza condivisa tra gli item del gruppo specifico. Anche qui, un valore più alto indica maggiore affidabilità.\nH: Questo indice rappresenta la quota della varianza unica spiegata dal Fattore Generale per ciascun fattore. Indica quanto della varianza unica è spiegata dal Fattore Generale piuttosto che da fattori specifici.\nFD: Questo indice rappresenta la distorsione fattoriale, che è una misura di quanto i dati si adattino bene al modello bifattoriale. Valori vicini a 1 indicano un buon adattamento.\n\nInfine, l’output ItemLevelIndices riguarda gli indici a livello di item in un modello bifattoriale.\n\nIECV: Questo indica la proporzione di varianza spiegata dal Fattore Generale per ciascun item. IECV sta per “Item Explained Common Variance” (Varianza Comune Spiegata dell’Item). Indica quanto della varianza totale dell’item può essere spiegata dal Fattore Generale del modello bifattoriale. Valori più alti indicano che il Fattore Generale contribuisce maggiormente a spiegare le variazioni osservate nell’item.\nRelParBias: Questo rappresenta il bias relativo dei parametri dell’item. Indica quanto i parametri dell’item sono influenzati dalla presenza del Fattore Generale e dai fattori specifici nel modello. Valori più alti indicano una maggiore influenza dei fattori specifici rispetto al Fattore Generale nell’item.\n\nPer i dati dell’esempio considerato, di seguito è riportata un’interpretazione succinta dei risultati chiave per ciascun gruppo principale di risultati:\n\n40.3.1 Model-Level Indices\n\nECV.SRS (Explained Common Variance): Il 67.28% della varianza osservata è spiegata dal modello.\nPUC (Percentage of Uncontaminated Correlations): Il 78.95% delle correlazioni tra gli item è “puro”, cioè non contaminato da altri fattori oltre al fattore generale.\nOmega.SRS: La consistenza interna complessiva del test è molto alta (0.942), indicando una buona affidabilità.\nOmegaH.SRS: Il 83.38% della varianza totale standardizzata è attribuibile al fattore generale, confermando che è un fattore dominante nel modello.\nARPB (Average Relative Parameter Bias): Un bias relativo medio basso (0.121) suggerisce che le stime dei parametri sono relativamente poco distorte.\n\n\n\n40.3.2 Factor-Level Indices\n\nECV (Explained Common Variance) per i fattori specifici:\n\nFunzione: 19.73% della varianza è spiegata dal fattore specifico “Funzione”, con l’80.27% attribuibile al fattore generale.\nDolore (Pain): 41.24% della varianza è spiegata dal fattore specifico “Dolore”.\nAutopercezione (SelfImage): 32.80% della varianza è spiegata dal fattore specifico “Autopercezione”.\nSalute Mentale (MentalHealth): 34.24% della varianza è spiegata dal fattore specifico “Salute Mentale”.\n\nOmega e OmegaH per ogni fattore specifico: Le misure Omega indicano la consistenza interna per ciascun sottogruppo di item, mentre OmegaH indica la proporzione della varianza attribuibile ai fattori specifici rispetto al fattore generale.\n\n\n\n40.3.3 Item-Level Indices\n\nIECV (Item Explained Common Variance): Valori come 0.937 per SRS_12 e quasi 1 per SRS_14, SRS_15 e SRS_18 indicano che questi item sono molto influenzati dal fattore generale.\nRelParBias (Relative Parameter Bias): La maggior parte degli item mostra un bias relativo basso, suggerendo che gli effetti dei fattori specifici su questi item sono correttamente rappresentati senza grande distorsione.\n\nIn sintesi, il modello bifattoriale sembra adattarsi bene ai dati, con un forte fattore generale che domina la struttura del test, supportato da alcuni fattori specifici che spiegano porzioni significative della varianza in diverse aree tematiche. Gli item individuati con alti valori di IECV sono particolarmente rappresentativi del fattore generale.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Modello bifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/05_bifactor.html#commenti-e-considerazioni-conclusive",
    "href": "chapters/cfa/05_bifactor.html#commenti-e-considerazioni-conclusive",
    "title": "40  Modello bifattoriale",
    "section": "40.4 Commenti e considerazioni conclusive",
    "text": "40.4 Commenti e considerazioni conclusive\nIn questo capitolo, abbiamo esplorato diversi indici derivati dall’analisi con un modello bifattoriale, ognuno dei quali rivela aspetti specifici delle proprietà psicometriche di uno strumento di misura. Questi indici sono di grande utilità per gli sviluppatori e i valutatori di scale, oltre a essere strumenti preziosi per i ricercatori e i professionisti che le impiegano nella pratica clinica e nella ricerca. Inoltre, contribuiscono allo sviluppo e alla comprensione dei costrutti psicologici che tali strumenti intendono misurare.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Modello bifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/05_bifactor.html#session-info",
    "href": "chapters/cfa/05_bifactor.html#session-info",
    "title": "40  Modello bifattoriale",
    "section": "40.5 Session Info",
    "text": "40.5 Session Info\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] BifactorIndicesCalculator_0.2.2 MASS_7.3-61                    \n [3] viridis_0.6.5                   viridisLite_0.4.2              \n [5] ggpubr_0.6.0                    ggExtra_0.10.1                 \n [7] gridExtra_2.3                   patchwork_1.3.0                \n [9] bayesplot_1.11.1                semTools_0.5-6                 \n[11] semPlot_1.1.6                   lavaan_0.6-19                  \n[13] psych_2.4.6.26                  scales_1.3.0                   \n[15] markdown_1.13                   knitr_1.49                     \n[17] lubridate_1.9.3                 forcats_1.0.0                  \n[19] stringr_1.5.1                   dplyr_1.1.4                    \n[21] purrr_1.0.2                     readr_2.1.5                    \n[23] tidyr_1.3.1                     tibble_3.2.1                   \n[25] ggplot2_3.5.1                   tidyverse_2.0.0                \n[27] here_1.0.1                     \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.17.1   jsonlite_1.8.9      magrittr_2.0.3     \n  [4] TH.data_1.1-2       estimability_1.5.1  farver_2.1.2       \n  [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n [10] Cairo_1.6-2         minqa_1.2.8         base64enc_0.1-3    \n [13] rstatix_0.7.2       htmltools_0.5.8.1   broom_1.0.7        \n [16] Formula_1.2-5       htmlwidgets_1.6.4   plyr_1.8.9         \n [19] sandwich_3.1-1      emmeans_1.10.5      zoo_1.8-12         \n [22] uuid_1.2-1          igraph_2.1.1        mime_0.12          \n [25] lifecycle_1.0.4     pkgconfig_2.0.3     Matrix_1.7-1       \n [28] R6_2.5.1            fastmap_1.2.0       shiny_1.9.1        \n [31] digest_0.6.37       OpenMx_2.21.13      fdrtool_1.2.18     \n [34] colorspace_2.1-1    rprojroot_2.0.4     Hmisc_5.2-0        \n [37] fansi_1.0.6         timechange_0.3.0    abind_1.4-8        \n [40] compiler_4.4.2      withr_3.0.2         glasso_1.11        \n [43] htmlTable_2.4.3     backports_1.5.0     carData_3.0-5      \n [46] ggsignif_0.6.4      corpcor_1.6.10      gtools_3.9.5       \n [49] tools_4.4.2         pbivnorm_0.6.0      foreign_0.8-87     \n [52] zip_2.3.1           httpuv_1.6.15       nnet_7.3-19        \n [55] glue_1.8.0          quadprog_1.5-8      promises_1.3.0     \n [58] nlme_3.1-166        lisrelToR_0.3       grid_4.4.2         \n [61] pbdZMQ_0.3-13       checkmate_2.3.2     cluster_2.1.6      \n [64] reshape2_1.4.4      generics_0.1.3      gtable_0.3.6       \n [67] tzdb_0.4.0          data.table_1.16.2   hms_1.1.3          \n [70] car_3.1-3           utf8_1.2.4          sem_3.1-16         \n [73] pillar_1.9.0        IRdisplay_1.1       rockchalk_1.8.157  \n [76] later_1.3.2         splines_4.4.2       cherryblossom_0.1.0\n [79] lattice_0.22-6      survival_3.7-0      kutils_1.73        \n [82] tidyselect_1.2.1    miniUI_0.1.1.1      pbapply_1.7-2      \n [85] airports_0.1.0      stats4_4.4.2        xfun_0.49          \n [88] qgraph_1.9.8        arm_1.14-4          stringi_1.8.4      \n [91] pacman_0.5.1        boot_1.3-31         evaluate_1.0.1     \n [94] codetools_0.2-20    mi_1.1              cli_3.6.3          \n [97] RcppParallel_5.1.9  IRkernel_1.3.2      rpart_4.1.23       \n[100] xtable_1.8-4        repr_1.1.7          munsell_0.5.1      \n[103] Rcpp_1.0.13-1       coda_0.19-4.1       png_0.1-8          \n[106] XML_3.99-0.17       parallel_4.4.2      usdata_0.3.1       \n[109] jpeg_0.1-10         lme4_1.1-35.5       mvtnorm_1.3-2      \n[112] openxlsx_4.2.7.1    crayon_1.5.3        openintro_2.5.0    \n[115] rlang_1.1.4         multcomp_1.4-26     mnormt_2.1.1",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Modello bifattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/06_efa_lavaan.html",
    "href": "chapters/cfa/06_efa_lavaan.html",
    "title": "41  Exploratory Structural Equation Modeling (ESEM)",
    "section": "",
    "text": "41.1 Introduzione\nNel modello di misurazione di Confirmatory Factor Analysis (CFA), comunemente adottato nella ricerca psicologica, solitamente sappiamo quali indicatori appartengono a ciascun fattore latente, una struttura denominata “a priori”. Questo approccio è utilizzato per verificare se la struttura fattoriale presunta corrisponde effettivamente ai dati raccolti.\nTuttavia, nonostante la popolarità della CFA, essa presenta delle limitazioni significative. I modelli CFA spesso risultano eccessivamente semplici e restrittivi, presupponendo “fattori puri”, cioè assumento che ciascun item saturi solamente sui suoi fattori latenti predeterminati, con saturazioni incrociate (ovvero, contributi di un item a fattori non primari) vincolate a zero. Questa restrizione può non riflettere adeguatamente la realtà di molte misure psicologiche, dove gli item tendono a riflettere più di un costrutto. Questo approccio può portare a una rappresentazione artificiale delle relazioni tra gli item e i fattori, risultando in statistiche di adattamento del modello sovrastimate e correlazioni tra fattori positivamente distorte. Studi di simulazione hanno mostrato che anche piccole saturazioni incrociate, se ignorate, possono portare ad una distorsione nelle stime dei parametri.\nUn altro problema è rappresentato dagli indici di bontà di adattamento utilizzati nei modelli CFA, che sono spesso troppo restrittivi per strumenti psicologici multifattoriali, rendendo quasi impossibile ottenere un “buon” adattamento senza significative modifiche ai modelli. Tuttavia, quando analizzati a livello di item e per affidabilità, i modelli che non mostrano un buon adattamento possono comunque indicare saturazioni ragionevoli e alti livelli di affidabilità.\nIn risposta a queste sfide, sono stati sviluppati approcci più flessibili e robusti, come l’Exploratory Structural Equation Modeling (ESEM).",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Exploratory Structural Equation Modeling (ESEM)</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/06_efa_lavaan.html#exploratory-structural-equation-modeling",
    "href": "chapters/cfa/06_efa_lavaan.html#exploratory-structural-equation-modeling",
    "title": "41  Exploratory Structural Equation Modeling (ESEM)",
    "section": "41.2 Exploratory Structural Equation Modeling",
    "text": "41.2 Exploratory Structural Equation Modeling\nL’ESEM combina elementi delle CFA e dell’Exploratory Factor Analysis (EFA) all’interno del tradizionale framework delle Equazioni Strutturali (SEM). Questo approccio rappresenta un compromesso tra la ricerca iterativa di soluzioni fattoriali ottimali, tipica dell’EFA, e la modellazione teorica restrittiva delle CFA.\nL’ESEM è essenzialmente un metodo confermativo che permette anche un’esplorazione attraverso l’uso di rotazioni mirate, mantenendo la presenza di caricamenti incrociati, seppur minimizzati. All’interno dell’ESEM, il ricercatore può prevedere a priori una struttura fattoriale, similmente a quanto avviene nelle CFA, ma con una maggiore flessibilità permessa dalla possibilità di modellare saturazioni incrociate.\nNell’ESEM, i fattori generali e specifici devono essere specificati come totalmente indipendenti, e le rotazioni ortogonali sono comuni nei modelli bifattoriali. I metodi di rotazione più usati nell’ESEM includono le rotazioni geomin e target, con rotazioni ortogonali adatte ai modelli più complessi.\nLe analisi di simulazione indicano che le correlazioni tra i fattori latenti ottenute con l’ESEM sono generalmente meno distorte e più vicine alle vere associazioni, rendendo i modelli ESEM più coerenti con le teorie sottostanti e le intenzioni degli strumenti psicometrici misurati.\nQuando un modello ESEM include solo una parte di misurazione, viene definito come “analisi fattoriale esplorativa” o EFA. Se il modello include anche una parte strutturale, come regressioni tra variabili latenti, è classificato come “modello di equazioni strutturali esplorativo” o ESEM.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Exploratory Structural Equation Modeling (ESEM)</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/06_efa_lavaan.html#un-esempio-pratico",
    "href": "chapters/cfa/06_efa_lavaan.html#un-esempio-pratico",
    "title": "41  Exploratory Structural Equation Modeling (ESEM)",
    "section": "41.3 Un Esempio Pratico",
    "text": "41.3 Un Esempio Pratico\nIn questo esempio pratico analizzeremo nuovamente i dati di Brown (2015), ovvero otto misure di personalità raccolte su un campione di 250 pazienti che hanno concluso un programma di psicoterapia. Utilizzeremo un’analisi EFA mediante la funzione efa() di lavaan.\nGli item sono i seguenti:\n\nanxiety (N1),\nhostility (N2),\ndepression (N3),\nself-consciousness (N4),\nwarmth (E1),\ngregariousness (E2),\nassertiveness (E3),\npositive emotions (E4).\n\n\nvarnames &lt;- c(\"N1\", \"N2\", \"N3\", \"N4\", \"E1\", \"E2\", \"E3\", \"E4\")\nsds &lt;- \"5.7  5.6  6.4  5.7  6.0  6.2  5.7  5.6\"\n\ncors &lt;- \"\n 1.000\n 0.767  1.000\n 0.731  0.709  1.000\n 0.778  0.738  0.762  1.000\n-0.351  -0.302  -0.356  -0.318  1.000\n-0.316  -0.280  -0.300  -0.267  0.675  1.000\n-0.296  -0.289  -0.297  -0.296  0.634  0.651  1.000\n-0.282  -0.254  -0.292  -0.245  0.534  0.593  0.566  1.000\"\n\npsychot_cor_mat &lt;- getCov(cors, names = varnames)\nn &lt;- 250\n\nDefiniamo un modello ad un solo fattore comune.\n\n# 1-factor model\nf1 &lt;- '\n    efa(\"efa\")*f1 =~ N1 + N2 + N3 + N4 + E1 + E2 + E3 + E4\n'\n\nDefiniamo un modello con due fattori comuni.\n\n# 2-factor model\nf2 &lt;- '\n    efa(\"efa\")*f1 +\n    efa(\"efa\")*f2 =~ N1 + N2 + N3 + N4 + E1 + E2 + E3 + E4\n'\n\nAdattiamo ai dati il modello ad un fattore comune.\n\nefa_f1 &lt;-cfa(\n    model = f1,\n    sample.cov = psychot_cor_mat,\n    sample.nobs = 250,\n    rotation = \"oblimin\"\n)\n\nEsaminiamo la soluzione ottenuta.\n\nsemPlot::semPaths(efa_f1,\n    what = \"col\", \n    whatLabels = \"std\", \n    style = \"mx\",\n    layout = \"tree2\", \n    nCharNodes = 7,\n    shapeMan = \"rectangle\", \n    sizeMan = 8, \n    sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\n\nsummary(\n    efa_f1,\n    fit.measures = TRUE,\n    standardized = TRUE,\n    rsquare = TRUE\n) |&gt;\n    print()\n\nlavaan 0.6-18 ended normally after 2 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        16\n\n  Rotation method                      OBLIMIN OBLIQUE\n  Oblimin gamma                                      0\n  Rotation algorithm (rstarts)                GPA (30)\n  Standardized metric                             TRUE\n  Row weights                                     None\n\n  Number of observations                           250\n\nModel Test User Model:\n                                                      \n  Test statistic                               375.327\n  Degrees of freedom                                20\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              1253.791\n  Degrees of freedom                                28\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.710\n  Tucker-Lewis Index (TLI)                       0.594\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2394.637\n  Loglikelihood unrestricted model (H1)      -2206.974\n                                                      \n  Akaike (AIC)                                4821.275\n  Bayesian (BIC)                              4877.618\n  Sample-size adjusted Bayesian (SABIC)       4826.897\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.267\n  90 Percent confidence interval - lower         0.243\n  90 Percent confidence interval - upper         0.291\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.187\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  f1 =~ efa                                                             \n    N1                0.879    0.051   17.333    0.000    0.879    0.880\n    N2                0.841    0.052   16.154    0.000    0.841    0.842\n    N3                0.841    0.052   16.175    0.000    0.841    0.843\n    N4                0.870    0.051   17.065    0.000    0.870    0.872\n    E1               -0.438    0.062   -7.041    0.000   -0.438   -0.439\n    E2               -0.398    0.063   -6.327    0.000   -0.398   -0.398\n    E3               -0.398    0.063   -6.342    0.000   -0.398   -0.399\n    E4               -0.364    0.063   -5.746    0.000   -0.364   -0.364\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                0.224    0.028    7.915    0.000    0.224    0.225\n   .N2                0.289    0.033    8.880    0.000    0.289    0.290\n   .N3                0.288    0.032    8.866    0.000    0.288    0.289\n   .N4                0.239    0.029    8.174    0.000    0.239    0.240\n   .E1                0.804    0.073   10.963    0.000    0.804    0.807\n   .E2                0.838    0.076   11.008    0.000    0.838    0.841\n   .E3                0.837    0.076   11.007    0.000    0.837    0.841\n   .E4                0.864    0.078   11.041    0.000    0.864    0.867\n    f1                1.000                               1.000    1.000\n\nR-Square:\n                   Estimate\n    N1                0.775\n    N2                0.710\n    N3                0.711\n    N4                0.760\n    E1                0.193\n    E2                0.159\n    E3                0.159\n    E4                0.133\n\n\n\n\nstandardizedSolution(efa_f1)\n\n\nA lavaan.data.frame: 17 x 9\n\n\nlhs\nop\nrhs\nest.std\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nf1\n=~\nN1\n0.8803717\n0.01823083\n48.290268\n0.000000e+00\n0.8446400\n0.9161035\n\n\nf1\n=~\nN2\n0.8424369\n0.02181993\n38.608605\n0.000000e+00\n0.7996706\n0.8852031\n\n\nf1\n=~\nN3\n0.8431190\n0.02175410\n38.756779\n0.000000e+00\n0.8004818\n0.8857563\n\n\nf1\n=~\nN4\n0.8719792\n0.01900765\n45.875162\n0.000000e+00\n0.8347249\n0.9092335\n\n\nf1\n=~\nE1\n-0.4389273\n0.05365312\n-8.180834\n2.220446e-16\n-0.5440854\n-0.3337691\n\n\nf1\n=~\nE2\n-0.3983268\n0.05578807\n-7.139999\n9.332535e-13\n-0.5076694\n-0.2889842\n\n\nf1\n=~\nE3\n-0.3991904\n0.05574480\n-7.161033\n8.006928e-13\n-0.5084482\n-0.2899326\n\n\nf1\n=~\nE4\n-0.3644271\n0.05741294\n-6.347474\n2.188791e-10\n-0.4769544\n-0.2518998\n\n\nN1\n~~\nN1\n0.2249456\n0.03209982\n7.007691\n2.422729e-12\n0.1620311\n0.2878601\n\n\nN2\n~~\nN2\n0.2903001\n0.03676382\n7.896353\n2.886580e-15\n0.2182443\n0.3623559\n\n\nN3\n~~\nN3\n0.2891503\n0.03668260\n7.882492\n3.108624e-15\n0.2172537\n0.3610469\n\n\nN4\n~~\nN4\n0.2396523\n0.03314856\n7.229643\n4.842793e-13\n0.1746823\n0.3046222\n\n\nE1\n~~\nE1\n0.8073429\n0.04709963\n17.141172\n0.000000e+00\n0.7150293\n0.8996564\n\n\nE2\n~~\nE2\n0.8413358\n0.04444377\n18.930344\n0.000000e+00\n0.7542276\n0.9284440\n\n\nE3\n~~\nE3\n0.8406470\n0.04450558\n18.888577\n0.000000e+00\n0.7534177\n0.9278764\n\n\nE4\n~~\nE4\n0.8671929\n0.04184566\n20.723604\n0.000000e+00\n0.7851769\n0.9492089\n\n\nf1\n~~\nf1\n1.0000000\n0.00000000\nNA\nNA\n1.0000000\n1.0000000\n\n\n\n\n\n\nlavaan::residuals(efa_f1, type = \"cor\") |&gt;\n    print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n       N1     N2     N3     N4     E1     E2     E3     E4\nN1  0.000                                                 \nN2  0.025  0.000                                          \nN3 -0.011 -0.001  0.000                                   \nN4  0.010  0.003  0.027  0.000                            \nE1  0.035  0.068  0.014  0.065  0.000                     \nE2  0.035  0.056  0.036  0.080  0.500  0.000              \nE3  0.055  0.047  0.040  0.052  0.459  0.492  0.000       \nE4  0.039  0.053  0.015  0.073  0.374  0.448  0.421  0.000\n\n\n\nAdattiamo ai dati il modello a due fattori comuni.\n\nefa_f2 &lt;- cfa(\n    model = f2,\n    sample.cov = psychot_cor_mat,\n    sample.nobs = 250,\n    rotation = \"oblimin\"\n)\n\nEsaminiamo la soluzione ottenuta.\n\nsemPlot::semPaths(efa_f2,\n    what = \"col\", \n    whatLabels = \"std\", \n    style = \"mx\",\n    layout = \"tree2\", \n    nCharNodes = 7,\n    shapeMan = \"rectangle\", \n    sizeMan = 8, \n    sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\n\nsummary(\n    efa_f2,\n    fit.measures = TRUE,\n    standardized = TRUE,\n    rsquare = TRUE\n) |&gt;\n    print()\n\nlavaan 0.6-18 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        25\n  Row rank of the constraints matrix                 2\n\n  Rotation method                      OBLIMIN OBLIQUE\n  Oblimin gamma                                      0\n  Rotation algorithm (rstarts)                GPA (30)\n  Standardized metric                             TRUE\n  Row weights                                     None\n\n  Number of observations                           250\n\nModel Test User Model:\n                                                      \n  Test statistic                                 9.811\n  Degrees of freedom                                13\n  P-value (Chi-square)                           0.709\n\nModel Test Baseline Model:\n\n  Test statistic                              1253.791\n  Degrees of freedom                                28\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.006\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2211.879\n  Loglikelihood unrestricted model (H1)      -2206.974\n                                                      \n  Akaike (AIC)                                4469.758\n  Bayesian (BIC)                              4550.752\n  Sample-size adjusted Bayesian (SABIC)       4477.840\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.048\n  P-value H_0: RMSEA &lt;= 0.050                    0.957\n  P-value H_0: RMSEA &gt;= 0.080                    0.001\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.010\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  f1 =~ efa                                                             \n    N1                0.874    0.053   16.592    0.000    0.874    0.876\n    N2                0.851    0.055   15.551    0.000    0.851    0.853\n    N3                0.826    0.054   15.179    0.000    0.826    0.828\n    N4                0.896    0.053   16.802    0.000    0.896    0.898\n    E1               -0.046    0.040   -1.138    0.255   -0.046   -0.046\n    E2                0.035    0.034    1.030    0.303    0.035    0.035\n    E3                0.000    0.040    0.010    0.992    0.000    0.000\n    E4               -0.006    0.049   -0.131    0.896   -0.006   -0.006\n  f2 =~ efa                                                             \n    N1               -0.017    0.032   -0.539    0.590   -0.017   -0.017\n    N2                0.011    0.035    0.322    0.748    0.011    0.011\n    N3               -0.035    0.036   -0.949    0.343   -0.035   -0.035\n    N4                0.031    0.031    0.994    0.320    0.031    0.031\n    E1                0.776    0.059   13.125    0.000    0.776    0.778\n    E2                0.854    0.058   14.677    0.000    0.854    0.855\n    E3                0.785    0.060   13.106    0.000    0.785    0.787\n    E4                0.695    0.063   10.955    0.000    0.695    0.697\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  f1 ~~                                                                 \n    f2               -0.432    0.059   -7.345    0.000   -0.432   -0.432\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .N1                0.218    0.028    7.790    0.000    0.218    0.219\n   .N2                0.279    0.032    8.693    0.000    0.279    0.280\n   .N3                0.287    0.032    8.907    0.000    0.287    0.289\n   .N4                0.216    0.029    7.578    0.000    0.216    0.217\n   .E1                0.361    0.044    8.226    0.000    0.361    0.362\n   .E2                0.292    0.043    6.787    0.000    0.292    0.293\n   .E3                0.379    0.046    8.315    0.000    0.379    0.381\n   .E4                0.509    0.053    9.554    0.000    0.509    0.511\n    f1                1.000                               1.000    1.000\n    f2                1.000                               1.000    1.000\n\nR-Square:\n                   Estimate\n    N1                0.781\n    N2                0.720\n    N3                0.711\n    N4                0.783\n    E1                0.638\n    E2                0.707\n    E3                0.619\n    E4                0.489\n\n\n\n\nstandardizedSolution(efa_f2) |&gt;\n    print()\n\n   lhs op rhs est.std    se      z pvalue ci.lower ci.upper\n1   f1 =~  N1   0.876 0.024 36.440  0.000    0.829    0.923\n2   f1 =~  N2   0.853 0.027 31.403  0.000    0.800    0.906\n3   f1 =~  N3   0.828 0.028 29.069  0.000    0.772    0.884\n4   f1 =~  N4   0.898 0.023 38.383  0.000    0.852    0.944\n5   f1 =~  E1  -0.046 0.040 -1.139  0.255   -0.125    0.033\n6   f1 =~  E2   0.035 0.034  1.031  0.303   -0.031    0.101\n7   f1 =~  E3   0.000 0.040  0.010  0.992   -0.078    0.079\n8   f1 =~  E4  -0.006 0.049 -0.131  0.896   -0.103    0.090\n9   f2 =~  N1  -0.017 0.032 -0.539  0.590   -0.079    0.045\n10  f2 =~  N2   0.011 0.035  0.322  0.748   -0.058    0.080\n11  f2 =~  N3  -0.035 0.037 -0.949  0.343   -0.106    0.037\n12  f2 =~  N4   0.031 0.031  0.994  0.320   -0.030    0.092\n13  f2 =~  E1   0.778 0.038 20.654  0.000    0.704    0.852\n14  f2 =~  E2   0.855 0.033 26.036  0.000    0.791    0.920\n15  f2 =~  E3   0.787 0.038 20.886  0.000    0.713    0.861\n16  f2 =~  E4   0.697 0.046 15.282  0.000    0.607    0.786\n17  N1 ~~  N1   0.219 0.032  6.905  0.000    0.157    0.281\n18  N2 ~~  N2   0.280 0.036  7.727  0.000    0.209    0.351\n19  N3 ~~  N3   0.289 0.036  7.909  0.000    0.217    0.360\n20  N4 ~~  N4   0.217 0.032  6.751  0.000    0.154    0.280\n21  E1 ~~  E1   0.362 0.047  7.673  0.000    0.269    0.454\n22  E2 ~~  E2   0.293 0.046  6.322  0.000    0.202    0.384\n23  E3 ~~  E3   0.381 0.049  7.816  0.000    0.285    0.476\n24  E4 ~~  E4   0.511 0.053  9.631  0.000    0.407    0.615\n25  f1 ~~  f1   1.000 0.000     NA     NA    1.000    1.000\n26  f2 ~~  f2   1.000 0.000     NA     NA    1.000    1.000\n27  f1 ~~  f2  -0.432 0.059 -7.345  0.000   -0.547   -0.317\n\n\nAnche se abbiamo introdotto finora soltanto la misura di bontà di adattamento del chi-quadrato, aggiungiamo qui il calcolo di altre misure di bontà di adattamento che discuteremo in seguito.\n\nfit_measures_robust &lt;- c(\n    \"chisq\", \"df\", \"pvalue\", \"cfi\", \"rmsea\", \"srmr\"\n)\n\nConfrontiamo le misure di bontà di adattamento del modello che ipotizza un solo fattore comune e il modello che ipotizza la presenza di due fattori comuni.\n\n# collect them for each model\nrbind(\n    fitmeasures(efa_f1, fit_measures_robust),\n    fitmeasures(efa_f2, fit_measures_robust)\n) %&gt;%\n    # wrangle\n    data.frame() %&gt;%\n    mutate(\n        chisq = round(chisq, digits = 0),\n        df = as.integer(df),\n        pvalue = ifelse(pvalue == 0, \"&lt; .001\", pvalue)\n    ) %&gt;%\n    mutate_at(vars(cfi:srmr), ~ round(., digits = 3)) |&gt;\n    print()\n\n  chisq df            pvalue  cfi rmsea  srmr\n1   375 20            &lt; .001 0.71 0.267 0.187\n2    10 13 0.709310449320098 1.00 0.000 0.010\n\n\n\nlavaan::residuals(efa_f2, type = \"cor\") |&gt;\n    print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n       N1     N2     N3     N4     E1     E2     E3     E4\nN1  0.000                                                 \nN2  0.018  0.000                                          \nN3 -0.014 -0.006  0.000                                   \nN4 -0.003 -0.013  0.017  0.000                            \nE1 -0.003  0.015 -0.012  0.000  0.000                     \nE2 -0.009 -0.004  0.006  0.007  0.006  0.000              \nE3  0.015 -0.008  0.011 -0.016  0.006 -0.010  0.000       \nE4 -0.001  0.000 -0.013  0.009 -0.024  0.006  0.016  0.000\n\n\n\nL’evidenza empirica supporta la superiorità del modello a due fattori rispetto a quello ad un solo fattore comune. In particolare, l’analisi fattoriale esplorativa svolta mediante la funzione efa() evidenzia la capacità del modello a due fattori di fornire una descrizione adeguata della struttura dei dati e di distinguere in modo sensato tra i due fattori ipotizzati.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Exploratory Structural Equation Modeling (ESEM)</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/06_efa_lavaan.html#riflessioni-conclusive",
    "href": "chapters/cfa/06_efa_lavaan.html#riflessioni-conclusive",
    "title": "41  Exploratory Structural Equation Modeling (ESEM)",
    "section": "41.4 Riflessioni Conclusive",
    "text": "41.4 Riflessioni Conclusive\nL’ESEM rappresenta un ponte significativo tra i modelli di misurazione tradizionali dell’Exploratory Factor Analysis (EFA) e il più esteso quadro del Confirmatory Factor Analysis/Structural Equation Modeling (CFA/SEM). Grazie a questo, l’ESEM combina i benefici dell’EFA con quelli del CFA/SEM, fornendo un approccio più flessibile e inclusivo nell’analisi dei dati. Tale integrazione ha segnato un progresso notevole nella ricerca statistica, evidenziando l’importanza dell’EFA che precedentemente era sottovalutata.\nL’ESEM e il quadro bifattoriale-ESEM, in particolare, offrono una rappresentazione più fedele e precisa della multidimensionalità dei costrutti psicometrici, che è spesso presente nelle misurazioni. Questo approccio riconosce e gestisce meglio la natura multidimensionale dei costrutti, a differenza dell’approccio tradizionale del CFA, che tende a sovrastimare le correlazioni tra i fattori quando non considera adeguatamente la loro natura gerarchica e interconnessa (Asparouhov et al., 2015; Morin et al., 2020).\nNonostante questi vantaggi, l’ESEM presenta alcune limitazioni che devono essere considerate:\n\nComplessità Computazionale: L’ESEM può essere più complesso e richiedere maggiori risorse computazionali rispetto agli approcci tradizionali, soprattutto quando si gestiscono grandi set di dati o modelli con molti fattori.\nInterpretazione dei Risultati: A causa della sua flessibilità, l’ESEM può produrre risultati che sono più difficili da interpretare. Ad esempio, la sovrapposizione tra i fattori può complicare l’interpretazione dei costrutti.\nRischio di Overfitting: La maggiore flessibilità dell’ESEM può anche portare a un rischio maggiore di overfitting, specialmente in campioni più piccoli o con modelli eccessivamente complessi.\nNecessità di Esperienza e Conoscenza: Per utilizzare efficacemente l’ESEM, è richiesta una comprensione approfondita della teoria sottostante e delle tecniche statistiche, che può essere una barriera per alcuni ricercatori.\n\nNonostante queste limitazioni, si prevede che i futuri sviluppi e le applicazioni dell’ESEM conducano a soluzioni più integrate e a un consenso più ampio sulle migliori pratiche nell’utilizzo di questo potente strumento statistico. Nel Capitolo 57 esploreremo il set-ESEM, una recente evoluzione di questa metodologia.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Exploratory Structural Equation Modeling (ESEM)</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/06_efa_lavaan.html#session-info",
    "href": "chapters/cfa/06_efa_lavaan.html#session-info",
    "title": "41  Exploratory Structural Equation Modeling (ESEM)",
    "section": "41.5 Session Info",
    "text": "41.5 Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] BifactorIndicesCalculator_0.2.2 ggokabeito_0.1.0               \n [3] viridis_0.6.5                   viridisLite_0.4.2              \n [5] ggpubr_0.6.0                    ggExtra_0.10.1                 \n [7] gridExtra_2.3                   patchwork_1.3.0                \n [9] bayesplot_1.11.1                semTools_0.5-6                 \n[11] semPlot_1.1.6                   lavaan_0.6-18                  \n[13] psych_2.4.6.26                  scales_1.3.0                   \n[15] markdown_1.13                   knitr_1.48                     \n[17] lubridate_1.9.3                 forcats_1.0.0                  \n[19] stringr_1.5.1                   dplyr_1.1.4                    \n[21] purrr_1.0.2                     readr_2.1.5                    \n[23] tidyr_1.3.1                     tibble_3.2.1                   \n[25] ggplot2_3.5.1                   tidyverse_2.0.0                \n[27] here_1.0.1                     \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.16.0   jsonlite_1.8.9      magrittr_2.0.3     \n  [4] TH.data_1.1-2       estimability_1.5.1  farver_2.1.2       \n  [7] nloptr_2.1.1        rmarkdown_2.28      vctrs_0.6.5        \n [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n [13] htmltools_0.5.8.1   broom_1.0.6         Formula_1.2-5      \n [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n [19] emmeans_1.10.4      zoo_1.8-12          uuid_1.2-1         \n [22] igraph_2.0.3        mime_0.12           lifecycle_1.0.4    \n [25] pkgconfig_2.0.3     Matrix_1.7-0        R6_2.5.1           \n [28] fastmap_1.2.0       shiny_1.9.1         numDeriv_2016.8-1.1\n [31] digest_0.6.37       OpenMx_2.21.12      fdrtool_1.2.18     \n [34] colorspace_2.1-1    rprojroot_2.0.4     Hmisc_5.1-3        \n [37] fansi_1.0.6         timechange_0.3.0    abind_1.4-8        \n [40] compiler_4.4.1      withr_3.0.1         glasso_1.11        \n [43] htmlTable_2.4.3     backports_1.5.0     carData_3.0-5      \n [46] ggsignif_0.6.4      MASS_7.3-61         corpcor_1.6.10     \n [49] gtools_3.9.5        tools_4.4.1         pbivnorm_0.6.0     \n [52] foreign_0.8-87      zip_2.3.1           httpuv_1.6.15      \n [55] nnet_7.3-19         glue_1.7.0          quadprog_1.5-8     \n [58] promises_1.3.0      nlme_3.1-166        lisrelToR_0.3      \n [61] grid_4.4.1          pbdZMQ_0.3-13       checkmate_2.3.2    \n [64] cluster_2.1.6       reshape2_1.4.4      generics_0.1.3     \n [67] gtable_0.3.5        tzdb_0.4.0          data.table_1.16.0  \n [70] hms_1.1.3           car_3.1-2           utf8_1.2.4         \n [73] sem_3.1-16          pillar_1.9.0        IRdisplay_1.1      \n [76] rockchalk_1.8.157   later_1.3.2         splines_4.4.1      \n [79] lattice_0.22-6      survival_3.7-0      kutils_1.73        \n [82] tidyselect_1.2.1    miniUI_0.1.1.1      pbapply_1.7-2      \n [85] stats4_4.4.1        xfun_0.47           qgraph_1.9.8       \n [88] arm_1.14-4          stringi_1.8.4       pacman_0.5.1       \n [91] boot_1.3-31         evaluate_1.0.0      codetools_0.2-20   \n [94] mi_1.1              cli_3.6.3           RcppParallel_5.1.9 \n [97] IRkernel_1.3.2      rpart_4.1.23        xtable_1.8-4       \n[100] repr_1.1.7          munsell_0.5.1       Rcpp_1.0.13        \n[103] coda_0.19-4.1       png_0.1-8           XML_3.99-0.17      \n[106] parallel_4.4.1      jpeg_0.1-10         lme4_1.1-35.5      \n[109] mvtnorm_1.3-1       openxlsx_4.2.7.1    crayon_1.5.3       \n[112] rlang_1.1.4         multcomp_1.4-26     mnormt_2.1.1       \n\n\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Exploratory Structural Equation Modeling (ESEM)</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html",
    "href": "chapters/cfa/07_fa_in_r.html",
    "title": "42  Strategia Integrata per un’Analisi Fattoriale",
    "section": "",
    "text": "42.1 Introduzione\nSecondo Saqr & López-Pernas (2024), non esiste una chiara distinzione tra quando usare l’Analisi Fattoriale Esplorativa (EFA) o l’Analisi Fattoriale Confermativa (CFA); spesso, entrambe le tecniche vengono utilizzate nello stesso studio. Questa sezione descrive una strategia metodica per integrare sia l’EFA che la CFA, articolata in tre passaggi che i ricercatori dovrebbero seguire quando i costrutti latenti sono parte del loro studio (sia perché sono il focus principale dello strumento, sia perché fungono da predittori o esiti nell’analisi): (1) esplorazione della struttura fattoriale, (2) costruzione del modello fattoriale e valutazione dell’adattamento, e (3) valutazione della generalizzabilità. Si presuppone che il ricercatore abbia già completato una fase preliminare di sviluppo dello strumento per un costrutto di interesse, come ad esempio la competenza interculturale, utilizzando variabili o comportamenti da ricerche precedenti, adattando uno strumento esistente, o sviluppandone uno nuovo basato sulla teoria. Inoltre, si suppone che siano stati raccolti dati da un campione rappresentativo della popolazione di interesse (ad esempio, studenti).",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#passo-1-esplorazione-della-struttura-fattoriale",
    "href": "chapters/cfa/07_fa_in_r.html#passo-1-esplorazione-della-struttura-fattoriale",
    "title": "42  Strategia Integrata per un’Analisi Fattoriale",
    "section": "42.2 Passo 1: Esplorazione della Struttura Fattoriale",
    "text": "42.2 Passo 1: Esplorazione della Struttura Fattoriale\nUna volta selezionate le variabili e raccolti i dati su di esse, il ricercatore dovrebbe iniziare con un’EFA. Se viene utilizzato uno strumento già validato o se si dispone di ipotesi forti sulla struttura fattoriale sottostante, si può verificare se il numero di fattori e le modalità di carico delle variabili sui fattori sono in linea con i risultati previsti. Le domande chiave da porsi sono: “Le variabili che si ritiene siano influenzate dallo stesso fattore sottostante caricano effettivamente su un unico fattore?” e, nel caso si supponga che le variabili siano causate da un unico fattore sottostante, “Queste variabili mostrano effettivamente forti carichi fattoriali su un solo fattore?”\nSe lo strumento è nuovo, occorre verificare se i fattori e lo schema dei carichi sono interpretabili. Le domande chiave diventano: “Le variabili che caricano principalmente sullo stesso fattore condividono effettivamente un contenuto comune?” e “Le variabili che caricano su fattori diversi sono effettivamente qualitativamente differenti in qualche modo?”\nRitornando all’esempio di un test di matematica, si potrebbe osservare che compiti di addizione, sottrazione, divisione e moltiplicazione caricano su 4 fattori distinti, identificabili preliminarmente come abilità in addizione, sottrazione, divisione e moltiplicazione. In questa fase, può essere necessario fare aggiustamenti, come rimuovere variabili che non presentano carichi significativi (ad esempio, inferiori a 0.3) su alcuna dimensione, e rieseguire l’EFA. È importante riflettere sulle ragioni dei bassi carichi fattoriali (ad esempio, una formulazione ambigua di un item) e non rimuovere variabili senza una motivazione solida.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#passo-2-costruzione-del-modello-fattoriale-e-valutazione-delladattamento",
    "href": "chapters/cfa/07_fa_in_r.html#passo-2-costruzione-del-modello-fattoriale-e-valutazione-delladattamento",
    "title": "42  Strategia Integrata per un’Analisi Fattoriale",
    "section": "42.3 Passo 2: Costruzione del Modello Fattoriale e Valutazione dell’Adattamento",
    "text": "42.3 Passo 2: Costruzione del Modello Fattoriale e Valutazione dell’Adattamento\nDopo aver scelto un modello con l’EFA, è necessario raffinare il modello e utilizzare la CFA per valutare quanto bene il modello si adatta ai dati (ossia, quanto le covarianze tra variabili previste dalla struttura fattoriale corrispondono alle covarianze osservate nel dataset). Nell’EFA, tutte le variabili potevano caricare su tutti i fattori; tuttavia, spesso si dispone di informazioni teoriche o empiriche che suggeriscono di limitare il numero di carichi trasversali. In questa fase, si possono eliminare relazioni tra fattori e variabili che non sono coerenti con la teoria, ma attenzione al valore dei carichi fattoriali. I carichi prossimi allo zero possono essere rimossi senza rischio, ma i carichi maggiori richiedono maggiore cautela, poiché, anche se inizialmente non sembrano sensati, i dati suggeriscono la loro presenza. Quindi, esaminateli attentamente; se, dopo un’analisi approfondita, possono essere inclusi nella teoria o nelle ipotesi iniziali, si possono mantenere; in caso contrario, si possono rimuovere e verificare se il modello continua a essere adatto ai dati.\nDopo aver scelto le relazioni variabile-fattore da rimuovere, si costruisce il modello CFA corrispondente e lo si adatta ai dati per verificare se è sufficientemente adatto. Se il modello non si adatta bene, si può tornare ai risultati dell’EFA e valutare se bisogna consentire ulteriori carichi diversi da zero o applicare altre modifiche. È importante aggiungere al modello solo relazioni che abbiano senso dal punto di vista teorico.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#passo-3-valutazione-della-generalizzabilità",
    "href": "chapters/cfa/07_fa_in_r.html#passo-3-valutazione-della-generalizzabilità",
    "title": "42  Strategia Integrata per un’Analisi Fattoriale",
    "section": "42.4 Passo 3: Valutazione della Generalizzabilità",
    "text": "42.4 Passo 3: Valutazione della Generalizzabilità\nDopo i passaggi precedenti, si dispone di un modello preliminare che si adatta bene sia alla teoria sia ai dati attuali. Tuttavia, l’obiettivo finale dovrebbe essere quello di creare strumenti che possano essere utilizzati per misurare costrutti in studi futuri, per cui è essenziale verificare che il modello preliminare si adatti anche a nuovi dati. Questo passaggio finale, chiamato validazione incrociata, consiste nell’adattare il modello a un nuovo dataset della stessa popolazione.\nPer valutare la generalizzabilità, si potrebbe raccogliere un secondo dataset. Tuttavia, nella pratica, raccogliere dati più di una volta per lo sviluppo di uno strumento è spesso poco pratico. Un’alternativa valida è dividere casualmente il dataset in due parti: un campione su cui eseguire i Passi 1 (esplorazione della struttura fattoriale con EFA) e 2 (costruzione del modello fattoriale e valutazione con CFA) e un campione di riserva, che verrà utilizzato per il Passo 3 (valutazione della generalizzabilità). Se il modello CFA si adatta anche al campione di riserva, si può essere più sicuri che lo strumento sia utilizzabile in studi futuri. In caso contrario, occorre investigare le fonti di disadattamento tra il modello CFA e il campione di riserva e utilizzare i risultati di questa analisi per aggiornare teoria e modello.\nQuesti passaggi rappresentano una strategia adatta per l’analisi fattoriale in qualsiasi studio che utilizzi strumenti per misurare dimensioni latenti. L’unica situazione in cui sarebbe teoricamente possibile adattare direttamente un modello CFA è quando si usa uno strumento già validato su un campione della stessa popolazione, ma anche in questo caso è consigliabile seguire tutti i passaggi per evitare risultati distorti.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#analisi-fattoriale-in-r",
    "href": "chapters/cfa/07_fa_in_r.html#analisi-fattoriale-in-r",
    "title": "42  Strategia Integrata per un’Analisi Fattoriale",
    "section": "42.5 Analisi Fattoriale in R",
    "text": "42.5 Analisi Fattoriale in R\nSaqr & López-Pernas (2024) forniscono un tutorial in cui sono descritti i passaggi essenziali per esplorare la struttura fattoriale utilizzando sia l’Analisi Fattoriale Esplorativa (EFA) che l’Analisi Fattoriale Confermativa (CFA) con R. In particolare, vengono affrontati: la verifica delle caratteristiche dei dati per valutare l’adeguatezza per EFA/CFA, la scelta del numero di fattori, la valutazione dell’adattamento globale e locale del modello, e la verifica della generalizzabilità del modello fattoriale finale.\nIl tutorial inizia con la preparazione necessaria: importare i dati, verificare l’idoneità dei dati per l’analisi fattoriale e predisporre un campione di riserva per la valutazione della generalizzabilità. Le sezioni successive illustrano come condurre un’EFA per definire una struttura fattoriale preliminare (Passo 1) e come affinare questo modello con la CFA (Passo 2). L’ultima sezione mostra come testare la generalizzabilità del modello fattoriale affinato tramite la validazione incrociata (Passo 3).\n\n42.5.1 Preparazione\n\nSaqr & López-Pernas (2024) utilizzano un dataset che raccoglie dati di un’indagine sul burnout degli insegnanti in Indonesia, con un totale di 876 rispondenti. Le domande coprono cinque ambiti: Concetto di Sé dell’Insegnante (TSC, 5 domande), Efficacia dell’Insegnante (TE, 5 domande), Esaurimento Emotivo (EE, 5 domande), Depersonalizzazione (DP, 3 domande) e Riduzione del Senso di Realizzazione Personale (RPA, 7 domande), per un totale di 25 variabili. Le risposte sono su una scala Likert a 5 punti (da 1 = “mai” a 5 = “sempre”).\n\n\ndataset &lt;- rio::import(\"https://github.com/lamethods/data/raw/main/4_teachersBurnout/2.%20Response.xlsx\")\nvar_names &lt;- colnames(dataset)\n\n\n\n42.5.2 I Dati Sono Adatti all’Analisi Fattoriale?\nPer condurre un’Analisi Fattoriale Esplorativa (EFA) o Confermativa (CFA) è importante che i dati presentino alcune caratteristiche. Anzitutto, le variabili dovrebbero essere continue. Anche se le variabili raramente sono veramente continue, possono essere trattate come tali se sono state misurate su una scala con almeno cinque categorie di risposta e se le risposte sono distribuite in modo ragionevolmente simmetrico.\nSe le variabili non sono continue, è comunque possibile eseguire un’analisi fattoriale, ma sarà necessario un metodo di stima specifico per dati categorici. Inoltre, è preferibile che tutte le variabili siano misurate sulla stessa scala; in alcuni casi, soprattutto nei dati educativi, questo potrebbe non essere garantito. Se le variabili sono su scale diverse o se, pur essendo sulla stessa scala, hanno intervalli di punteggio molto diversi (ad esempio, alcune variabili con punteggi da 1 a 5 e altre con punteggi solo da 2 a 4), è opportuno trasformare le variabili per rendere le scale più omogenee prima di procedere con l’analisi fattoriale.\nPer ispezionare l’intervallo di ogni variabile, è possibile utilizzare il seguente comando:\n\ndescribe(dataset)\n\n\nA psych: 23 x 13\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nTSC1\n1\n876\n3.65\n0.685\n4\n3.62\n0.00\n1\n5\n4\n-0.0924\n0.0630\n0.0231\n\n\nTSC2\n2\n876\n3.81\n0.639\n4\n3.78\n0.00\n2\n5\n3\n-0.0729\n-0.1412\n0.0216\n\n\nTSC3\n3\n876\n3.73\n0.640\n4\n3.71\n0.00\n2\n5\n3\n-0.1658\n-0.0184\n0.0216\n\n\nTSC4\n4\n876\n3.71\n0.673\n4\n3.67\n0.00\n2\n5\n3\n-0.0258\n-0.2521\n0.0228\n\n\nTSC5\n5\n876\n3.82\n0.652\n4\n3.79\n0.00\n2\n5\n3\n-0.0996\n-0.1293\n0.0220\n\n\nTE1\n6\n876\n4.06\n0.712\n4\n4.10\n0.00\n1\n5\n4\n-0.4669\n0.3770\n0.0241\n\n\nTE2\n7\n876\n4.04\n0.698\n4\n4.07\n0.00\n2\n5\n3\n-0.2200\n-0.4490\n0.0236\n\n\nTE3\n8\n876\n4.12\n0.705\n4\n4.17\n0.00\n1\n5\n4\n-0.7210\n1.6004\n0.0238\n\n\nTE4\n9\n876\n4.11\n0.690\n4\n4.15\n0.00\n1\n5\n4\n-0.4737\n0.5071\n0.0233\n\n\nTE5\n10\n876\n3.90\n0.754\n4\n3.92\n0.00\n1\n5\n4\n-0.4117\n0.1647\n0.0255\n\n\nEE1\n11\n876\n3.81\n0.759\n4\n3.81\n0.00\n1\n5\n4\n-0.3458\n0.2314\n0.0256\n\n\nEE2\n12\n876\n3.73\n0.848\n4\n3.75\n1.48\n1\n5\n4\n-0.3696\n0.1226\n0.0287\n\n\nEE3\n13\n876\n3.88\n0.828\n4\n3.91\n1.48\n1\n5\n4\n-0.3128\n-0.3990\n0.0280\n\n\nEE4\n14\n876\n3.69\n0.796\n4\n3.67\n1.48\n1\n5\n4\n-0.0341\n-0.4074\n0.0269\n\n\nEE5\n15\n876\n3.99\n0.811\n4\n4.03\n1.48\n1\n5\n4\n-0.4273\n-0.2694\n0.0274\n\n\nDE1\n16\n876\n3.92\n0.677\n4\n3.93\n0.00\n1\n5\n4\n-0.5261\n1.2456\n0.0229\n\n\nDE2\n17\n876\n3.60\n0.682\n4\n3.58\n1.48\n1\n5\n4\n-0.2190\n0.6393\n0.0230\n\n\nDE3\n18\n876\n3.82\n0.698\n4\n3.79\n0.00\n1\n5\n4\n-0.1368\n0.0125\n0.0236\n\n\nRPA1\n19\n876\n3.93\n0.834\n4\n3.97\n1.48\n1\n5\n4\n-0.5931\n0.5042\n0.0282\n\n\nRPA2\n20\n876\n3.94\n0.805\n4\n3.99\n0.00\n1\n5\n4\n-0.7855\n1.2227\n0.0272\n\n\nRPA3\n21\n876\n3.88\n0.789\n4\n3.91\n0.00\n1\n5\n4\n-0.5879\n0.7489\n0.0266\n\n\nRPA4\n22\n876\n3.87\n0.765\n4\n3.89\n0.00\n1\n5\n4\n-0.4781\n0.3333\n0.0258\n\n\nRPA5\n23\n876\n3.84\n0.786\n4\n3.86\n0.00\n1\n5\n4\n-0.5347\n0.6706\n0.0265\n\n\n\n\n\nTutte le variabili sono state valutate su scale Likert a 5 punti, e dall’output si nota che hanno intervalli di punteggio molto simili. Pertanto, è possibile trattarle come variabili continue, senza necessità di trasformazione.\nIn secondo luogo, la dimensione del campione deve essere sufficientemente ampia. Esistono alcune regole empiriche a riguardo. Una regola di base suggerisce un campione di circa 200 osservazioni, anche se campioni più piccoli possono essere adeguati per modelli semplici (con pochi fattori e/o relazioni forti tra fattori e variabili osservate), mentre modelli più complessi (con più fattori e/o relazioni più deboli) richiedono campioni più ampi. Altre regole si basano sul rapporto tra la dimensione del campione e il numero di parametri da stimare (cioè carichi fattoriali, intercetti e varianze degli errori). Bentler e Chou raccomandano 5 osservazioni per ogni parametro da stimare, mentre Jackson suggerisce di avere almeno 10, preferibilmente 20 osservazioni per ogni parametro (ad esempio, per un modello con un fattore e 10 variabili, bisognerebbe mirare a 300 casi, calcolando i 30 parametri da stimare: 10 carichi fattoriali, 10 intercetti e 10 varianze degli errori).\n\nnrow(dataset)\n\n876\n\n\nPer il dataset di esempio con 25 variabili, che si presume misurino 5 costrutti latenti, è necessario stimare 25 intercetti, 25 varianze residue e 125 carichi fattoriali (5 × 25 = 125). In totale, questo comporta la stima di 175 parametri. Guardando l’output, si può concludere che la dimensione del campione è sufficientemente grande per l’EFA e la CFA, seguendo le linee guida di Bentler e Chou (5 × 175 = 875), ma non per quelle di Johnson. Poiché il campione non raggiunge il doppio delle dimensioni raccomandate, non si dovrebbe creare un campione di riserva; la validazione del modello potrebbe quindi essere riservata a uno studio futuro. Tuttavia, a scopo illustrativo, sarà comunque mostrato come creare un sottoinsieme di riserva per valutare la generalizzabilità del modello fattoriale finale.\nInoltre, è necessario che ci siano correlazioni sufficientemente alte tra le variabili. Altrimenti, non avrebbe senso esaminare la struttura fattoriale. Per verificare che le variabili nel dataset siano correlate, è possibile utilizzare il test di Bartlett [28], che verifica se la matrice di correlazione è una matrice identità (ovvero una matrice in cui tutti gli elementi fuori diagonale sono zero), indicando quindi che le variabili non sono correlate. L’ipotesi nulla di questo test afferma che la matrice di correlazione è una matrice identità. Se l’ipotesi nulla viene rifiutata, si può concludere che le variabili sono correlate e, quindi, procedere con l’analisi fattoriale. Con il comando seguente, si verifica se il p-value del test di Bartlett è inferiore al livello di significatività di 0,05 e, quindi, se l’ipotesi nulla di “assenza di correlazione tra variabili” può essere rifiutata:\n\n(cortest.bartlett(\n    R = cor(dataset[, var_names]), n =\n        nrow(dataset)\n)$p.value) &lt; 0.05\n\nTRUE\n\n\nIl p-value è effettivamente inferiore a 0,05, quindi le variabili sono correlate.\nOltre a verificare le correlazioni tra variabili, è utile determinare se esiste una quantità sufficiente di varianza comune tra di esse. Questo può essere valutato con il test di Kaiser-Meyer-Olkin (KMO) [29]. La statistica KMO misura la proporzione di varianza totale tra variabili che potrebbe essere varianza comune. Più alta è questa proporzione, più alto è il valore KMO, e quindi i dati saranno più adatti all’analisi fattoriale. Kaiser suggerisce che il valore KMO dovrebbe essere almeno 0,8 per avere dati adeguati (e almeno 0,9 per dati eccellenti). Con il seguente comando è possibile ottenere i risultati:\n\nKMO(dataset)\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = dataset)\nOverall MSA =  0.94\nMSA for each item = \nTSC1 TSC2 TSC3 TSC4 TSC5  TE1  TE2  TE3  TE4  TE5  EE1  EE2  EE3  EE4  EE5 \n0.96 0.96 0.95 0.94 0.96 0.93 0.96 0.94 0.94 0.96 0.95 0.94 0.95 0.94 0.97 \n DE1  DE2  DE3 RPA1 RPA2 RPA3 RPA4 RPA5 \n0.87 0.86 0.92 0.91 0.91 0.95 0.94 0.96 \n\n\nIl valore complessivo di KMO è pari a 0,94, indicando che i dati sono eccellenti per l’analisi fattoriale.\nOltre alle caratteristiche essenziali dei dati, è importante considerare la non-normalità delle variabili e i dati mancanti. Se le variabili non sono distribuite normalmente, è necessario utilizzare un metodo di stima robusto. Inoltre, se ci sono valori mancanti per una o più variabili, questo deve essere gestito nella stima. Di seguito sarà descritto come procedere. La normalità può essere valutata osservando gli istogrammi delle variabili: come mostrato nella figura successiva, le distribuzioni delle variabili presentano una leggera asimmetria a sinistra. Perciò, è consigliabile utilizzare un metodo di stima robusto rispetto alla non-normalità.\n\ndataset |&gt;\n    pivot_longer(2:ncol(dataset),\n        names_to = \"Variable\", values_to = \"Score\"\n    ) |&gt;\n    ggplot(aes(x = Score)) +\n    geom_histogram(bins = 6) +\n    scale_x_continuous(\n        limits = c(0, 6), breaks =\n            c(1, 2, 3, 4, 5)\n    ) +\n    facet_wrap(\"Variable\",\n        ncol = 6, scales =\n            \"free\"\n    )\n\n\n\n\n\n\n\n\nSuccessivamente, possiamo verificare la presenza di dati mancanti con il seguente comando:\n\ncolSums(is.na(dataset)) |&gt; print()\n\nTSC1 TSC2 TSC3 TSC4 TSC5  TE1  TE2  TE3  TE4  TE5  EE1  EE2  EE3  EE4  EE5 \n   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0 \n DE1  DE2  DE3 RPA1 RPA2 RPA3 RPA4 RPA5 \n   0    0    0    0    0    0    0    0",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#separare-un-campione-di-riserva",
    "href": "chapters/cfa/07_fa_in_r.html#separare-un-campione-di-riserva",
    "title": "42  Strategia Integrata per un’Analisi Fattoriale",
    "section": "42.6 Separare un Campione di Riserva",
    "text": "42.6 Separare un Campione di Riserva\nUna volta verificato che i dati siano adatti all’analisi fattoriale, puoi considerare di separare un campione di riserva per valutare la generalizzabilità dei risultati. Tuttavia, è importante tenere conto della dimensione del campione. Come indicato sopra, la dimensione minima richiesta del campione dovrebbe essere almeno 5 (preferibilmente 10 o 20) volte il numero di parametri che stai stimando. Non separare un campione di riserva a meno che il campione non sia circa il doppio della dimensione minima richiesta, altrimenti potresti non avere dati sufficienti per costruire un modello appropriato. In questo caso, la validazione del modello finale dovrà essere effettuata in studi futuri. Tuttavia, il numero di parametri per un modello CFA è generalmente inferiore rispetto a un modello EFA, quindi è accettabile se il campione di riserva è leggermente più piccolo rispetto al campione di costruzione del modello.\nCome determinato in precedenza, il campione non è due volte la dimensione minima richiesta per un modello con 25 variabili e 5 fattori latenti, ma, a scopo illustrativo, separeremo comunque un campione di riserva. Per fare ciò, si possono assegnare casualmente 438 righe a un dataset di costruzione del modello e a un dataset di riserva. Ecco i comandi per farlo:\n\nImpostare il seed: Con la prima riga di codice, imposti un seed per ottenere la stessa sequenza casuale ogni volta che esegui il codice, garantendo così la replicabilità dei risultati.\nCreare il vettore di classificazione: Crei un vettore chiamato ind che contiene 438 volte i termini “model.building” e “holdout” rispettivamente, in ordine casuale. Ottieni così un totale di 876 classificazioni, una per ciascun partecipante (cioè, una per riga) nei tuoi dati.\nDividere il dataset: Crei una lista temporanea chiamata tmp che contiene due dataset: per ciascun numero di riga, la funzione split() verifica se è assegnata l’etichetta “model.building” o “holdout” e assegna la riga al rispettivo dataset. Ad esempio, se i primi tre elementi del vettore ind sono “model.building”, “model.building” e “holdout”, le prime due righe del dataset vengono assegnate al dataset di costruzione del modello, mentre la terza riga va al dataset di riserva.\nEstrarre i dataset finali: Nell’ultimo passaggio, estrai i due nuovi dataset dalla lista e li memorizzi negli oggetti chiamati “model.building” e “holdout”.\n\n\nset.seed(19)\nind &lt;- sample(\n    c(rep(\"model.building\", 438), rep(\"holdout\", 438))\n)\ntmp &lt;- split(dataset, ind)\nmodel.building &lt;- tmp$model.building\nholdout &lt;- tmp$holdout",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#passo-1-esplorare-la-struttura-fattoriale",
    "href": "chapters/cfa/07_fa_in_r.html#passo-1-esplorare-la-struttura-fattoriale",
    "title": "42  Strategia Integrata per un’Analisi Fattoriale",
    "section": "42.7 Passo 1: Esplorare la Struttura Fattoriale",
    "text": "42.7 Passo 1: Esplorare la Struttura Fattoriale\nIl primo passo nell’esplorare la struttura fattoriale consiste nel determinare quante dimensioni potrebbero sottostare al costrutto di interesse. In questo tutorial, vedremo come farlo utilizzando due metodi comuni: l’analisi parallela e il criterio di informazione bayesiano (BIC). Questi metodi si completano a vicenda: l’analisi parallela fornisce un intervallo per il numero di dimensioni, mentre il BIC ci aiuta a scegliere il numero specifico che meglio si adatta ai dati.\n\n42.7.1 Analisi Parallela\nL’analisi parallela è un metodo basato su simulazioni che individua il numero di fattori confrontando la varianza spiegata da un certo numero di fattori nei dati con la varianza spiegata dallo stesso numero di fattori in dataset simulati (creati con lo stesso numero di variabili e osservazioni ma senza correlazioni). Se la varianza spiegata nei dati è maggiore rispetto a quella nei dati simulati, significa che il fattore è significativo e non dovuto al caso. Il numero di fattori selezionato è quello per cui la varianza spiegata nei dati reali è maggiore di quella nei dati simulati. Dettagli tecnici su questo metodo si trovano nel file di aiuto della funzione fa.parallel().\n\n\n42.7.2 Uso dell’Analisi Parallela\nPer applicare l’analisi parallela, si specificano i dati di costruzione del modello e tutte le colonne delle variabili d’interesse. Con l’argomento fa = \"fa\", si specifica di cercare il miglior numero di fattori per l’analisi fattoriale (e non per l’analisi dei componenti principali, un metodo diverso). Il risultato include un messaggio e un grafico: il messaggio indica che probabilmente ci sono cinque fattori sottostanti ai dati. Nel grafico, si nota che, oltre cinque fattori, la varianza spiegata nei dati è inferiore rispetto a quella nei dati simulati.\nL’analisi parallela è puramente basata sui dati e dipende dal campione specifico, quindi i cinque fattori sono un punto di partenza e dovrebbero essere considerati come un intervallo plausibile di fattori (più o meno uno o più fattori, se necessario).\n\n\n42.7.3 Criterio di Informazione Bayesiano (BIC)\nPer decidere il numero finale di fattori, si eseguono più modelli fattoriali con diversi numeri di fattori (quelli plausibili indicati dall’analisi parallela) e li si confronta per interpretabilità (cioè, il senso teorico delle relazioni tra variabili e fattori) e adattamento, utilizzando il BIC. Il BIC permette di selezionare il modello equilibrando la qualità dell’adattamento ai dati con la semplicità del modello, penalizzando ogni parametro aggiuntivo. Più basso è il valore BIC, meglio il modello si adatta ai dati. Se il BIC più basso corrisponde al modello con cinque fattori, si ha un supporto per la soluzione a cinque fattori, ma la decisione finale dovrebbe sempre tener conto anche dell’interpretabilità.\n\nfa.parallel(x = model.building[, var_names], fa = \"fa\")\n\nParallel analysis suggests that the number of factors =  5  and the number of components =  NA \n\n\n\n\n\n\n\n\n\n\n\n42.7.4 Analisi Fattoriale Esplorativa\nL’analisi fattoriale esplorativa può essere eseguita con il seguente comando:\n\nEFA &lt;- efa(\n    data = model.building[, var_names],\n    nfactors = 4:6,\n    rotation = \"geomin\", \n    estimator = \"MLR\",\n    meanstructure = TRUE\n)\n\nLa funzione efa() è inclusa nel pacchetto lavaan.\n\nPrimo argomento: Con data, si specifica il dataset su cui eseguire l’analisi fattoriale.\nArgomento nfactors: Qui si indica l’intervallo di fattori per cui si vogliono ottenere i risultati.\nArgomento rotation: Questo serve a “identificare” il modello, un passaggio necessario solo per l’EFA (Analisi Fattoriale Esplorativa) e non per la CFA (Analisi Fattoriale Confermativa). In CFA, infatti, si impongono restrizioni che fissano la struttura del modello, ma in EFA, senza queste restrizioni, si ottengono infinite soluzioni con lo stesso livello di adattamento. Di conseguenza, è possibile “ruotare” la matrice dei carichi fattoriali in molti modi senza cambiare la posizione relativa dei dati osservati, ma solo orientando diversamente gli assi dei fattori latenti.\n\nImpostazione dei parametri\nNell’argomento rotation, è possibile usare geomin, una rotazione che consente ai fattori di essere correlati, il che è realistico in contesti educativi. Per altre opzioni di rotazione, consultare [23].\n\nArgomento estimator: permette di scegliere il metodo di stima. Il valore predefinito è la stima di massima verosimiglianza (“ML”), ma qui si usa “MLR” (massima verosimiglianza robusta) per gestire lievi violazioni della normalità.\nDati mancanti: Se i dati contengono valori mancanti, si può aggiungere l’argomento missing = \"fiml\", che applica un metodo di stima che usa tutte le informazioni disponibili (Full Information Maximum Likelihood, FIML), ideale se i dati mancanti sono almeno MAR (Missing At Random).\nmeanstructure = TRUE: Se impostato su TRUE, stima anche gli intercetti delle variabili osservate oltre a varianze e covarianze. Se si utilizza l’argomento missing, meanstructure sarà automaticamente impostato su TRUE.\n\nInfine, per estrarre e ordinare i valori BIC in ordine crescente, è possibile utilizzare il seguente comando:\n\nsort(fitMeasures(EFA)[\"bic\", ]) |&gt; print()\n\nnfactors = 5 nfactors = 4 nfactors = 6 \n       18142        18167        18189 \n\n\nL’output indica che il modello con cinque fattori è quello migliore secondo il BIC. Di conseguenza, le due tecniche per determinare il numero di fattori sono concordi. Inoltre, dall’articolo originale da cui sono stati ottenuti i dati per questo tutorial, sappiamo che il numero atteso di fattori era anch’esso cinque. Pertanto, ha senso continuare la costruzione del modello utilizzando la soluzione a cinque fattori in questo tutorial.\nCon il seguente comando, è possibile ottenere i carichi fattoriali per i cinque fattori. Lavaan, di default, fornisce i carichi standardizzati, che possono essere interpretati come correlazioni tra variabili e fattori. Nell’output, vengono mostrati tutti i carichi superiori (in valore assoluto) a 0.3. Osservando i risultati, si nota una struttura semplice, con ogni variabile che carica su un solo fattore.\nUn’eccezione è rappresentata dalla variabile DE1, che ha un carico positivo sul fattore 4 (insieme alle altre variabili DE) e un carico negativo sul fattore 3 (insieme alle variabili EE). A parte questo cross-loading, i risultati sono in linea con il modello teorico: tutte le variabili TSC, TE, EE, DE e RPA caricano rispettivamente su un singolo fattore.\nNel prossimo passaggio, il modello può essere ulteriormente affinato in base all’adattamento. Poiché il modello senza cross-loading è completamente coerente con la teoria, nel CFA della sezione successiva si fisserà il carico della variabile DE1 sul fattore 3 a zero. Tuttavia, se il modello CFA non dovesse adattarsi bene, reintrodurre questo cross-loading sarebbe la prima modifica logica da considerare.\n\nEFA$nf5\n\n\n         f1      f2      f3      f4      f5 \nTSC1  0.584*                       *      .*\nTSC2  0.487*                       *      .*\nTSC3  0.637*                      .*       *\nTSC4  0.578*      .*              .*       *\nTSC5  0.547*                              . \nTE1           0.728*              .         \nTE2       .   0.672*                        \nTE3           0.708*      .                 \nTE4           0.651*              .*        \nTE5           0.337*      .*      .*        \nEE1               .   0.469*      .         \nEE2       .*          0.689*                \nEE3                   0.768*                \nEE4       .*          0.732*              . \nEE5               .   0.479*      .*        \nDE1                  -0.353*  0.744*      . \nDE2               .*          0.821*        \nDE3                       .*  0.755*        \nRPA1                                  0.851*\nRPA2                                  0.906*\nRPA3                                  0.624*\nRPA4                      .       .   0.350*\nRPA5                      .       .   0.338*",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#passo-2-costruire-il-modello-fattoriale-e-valutare-ladattamento",
    "href": "chapters/cfa/07_fa_in_r.html#passo-2-costruire-il-modello-fattoriale-e-valutare-ladattamento",
    "title": "42  Strategia Integrata per un’Analisi Fattoriale",
    "section": "42.8 Passo 2: Costruire il Modello Fattoriale e Valutare l’Adattamento",
    "text": "42.8 Passo 2: Costruire il Modello Fattoriale e Valutare l’Adattamento\nIl primo passo per costruire il modello è descriverlo usando la sintassi specifica di lavaan.\nNel modello seguente, vengono definiti i 5 fattori (TSC, TE, EE, DE e RPA), misurati da gruppi di variabili diversi (in linea con la teoria e i risultati dell’EFA del passaggio precedente), separati con “+”. Inoltre, si specifica che le correlazioni tra fattori devono essere stimate. Gli intercetti non sono inclusi esplicitamente, ma si possono aggiungere usando l’argomento meanstructure = TRUE al comando di stima del modello CFA.\n\nCFA_model &lt;- \"\n    # Regressing items on factors\n    TSC =~ TSC1 + TSC2 + TSC3 + TSC5\n    TE =~ TE1 + TE2 + TE3 + TE5\n    EE =~ EE1 + EE2 + EE3 + EE4\n    DE =~ DE1 + DE2 + DE3\n    RPA =~ RPA1 + RPA2 + RPA3 + RPA4\n    # Correlations between factors\n    TSC ~~ TE\n    TSC ~~ EE\n    TSC ~~ DE\n    TSC ~~ RPA\n    TE ~~ EE\n    TE ~~ DE\n    TE ~~ RPA\n    EE ~~ DE\n    EE ~~ RPA\n    DE ~~ RPA\n\"\n\n\nCFA &lt;- cfa(\n    model = CFA_model, \n    data = model.building[, var_names],\n    estimator = \"MLR\", \n    std.lv = TRUE,\n    meanstructure = TRUE\n)\n\nDopo aver eseguito la CFA, è possibile valutare quanto bene il modello si adatta ai dati.\nEsistono due tipi di misure di adattamento: globale e locale. Inizia con le misure di adattamento globale, che indicano quanto bene l’intero modello si adatta ai dati. Le principali misure globali di adattamento sono:\n\nTest di significatività Chi-quadro: verifica se il modello si adatta perfettamente ai dati, cioè se è in grado di riprodurre esattamente le relazioni osservate tra le variabili.\nIndice di adattamento comparativo (CFI): confronta l’adattamento del modello con quello di un modello che assume zero correlazioni tra le variabili.\nErrore quadratico medio di approssimazione (RMSEA): quantifica l’adattamento approssimativo tra modello e dati, senza richiedere un adattamento perfetto.\nResiduo quadratico medio standardizzato (SRMR): riassume la differenza tra la matrice di covarianza campionaria e quella del modello, in un solo valore.\n\nA differenza del test Chi-quadro, il CFI, RMSEA e SRMR non sono veri test di ipotesi e valutano l’adattamento approssimato.\nRegole pratiche per interpretare le misure di adattamento: - Il test Chi-quadro dovrebbe essere non significativo (non è sempre affidabile con campioni ampi, in cui il test tende a rifiutare l’adattamento perfetto). - CFI: dovrebbe essere maggiore di 0,9. - RMSEA: la stima puntuale e il limite superiore dell’intervallo di confidenza al 95% dovrebbero essere inferiori a 0,05. - SRMR: dovrebbe essere inferiore a 0,08.\nÈ possibile ottenere queste misure di adattamento e la loro interpretazione con il seguente comando:\n\nglobalFit(CFA)\n\nResults------------------------------------------------------------------------ \n \nChi-Square (142) = 319 with p-value\n          = 1.33e-15\n\nCFI = 0.948\n\nRMSEA = 0.0533; lower bound = 0.0459;\n      upper bound = 0.0608\n\nSRMR = 0.0435\n\nInterpretations--------------------------------------------------------------- \n \nThe hypothesis of perfect fit *is* rejected according to the Chi-\n          Square test statistics because the p-value is smaller than 0.05 \n \nThe hypothesis of approximate model fit *is not* rejected according\n          to the CFI because the value is larger than 0.9. \n \nThe hypothesis of approximate model fit *is* rejected according\n         to the RMSEA because the point estimate is larger or equal to\n         0.05. \n \nThe hypothesis of approximate model fit *is not* rejected according\n         to the SRMR because the value is smaller than 0.08. \n \n\n\nAnalizzando l’output, si vede che il test di significatività Chi-quadro ha rifiutato l’adattamento perfetto, ma l’adattamento approssimato è confermato dal CFI e dallo SRMR.\nIdealmente, almeno tre delle misure di adattamento dovrebbero indicare un buon adattamento; tuttavia, per semplicità, in questo tutorial si procede con il modello senza ulteriori modifiche. In pratica, potresti affinare ulteriormente il modello, ad esempio reintroducendo il cross-loading tra DE1 e il fattore 3, e valutare nuovamente l’adattamento con il comando globalFit().\nLe misure sopra considerate valutano l’adattamento globale del modello, ossia quanto bene il modello nel suo complesso rappresenta i dati. Tuttavia, queste misure possono non rilevare problemi locali di adattamento tra modello e dati. Ad esempio, se il modello a 5 fattori descrive bene tutte le variabili tranne una, l’adattamento globale potrebbe comunque risultare buono, ma le stime per quella variabile saranno errate.\nPer questo motivo, è importante controllare anche l’adattamento locale, ovvero se ogni parte del modello si adatta bene ai dati. Esistono diverse misure per valutare l’adattamento locale [38], ma il metodo più semplice consiste nel confrontare le differenze assolute tra la matrice di covarianza del campione e quella implicata dal modello. Queste due matrici sono le stesse usate per calcolare l’SRMR, ma anziché ottenere un singolo valore complessivo, puoi osservare la differenza per ogni varianza e covarianza separatamente. Con il seguente comando, puoi vedere le deviazioni tra le due matrici per ogni coppia di variabili e identificare la differenza massima:\n\nlocalFit(CFA) |&gt; print()\n\n$local_misfit\n      TSC1  TSC2  TSC3  TSC5   TE1   TE2   TE3   TE5   EE1   EE2   EE3\nTSC1 0.000                                                            \nTSC2 0.012 0.000                                                      \nTSC3 0.007 0.012 0.000                                                \nTSC5 0.007 0.002 0.010 0.000                                          \nTE1  0.019 0.000 0.009 0.010 0.000                                    \nTE2  0.025 0.014 0.031 0.021 0.011 0.000                              \nTE3  0.013 0.010 0.048 0.005 0.003 0.008 0.000                        \nTE5  0.025 0.028 0.032 0.022 0.012 0.026 0.005 0.000                  \nEE1  0.013 0.010 0.004 0.016 0.042 0.044 0.001 0.072 0.000            \nEE2  0.004 0.009 0.025 0.003 0.029 0.050 0.027 0.043 0.002 0.000      \nEE3  0.013 0.015 0.039 0.013 0.021 0.042 0.006 0.081 0.012 0.001 0.000\nEE4  0.002 0.002 0.000 0.013 0.042 0.021 0.006 0.039 0.017 0.017 0.010\nDE1  0.011 0.019 0.015 0.002 0.010 0.026 0.011 0.036 0.010 0.048 0.042\nDE2  0.014 0.018 0.030 0.011 0.008 0.025 0.032 0.059 0.058 0.031 0.012\nDE3  0.000 0.008 0.041 0.021 0.023 0.006 0.012 0.019 0.048 0.015 0.022\nRPA1 0.008 0.015 0.034 0.011 0.013 0.022 0.001 0.012 0.011 0.018 0.019\nRPA2 0.006 0.008 0.044 0.007 0.021 0.004 0.009 0.008 0.015 0.016 0.002\nRPA3 0.041 0.016 0.012 0.003 0.006 0.010 0.017 0.034 0.035 0.008 0.022\nRPA4 0.020 0.000 0.003 0.031 0.001 0.027 0.031 0.039 0.042 0.035 0.031\n       EE4   DE1   DE2   DE3  RPA1  RPA2  RPA3  RPA4\nTSC1                                                \nTSC2                                                \nTSC3                                                \nTSC5                                                \nTE1                                                 \nTE2                                                 \nTE3                                                 \nTE5                                                 \nEE1                                                 \nEE2                                                 \nEE3                                                 \nEE4  0.000                                          \nDE1  0.040 0.000                                    \nDE2  0.052 0.004 0.000                              \nDE3  0.012 0.002 0.006 0.000                        \nRPA1 0.041 0.008 0.006 0.002 0.000                  \nRPA2 0.053 0.010 0.025 0.024 0.024 0.000            \nRPA3 0.009 0.002 0.016 0.021 0.009 0.017 0.000      \nRPA4 0.053 0.006 0.056 0.074 0.046 0.011 0.052 0.000\n\n$max_misfit\n[1] 0.0805\n\n\n\nDall’analisi dell’adattamento locale, si può concludere che non ci sono problemi di adattamento locale, dato che la differenza massima tra le due matrici è solo 0,08, un valore piccolo rispetto alla scala delle variabili osservate.\nSe ci fossero problemi di adattamento locale, ad esempio se la correlazione tra due variabili osservate fosse significativamente maggiore di quanto previsto dal modello, si potrebbero apportare modifiche mirate al modello, come aggiungere una covarianza aggiuntiva tra queste variabili. Tuttavia, tali aggiustamenti dovrebbero sempre avere una base teorica solida! Non aggiungere mai parametri al modello solo per migliorare l’adattamento.\nCon questa valutazione dell’adattamento si conclude il passaggio. Ora è possibile esaminare i carichi del modello finale con il seguente comando:\n\ninspect(object = CFA, what = \"std\")$lambda\n\n\nA lavaan.matrix: 19 x 5 of type dbl\n\n\n\nTSC\nTE\nEE\nDE\nRPA\n\n\n\n\nTSC1\n0.657\n0.000\n0.000\n0.000\n0.000\n\n\nTSC2\n0.692\n0.000\n0.000\n0.000\n0.000\n\n\nTSC3\n0.628\n0.000\n0.000\n0.000\n0.000\n\n\nTSC5\n0.726\n0.000\n0.000\n0.000\n0.000\n\n\nTE1\n0.000\n0.789\n0.000\n0.000\n0.000\n\n\nTE2\n0.000\n0.745\n0.000\n0.000\n0.000\n\n\nTE3\n0.000\n0.788\n0.000\n0.000\n0.000\n\n\nTE5\n0.000\n0.649\n0.000\n0.000\n0.000\n\n\nEE1\n0.000\n0.000\n0.739\n0.000\n0.000\n\n\nEE2\n0.000\n0.000\n0.802\n0.000\n0.000\n\n\nEE3\n0.000\n0.000\n0.786\n0.000\n0.000\n\n\nEE4\n0.000\n0.000\n0.760\n0.000\n0.000\n\n\nDE1\n0.000\n0.000\n0.000\n0.665\n0.000\n\n\nDE2\n0.000\n0.000\n0.000\n0.640\n0.000\n\n\nDE3\n0.000\n0.000\n0.000\n0.738\n0.000\n\n\nRPA1\n0.000\n0.000\n0.000\n0.000\n0.849\n\n\nRPA2\n0.000\n0.000\n0.000\n0.000\n0.854\n\n\nRPA3\n0.000\n0.000\n0.000\n0.000\n0.788\n\n\nRPA4\n0.000\n0.000\n0.000\n0.000\n0.587",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#passo-3-valutare-la-generalizzabilità",
    "href": "chapters/cfa/07_fa_in_r.html#passo-3-valutare-la-generalizzabilità",
    "title": "42  Strategia Integrata per un’Analisi Fattoriale",
    "section": "42.9 Passo 3: Valutare la Generalizzabilità",
    "text": "42.9 Passo 3: Valutare la Generalizzabilità\nL’ultimo passo consiste nel valutare la generalizzabilità del modello CFA ottenuto nel Passo 2, adattando lo stesso modello al campione di riserva. Se il modello si adatta bene anche a questo campione alternativo, puoi essere più sicuro che il tuo modello fattoriale sia applicabile in modo più ampio e sia in grado di rappresentare la struttura sottostante del tuo strumento di misura anche in studi e campioni futuri.\nPer valutare la generalizzabilità, è possibile usare lo stesso codice del Passo 2, ma specificando il campione di riserva nell’argomento data.\n\nCFA_holdout &lt;- cfa(\n    model = CFA_model, \n    data = holdout[, var_names],\n    estimator = \"MLR\", \n    std.lv = TRUE,\n    meanstructure = TRUE\n)\n\nDopo aver adattato il modello CFA al campione di riserva, puoi ottenere nuovamente le misure di adattamento e la loro interpretazione usando il comando globalFit().\n\nglobalFit(CFA_holdout)\n\nResults------------------------------------------------------------------------ \n \nChi-Square (142) = 340 with p-value\n          = 0\n\nCFI = 0.943\n\nRMSEA = 0.0564; lower bound = 0.049;\n      upper bound = 0.0638\n\nSRMR = 0.0416\n\nInterpretations--------------------------------------------------------------- \n \nThe hypothesis of perfect fit *is* rejected according to the Chi-\n          Square test statistics because the p-value is smaller than 0.05 \n \nThe hypothesis of approximate model fit *is not* rejected according\n          to the CFI because the value is larger than 0.9. \n \nThe hypothesis of approximate model fit *is* rejected according\n         to the RMSEA because the point estimate is larger or equal to\n         0.05. \n \nThe hypothesis of approximate model fit *is not* rejected according\n         to the SRMR because the value is smaller than 0.08. \n \n\n\nDall’analisi dell’output, si nota che l’adattamento del modello al campione di riserva è molto simile a quello ottenuto con i dati di costruzione del modello. Anche in questo caso, il test Chi-quadro rifiuta l’adattamento perfetto, ma le misure CFI e SRMR confermano un buon adattamento approssimativo.\nL’adattamento locale può essere verificato utilizzando lo stesso comando del Passo 2, questa volta applicato ai risultati del campione di riserva.\n\nlocalFit(CFA_holdout) |&gt; print()\n\n$local_misfit\n      TSC1  TSC2  TSC3  TSC5   TE1   TE2   TE3   TE5   EE1   EE2   EE3\nTSC1 0.000                                                            \nTSC2 0.010 0.000                                                      \nTSC3 0.012 0.015 0.000                                                \nTSC5 0.007 0.007 0.005 0.000                                          \nTE1  0.023 0.023 0.003 0.014 0.000                                    \nTE2  0.027 0.012 0.019 0.008 0.008 0.000                              \nTE3  0.012 0.010 0.024 0.008 0.012 0.002 0.000                        \nTE5  0.019 0.014 0.008 0.002 0.046 0.006 0.013 0.000                  \nEE1  0.037 0.023 0.009 0.005 0.035 0.009 0.002 0.011 0.000            \nEE2  0.028 0.003 0.003 0.019 0.038 0.016 0.069 0.008 0.033 0.000      \nEE3  0.032 0.047 0.012 0.017 0.024 0.017 0.004 0.071 0.026 0.019 0.000\nEE4  0.006 0.033 0.003 0.002 0.027 0.002 0.002 0.048 0.015 0.020 0.004\nDE1  0.056 0.005 0.007 0.007 0.005 0.020 0.003 0.032 0.037 0.072 0.020\nDE2  0.005 0.029 0.032 0.061 0.012 0.014 0.046 0.006 0.024 0.038 0.034\nDE3  0.012 0.019 0.022 0.002 0.034 0.016 0.014 0.005 0.057 0.032 0.050\nRPA1 0.019 0.009 0.012 0.028 0.018 0.001 0.003 0.004 0.030 0.031 0.037\nRPA2 0.009 0.020 0.023 0.001 0.017 0.016 0.018 0.004 0.003 0.045 0.008\nRPA3 0.000 0.007 0.004 0.009 0.000 0.006 0.000 0.028 0.011 0.021 0.025\nRPA4 0.018 0.015 0.006 0.014 0.021 0.019 0.040 0.036 0.049 0.023 0.046\n       EE4   DE1   DE2   DE3  RPA1  RPA2  RPA3  RPA4\nTSC1                                                \nTSC2                                                \nTSC3                                                \nTSC5                                                \nTE1                                                 \nTE2                                                 \nTE3                                                 \nTE5                                                 \nEE1                                                 \nEE2                                                 \nEE3                                                 \nEE4  0.000                                          \nDE1  0.036 0.000                                    \nDE2  0.018 0.020 0.000                              \nDE3  0.020 0.019 0.006 0.000                        \nRPA1 0.003 0.014 0.029 0.004 0.000                  \nRPA2 0.051 0.006 0.010 0.005 0.020 0.000            \nRPA3 0.002 0.030 0.006 0.026 0.016 0.017 0.000      \nRPA4 0.023 0.008 0.028 0.046 0.057 0.005 0.093 0.000\n\n$max_misfit\n[1] 0.0931\n\n\n\nI risultati mostrano che l’adattamento locale è sufficiente anche per il campione di riserva (la differenza massima assoluta tra le due matrici è solo 0,09) ed è nuovamente paragonabile a quello ottenuto con i dati di costruzione del modello.\nInfine, è possibile esaminare i carichi del modello finale adattato al campione di riserva.\n\ninspect(object = CFA_holdout, what = \"std\")$lambda |&gt; print()\n\n       TSC    TE    EE    DE   RPA\nTSC1 0.679 0.000 0.000 0.000 0.000\nTSC2 0.689 0.000 0.000 0.000 0.000\nTSC3 0.691 0.000 0.000 0.000 0.000\nTSC5 0.702 0.000 0.000 0.000 0.000\nTE1  0.000 0.694 0.000 0.000 0.000\nTE2  0.000 0.772 0.000 0.000 0.000\nTE3  0.000 0.819 0.000 0.000 0.000\nTE5  0.000 0.677 0.000 0.000 0.000\nEE1  0.000 0.000 0.749 0.000 0.000\nEE2  0.000 0.000 0.794 0.000 0.000\nEE3  0.000 0.000 0.781 0.000 0.000\nEE4  0.000 0.000 0.801 0.000 0.000\nDE1  0.000 0.000 0.000 0.677 0.000\nDE2  0.000 0.000 0.000 0.659 0.000\nDE3  0.000 0.000 0.000 0.766 0.000\nRPA1 0.000 0.000 0.000 0.000 0.851\nRPA2 0.000 0.000 0.000 0.000 0.867\nRPA3 0.000 0.000 0.000 0.000 0.700\nRPA4 0.000 0.000 0.000 0.000 0.618\n\n\nAnche in questo caso, i risultati del campione di costruzione e del campione di riserva sono molto simili, con carichi fattoriali paragonabili a quelli precedenti.\nPoiché il modello si adatta sufficientemente bene al campione di riserva (almeno per questa illustrazione; idealmente, almeno 3 su 4 misure di adattamento dovrebbero indicare un buon adattamento, cosa che qui non si è verificata del tutto) e i parametri stimati sono simili tra i due dataset, si può concludere che la generalizzabilità del modello è soddisfacente. Se il modello non si fosse adattato adeguatamente al campione di riserva, si dovrebbe concludere che, pur essendo adatto ai dati di costruzione, il modello CFA del Passo 2 potrebbe non rappresentare una struttura applicabile in modo generale. In quel caso, la struttura fattoriale richiederebbe ulteriori perfezionamenti. Dato che il campione di riserva è stato già usato, questa revisione richiederebbe una nuova raccolta di dati, da suddividere nuovamente in un campione di costruzione e uno di riserva, per ripetere i tre passaggi.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#riflessioni-conclusive",
    "href": "chapters/cfa/07_fa_in_r.html#riflessioni-conclusive",
    "title": "42  Strategia Integrata per un’Analisi Fattoriale",
    "section": "42.10 Riflessioni conclusive",
    "text": "42.10 Riflessioni conclusive\nL’analisi fattoriale è un ottimo metodo per studiare costrutti non direttamente osservabili. Ha una vasta gamma di applicazioni interdisciplinari e ha sviluppi estesi, come l’analisi fattoriale multigruppo, che sarà trattata nel capitolo successivo con una discussione sull’importanza dell’invarianza di misura. Questo capitolo serve principalmente come introduzione ai concetti di base e per stimolare l’interesse e la sicurezza nell’applicazione autonoma del metodo.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/07_fa_in_r.html#session-info",
    "href": "chapters/cfa/07_fa_in_r.html#session-info",
    "title": "42  Strategia Integrata per un’Analisi Fattoriale",
    "section": "42.11 Session Info",
    "text": "42.11 Session Info\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] devtools_2.4.5    usethis_3.0.0     effectsize_0.8.9  MASS_7.3-61      \n [5] viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1   \n [9] gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6   \n[13] semPlot_1.1.6     lavaan_0.6-19     psych_2.4.6.26    scales_1.3.0     \n[17] markdown_1.13     knitr_1.49        lubridate_1.9.3   forcats_1.0.0    \n[21] stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2       readr_2.1.5      \n[25] tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n[29] here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.2       later_1.3.2         pbdZMQ_0.3-13      \n  [4] R.oo_1.27.0         cellranger_1.1.0    datawizard_0.13.0  \n  [7] XML_3.99-0.17       rpart_4.1.23        lifecycle_1.0.4    \n [10] rstatix_0.7.2       rprojroot_2.0.4     lattice_0.22-6     \n [13] insight_0.20.5      rockchalk_1.8.157   backports_1.5.0    \n [16] magrittr_2.0.3      openxlsx_4.2.7.1    Hmisc_5.2-0        \n [19] rmarkdown_2.29      remotes_2.5.0       httpuv_1.6.15      \n [22] qgraph_1.9.8        zip_2.3.1           sessioninfo_1.2.2  \n [25] pkgbuild_1.4.5      pbapply_1.7-2       minqa_1.2.8        \n [28] pkgload_1.4.0       multcomp_1.4-26     abind_1.4-8        \n [31] quadprog_1.5-8      R.utils_2.12.3      nnet_7.3-19        \n [34] TH.data_1.1-2       sandwich_3.1-1      openintro_2.5.0    \n [37] arm_1.14-4          airports_0.1.0      codetools_0.2-20   \n [40] tidyselect_1.2.1    farver_2.1.2        lme4_1.1-35.5      \n [43] stats4_4.4.2        base64enc_0.1-3     jsonlite_1.8.9     \n [46] ellipsis_0.3.2      Formula_1.2-5       survival_3.7-0     \n [49] emmeans_1.10.5      tools_4.4.2         rio_1.2.3          \n [52] Rcpp_1.0.13-1       glue_1.8.0          mnormt_2.1.1       \n [55] xfun_0.49           IRdisplay_1.1       numDeriv_2016.8-1.1\n [58] withr_3.0.2         fastmap_1.2.0       boot_1.3-31        \n [61] fansi_1.0.6         digest_0.6.37       mi_1.1             \n [64] timechange_0.3.0    R6_2.5.1            mime_0.12          \n [67] estimability_1.5.1  colorspace_2.1-1    Cairo_1.6-2        \n [70] gtools_3.9.5        jpeg_0.1-10         R.methodsS3_1.8.2  \n [73] utf8_1.2.4          generics_0.1.3      data.table_1.16.2  \n [76] corpcor_1.6.10      usdata_0.3.1        htmlwidgets_1.6.4  \n [79] parameters_0.23.0   pkgconfig_2.0.3     sem_3.1-16         \n [82] gtable_0.3.6        htmltools_0.5.8.1   carData_3.0-5      \n [85] profvis_0.4.0       png_0.1-8           rstudioapi_0.17.1  \n [88] tzdb_0.4.0          reshape2_1.4.4      uuid_1.2-1         \n [91] coda_0.19-4.1       checkmate_2.3.2     nlme_3.1-166       \n [94] curl_6.0.0          nloptr_2.1.1        cachem_1.1.0       \n [97] repr_1.1.7          zoo_1.8-12          parallel_4.4.2     \n[100] miniUI_0.1.1.1      foreign_0.8-87      pillar_1.9.0       \n[103] grid_4.4.2          vctrs_0.6.5         urlchecker_1.0.1   \n[106] promises_1.3.0      car_3.1-3           OpenMx_2.21.13     \n[109] xtable_1.8-4        cluster_2.1.6       htmlTable_2.4.3    \n[112] evaluate_1.0.1      pbivnorm_0.6.0      mvtnorm_1.3-2      \n[115] cli_3.6.3           kutils_1.73         compiler_4.4.2     \n[118] rlang_1.1.4         crayon_1.5.3        ggsignif_0.6.4     \n[121] labeling_0.4.3      fdrtool_1.2.18      fs_1.6.5           \n[124] plyr_1.8.9          stringi_1.8.4       munsell_0.5.1      \n[127] lisrelToR_0.3       bayestestR_0.15.0   pacman_0.5.1       \n[130] Matrix_1.7-1        IRkernel_1.3.2      hms_1.1.3          \n[133] glasso_1.11         shiny_1.9.1         memoise_2.0.1      \n[136] igraph_2.1.1        broom_1.0.7         RcppParallel_5.1.9 \n[139] readxl_1.4.3        cherryblossom_0.1.0\n\n\n\n\n\n\nSaqr, M., & López-Pernas, S. (2024). Learning analytics methods and tutorials: A practical guide using R. Springer Nature.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Strategia Integrata per un'Analisi Fattoriale</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/E_01.html",
    "href": "chapters/cfa/E_01.html",
    "title": "43  ✏️ Esercizi",
    "section": "",
    "text": "source(\"../_common.R\")\nsuppressPackageStartupMessages({\n    library(\"lavaan\")\n    library(\"semTools\")\n})\nset.seed(42)\n\nE1. Si ripeta l’esercizio che abbiamo svolto in precedenza usando l’analisi fattoriale esplorativa, questa volta usando la CFA in lavaan. I dati sono forniti da Brown (2015) e riguardano a otto misure di personalità raccolte su un campione di 250 pazienti che hanno concluso un programma di psicoterapia:\n\nanxiety (N1),\nhostility (N2),\ndepression (N3),\nself-consciousness (N4),\nwarmth (E1),\ngregariousness (E2),\nassertiveness (E3),\npositive emotions (E4).\n\n\nvarnames &lt;- c(\"N1\", \"N2\", \"N3\", \"N4\", \"E1\", \"E2\", \"E3\", \"E4\")\nsds &lt;- \"5.7  5.6  6.4  5.7  6.0  6.2  5.7  5.6\"\n\ncors &lt;- \"\n 1.000\n 0.767  1.000\n 0.731  0.709  1.000\n 0.778  0.738  0.762  1.000\n-0.351  -0.302  -0.356  -0.318  1.000\n-0.316  -0.280  -0.300  -0.267  0.675  1.000\n-0.296  -0.289  -0.297  -0.296  0.634  0.651  1.000\n-0.282  -0.254  -0.292  -0.245  0.534  0.593  0.566  1.000\"\n\npsychot_cor_mat &lt;- getCov(cors, names = varnames)\nn &lt;- 250\n\nIl modello con due fattori ortogonali può essere adattato ai dati nel modo seguente.\n\ncfa_mod &lt;- \"\n  N =~ N1 + N2 + N3 + N4\n  E =~ E1 + E2 + E3 + E4\n\"\n\n\nfit_cfa &lt;- lavaan::cfa(\n    cfa_mod,\n    sample.cov = psychot_cor_mat,\n    sample.nobs = n,\n    orthogonal = TRUE,\n    std.lv = TRUE\n)\n\n\nsemPlot::semPaths(fit_cfa,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\nEsaminiamo le saturazioni fattoriali:\n\nparameterEstimates(fit_cfa, standardized = TRUE) %&gt;%\n    dplyr::filter(op == \"=~\") %&gt;%\n    dplyr::select(\n        \"Latent Factor\" = lhs,\n        Indicator = rhs,\n        B = est,\n        SE = se,\n        Z = z,\n        \"p-value\" = pvalue,\n        Beta = std.all\n    ) %&gt;%\n    knitr::kable(\n        digits = 3, booktabs = TRUE, format = \"markdown\",\n        caption = \"Factor Loadings\"\n    )\n\n\n\nTable: Factor Loadings\n\n|Latent Factor |Indicator |     B|    SE|      Z| p-value|  Beta|\n|:-------------|:---------|-----:|-----:|------:|-------:|-----:|\n|N             |N1        | 0.882| 0.051| 17.422|       0| 0.884|\n|N             |N2        | 0.847| 0.052| 16.340|       0| 0.849|\n|N             |N3        | 0.840| 0.052| 16.134|       0| 0.842|\n|N             |N4        | 0.882| 0.051| 17.432|       0| 0.884|\n|E             |E1        | 0.795| 0.056| 14.276|       0| 0.796|\n|E             |E2        | 0.838| 0.054| 15.369|       0| 0.839|\n|E             |E3        | 0.788| 0.056| 14.097|       0| 0.789|\n|E             |E4        | 0.697| 0.058| 11.942|       0| 0.699|\n\n\nIl risultato sembra sensato: le saturazioni su ciascun fattore sono molto alte. Tuttavia, la matrice delle correlazioni residue\n\ncor_table &lt;- residuals(fit_cfa, type = \"cor\")$cov\nknitr::kable(\n    cor_table,\n    digits = 3,\n    format = \"markdown\",\n    booktabs = TRUE\n)\n\n\n\n|   |     N1|     N2|     N3|     N4|     E1|     E2|     E3|     E4|\n|:--|------:|------:|------:|------:|------:|------:|------:|------:|\n|N1 |  0.000|  0.017| -0.013| -0.003| -0.351| -0.316| -0.296| -0.282|\n|N2 |  0.017|  0.000| -0.006| -0.012| -0.302| -0.280| -0.289| -0.254|\n|N3 | -0.013| -0.006|  0.000|  0.018| -0.356| -0.300| -0.297| -0.292|\n|N4 | -0.003| -0.012|  0.018|  0.000| -0.318| -0.267| -0.296| -0.245|\n|E1 | -0.351| -0.302| -0.356| -0.318|  0.000|  0.007|  0.006| -0.022|\n|E2 | -0.316| -0.280| -0.300| -0.267|  0.007|  0.000| -0.011|  0.007|\n|E3 | -0.296| -0.289| -0.297| -0.296|  0.006| -0.011|  0.000|  0.015|\n|E4 | -0.282| -0.254| -0.292| -0.245| -0.022|  0.007|  0.015|  0.000|\n\n\nrivela che il modello ipotizzato dall’analisi fattoriale confermativa non è adeguato.\n\nfit2_cfa &lt;- lavaan::cfa(\n    cfa_mod,\n    sample.cov = psychot_cor_mat,\n    sample.nobs = n,\n    orthogonal = FALSE,\n    std.lv = TRUE\n)\n\n\nsemPlot::semPaths(fit2_cfa,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\nEsaminiamo le saturazioni fattoriali.\n\nparameterEstimates(fit2_cfa, standardized = TRUE) %&gt;%\n    dplyr::filter(op == \"=~\") %&gt;%\n    dplyr::select(\n        \"Latent Factor\" = lhs,\n        Indicator = rhs,\n        B = est,\n        SE = se,\n        Z = z,\n        \"p-value\" = pvalue,\n        Beta = std.all\n    ) %&gt;%\n    knitr::kable(\n        digits = 3, booktabs = TRUE, format = \"markdown\",\n        caption = \"Factor Loadings\"\n    )\n\n\n\nTable: Factor Loadings\n\n|Latent Factor |Indicator |     B|    SE|      Z| p-value|  Beta|\n|:-------------|:---------|-----:|-----:|------:|-------:|-----:|\n|N             |N1        | 0.883| 0.051| 17.472|       0| 0.885|\n|N             |N2        | 0.847| 0.052| 16.337|       0| 0.849|\n|N             |N3        | 0.842| 0.052| 16.190|       0| 0.844|\n|N             |N4        | 0.880| 0.051| 17.381|       0| 0.882|\n|E             |E1        | 0.800| 0.055| 14.465|       0| 0.802|\n|E             |E2        | 0.832| 0.054| 15.294|       0| 0.834|\n|E             |E3        | 0.788| 0.056| 14.150|       0| 0.789|\n|E             |E4        | 0.698| 0.058| 11.974|       0| 0.699|\n\n\nEsaminiamo i residui.\n\ncor_table &lt;- residuals(fit2_cfa, type = \"cor\")$cov\nknitr::kable(\n    cor_table,\n    digits = 3,\n    format = \"markdown\",\n    booktabs = TRUE\n)\n\n\n\n|   |     N1|     N2|     N3|     N4|     E1|     E2|     E3|     E4|\n|:--|------:|------:|------:|------:|------:|------:|------:|------:|\n|N1 |  0.000|  0.016| -0.015| -0.002| -0.042|  0.005|  0.008| -0.013|\n|N2 |  0.016|  0.000| -0.007| -0.010| -0.006|  0.028|  0.002|  0.004|\n|N3 | -0.015| -0.007|  0.000|  0.018| -0.062|  0.006| -0.007| -0.035|\n|N4 | -0.002| -0.010|  0.018|  0.000| -0.010|  0.053|  0.007|  0.023|\n|E1 | -0.042| -0.006| -0.062| -0.010|  0.000|  0.006|  0.001| -0.027|\n|E2 |  0.005|  0.028|  0.006|  0.053|  0.006|  0.000| -0.007|  0.010|\n|E3 |  0.008|  0.002| -0.007|  0.007|  0.001| -0.007|  0.000|  0.014|\n|E4 | -0.013|  0.004| -0.035|  0.023| -0.027|  0.010|  0.014|  0.000|\n\n\nSistemiamo le saturazioni fattoriali in una matrice 8 \\(\\times\\) 2:\n\nlambda &lt;- inspect(fit2_cfa, what = \"std\")$lambda\nlambda\n\n\nA lavaan.matrix: 8 x 2 of type dbl\n\n\n\nN\nE\n\n\n\n\nN1\n0.8848214\n0.0000000\n\n\nN2\n0.8485128\n0.0000000\n\n\nN3\n0.8436432\n0.0000000\n\n\nN4\n0.8819736\n0.0000000\n\n\nE1\n0.0000000\n0.8018485\n\n\nE2\n0.0000000\n0.8337599\n\n\nE3\n0.0000000\n0.7894530\n\n\nE4\n0.0000000\n0.6990366\n\n\n\n\n\nOtteniamo la matrice di intercorrelazoni fattoriali.\n\nPhi &lt;- inspect(fit2_cfa, what = \"std\")$psi\nPhi\n\n\nA lavaan.matrix.symmetric: 2 x 2 of type dbl\n\n\n\nN\nE\n\n\n\n\nN\n1.000000\n-0.434962\n\n\nE\n-0.434962\n1.000000\n\n\n\n\n\nOtteniamo la matrice di varianze residue.\n\nPsi &lt;- inspect(fit2_cfa, what = \"std\")$theta\nPsi\n\n\nA lavaan.matrix.symmetric: 8 x 8 of type dbl\n\n\n\nN1\nN2\nN3\nN4\nE1\nE2\nE3\nE4\n\n\n\n\nN1\n0.217091\n0.0000000\n0.0000000\n0.0000000\n0.000000\n0.0000000\n0.000000\n0.0000000\n\n\nN2\n0.000000\n0.2800261\n0.0000000\n0.0000000\n0.000000\n0.0000000\n0.000000\n0.0000000\n\n\nN3\n0.000000\n0.0000000\n0.2882661\n0.0000000\n0.000000\n0.0000000\n0.000000\n0.0000000\n\n\nN4\n0.000000\n0.0000000\n0.0000000\n0.2221225\n0.000000\n0.0000000\n0.000000\n0.0000000\n\n\nE1\n0.000000\n0.0000000\n0.0000000\n0.0000000\n0.357039\n0.0000000\n0.000000\n0.0000000\n\n\nE2\n0.000000\n0.0000000\n0.0000000\n0.0000000\n0.000000\n0.3048445\n0.000000\n0.0000000\n\n\nE3\n0.000000\n0.0000000\n0.0000000\n0.0000000\n0.000000\n0.0000000\n0.376764\n0.0000000\n\n\nE4\n0.000000\n0.0000000\n0.0000000\n0.0000000\n0.000000\n0.0000000\n0.000000\n0.5113478\n\n\n\n\n\nMediante i parametri del modello la matrice di correlazione si riproduce nel modo seguente:\n\\[\n\\boldsymbol{\\Sigma} =\\boldsymbol{\\Lambda} \\boldsymbol{\\Phi} \\boldsymbol{\\Lambda}^{\\mathsf{T}} + \\boldsymbol{\\Psi}.\n\\]\nIn \\(\\textsf{R}\\) scriviamo:\n\nR_hat &lt;- lambda %*% Phi %*% t(lambda) + Psi\nR_hat %&gt;%\n    round(3)\n\n\nA lavaan.matrix.symmetric: 8 x 8 of type dbl\n\n\n\nN1\nN2\nN3\nN4\nE1\nE2\nE3\nE4\n\n\n\n\nN1\n1.000\n0.751\n0.746\n0.780\n-0.309\n-0.321\n-0.304\n-0.269\n\n\nN2\n0.751\n1.000\n0.716\n0.748\n-0.296\n-0.308\n-0.291\n-0.258\n\n\nN3\n0.746\n0.716\n1.000\n0.744\n-0.294\n-0.306\n-0.290\n-0.257\n\n\nN4\n0.780\n0.748\n0.744\n1.000\n-0.308\n-0.320\n-0.303\n-0.268\n\n\nE1\n-0.309\n-0.296\n-0.294\n-0.308\n1.000\n0.669\n0.633\n0.561\n\n\nE2\n-0.321\n-0.308\n-0.306\n-0.320\n0.669\n1.000\n0.658\n0.583\n\n\nE3\n-0.304\n-0.291\n-0.290\n-0.303\n0.633\n0.658\n1.000\n0.552\n\n\nE4\n-0.269\n-0.258\n-0.257\n-0.268\n0.561\n0.583\n0.552\n1.000\n\n\n\n\n\nLe correlazioni residue sono:\n\n(psychot_cor_mat - R_hat) %&gt;%\n    round(3)\n\n\nA lavaan.matrix.symmetric: 8 x 8 of type dbl\n\n\n\nN1\nN2\nN3\nN4\nE1\nE2\nE3\nE4\n\n\n\n\nN1\n0.000\n0.016\n-0.015\n-0.002\n-0.042\n0.005\n0.008\n-0.013\n\n\nN2\n0.016\n0.000\n-0.007\n-0.010\n-0.006\n0.028\n0.002\n0.004\n\n\nN3\n-0.015\n-0.007\n0.000\n0.018\n-0.062\n0.006\n-0.007\n-0.035\n\n\nN4\n-0.002\n-0.010\n0.018\n0.000\n-0.010\n0.053\n0.007\n0.023\n\n\nE1\n-0.042\n-0.006\n-0.062\n-0.010\n0.000\n0.006\n0.001\n-0.027\n\n\nE2\n0.005\n0.028\n0.006\n0.053\n0.006\n0.000\n-0.007\n0.010\n\n\nE3\n0.008\n0.002\n-0.007\n0.007\n0.001\n-0.007\n0.000\n0.014\n\n\nE4\n-0.013\n0.004\n-0.035\n0.023\n-0.027\n0.010\n0.014\n0.000\n\n\n\n\n\nCalcoliamo la correlazione predetta dal modello tra le variabili \\(Y_1\\) e \\(Y_2\\):\n\nlambda[1, 1] * lambda[2, 1] + lambda[1, 2] * lambda[2, 2] +\n    lambda[1, 1] * lambda[2, 2] * Phi[1, 2] +\n    lambda[1, 2] * lambda[2, 1] * Phi[1, 2]\n\n0.750782309575684\n\n\nQuesto risultato è molto simile al valore contenuto dell’elemento (1, 2) della matrice di correlazioni osservate:\n\npsychot_cor_mat[1, 2]\n\n0.767\n\n\nUsando le funzonalità di lavaan la matrice di correlazione predetta si ottiene con:\n\nfitted(fit2_cfa)$cov |&gt;\n    print()\n\n       N1     N2     N3     N4     E1     E2     E3     E4\nN1  0.996                                                 \nN2  0.748  0.996                                          \nN3  0.743  0.713  0.996                                   \nN4  0.777  0.745  0.741  0.996                            \nE1 -0.307 -0.295 -0.293 -0.306  0.996                     \nE2 -0.320 -0.306 -0.305 -0.319  0.666  0.996              \nE3 -0.303 -0.290 -0.289 -0.302  0.630  0.656  0.996       \nE4 -0.268 -0.257 -0.255 -0.267  0.558  0.580  0.550  0.996\n\n\nLa matrice dei residui è\n\nresid(fit2_cfa)$cov |&gt;\n    print()\n\n       N1     N2     N3     N4     E1     E2     E3     E4\nN1  0.000                                                 \nN2  0.016  0.000                                          \nN3 -0.015 -0.007  0.000                                   \nN4 -0.002 -0.010  0.018  0.000                            \nE1 -0.042 -0.006 -0.062 -0.010  0.000                     \nE2  0.005  0.028  0.006  0.053  0.006  0.000              \nE3  0.008  0.002 -0.007  0.007  0.001 -0.007  0.000       \nE4 -0.013  0.004 -0.035  0.023 -0.026  0.010  0.014  0.000\n\n\nLa matrice dei residui standardizzati è\n\nresid(fit2_cfa, type = \"standardized\")$cov |&gt;\n    print()\n\n       N1     N2     N3     N4     E1     E2     E3     E4\nN1  0.000                                                 \nN2  1.674  0.000                                          \nN3 -1.769 -0.569  0.000                                   \nN4 -0.350 -1.152  1.746  0.000                            \nE1 -1.214 -0.161 -1.646 -0.294  0.000                     \nE2  0.154  0.794  0.168  1.626  0.637  0.000              \nE3  0.219  0.062 -0.191  0.193  0.075 -0.693  0.000       \nE4 -0.314  0.092 -0.824  0.552 -1.481  0.624  0.690  0.000\n\n\nI valori precedenti possono essere considerati come punti z, dove i valori con un valore assoluto maggiore di 2 possono essere ritenuti problematici. Tuttavia, è importante considerare che in questo modo si stanno eseguendo molteplici confronti, pertanto, si dovrebbe considerare l’opportunità di applicare una qualche forma di correzione per i confronti multipli.\nE2. Si utilizzino i dati dass21.txt che corrispondono alla somministrazione del test DASS-21 a 334 partecipanti. Lo schema di codifica si può trovare seguendo questo link. Si adatti ai dati un modello a tre fattori usando l’analisi fattoriale esplorativa con la funzione lavaan::efa(). Usando le saturazioni fattoriali e la matrice di inter-correlazioni fattoriali, si trovi la matrice di correlazioni riprodotta dal modello. Senza usare l’albebra matriciale, si trovi la correlazione predetta tra gli indicatori DASS-1 e DASS-2.\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/E_02_bifactor.html",
    "href": "chapters/cfa/E_02_bifactor.html",
    "title": "44  ✏️ Esercizi",
    "section": "",
    "text": "44.1 Introduzione\nIn questo esercizio replicheremo la validazione della Strengths and Weaknesses of ADHD Symptoms and Normal Behavior Scale descritta nell’articolo di Blume et al. (2020).\nGli adulti con sintomi di disturbo da deficit di attenzione/iperattività (ADHD; American Psychiatric Association [APA], 2013) presentano sintomi di disattenzione (ad esempio, difficoltà a mantenere l’attenzione sul lavoro, durante compiti o attività), iperattività-impulsività (ad esempio, interrompere o intromettersi nelle conversazioni, parlare in modo eccessivo), o una combinazione di entrambi. Questi sintomi sono associati a compromissioni nel funzionamento accademico (ad esempio, tassi più bassi di diploma e laurea), lavorativo (ad esempio, redditi complessivamente inferiori) e sociale (ad esempio, meno amici, tassi di divorzio più alti).\nL’ADHD si manifesta inizialmente durante l’infanzia e persiste nell’età adulta in circa la metà dei casi, con una prevalenza stimata del 2.5% negli adulti. Clinicamente, l’ADHD si presenta in tre modalità principali:\nSwanson e colleghi (2012) hanno introdotto la scala Strengths and Weaknesses of ADHD-Symptoms and Normal-Behavior (SWAN), che valuta i sintomi di disattenzione e iperattività-impulsività nei bambini in età scolare tramite un report di terze parti. La scala, composta da 18 item, si basa sui criteri sintomatici definiti nel Diagnostic and Statistical Manual of Mental Disorders (DSM-IV; APA, 2000) e confermati nel DSM-5 (APA, 2013). La SWAN è stata progettata per valutare il comportamento dei bambini, concentrandosi su situazioni scolastiche, di gioco e domestiche. La scala utilizza un punteggio a 7 punti, con ancore che rappresentano gli estremi negativi (“molto al di sotto della media”) e positivi (“molto al di sopra della media”), confrontando il comportamento del bambino con quello di altri coetanei. La SWAN è stata la prima scala a valutare i sintomi dell’ADHD in modo realmente dimensionale.\nBlume et al. (2020) adattano la versione tedesca esistente, SWAN-DE (Schulz-Zhecheva et al., 2019), in una versione self-report per adulti, denominandola German Strengths and Weaknesses of ADHD and Normal-Behavior Scale Self-Report (SWAN-DE-SB).",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/E_02_bifactor.html#introduzione",
    "href": "chapters/cfa/E_02_bifactor.html#introduzione",
    "title": "44  ✏️ Esercizi",
    "section": "",
    "text": "Presentazione prevalentemente disattenta: predominano i sintomi di disattenzione;\nPresentazione prevalentemente iperattiva-impulsiva: predominano i sintomi di iperattività-impulsività;\nPresentazione combinata: sono presenti livelli significativi di entrambi i tipi di sintomi.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/cfa/E_02_bifactor.html#validazione",
    "href": "chapters/cfa/E_02_bifactor.html#validazione",
    "title": "44  ✏️ Esercizi",
    "section": "44.2 Validazione",
    "text": "44.2 Validazione\nDi seguito è fornita una parte dello script R fornito dagli autori per l’analisi statistica dai dati grezzi fino alla formulazione del modello bifattoriale.\n\ndata &lt;- rio::import(here::here(\"data\", \"blume_2024\", \"data_total_OSF.csv\"))\n\ndata$X &lt;- NULL # delete column without information\n\ndata$SW_mean &lt;- as.numeric(data$SW_mean) # convert from character to numeric\ndata$SW_AD_mean &lt;- as.numeric(data$SW_AD_mean)\ndata$SW_HI_mean &lt;- as.numeric(data$SW_HI_mean)\n\ndata$CA_mean &lt;- as.numeric(data$CA_mean)\ndata$CA_AD_mean &lt;- as.numeric(data$CA_AD_mean)\ndata$CA_HI_mean &lt;- as.numeric(data$CA_HI_mean)\n\ndata$HA_mean &lt;- as.numeric(data$HA_mean)\ndata$HA_AD_mean &lt;- as.numeric(data$HA_AD_mean)\ndata$HA_HI_mean &lt;- as.numeric(data$HA_HI_mean)\n\n\ndata_clin &lt;- rio::import(here::here(\"data\", \"blume_2024\", \"data_clinical_OSF.csv\"))\n\ndata_clin$X &lt;- NULL # delete column without information\n\ndata_clin$SW_mean &lt;- as.numeric(data_clin$SW_mean) # convert from character to numeric\ndata_clin$SW_AD_mean &lt;- as.numeric(data_clin$SW_AD_mean)\ndata_clin$SW_HI_mean &lt;- as.numeric(data_clin$SW_HI_mean)\n\ndata_clin$CA_mean &lt;- as.numeric(data_clin$CA_mean)\ndata_clin$CA_AD_mean &lt;- as.numeric(data_clin$CA_AD_mean)\ndata_clin$CA_HI_mean &lt;- as.numeric(data_clin$CA_HI_mean)\n\ndata_clin$HA_mean &lt;- as.numeric(data_clin$HA_mean)\ndata_clin$HA_AD_mean &lt;- as.numeric(data_clin$HA_AD_mean)\ndata_clin$HA_HI_mean &lt;- as.numeric(data_clin$HA_HI_mean)\n\n\n# Information on missing data in the general population sample\nSWAN_vars &lt;- colnames(data)[str_detect(colnames(data), \"SW01\")]\nsum(is.na(data[, SWAN_vars])) # 1 data point missing\nsum(!is.na(data[, SWAN_vars])) # 7163 not missing -&gt; 0.01% missing\n\n1\n\n\n7163\n\n\n\n# age\n\nsem_age1 &lt;- \"\n        SW_GF =~ SW01_01 + SW01_02 + SW01_03 + SW01_04 + SW01_05 + SW01_06\n                           + SW01_07 + SW01_08 + SW01_09 + SW01_10 + SW01_11 + SW01_12\n                           + SW01_13 + SW01_14 + SW01_15 + SW01_16 + SW01_17 + SW01_18;\n        SW_GF ~ age\n\"\nfit_age1 &lt;- sem(sem_age1, data = data)\n\n# Regressions:\n#                   Estimate  Std.Err  z-value  P(&gt;|z|)\n#  SW_GF ~\n#    age               0.001    0.003    0.285    0.775\n\n\nsem_age2 &lt;- \"\n        SW_AD =~ SW01_01 + SW01_02 + SW01_03 + SW01_04 + SW01_05 + SW01_06\n                + SW01_07 + SW01_08 + SW01_09;\n        SW_AD ~ age\n\"\nfit_age2 &lt;- sem(sem_age2, data = data)\n\n# Regressions:\n#                   Estimate  Std.Err  z-value  P(&gt;|z|)\n#  SW_AD ~\n#    age               0.004    0.003    1.018    0.309\n\nsem_age3 &lt;- \"\n        SW_HI =~ SW01_10 + SW01_11 + SW01_12 + SW01_13 + SW01_14 + SW01_15\n                + SW01_16 + SW01_17 + SW01_18;\n        SW_HI ~ age\n\"\nfit_age3 &lt;- sem(sem_age3, data = data)\n\n# Regressions:\n#                   Estimate  Std.Err  z-value  P(&gt;|z|)\n#  SW_HI ~\n#    age              -0.002    0.005   -0.530    0.596\n\n\nglimpse(data)\n\nRows: 398\nColumns: 84\n$ V1             &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, ~\n$ id             &lt;int&gt; 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,~\n$ gender         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\n$ age            &lt;int&gt; 36, 26, 21, 21, 21, 19, 22, 25, 28, 20, 19, 32, 18,~\n$ diagnosis_ever &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ~\n$ diagnosis_now  &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ~\n$ medication     &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ~\n$ education      &lt;int&gt; 4, 3, 3, 3, 3, 3, 3, 4, 4, 3, 3, 4, 3, 3, 3, 4, 3, ~\n$ SW01_01        &lt;int&gt; 2, 4, 4, 4, 4, 6, 4, 6, 3, 4, 5, 6, 4, 4, 3, 5, 4, ~\n$ SW01_02        &lt;int&gt; 2, 6, 3, 3, 5, 5, 4, 4, 4, 1, 5, 5, 3, 4, 3, 5, 1, ~\n$ SW01_03        &lt;int&gt; 4, 6, 5, 3, 4, 6, 6, 6, 5, 4, 5, 5, 5, 6, 2, 5, 4, ~\n$ SW01_04        &lt;int&gt; 4, 6, 3, 4, 5, 6, 5, 5, 5, 3, 6, 5, 4, 5, 3, 5, 1, ~\n$ SW01_05        &lt;int&gt; 3, 3, 5, 4, 5, 6, 6, 6, 6, 5, 4, 6, 5, 5, 6, 5, 2, ~\n$ SW01_06        &lt;int&gt; 4, 3, 3, 3, 5, 6, 6, 5, 3, 1, 6, 5, 5, 5, 2, 5, 2, ~\n$ SW01_07        &lt;int&gt; 4, 4, 3, 3, 5, 6, 6, 6, 6, 3, 3, 5, 4, 5, 5, 5, 4, ~\n$ SW01_08        &lt;int&gt; 3, 3, 2, 3, 3, 4, 3, 5, 5, 1, 3, 4, 3, 2, 1, 2, 3, ~\n$ SW01_09        &lt;int&gt; 3, 4, 3, 4, 5, 4, 0, 6, 6, 5, 1, 5, 3, 6, 4, 4, 4, ~\n$ SW01_10        &lt;int&gt; 3, 5, 2, 3, 4, 3, 6, 3, 4, 5, 3, 5, 3, 4, 3, 3, 2, ~\n$ SW01_11        &lt;int&gt; 3, 6, 3, 3, 4, 5, 6, 6, 4, 6, 5, 5, 3, 6, 3, 3, 5, ~\n$ SW01_12        &lt;int&gt; 3, 3, 3, 3, 3, 6, 6, 6, 3, 6, 4, 5, 2, 3, 3, 3, 2, ~\n$ SW01_13        &lt;int&gt; 4, 6, 3, 3, 3, 6, 2, 6, 5, 2, 5, 6, 3, 5, 4, 3, 1, ~\n$ SW01_14        &lt;int&gt; 3, 1, 3, 3, 3, 6, 6, 6, 5, 6, 5, 4, 4, 5, 1, 2, 5, ~\n$ SW01_15        &lt;int&gt; 3, 4, 3, 3, 5, 6, 3, 6, 4, 4, 5, 5, 3, 4, 5, 2, 2, ~\n$ SW01_16        &lt;int&gt; 2, 6, 3, 3, 5, 5, 2, 6, 3, 6, 5, 5, 3, 4, 3, 2, 2, ~\n$ SW01_17        &lt;int&gt; 3, 4, 4, 3, 3, 5, 3, 6, 3, 6, 5, 5, 3, 4, 1, 1, 4, ~\n$ SW01_18        &lt;int&gt; 3, 4, 3, 3, 3, 6, 3, 6, 4, 2, 5, 5, 3, 1, 3, 2, 0, ~\n$ HA01_01        &lt;int&gt; 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 2, ~\n$ HA01_02        &lt;int&gt; 2, 1, 0, 1, 0, 0, 1, 0, 0, 2, 0, 2, 1, 0, 0, 0, 2, ~\n$ HA01_03        &lt;int&gt; 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 2, 0, 1, 1, 0, 1, ~\n$ HA01_04        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, ~\n$ HA01_05        &lt;int&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, ~\n$ HA01_06        &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ~\n$ HA01_07        &lt;int&gt; 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, ~\n$ HA01_08        &lt;int&gt; 1, 0, 1, 1, 0, 0, 1, 1, 0, 2, 1, 1, 1, 1, 2, 1, 2, ~\n$ HA01_09        &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, ~\n$ HA01_10        &lt;int&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, ~\n$ HA01_11        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\n$ HA01_12        &lt;int&gt; 1, 3, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 2, 0, 1, ~\n$ HA01_13        &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\n$ HA01_14        &lt;int&gt; 1, 2, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 1, 1, 2, 0, ~\n$ HA01_15        &lt;int&gt; 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 2, 2, ~\n$ HA01_16        &lt;int&gt; 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, ~\n$ HA01_17        &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, ~\n$ HA01_18        &lt;int&gt; 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, ~\n$ HA01_19        &lt;int&gt; 0, 1, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 1, 0, 1, 3, 2, ~\n$ HA01_20        &lt;int&gt; 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, ~\n$ HA01_21        &lt;int&gt; 1, 3, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 1, 1, ~\n$ HA01_22        &lt;int&gt; 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ~\n$ CA01_01        &lt;int&gt; 1, 0, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 2, 2, 2, ~\n$ CA01_02        &lt;int&gt; 1, 2, 1, 0, 0, 1, 2, 1, 3, 0, 0, 1, 0, 0, 1, 3, 0, ~\n$ CA01_03        &lt;int&gt; 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 2, ~\n$ CA01_04        &lt;int&gt; 1, 0, 0, 0, 2, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, ~\n$ CA01_05        &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, ~\n$ CA01_06        &lt;int&gt; 2, 2, 2, 1, 2, 1, 0, 1, 1, 3, 2, 0, 1, 2, 0, 3, 0, ~\n$ CA01_07        &lt;int&gt; 2, 3, 2, 1, 1, 1, 1, 0, 1, 2, 1, 0, 0, 1, 1, 1, 0, ~\n$ CA01_08        &lt;int&gt; 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, ~\n$ CA01_09        &lt;int&gt; 0, 1, 2, 0, 0, 2, 0, 0, 0, 2, 1, 0, 1, 2, 1, 0, 1, ~\n$ CA01_10        &lt;int&gt; 1, 0, 1, 1, 1, 1, 2, 0, 1, 1, 0, 2, 1, 1, 1, 3, 0, ~\n$ CA01_11        &lt;int&gt; 1, 2, 2, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, ~\n$ CA01_12        &lt;int&gt; 2, 1, 3, 1, 0, 0, 1, 1, 0, 3, 1, 2, 0, 1, 2, 0, 1, ~\n$ CA01_13        &lt;int&gt; 2, 2, 2, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 1, 0, 0, ~\n$ CA01_14        &lt;int&gt; 1, 1, 0, 0, 1, 0, 2, 0, 0, 2, 1, 2, 0, 0, 1, 1, 2, ~\n$ CA01_15        &lt;int&gt; 2, 3, 2, 2, 2, 1, 2, 0, 1, 1, 1, 2, 0, 1, 2, 3, 1, ~\n$ CA01_16        &lt;int&gt; 2, 3, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, ~\n$ CA01_17        &lt;int&gt; 2, 1, 2, 1, 2, 0, 0, 0, 0, 2, 0, 1, 0, 1, 1, 3, 3, ~\n$ CA01_18        &lt;int&gt; 2, 2, 2, 1, 1, 0, 0, 1, 1, 3, 0, 2, 1, 0, 1, 2, 3, ~\n$ CA01_19        &lt;int&gt; 0, 0, 2, 0, 1, 0, 1, 0, 1, 0, 2, 1, 0, 1, 0, 1, 1, ~\n$ CA01_20        &lt;int&gt; 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 2, 1, 0, 1, 1, 0, 1, ~\n$ CA01_21        &lt;int&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 3, 1, 2, 0, 0, 0, 2, 1, ~\n$ CA01_22        &lt;int&gt; 2, 1, 3, 0, 0, 0, 3, 0, 0, 1, 0, 2, 0, 0, 1, 0, 2, ~\n$ CA01_23        &lt;int&gt; 1, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ~\n$ CA01_24        &lt;int&gt; 0, 0, 1, 0, 0, 0, 2, 1, 0, 3, 0, 1, 0, 0, 0, 0, 0, ~\n$ CA01_25        &lt;int&gt; 1, 3, 3, 0, 1, 0, 0, 0, 1, 2, 0, 1, 0, 1, 3, 0, 3, ~\n$ CA01_26        &lt;int&gt; 1, 3, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ~\n$ diagnosis_type &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~\n$ SW_mean        &lt;dbl&gt; 3.11, 4.33, 3.22, 3.22, 4.11, 5.39, 4.28, 5.56, 4.3~\n$ SW_AD_mean     &lt;dbl&gt; 3.22, 4.33, 3.44, 3.44, 4.56, 5.44, 4.44, 5.44, 4.7~\n$ SW_HI_mean     &lt;dbl&gt; 3.00, 4.33, 3.00, 3.00, 3.67, 5.33, 4.11, 5.67, 3.8~\n$ CA_mean        &lt;dbl&gt; 1.154, 1.308, 1.423, 0.462, 0.692, 0.538, 0.692, 0.~\n$ CA_AD_mean     &lt;dbl&gt; 1.0, 1.0, 0.8, 0.6, 1.0, 0.0, 0.2, 0.2, 0.2, 2.0, 0~\n$ CA_HI_mean     &lt;dbl&gt; 1.3, 1.1, 1.3, 0.5, 0.7, 0.8, 0.4, 0.1, 0.7, 1.1, 0~\n$ HA_mean        &lt;dbl&gt; 0.5000, 0.9091, 0.1364, 0.3636, 0.1364, 0.1364, 0.5~\n$ HA_AD_mean     &lt;dbl&gt; 0.778, 0.667, 0.111, 0.444, 0.222, 0.000, 0.333, 0.~\n$ HA_HI_mean     &lt;dbl&gt; 0.333, 0.778, 0.222, 0.333, 0.111, 0.333, 0.667, 0.~\n\n\n\n# education\nsem_education1 &lt;- \"\n        SW_GF =~ SW01_01 + SW01_02 + SW01_03 + SW01_04 + SW01_05 + SW01_06\n                           + SW01_07 + SW01_08 + SW01_09 + SW01_10 + SW01_11 + SW01_12\n                           + SW01_13 + SW01_14 + SW01_15 + SW01_16 + SW01_17 + SW01_18;\n        SW_GF ~ education\n\"\nfit_education1 &lt;- sem(sem_education1, data = data)\n\n# Regressions:\n#                   Estimate  Std.Err  z-value  P(&gt;|z|)\n#  SW_GF ~\n#    education         0.170    0.059    2.897    0.004\n\nsem_education2 &lt;- \"\n        SW_AD =~ SW01_01 + SW01_02 + SW01_03 + SW01_04 + SW01_05 + SW01_06\n                + SW01_07 + SW01_08 + SW01_09;\n        SW_AD ~ education\n\"\nfit_education2 &lt;- sem(sem_education2, data = data)\n\n# Regressions:\n#                   Estimate  Std.Err  z-value  P(&gt;|z|)\n#  SW_AD ~\n#    education         0.209    0.066    3.196    0.001\n\nsem_education3 &lt;- \"\n        SW_HI =~ SW01_10 + SW01_11 + SW01_12 + SW01_13 + SW01_14 + SW01_15\n                + SW01_16 + SW01_17 + SW01_18;\n        SW_HI ~ education\n\"\nfit_education3 &lt;- sem(sem_education3, data = data)\n\n# Regressions:\n#                   Estimate  Std.Err  z-value  P(&gt;|z|)\n#  SW_HI ~\n#    education         0.172    0.086    2.016    0.044\n\n# interactions\n\n\nSWAN_vars &lt;- colnames(data)[str_detect(colnames(data), \"SW01\")]\n\nSW_AD &lt;- colnames(data[, c(\n    \"SW01_01\",\n    \"SW01_02\",\n    \"SW01_03\",\n    \"SW01_04\",\n    \"SW01_05\",\n    \"SW01_06\",\n    \"SW01_07\",\n    \"SW01_08\",\n    \"SW01_09\"\n)])\n\nSW_HI &lt;- colnames(data[, c(\n    \"SW01_10\",\n    \"SW01_11\",\n    \"SW01_12\",\n    \"SW01_13\",\n    \"SW01_14\",\n    \"SW01_15\",\n    \"SW01_16\",\n    \"SW01_17\",\n    \"SW01_18\"\n)])\n\n# Cronbachs alphas\npsych::alpha(data[, SWAN_vars]) # 0.90\npsych::alpha(data[, SW_AD]) # 0.85\npsych::alpha(data[, SW_HI]) # 0.87\n\n\nReliability analysis   \nCall: psych::alpha(x = data[, SWAN_vars])\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r\n       0.9       0.9    0.92      0.33 8.8 0.0075  3.8 0.83     0.33\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.88   0.9  0.91\nDuhachek  0.88   0.9  0.91\n\n Reliability if an item is dropped:\n        raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r\nSW01_01      0.89      0.90    0.91      0.33 8.5   0.0078 0.014  0.34\nSW01_02      0.89      0.89    0.91      0.32 8.1   0.0081 0.013  0.32\nSW01_03      0.89      0.89    0.91      0.33 8.3   0.0080 0.014  0.32\nSW01_04      0.89      0.89    0.91      0.33 8.3   0.0080 0.013  0.33\nSW01_05      0.89      0.90    0.91      0.34 8.6   0.0077 0.012  0.33\nSW01_06      0.89      0.89    0.91      0.33 8.5   0.0078 0.014  0.33\nSW01_07      0.89      0.89    0.91      0.33 8.3   0.0080 0.013  0.33\nSW01_08      0.90      0.90    0.91      0.34 8.7   0.0077 0.013  0.34\nSW01_09      0.89      0.89    0.91      0.33 8.3   0.0080 0.014  0.32\nSW01_10      0.89      0.89    0.91      0.32 8.2   0.0081 0.014  0.32\nSW01_11      0.89      0.89    0.91      0.32 8.2   0.0081 0.013  0.32\nSW01_12      0.89      0.89    0.91      0.32 8.0   0.0082 0.012  0.32\nSW01_13      0.89      0.89    0.91      0.32 8.2   0.0081 0.013  0.32\nSW01_14      0.89      0.89    0.91      0.33 8.3   0.0080 0.014  0.33\nSW01_15      0.90      0.90    0.91      0.34 8.7   0.0077 0.012  0.33\nSW01_16      0.89      0.90    0.91      0.34 8.6   0.0077 0.011  0.33\nSW01_17      0.89      0.89    0.91      0.33 8.4   0.0079 0.013  0.33\nSW01_18      0.89      0.89    0.91      0.33 8.3   0.0080 0.013  0.33\n\n Item statistics \n          n raw.r std.r r.cor r.drop mean  sd\nSW01_01 398  0.54  0.54  0.51   0.48  3.9 1.3\nSW01_02 397  0.68  0.68  0.67   0.63  3.5 1.3\nSW01_03 398  0.63  0.64  0.61   0.58  4.4 1.2\nSW01_04 398  0.63  0.63  0.61   0.57  3.9 1.5\nSW01_05 398  0.53  0.52  0.50   0.45  4.1 1.5\nSW01_06 398  0.55  0.55  0.52   0.48  3.9 1.4\nSW01_07 398  0.63  0.63  0.62   0.57  4.1 1.3\nSW01_08 398  0.48  0.48  0.43   0.41  2.6 1.3\nSW01_09 398  0.62  0.61  0.58   0.55  3.6 1.5\nSW01_10 398  0.67  0.67  0.64   0.61  3.5 1.5\nSW01_11 398  0.66  0.67  0.65   0.61  4.4 1.3\nSW01_12 398  0.73  0.73  0.73   0.69  3.7 1.3\nSW01_13 398  0.67  0.67  0.66   0.61  4.0 1.4\nSW01_14 398  0.62  0.62  0.59   0.56  3.4 1.5\nSW01_15 398  0.50  0.50  0.46   0.43  3.8 1.4\nSW01_16 398  0.53  0.53  0.50   0.46  3.9 1.4\nSW01_17 398  0.58  0.58  0.55   0.52  3.5 1.3\nSW01_18 398  0.64  0.64  0.62   0.58  3.6 1.4\n\nNon missing response frequency for each item\n           0    1    2    3    4    5    6 miss\nSW01_01 0.01 0.05 0.10 0.17 0.31 0.31 0.06    0\nSW01_02 0.02 0.07 0.10 0.31 0.22 0.25 0.03    0\nSW01_03 0.01 0.02 0.06 0.17 0.23 0.36 0.17    0\nSW01_04 0.02 0.05 0.08 0.25 0.21 0.25 0.15    0\nSW01_05 0.01 0.06 0.09 0.17 0.21 0.26 0.20    0\nSW01_06 0.02 0.04 0.09 0.23 0.23 0.28 0.13    0\nSW01_07 0.01 0.03 0.07 0.23 0.26 0.27 0.14    0\nSW01_08 0.06 0.13 0.23 0.38 0.12 0.06 0.03    0\nSW01_09 0.03 0.07 0.10 0.28 0.25 0.17 0.11    0\nSW01_10 0.03 0.07 0.14 0.31 0.19 0.18 0.09    0\nSW01_11 0.01 0.02 0.03 0.24 0.22 0.24 0.25    0\nSW01_12 0.00 0.03 0.06 0.42 0.20 0.16 0.12    0\nSW01_13 0.00 0.04 0.08 0.28 0.20 0.23 0.17    0\nSW01_14 0.03 0.09 0.17 0.24 0.21 0.17 0.10    0\nSW01_15 0.01 0.06 0.10 0.23 0.24 0.25 0.12    0\nSW01_16 0.01 0.03 0.13 0.25 0.22 0.21 0.16    0\nSW01_17 0.01 0.04 0.13 0.37 0.21 0.18 0.06    0\nSW01_18 0.01 0.05 0.15 0.30 0.18 0.21 0.10    0\n\n\n\nReliability analysis   \nCall: psych::alpha(x = data[, SW_AD])\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r\n      0.85      0.85    0.85      0.38 5.5 0.012  3.8 0.92     0.38\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.82  0.85  0.87\nDuhachek  0.82  0.85  0.87\n\n Reliability if an item is dropped:\n        raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nSW01_01      0.84      0.84    0.83      0.39 5.1    0.012 0.0107  0.39\nSW01_02      0.82      0.82    0.81      0.36 4.4    0.014 0.0094  0.33\nSW01_03      0.83      0.83    0.83      0.38 5.0    0.013 0.0109  0.38\nSW01_04      0.82      0.82    0.82      0.36 4.6    0.014 0.0092  0.35\nSW01_05      0.83      0.83    0.82      0.38 4.9    0.013 0.0083  0.39\nSW01_06      0.83      0.83    0.83      0.39 5.1    0.012 0.0104  0.39\nSW01_07      0.82      0.82    0.81      0.36 4.5    0.014 0.0087  0.35\nSW01_08      0.84      0.85    0.84      0.41 5.5    0.012 0.0080  0.39\nSW01_09      0.83      0.83    0.83      0.38 5.0    0.013 0.0115  0.38\n\n Item statistics \n          n raw.r std.r r.cor r.drop mean  sd\nSW01_01 398  0.61  0.62  0.55   0.50  3.9 1.3\nSW01_02 397  0.77  0.78  0.76   0.69  3.5 1.3\nSW01_03 398  0.64  0.65  0.58   0.54  4.4 1.2\nSW01_04 398  0.74  0.73  0.70   0.64  3.9 1.5\nSW01_05 398  0.68  0.67  0.63   0.56  4.1 1.5\nSW01_06 398  0.63  0.63  0.56   0.51  3.9 1.4\nSW01_07 398  0.75  0.75  0.73   0.67  4.1 1.3\nSW01_08 398  0.53  0.54  0.44   0.40  2.6 1.3\nSW01_09 398  0.66  0.65  0.58   0.53  3.6 1.5\n\nNon missing response frequency for each item\n           0    1    2    3    4    5    6 miss\nSW01_01 0.01 0.05 0.10 0.17 0.31 0.31 0.06    0\nSW01_02 0.02 0.07 0.10 0.31 0.22 0.25 0.03    0\nSW01_03 0.01 0.02 0.06 0.17 0.23 0.36 0.17    0\nSW01_04 0.02 0.05 0.08 0.25 0.21 0.25 0.15    0\nSW01_05 0.01 0.06 0.09 0.17 0.21 0.26 0.20    0\nSW01_06 0.02 0.04 0.09 0.23 0.23 0.28 0.13    0\nSW01_07 0.01 0.03 0.07 0.23 0.26 0.27 0.14    0\nSW01_08 0.06 0.13 0.23 0.38 0.12 0.06 0.03    0\nSW01_09 0.03 0.07 0.10 0.28 0.25 0.17 0.11    0\n\n\n\nReliability analysis   \nCall: psych::alpha(x = data[, SW_HI])\n\n  raw_alpha std.alpha G6(smc) average_r S/N  ase mean   sd median_r\n      0.87      0.87    0.87      0.43 6.7 0.01  3.8 0.97     0.42\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.85  0.87  0.89\nDuhachek  0.85  0.87  0.89\n\n Reliability if an item is dropped:\n        raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nSW01_10      0.86      0.86    0.86      0.43 6.0    0.011 0.0073  0.41\nSW01_11      0.85      0.85    0.85      0.42 5.7    0.011 0.0059  0.41\nSW01_12      0.85      0.85    0.84      0.41 5.6    0.012 0.0059  0.41\nSW01_13      0.85      0.85    0.85      0.42 5.7    0.012 0.0069  0.41\nSW01_14      0.86      0.86    0.86      0.44 6.3    0.011 0.0079  0.44\nSW01_15      0.86      0.87    0.86      0.45 6.4    0.010 0.0065  0.44\nSW01_16      0.85      0.86    0.85      0.43 6.0    0.011 0.0082  0.41\nSW01_17      0.86      0.86    0.86      0.43 6.1    0.011 0.0083  0.43\nSW01_18      0.85      0.85    0.85      0.42 5.8    0.011 0.0086  0.40\n\n Item statistics \n          n raw.r std.r r.cor r.drop mean  sd\nSW01_10 398  0.69  0.69  0.63   0.59  3.5 1.5\nSW01_11 398  0.73  0.74  0.72   0.65  4.4 1.3\nSW01_12 398  0.77  0.77  0.75   0.70  3.7 1.3\nSW01_13 398  0.74  0.75  0.71   0.66  4.0 1.4\nSW01_14 398  0.66  0.65  0.58   0.54  3.4 1.5\nSW01_15 398  0.62  0.62  0.55   0.51  3.8 1.4\nSW01_16 398  0.70  0.70  0.65   0.60  3.9 1.4\nSW01_17 398  0.66  0.67  0.61   0.56  3.5 1.3\nSW01_18 398  0.73  0.73  0.69   0.64  3.6 1.4\n\nNon missing response frequency for each item\n           0    1    2    3    4    5    6 miss\nSW01_10 0.03 0.07 0.14 0.31 0.19 0.18 0.09    0\nSW01_11 0.01 0.02 0.03 0.24 0.22 0.24 0.25    0\nSW01_12 0.00 0.03 0.06 0.42 0.20 0.16 0.12    0\nSW01_13 0.00 0.04 0.08 0.28 0.20 0.23 0.17    0\nSW01_14 0.03 0.09 0.17 0.24 0.21 0.17 0.10    0\nSW01_15 0.01 0.06 0.10 0.23 0.24 0.25 0.12    0\nSW01_16 0.01 0.03 0.13 0.25 0.22 0.21 0.16    0\nSW01_17 0.01 0.04 0.13 0.37 0.21 0.18 0.06    0\nSW01_18 0.01 0.05 0.15 0.30 0.18 0.21 0.10    0\n\n\n\npsych::omega(data[SWAN_vars], nfactors = 2)\n\nOmega \nCall: omegah(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip, \n    digits = digits, title = title, sl = sl, labels = labels, \n    plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option = option, \n    covar = covar)\nAlpha:                 0.9 \nG.6:                   0.92 \nOmega Hierarchical:    0.58 \nOmega H asymptotic:    0.63 \nOmega Total            0.91 \n\nSchmid Leiman Factor loadings greater than  0.2 \n           g   F1*   F2*   h2   h2   u2   p2  com\nSW01_01 0.40  0.34       0.28 0.28 0.72 0.56 2.06\nSW01_02 0.54  0.50       0.55 0.55 0.45 0.53 2.03\nSW01_03 0.49  0.32  0.20 0.38 0.38 0.62 0.62 2.12\nSW01_04 0.50  0.54       0.54 0.54 0.46 0.46 1.99\nSW01_05 0.40  0.52       0.44 0.44 0.56 0.37 1.95\nSW01_06 0.41  0.36       0.30 0.30 0.70 0.55 2.05\nSW01_07 0.50  0.55       0.55 0.55 0.45 0.45 1.98\nSW01_08 0.34  0.27       0.20 0.20 0.80 0.59 2.08\nSW01_09 0.46  0.35       0.36 0.36 0.64 0.60 2.10\nSW01_10 0.51        0.36 0.43 0.43 0.57 0.61 2.11\nSW01_11 0.52        0.46 0.49 0.49 0.51 0.55 2.05\nSW01_12 0.58        0.44 0.56 0.56 0.44 0.60 2.09\nSW01_13 0.52        0.49 0.51 0.51 0.49 0.53 2.03\nSW01_14 0.46        0.34 0.35 0.35 0.65 0.61 2.10\nSW01_15 0.35        0.44 0.32 0.32 0.68 0.38 1.95\nSW01_16 0.39        0.59 0.53 0.53 0.47 0.29 1.93\nSW01_17 0.44        0.43 0.38 0.38 0.62 0.50 2.01\nSW01_18 0.48        0.50 0.48 0.48 0.52 0.48 2.00\n\nWith Sums of squares  of:\n  g F1* F2*  h2 \n3.9 1.8 2.0 3.5 \n\ngeneral/max  1.13   max/min =   1.91\nmean percent general =  0.52    with sd =  0.09 and cv of  0.18 \nExplained Common Variance of the general factor =  0.51 \n\nThe degrees of freedom are 118  and the fit is  0.99 \nThe number of observations was  398  with Chi Square =  386  with prob &lt;  3.4e-30\nThe root mean square of the residuals is  0.05 \nThe df corrected root mean square of the residuals is  0.05\nRMSEA index =  0.075  and the 10 % confidence intervals are  0.067 0.084\nBIC =  -320\n\nCompare this with the adequacy of just a general factor and no group factors\nThe degrees of freedom for just the general factor are 135  and the fit is  2.52 \nThe number of observations was  398  with Chi Square =  980  with prob &lt;  3.5e-128\nThe root mean square of the residuals is  0.15 \nThe df corrected root mean square of the residuals is  0.16 \n\nRMSEA index =  0.125  and the 10 % confidence intervals are  0.118 0.133\nBIC =  172 \n\nMeasures of factor score adequacy             \n                                                 g  F1*  F2*\nCorrelation of scores with factors            0.76 0.73 0.74\nMultiple R square of scores with factors      0.58 0.54 0.54\nMinimum correlation of factor score estimates 0.17 0.08 0.09\n\n Total, General and Subset omega for each subset\n                                                 g  F1*  F2*\nOmega total for total scores and subscales    0.91 0.84 0.87\nOmega general for total scores and subscales  0.58 0.45 0.46\nOmega group for total scores and subscales    0.26 0.39 0.41\n\n\n\n\n\n\n\n\n\n\n# Correlation Matrix\ncorr.test(data[, c(SW_AD, \"SW_AD_mean\")]) # 0.53 - 0.77\ncorr.test(data[, c(SW_HI, \"SW_HI_mean\")]) # 0.62 - 0.77\ncorr.test(data[, c(SWAN_vars, \"SW_mean\")]) # 0.48 - 0.73\n\nCall:corr.test(x = data[, c(SW_AD, \"SW_AD_mean\")])\nCorrelation matrix \n           SW01_01 SW01_02 SW01_03 SW01_04 SW01_05 SW01_06 SW01_07 SW01_08\nSW01_01       1.00    0.49    0.32    0.31    0.38    0.36    0.32    0.22\nSW01_02       0.49    1.00    0.47    0.53    0.38    0.53    0.46    0.41\nSW01_03       0.32    0.47    1.00    0.44    0.32    0.31    0.40    0.23\nSW01_04       0.31    0.53    0.44    1.00    0.53    0.39    0.54    0.30\nSW01_05       0.38    0.38    0.32    0.53    1.00    0.28    0.61    0.20\nSW01_06       0.36    0.53    0.31    0.39    0.28    1.00    0.40    0.27\nSW01_07       0.32    0.46    0.40    0.54    0.61    0.40    1.00    0.33\nSW01_08       0.22    0.41    0.23    0.30    0.20    0.27    0.33    1.00\nSW01_09       0.33    0.40    0.41    0.39    0.34    0.28    0.48    0.29\nSW_AD_mean    0.61    0.77    0.64    0.74    0.68    0.63    0.75    0.53\n           SW01_09 SW_AD_mean\nSW01_01       0.33       0.61\nSW01_02       0.40       0.77\nSW01_03       0.41       0.64\nSW01_04       0.39       0.74\nSW01_05       0.34       0.68\nSW01_06       0.28       0.63\nSW01_07       0.48       0.75\nSW01_08       0.29       0.53\nSW01_09       1.00       0.66\nSW_AD_mean    0.66       1.00\nSample Size \n           SW01_01 SW01_02 SW01_03 SW01_04 SW01_05 SW01_06 SW01_07 SW01_08\nSW01_01        398     397     398     398     398     398     398     398\nSW01_02        397     397     397     397     397     397     397     397\nSW01_03        398     397     398     398     398     398     398     398\nSW01_04        398     397     398     398     398     398     398     398\nSW01_05        398     397     398     398     398     398     398     398\nSW01_06        398     397     398     398     398     398     398     398\nSW01_07        398     397     398     398     398     398     398     398\nSW01_08        398     397     398     398     398     398     398     398\nSW01_09        398     397     398     398     398     398     398     398\nSW_AD_mean     398     397     398     398     398     398     398     398\n           SW01_09 SW_AD_mean\nSW01_01        398        398\nSW01_02        397        397\nSW01_03        398        398\nSW01_04        398        398\nSW01_05        398        398\nSW01_06        398        398\nSW01_07        398        398\nSW01_08        398        398\nSW01_09        398        398\nSW_AD_mean     398        398\nProbability values (Entries above the diagonal are adjusted for multiple tests.) \n           SW01_01 SW01_02 SW01_03 SW01_04 SW01_05 SW01_06 SW01_07 SW01_08\nSW01_01          0       0       0       0       0       0       0       0\nSW01_02          0       0       0       0       0       0       0       0\nSW01_03          0       0       0       0       0       0       0       0\nSW01_04          0       0       0       0       0       0       0       0\nSW01_05          0       0       0       0       0       0       0       0\nSW01_06          0       0       0       0       0       0       0       0\nSW01_07          0       0       0       0       0       0       0       0\nSW01_08          0       0       0       0       0       0       0       0\nSW01_09          0       0       0       0       0       0       0       0\nSW_AD_mean       0       0       0       0       0       0       0       0\n           SW01_09 SW_AD_mean\nSW01_01          0          0\nSW01_02          0          0\nSW01_03          0          0\nSW01_04          0          0\nSW01_05          0          0\nSW01_06          0          0\nSW01_07          0          0\nSW01_08          0          0\nSW01_09          0          0\nSW_AD_mean       0          0\n\n To see confidence intervals of the correlations, print with the short=FALSE option\n\n\nCall:corr.test(x = data[, c(SW_HI, \"SW_HI_mean\")])\nCorrelation matrix \n           SW01_10 SW01_11 SW01_12 SW01_13 SW01_14 SW01_15 SW01_16 SW01_17\nSW01_10       1.00    0.49    0.58    0.42    0.44    0.30    0.33    0.34\nSW01_11       0.49    1.00    0.61    0.61    0.40    0.30    0.42    0.47\nSW01_12       0.58    0.61    1.00    0.55    0.50    0.34    0.39    0.41\nSW01_13       0.42    0.61    0.55    1.00    0.37    0.38    0.48    0.37\nSW01_14       0.44    0.40    0.50    0.37    1.00    0.32    0.32    0.36\nSW01_15       0.30    0.30    0.34    0.38    0.32    1.00    0.50    0.32\nSW01_16       0.33    0.42    0.39    0.48    0.32    0.50    1.00    0.46\nSW01_17       0.34    0.47    0.41    0.37    0.36    0.32    0.46    1.00\nSW01_18       0.44    0.38    0.49    0.51    0.37    0.44    0.49    0.47\nSW_HI_mean    0.69    0.73    0.77    0.74    0.66    0.62    0.70    0.66\n           SW01_18 SW_HI_mean\nSW01_10       0.44       0.69\nSW01_11       0.38       0.73\nSW01_12       0.49       0.77\nSW01_13       0.51       0.74\nSW01_14       0.37       0.66\nSW01_15       0.44       0.62\nSW01_16       0.49       0.70\nSW01_17       0.47       0.66\nSW01_18       1.00       0.73\nSW_HI_mean    0.73       1.00\nSample Size \n[1] 398\nProbability values (Entries above the diagonal are adjusted for multiple tests.) \n           SW01_10 SW01_11 SW01_12 SW01_13 SW01_14 SW01_15 SW01_16 SW01_17\nSW01_10          0       0       0       0       0       0       0       0\nSW01_11          0       0       0       0       0       0       0       0\nSW01_12          0       0       0       0       0       0       0       0\nSW01_13          0       0       0       0       0       0       0       0\nSW01_14          0       0       0       0       0       0       0       0\nSW01_15          0       0       0       0       0       0       0       0\nSW01_16          0       0       0       0       0       0       0       0\nSW01_17          0       0       0       0       0       0       0       0\nSW01_18          0       0       0       0       0       0       0       0\nSW_HI_mean       0       0       0       0       0       0       0       0\n           SW01_18 SW_HI_mean\nSW01_10          0          0\nSW01_11          0          0\nSW01_12          0          0\nSW01_13          0          0\nSW01_14          0          0\nSW01_15          0          0\nSW01_16          0          0\nSW01_17          0          0\nSW01_18          0          0\nSW_HI_mean       0          0\n\n To see confidence intervals of the correlations, print with the short=FALSE option\n\n\nCall:corr.test(x = data[, c(SWAN_vars, \"SW_mean\")])\nCorrelation matrix \n        SW01_01 SW01_02 SW01_03 SW01_04 SW01_05 SW01_06 SW01_07 SW01_08\nSW01_01    1.00    0.49    0.32    0.31    0.38    0.36    0.32    0.22\nSW01_02    0.49    1.00    0.47    0.53    0.38    0.53    0.46    0.41\nSW01_03    0.32    0.47    1.00    0.44    0.32    0.31    0.40    0.23\nSW01_04    0.31    0.53    0.44    1.00    0.53    0.39    0.54    0.30\nSW01_05    0.38    0.38    0.32    0.53    1.00    0.28    0.61    0.20\nSW01_06    0.36    0.53    0.31    0.39    0.28    1.00    0.40    0.27\nSW01_07    0.32    0.46    0.40    0.54    0.61    0.40    1.00    0.33\nSW01_08    0.22    0.41    0.23    0.30    0.20    0.27    0.33    1.00\nSW01_09    0.33    0.40    0.41    0.39    0.34    0.28    0.48    0.29\nSW01_10    0.34    0.43    0.34    0.38    0.23    0.30    0.29    0.28\nSW01_11    0.22    0.35    0.41    0.35    0.21    0.25    0.29    0.19\nSW01_12    0.30    0.38    0.38    0.42    0.28    0.35    0.37    0.26\nSW01_13    0.22    0.35    0.40    0.29    0.23    0.24    0.33    0.18\nSW01_14    0.24    0.32    0.33    0.31    0.26    0.20    0.29    0.34\nSW01_15    0.29    0.13    0.21    0.12    0.12    0.19    0.15    0.14\nSW01_16    0.17    0.18    0.28    0.09    0.03    0.17    0.09    0.15\nSW01_17    0.17    0.28    0.37    0.24    0.16    0.23    0.28    0.17\nSW01_18    0.25    0.34    0.30    0.21    0.17    0.28    0.25    0.28\nSW_mean    0.54    0.68    0.63    0.63    0.53    0.55    0.63    0.48\n        SW01_09 SW01_10 SW01_11 SW01_12 SW01_13 SW01_14 SW01_15 SW01_16\nSW01_01    0.33    0.34    0.22    0.30    0.22    0.24    0.29    0.17\nSW01_02    0.40    0.43    0.35    0.38    0.35    0.32    0.13    0.18\nSW01_03    0.41    0.34    0.41    0.38    0.40    0.33    0.21    0.28\nSW01_04    0.39    0.38    0.35    0.42    0.29    0.31    0.12    0.09\nSW01_05    0.34    0.23    0.21    0.28    0.23    0.26    0.12    0.03\nSW01_06    0.28    0.30    0.25    0.35    0.24    0.20    0.19    0.17\nSW01_07    0.48    0.29    0.29    0.37    0.33    0.29    0.15    0.09\nSW01_08    0.29    0.28    0.19    0.26    0.18    0.34    0.14    0.15\nSW01_09    1.00    0.35    0.33    0.38    0.36    0.37    0.20    0.23\nSW01_10    0.35    1.00    0.49    0.58    0.42    0.44    0.30    0.33\nSW01_11    0.33    0.49    1.00    0.61    0.61    0.40    0.30    0.42\nSW01_12    0.38    0.58    0.61    1.00    0.55    0.50    0.34    0.39\nSW01_13    0.36    0.42    0.61    0.55    1.00    0.37    0.38    0.48\nSW01_14    0.37    0.44    0.40    0.50    0.37    1.00    0.32    0.32\nSW01_15    0.20    0.30    0.30    0.34    0.38    0.32    1.00    0.50\nSW01_16    0.23    0.33    0.42    0.39    0.48    0.32    0.50    1.00\nSW01_17    0.27    0.34    0.47    0.41    0.37    0.36    0.32    0.46\nSW01_18    0.27    0.44    0.38    0.49    0.51    0.37    0.44    0.49\nSW_mean    0.62    0.67    0.66    0.73    0.67    0.62    0.50    0.53\n        SW01_17 SW01_18 SW_mean\nSW01_01    0.17    0.25    0.54\nSW01_02    0.28    0.34    0.68\nSW01_03    0.37    0.30    0.63\nSW01_04    0.24    0.21    0.63\nSW01_05    0.16    0.17    0.53\nSW01_06    0.23    0.28    0.55\nSW01_07    0.28    0.25    0.63\nSW01_08    0.17    0.28    0.48\nSW01_09    0.27    0.27    0.62\nSW01_10    0.34    0.44    0.67\nSW01_11    0.47    0.38    0.66\nSW01_12    0.41    0.49    0.73\nSW01_13    0.37    0.51    0.67\nSW01_14    0.36    0.37    0.62\nSW01_15    0.32    0.44    0.50\nSW01_16    0.46    0.49    0.53\nSW01_17    1.00    0.47    0.58\nSW01_18    0.47    1.00    0.64\nSW_mean    0.58    0.64    1.00\nSample Size \n        SW01_01 SW01_02 SW01_03 SW01_04 SW01_05 SW01_06 SW01_07 SW01_08\nSW01_01     398     397     398     398     398     398     398     398\nSW01_02     397     397     397     397     397     397     397     397\nSW01_03     398     397     398     398     398     398     398     398\nSW01_04     398     397     398     398     398     398     398     398\nSW01_05     398     397     398     398     398     398     398     398\nSW01_06     398     397     398     398     398     398     398     398\nSW01_07     398     397     398     398     398     398     398     398\nSW01_08     398     397     398     398     398     398     398     398\nSW01_09     398     397     398     398     398     398     398     398\nSW01_10     398     397     398     398     398     398     398     398\nSW01_11     398     397     398     398     398     398     398     398\nSW01_12     398     397     398     398     398     398     398     398\nSW01_13     398     397     398     398     398     398     398     398\nSW01_14     398     397     398     398     398     398     398     398\nSW01_15     398     397     398     398     398     398     398     398\nSW01_16     398     397     398     398     398     398     398     398\nSW01_17     398     397     398     398     398     398     398     398\nSW01_18     398     397     398     398     398     398     398     398\nSW_mean     398     397     398     398     398     398     398     398\n        SW01_09 SW01_10 SW01_11 SW01_12 SW01_13 SW01_14 SW01_15 SW01_16\nSW01_01     398     398     398     398     398     398     398     398\nSW01_02     397     397     397     397     397     397     397     397\nSW01_03     398     398     398     398     398     398     398     398\nSW01_04     398     398     398     398     398     398     398     398\nSW01_05     398     398     398     398     398     398     398     398\nSW01_06     398     398     398     398     398     398     398     398\nSW01_07     398     398     398     398     398     398     398     398\nSW01_08     398     398     398     398     398     398     398     398\nSW01_09     398     398     398     398     398     398     398     398\nSW01_10     398     398     398     398     398     398     398     398\nSW01_11     398     398     398     398     398     398     398     398\nSW01_12     398     398     398     398     398     398     398     398\nSW01_13     398     398     398     398     398     398     398     398\nSW01_14     398     398     398     398     398     398     398     398\nSW01_15     398     398     398     398     398     398     398     398\nSW01_16     398     398     398     398     398     398     398     398\nSW01_17     398     398     398     398     398     398     398     398\nSW01_18     398     398     398     398     398     398     398     398\nSW_mean     398     398     398     398     398     398     398     398\n        SW01_17 SW01_18 SW_mean\nSW01_01     398     398     398\nSW01_02     397     397     397\nSW01_03     398     398     398\nSW01_04     398     398     398\nSW01_05     398     398     398\nSW01_06     398     398     398\nSW01_07     398     398     398\nSW01_08     398     398     398\nSW01_09     398     398     398\nSW01_10     398     398     398\nSW01_11     398     398     398\nSW01_12     398     398     398\nSW01_13     398     398     398\nSW01_14     398     398     398\nSW01_15     398     398     398\nSW01_16     398     398     398\nSW01_17     398     398     398\nSW01_18     398     398     398\nSW_mean     398     398     398\nProbability values (Entries above the diagonal are adjusted for multiple tests.) \n        SW01_01 SW01_02 SW01_03 SW01_04 SW01_05 SW01_06 SW01_07 SW01_08\nSW01_01       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_02       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_03       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_04       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_05       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_06       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_07       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_08       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_09       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_10       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_11       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_12       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_13       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_14       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_15       0    0.01       0    0.02    0.02       0    0.00       0\nSW01_16       0    0.00       0    0.06    0.59       0    0.06       0\nSW01_17       0    0.00       0    0.00    0.00       0    0.00       0\nSW01_18       0    0.00       0    0.00    0.00       0    0.00       0\nSW_mean       0    0.00       0    0.00    0.00       0    0.00       0\n        SW01_09 SW01_10 SW01_11 SW01_12 SW01_13 SW01_14 SW01_15 SW01_16\nSW01_01       0       0       0       0       0       0    0.00    0.01\nSW01_02       0       0       0       0       0       0    0.07    0.01\nSW01_03       0       0       0       0       0       0    0.00    0.00\nSW01_04       0       0       0       0       0       0    0.11    0.18\nSW01_05       0       0       0       0       0       0    0.11    0.59\nSW01_06       0       0       0       0       0       0    0.00    0.01\nSW01_07       0       0       0       0       0       0    0.02    0.18\nSW01_08       0       0       0       0       0       0    0.03    0.02\nSW01_09       0       0       0       0       0       0    0.00    0.00\nSW01_10       0       0       0       0       0       0    0.00    0.00\nSW01_11       0       0       0       0       0       0    0.00    0.00\nSW01_12       0       0       0       0       0       0    0.00    0.00\nSW01_13       0       0       0       0       0       0    0.00    0.00\nSW01_14       0       0       0       0       0       0    0.00    0.00\nSW01_15       0       0       0       0       0       0    0.00    0.00\nSW01_16       0       0       0       0       0       0    0.00    0.00\nSW01_17       0       0       0       0       0       0    0.00    0.00\nSW01_18       0       0       0       0       0       0    0.00    0.00\nSW_mean       0       0       0       0       0       0    0.00    0.00\n        SW01_17 SW01_18 SW_mean\nSW01_01    0.01    0.00       0\nSW01_02    0.00    0.00       0\nSW01_03    0.00    0.00       0\nSW01_04    0.00    0.00       0\nSW01_05    0.02    0.01       0\nSW01_06    0.00    0.00       0\nSW01_07    0.00    0.00       0\nSW01_08    0.01    0.00       0\nSW01_09    0.00    0.00       0\nSW01_10    0.00    0.00       0\nSW01_11    0.00    0.00       0\nSW01_12    0.00    0.00       0\nSW01_13    0.00    0.00       0\nSW01_14    0.00    0.00       0\nSW01_15    0.00    0.00       0\nSW01_16    0.00    0.00       0\nSW01_17    0.00    0.00       0\nSW01_18    0.00    0.00       0\nSW_mean    0.00    0.00       0\n\n To see confidence intervals of the correlations, print with the short=FALSE option\n\n\n\n# Model 1: Bifactor Model\n# Model specification\nswan_model_2 &lt;- \"\nSW_GF =~ NA*SW01_01 + SW01_02 + SW01_03 + SW01_04 + SW01_05 + SW01_06 + SW01_07 +\n    SW01_08 + SW01_09 + SW01_10 + SW01_11 + SW01_12 + SW01_13 + SW01_14 + SW01_15 +\n    SW01_16 + SW01_17 + SW01_18;\nSW_AD =~ NA*SW01_01 + SW01_02 + SW01_03 + SW01_04 + SW01_05 + SW01_06 + SW01_07 +\n    SW01_08 + SW01_09; SW_HI =~ NA*SW01_10 + SW01_11 + SW01_12 + SW01_13 + SW01_14 +\n    SW01_15 + SW01_16 + SW01_17 + SW01_18;\nSW_GF ~~ 1*SW_GF; SW_AD ~~ 1*SW_AD; SW_HI ~~ 1*SW_HI; SW_GF ~~ 0*SW_AD;\nSW_AD ~~ 0*SW_HI; SW_HI ~~ 0*SW_GF\n\"\n\n\n# Model calculation\nswan_m2_cfa &lt;- cfa(swan_model_2,\n    data = data,\n    std.lv = TRUE,\n    missing = \"fiml\",\n    estimator = \"MLR\"\n)\n# Summary\nsummary(swan_m2_cfa, standardized = TRUE, fit = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 66 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        72\n\n  Number of observations                           398\n  Number of missing patterns                         2\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               352.858     289.028\n  Degrees of freedom                               117         117\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.221\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              2928.538    2257.491\n  Degrees of freedom                               153         153\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.297\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.915       0.918\n  Tucker-Lewis Index (TLI)                       0.889       0.893\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.925\n  Robust Tucker-Lewis Index (TLI)                            0.902\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -11152.042  -11152.042\n  Scaling correction factor                                  1.231\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -10975.613  -10975.613\n  Scaling correction factor                                  1.225\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               22448.083   22448.083\n  Bayesian (BIC)                             22735.108   22735.108\n  Sample-size adjusted Bayesian (SABIC)      22506.649   22506.649\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.071       0.061\n  90 Percent confidence interval - lower         0.063       0.053\n  90 Percent confidence interval - upper         0.080       0.069\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.014\n  P-value H_0: RMSEA &gt;= 0.080                    0.045       0.000\n                                                                  \n  Robust RMSEA                                               0.066\n  90 Percent confidence interval - lower                     0.056\n  90 Percent confidence interval - upper                     0.076\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.005\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.012\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.042       0.042\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SW_GF =~                                                              \n    SW01_01           0.511    0.089    5.754    0.000    0.511    0.393\n    SW01_02           0.743    0.121    6.161    0.000    0.743    0.563\n    SW01_03           0.676    0.076    8.842    0.000    0.676    0.543\n    SW01_04           0.747    0.107    6.989    0.000    0.747    0.513\n    SW01_05           0.496    0.095    5.207    0.000    0.496    0.331\n    SW01_06           0.602    0.106    5.673    0.000    0.602    0.427\n    SW01_07           0.587    0.083    7.087    0.000    0.587    0.452\n    SW01_08           0.485    0.102    4.761    0.000    0.485    0.367\n    SW01_09           0.749    0.089    8.409    0.000    0.749    0.507\n    SW01_10           1.033    0.075   13.834    0.000    1.033    0.698\n    SW01_11           0.931    0.095    9.751    0.000    0.931    0.712\n    SW01_12           0.991    0.079   12.618    0.000    0.991    0.793\n    SW01_13           0.900    0.103    8.718    0.000    0.900    0.650\n    SW01_14           0.913    0.082   11.154    0.000    0.913    0.595\n    SW01_15           0.518    0.130    3.975    0.000    0.518    0.361\n    SW01_16           0.587    0.160    3.667    0.000    0.587    0.418\n    SW01_17           0.636    0.084    7.588    0.000    0.636    0.508\n    SW01_18           0.780    0.099    7.865    0.000    0.780    0.554\n  SW_AD =~                                                              \n    SW01_01           0.451    0.106    4.248    0.000    0.451    0.346\n    SW01_02           0.569    0.166    3.420    0.001    0.569    0.431\n    SW01_03           0.346    0.109    3.166    0.002    0.346    0.278\n    SW01_04           0.749    0.098    7.639    0.000    0.749    0.514\n    SW01_05           0.945    0.107    8.810    0.000    0.945    0.630\n    SW01_06           0.487    0.139    3.511    0.000    0.487    0.345\n    SW01_07           0.806    0.083    9.754    0.000    0.806    0.621\n    SW01_08           0.319    0.110    2.902    0.004    0.319    0.241\n    SW01_09           0.470    0.096    4.914    0.000    0.470    0.318\n  SW_HI =~                                                              \n    SW01_10           0.078    0.161    0.482    0.630    0.078    0.053\n    SW01_11           0.217    0.219    0.993    0.321    0.217    0.166\n    SW01_12           0.127    0.192    0.663    0.507    0.127    0.102\n    SW01_13           0.426    0.188    2.261    0.024    0.426    0.308\n    SW01_14           0.171    0.157    1.087    0.277    0.171    0.112\n    SW01_15           0.731    0.120    6.108    0.000    0.731    0.510\n    SW01_16           0.940    0.127    7.401    0.000    0.940    0.669\n    SW01_17           0.432    0.111    3.878    0.000    0.432    0.345\n    SW01_18           0.589    0.126    4.670    0.000    0.589    0.419\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SW_GF ~~                                                              \n    SW_AD             0.000                               0.000    0.000\n  SW_AD ~~                                                              \n    SW_HI             0.000                               0.000    0.000\n  SW_GF ~~                                                              \n    SW_HI             0.000                               0.000    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .SW01_01           3.889    0.065   59.577    0.000    3.889    2.986\n   .SW01_02           3.529    0.066   53.265    0.000    3.529    2.674\n   .SW01_03           4.354    0.062   69.765    0.000    4.354    3.497\n   .SW01_04           3.920    0.073   53.716    0.000    3.920    2.693\n   .SW01_05           4.088    0.075   54.417    0.000    4.088    2.728\n   .SW01_06           3.942    0.071   55.693    0.000    3.942    2.792\n   .SW01_07           4.093    0.065   62.954    0.000    4.093    3.156\n   .SW01_08           2.643    0.066   39.895    0.000    2.643    2.000\n   .SW01_09           3.578    0.074   48.304    0.000    3.578    2.421\n   .SW01_10           3.457    0.074   46.630    0.000    3.457    2.337\n   .SW01_11           4.372    0.066   66.679    0.000    4.372    3.342\n   .SW01_12           3.741    0.063   59.687    0.000    3.741    2.992\n   .SW01_13           3.982    0.069   57.385    0.000    3.982    2.876\n   .SW01_14           3.440    0.077   44.746    0.000    3.440    2.243\n   .SW01_15           3.814    0.072   53.068    0.000    3.814    2.660\n   .SW01_16           3.897    0.070   55.295    0.000    3.897    2.772\n   .SW01_17           3.515    0.063   56.031    0.000    3.515    2.809\n   .SW01_18           3.606    0.071   51.085    0.000    3.606    2.561\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    SW_GF             1.000                               1.000    1.000\n    SW_AD             1.000                               1.000    1.000\n    SW_HI             1.000                               1.000    1.000\n   .SW01_01           1.232    0.103   11.957    0.000    1.232    0.726\n   .SW01_02           0.866    0.093    9.326    0.000    0.866    0.497\n   .SW01_03           0.974    0.082   11.901    0.000    0.974    0.628\n   .SW01_04           1.000    0.108    9.273    0.000    1.000    0.472\n   .SW01_05           1.107    0.187    5.905    0.000    1.107    0.493\n   .SW01_06           1.394    0.107   13.018    0.000    1.394    0.699\n   .SW01_07           0.689    0.110    6.242    0.000    0.689    0.409\n   .SW01_08           1.410    0.117   12.010    0.000    1.410    0.807\n   .SW01_09           1.401    0.123   11.414    0.000    1.401    0.642\n   .SW01_10           1.115    0.110   10.161    0.000    1.115    0.510\n   .SW01_11           0.798    0.087    9.133    0.000    0.798    0.466\n   .SW01_12           0.565    0.083    6.808    0.000    0.565    0.361\n   .SW01_13           0.925    0.086   10.779    0.000    0.925    0.482\n   .SW01_14           1.490    0.130   11.447    0.000    1.490    0.633\n   .SW01_15           1.254    0.153    8.218    0.000    1.254    0.610\n   .SW01_16           0.748    0.144    5.194    0.000    0.748    0.378\n   .SW01_17           0.976    0.097   10.010    0.000    0.976    0.623\n   .SW01_18           1.026    0.113    9.121    0.000    1.026    0.518\n\n\n\n\nmi &lt;- modindices(swan_m2_cfa, minimum.value = 10, sort = TRUE)\nprint(mi)\n\n        lhs op     rhs   mi    epc sepc.lv sepc.all sepc.nox\n120 SW01_02 ~~ SW01_06 28.3  0.326   0.326    0.297    0.297\n163 SW01_05 ~~ SW01_07 26.2  0.401   0.401    0.460    0.460\n226 SW01_11 ~~ SW01_13 24.3  0.252   0.252    0.293    0.293\n231 SW01_11 ~~ SW01_18 17.9 -0.227  -0.227   -0.251   -0.251\n113 SW01_01 ~~ SW01_15 17.9  0.288   0.288    0.232    0.232\n100 SW01_01 ~~ SW01_02 17.8  0.242   0.242    0.235    0.235\n119 SW01_02 ~~ SW01_05 15.5 -0.258  -0.258   -0.263   -0.263\n121 SW01_02 ~~ SW01_07 13.8 -0.198  -0.198   -0.256   -0.256\n126 SW01_02 ~~ SW01_12 12.6 -0.159  -0.159   -0.228   -0.228\n88    SW_AD =~ SW01_16 12.0 -0.261  -0.261   -0.185   -0.185\n162 SW01_05 ~~ SW01_06 11.8 -0.265  -0.265   -0.213   -0.213\n94    SW_HI =~ SW01_04 10.6 -0.235  -0.235   -0.162   -0.162\n105 SW01_01 ~~ SW01_07 10.3 -0.187  -0.187   -0.203   -0.203\n203 SW01_08 ~~ SW01_14 10.2  0.246   0.246    0.169    0.169\n\n\n\n# Model 2: Bifactor Model with Modification Items 5&7 and 2&6\n# Model specification\nswan_model_3 &lt;- \"\n    SW_GF =~ NA*SW01_01 + SW01_02 + SW01_03 + SW01_04 + SW01_05 + SW01_06 + \n        SW01_07 + SW01_08 + SW01_09 + SW01_10 + SW01_11 + SW01_12 + SW01_13 + \n        SW01_14 + SW01_15 + SW01_16 + SW01_17 + SW01_18; \n    SW_AD =~ NA*SW01_01 + SW01_02 + SW01_03 + SW01_04 + SW01_05 + SW01_06 + \n        SW01_07 + SW01_08 + SW01_09; \n    SW_HI =~ NA*SW01_10 + SW01_11 + SW01_12 + SW01_13 + SW01_14 + SW01_15 + \n        SW01_16 + SW01_17 + SW01_18; SW_GF ~~ 1*SW_GF; \n    SW_AD ~~ 1*SW_AD; \n    SW_HI ~~ 1*SW_HI; \n    SW_GF ~~ 0*SW_AD; \n    SW_AD ~~ 0*SW_HI; \n    SW_HI ~~ 0*SW_GF; \n    SW01_05 ~~ SW01_07; \n    SW01_02 ~~ SW01_06\n\"\n\n\n# Model calculation\nswan_m3_cfa &lt;- cfa(swan_model_3,\n    data = data,\n    std.lv = TRUE,\n    missing = \"fiml\",\n    estimator = \"MLR\"\n)\n\n# Summary\ns &lt;- summary(swan_m3_cfa, standardized = TRUE, fit = TRUE) # standardised factor loading is std.all\ns |&gt; print()\n\nlavaan 0.6-19 ended normally after 63 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        74\n\n  Number of observations                           398\n  Number of missing patterns                         2\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               308.602     248.615\n  Degrees of freedom                               115         115\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.241\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              2928.538    2257.491\n  Degrees of freedom                               153         153\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.297\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.930       0.937\n  Tucker-Lewis Index (TLI)                       0.907       0.916\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.941\n  Robust Tucker-Lewis Index (TLI)                            0.921\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -11129.914  -11129.914\n  Scaling correction factor                                  1.199\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -10975.613  -10975.613\n  Scaling correction factor                                  1.225\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               22407.828   22407.828\n  Bayesian (BIC)                             22702.825   22702.825\n  Sample-size adjusted Bayesian (SABIC)      22468.020   22468.020\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.065       0.054\n  90 Percent confidence interval - lower         0.056       0.046\n  90 Percent confidence interval - upper         0.074       0.062\n  P-value H_0: RMSEA &lt;= 0.050                    0.003       0.204\n  P-value H_0: RMSEA &gt;= 0.080                    0.002       0.000\n                                                                  \n  Robust RMSEA                                               0.059\n  90 Percent confidence interval - lower                     0.049\n  90 Percent confidence interval - upper                     0.070\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.069\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.040       0.040\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SW_GF =~                                                              \n    SW01_01           0.490    0.081    6.054    0.000    0.490    0.376\n    SW01_02           0.700    0.085    8.216    0.000    0.700    0.531\n    SW01_03           0.661    0.071    9.378    0.000    0.661    0.531\n    SW01_04           0.728    0.094    7.717    0.000    0.728    0.500\n    SW01_05           0.505    0.096    5.254    0.000    0.505    0.337\n    SW01_06           0.572    0.085    6.726    0.000    0.572    0.405\n    SW01_07           0.595    0.082    7.255    0.000    0.595    0.459\n    SW01_08           0.466    0.091    5.122    0.000    0.466    0.353\n    SW01_09           0.741    0.085    8.714    0.000    0.741    0.501\n    SW01_10           1.026    0.075   13.696    0.000    1.026    0.694\n    SW01_11           0.947    0.075   12.631    0.000    0.947    0.724\n    SW01_12           1.009    0.061   16.426    0.000    1.009    0.807\n    SW01_13           0.922    0.079   11.671    0.000    0.922    0.666\n    SW01_14           0.919    0.079   11.582    0.000    0.919    0.599\n    SW01_15           0.542    0.111    4.860    0.000    0.542    0.378\n    SW01_16           0.614    0.138    4.449    0.000    0.614    0.437\n    SW01_17           0.650    0.072    9.054    0.000    0.650    0.519\n    SW01_18           0.794    0.089    8.875    0.000    0.794    0.564\n  SW_AD =~                                                              \n    SW01_01           0.498    0.093    5.342    0.000    0.498    0.383\n    SW01_02           0.656    0.108    6.056    0.000    0.656    0.497\n    SW01_03           0.399    0.091    4.398    0.000    0.399    0.321\n    SW01_04           0.778    0.095    8.199    0.000    0.778    0.535\n    SW01_05           0.803    0.112    7.197    0.000    0.803    0.536\n    SW01_06           0.495    0.098    5.071    0.000    0.495    0.351\n    SW01_07           0.691    0.096    7.156    0.000    0.691    0.532\n    SW01_08           0.368    0.092    4.002    0.000    0.368    0.278\n    SW01_09           0.492    0.091    5.391    0.000    0.492    0.333\n  SW_HI =~                                                              \n    SW01_10           0.055    0.131    0.421    0.674    0.055    0.037\n    SW01_11           0.174    0.167    1.041    0.298    0.174    0.133\n    SW01_12           0.078    0.134    0.582    0.560    0.078    0.063\n    SW01_13           0.385    0.141    2.736    0.006    0.385    0.278\n    SW01_14           0.141    0.127    1.110    0.267    0.141    0.092\n    SW01_15           0.712    0.120    5.959    0.000    0.712    0.497\n    SW01_16           0.929    0.140    6.624    0.000    0.929    0.661\n    SW01_17           0.412    0.098    4.193    0.000    0.412    0.329\n    SW01_18           0.569    0.119    4.768    0.000    0.569    0.404\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SW_GF ~~                                                              \n    SW_AD             0.000                               0.000    0.000\n  SW_AD ~~                                                              \n    SW_HI             0.000                               0.000    0.000\n  SW_GF ~~                                                              \n    SW_HI             0.000                               0.000    0.000\n .SW01_05 ~~                                                            \n   .SW01_07           0.323    0.102    3.158    0.002    0.323    0.301\n .SW01_02 ~~                                                            \n   .SW01_06           0.263    0.082    3.198    0.001    0.263    0.243\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .SW01_01           3.889    0.065   59.577    0.000    3.889    2.986\n   .SW01_02           3.529    0.066   53.343    0.000    3.529    2.676\n   .SW01_03           4.354    0.062   69.765    0.000    4.354    3.497\n   .SW01_04           3.920    0.073   53.716    0.000    3.920    2.693\n   .SW01_05           4.088    0.075   54.417    0.000    4.088    2.728\n   .SW01_06           3.942    0.071   55.693    0.000    3.942    2.792\n   .SW01_07           4.093    0.065   62.954    0.000    4.093    3.156\n   .SW01_08           2.643    0.066   39.895    0.000    2.643    2.000\n   .SW01_09           3.578    0.074   48.304    0.000    3.578    2.421\n   .SW01_10           3.457    0.074   46.630    0.000    3.457    2.337\n   .SW01_11           4.372    0.066   66.679    0.000    4.372    3.342\n   .SW01_12           3.741    0.063   59.687    0.000    3.741    2.992\n   .SW01_13           3.982    0.069   57.385    0.000    3.982    2.876\n   .SW01_14           3.440    0.077   44.746    0.000    3.440    2.243\n   .SW01_15           3.814    0.072   53.068    0.000    3.814    2.660\n   .SW01_16           3.897    0.070   55.295    0.000    3.897    2.772\n   .SW01_17           3.515    0.063   56.031    0.000    3.515    2.809\n   .SW01_18           3.606    0.071   51.085    0.000    3.606    2.561\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    SW_GF             1.000                               1.000    1.000\n    SW_AD             1.000                               1.000    1.000\n    SW_HI             1.000                               1.000    1.000\n   .SW01_01           1.208    0.104   11.630    0.000    1.208    0.712\n   .SW01_02           0.820    0.104    7.848    0.000    0.820    0.471\n   .SW01_03           0.954    0.081   11.782    0.000    0.954    0.615\n   .SW01_04           0.983    0.121    8.150    0.000    0.983    0.464\n   .SW01_05           1.346    0.156    8.646    0.000    1.346    0.599\n   .SW01_06           1.422    0.107   13.257    0.000    1.422    0.713\n   .SW01_07           0.851    0.106    8.032    0.000    0.851    0.506\n   .SW01_08           1.395    0.117   11.930    0.000    1.395    0.798\n   .SW01_09           1.393    0.123   11.303    0.000    1.393    0.638\n   .SW01_10           1.132    0.112   10.084    0.000    1.132    0.518\n   .SW01_11           0.784    0.084    9.295    0.000    0.784    0.458\n   .SW01_12           0.540    0.069    7.828    0.000    0.540    0.345\n   .SW01_13           0.918    0.086   10.667    0.000    0.918    0.479\n   .SW01_14           1.487    0.130   11.479    0.000    1.487    0.632\n   .SW01_15           1.255    0.156    8.049    0.000    1.255    0.611\n   .SW01_16           0.736    0.162    4.547    0.000    0.736    0.373\n   .SW01_17           0.975    0.098    9.989    0.000    0.975    0.622\n   .SW01_18           1.028    0.117    8.795    0.000    1.028    0.519\n\n\n\n\n# Figure Structural Model (Model 3)\nm &lt;- matrix(nrow = 18, ncol = 3)\nm[, 1] &lt;- c(rep(0, 4), \"SW_A\", rep(0, 8), \"SW_H\", rep(0, 4))\nm[, 2] &lt;- c(SWAN_vars)\nm[, 3] &lt;- c(rep(0, 9), \"SW_G\", rep(0, 8))\n\nstr_model &lt;- semPaths(swan_m3_cfa,\n    layout = m,\n    intercepts = FALSE,\n    what = \"std\",\n    style = \"lisrel\",\n    edge.color = \"grey10\",\n    fade = FALSE,\n    edge.label.cex = 0.6,\n    sizeMan = 6,\n    sizeInt = 5,\n    sizeLat = 8,\n    sizeMan2 = 3,\n    esize = 1,\n    residuals = FALSE,\n    curvePivot = FALSE\n)\n\n\n\n\n\n\n\n\n\n\n\n\nBlume, F., Buhr, L., Kuehnhausen, J., Köpke, R., Weber, L. A., Fallgatter, A. J., Ethofer, T., & Gawrilow, C. (2020). Validation of the Self-Report Version of the German Strengths and Weaknesses of ADHD Symptoms and Normal Behavior Scale (SWAN-DE-SB). Assessment, 10731911241236699.",
    "crumbs": [
      "CFA",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html",
    "href": "chapters/sem/01_sem_intro.html",
    "title": "45  Introduzione ai Modelli SEM",
    "section": "",
    "text": "45.1 Introduzione\nI Modelli di Equazioni Strutturali (SEM) rappresentano un’evoluzione dell’analisi fattoriale, sia esplorativa che confermativa. Un tipico modello SEM si compone di due parti principali:\nL’obiettivo primario dei SEM è testare teorie specifiche attraverso la specificazione di modelli che rappresentino le predizioni di queste teorie, utilizzando costrutti plausibili misurati con variabili osservate appropriate. In questo senso, i SEM fungono da ponte tra teoria e osservazione, permettendo di tradurre concetti astratti in entità misurabili e di analizzare le loro interrelazioni in modo strutturato e teoricamente valido.\nTuttavia, è fondamentale mantenere un approccio critico nell’utilizzo dei SEM. Come tutti i modelli statistici, i SEM si basano su semplificazioni e approssimazioni della realtà. Questo concetto è ben espresso dal noto aforisma: “Tutti i modelli sono sbagliati, ma alcuni sono utili”. Questa affermazione sottolinea l’importanza di riconoscere che, anche se un modello mostra un buon adattamento ai dati, non è necessariamente una rappresentazione fedele della realtà.\nÈ possibile che modelli fondalmentalmente imprecisi si adattino bene ai dati, portando a conclusioni errate o fuorvianti. Di conseguenza, la selezione dei modelli va oltre un mero esercizio statistico; diventa un processo di sviluppo e raffinamento di teorie più solide. La critica e la revisione dei modelli, basate su evidenze empiriche e considerazioni teoriche, sono quindi componenti essenziali del processo scientifico nelle scienze sociali e psicologiche.\nPer condurre un’analisi mediante i modelli di equazioni strutturali, è utile impiegare i diagrammi di percorso (path diagrams). I modelli SEM sono costituiti da un insieme di equazioni che mirano a riprodurre le covarianze osservate attraverso le relazioni strutturali tra le variabili latenti ipotizzate e le variabili manifeste. I path diagram forniscono una rappresentazione grafica fedele di questi sistemi di equazioni, nel senso che tutte le informazioni contenute in un sistema di equazioni SEM sono rappresentate nel path diagram.\nIn sintesi, i SEM offrono potenti strumenti per l’analisi e l’interpretazione dei dati in psicologia. Tuttavia, il loro impiego richiede un attento equilibrio tra comprensione teorica, competenza statistica e spirito critico. Con un approccio consapevole e ben informato, i SEM possono giocare un ruolo importante nello sviluppo della ricerca psicologica, conducendo a una comprensione più profonda e articolata dei fenomeni studiati.\nPer introdurre i modelli SEM iniziamo a considerare il caso più semplice, ovvero il modello di regressione multipla espresso come un modello di equazioni strutturali. Utilizzeremo un campione di dati reali e ci concentreremo sulle 3 sottoscale del DASS-21: ansia, stress e depressione. Il campione è costituito da 526 studenti universitari di psicologia.\ndat &lt;- read.csv(\n    here::here(\"data\", \"dass_rosenberg_scs.csv\"),\n    header = TRUE\n)\ndat |&gt;\n    head()\n\n\nA data.frame: 6 x 11\n\n\n\nstress\nanxiety\ndepression\nrosenberg\nself_kindness\ncommon_humanity\nmindfulness\nself_judgment\nisolation\nover_identification\nscs_ts\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n7\n6\n4\n31\n17\n16\n16\n11\n8\n10\n98\n\n\n2\n3\n2\n1\n32\n14\n14\n16\n16\n11\n13\n82\n\n\n3\n1\n0\n1\n31\n20\n16\n16\n13\n6\n9\n102\n\n\n4\n12\n11\n13\n34\n12\n6\n6\n10\n7\n15\n70\n\n\n5\n10\n6\n12\n25\n16\n17\n13\n17\n16\n18\n73\n\n\n6\n5\n1\n2\n31\n14\n14\n10\n12\n8\n11\n85\ndim(dat)\n\n\n52611\nEsaminiamo i diagrammi di dispersione tra le varie misure presenti nel campione per verificare che la relazione tra le variabili sia lineare.\nd_mr &lt;- dat |&gt;\n    dplyr::select(stress, anxiety, depression, rosenberg, scs_ts)\n\npairs(d_mr)\ny &lt;- d_mr$scs_ts |&gt; as.matrix()\ndim(y)\n\n\n5261",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#introduzione",
    "href": "chapters/sem/01_sem_intro.html#introduzione",
    "title": "45  Introduzione ai Modelli SEM",
    "section": "",
    "text": "La parte di misurazione: collega le variabili latenti a un insieme di variabili osservate o indicatori.\nLa parte strutturale: modella le relazioni ipotizzate tra le variabili latenti.\n\n\n\n\n\n\n\n\n\n\n\n\n\n45.1.1 Modello di Regressione Lineare Multipla\nIl modello generale di regressione lineare multipla (MLR) si esprime attraverso la seguente equazione:\n\\[\ny_i = \\beta_0 + \\beta_1 x_{1i} + \\cdots + \\beta_p x_{pi} + \\epsilon_i\n\\]\ndove: - \\(i = 1, \\ldots, N\\) indica l’\\(i\\)-esima osservazione, - \\(\\beta_0\\) è l’intercetta del modello, - \\(\\beta_1, \\ldots, \\beta_p\\) sono i coefficienti di regressione associati a ciascuna variabile indipendente, - \\(\\epsilon_i\\) è il termine di errore associato all’\\(i\\)-esima osservazione, - Si presume che il termine di errore sia indipendente dalle variabili esplicative \\(X_s\\).\nIn questa formula, ogni \\(y_i\\) rappresenta il valore della variabile dipendente per l’\\(i\\)-esima osservazione, i coefficienti \\(\\beta\\) quantificano l’impatto di ogni variabile indipendente sulla variabile dipendente, e \\(\\epsilon_i\\) rappresenta l’errore o la varianza non spiegata nella previsione di \\(y_i\\). Questa struttura consente di modellare relazioni lineari tra una variabile dipendente e più variabili indipendenti.\nIl modello MLR può essere rappresentato anche in forma matriciale come segue:\n\\[\ny = X\\beta + \\epsilon\n\\]\ndove: - \\(y\\) è un vettore \\(N \\times 1\\) dei valori osservati della variabile risposta, - \\(X\\) è una matrice di progettazione \\(N \\times (p+1)\\) che incorpora tutte le \\(p\\) variabili indipendenti e una colonna di uno per l’intercetta, - \\(\\beta\\) è un vettore di dimensione \\((p+1)\\) che contiene i parametri di regressione, inclusa l’intercetta, - \\(\\epsilon\\) rappresenta il vettore dei termini di errore.\nLa matrice \\(X\\) e i vettori \\(y\\) e \\(\\epsilon\\) sono definiti nel modo seguente:\n\\[\ny =\n\\begin{pmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_N\n\\end{pmatrix}, \\quad\n\\epsilon =\n\\begin{pmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\vdots \\\\\n\\epsilon_N\n\\end{pmatrix}, \\quad\nX =\n\\begin{pmatrix}\n1 & x_{11} & \\cdots & x_{p1} \\\\\n1 & x_{12} & \\cdots & x_{p2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{1N} & \\cdots & x_{pN}\n\\end{pmatrix}\n\\]\nOgni riga di \\(X\\) corrisponde a un’osservazione e include i valori delle variabili indipendenti per quella osservazione più un uno per l’intercetta.\n\n\n45.1.2 Metodo dei Minimi Quadrati\nIl metodo dei minimi quadrati (LSE) mira a trovare i parametri \\(\\beta\\) che minimizzano la somma dei quadrati degli errori, espressa come:\n\\[\n\\text{SSE} = \\epsilon_i^2 = \\epsilon' \\epsilon = (y - X\\beta)'(y - X\\beta) = y'y - 2\\beta'X'y + \\beta'X'X\\beta\n\\]\nMinimizzando la SSE rispetto a \\(\\beta\\) e impostando la derivata pari a zero, si ottiene:\n\\[\nX'X\\hat{\\beta} = X'y\n\\]\nSe la matrice \\(X'X\\) è invertibile, la soluzione è:\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'y\n\\]\nQuesta procedura di minimizzazione degli errori al quadrato è essenziale per determinare i coefficienti che descrivono al meglio la relazione tra le variabili indipendenti e la variabile dipendente, minimizzando la discrepanza tra i valori osservati e quelli predetti dal modello.\nOra, esaminiamo queste relazioni utilizzando i dati specifici a disposizione.\n\ndass &lt;- d_mr |&gt;\n    dplyr::select(depression, anxiety, stress)\n\n\nX &lt;- model.matrix(~ depression + anxiety + stress, data = dass)\nhead(X)\n\n\nA matrix: 6 x 4 of type dbl\n\n\n\n(Intercept)\ndepression\nanxiety\nstress\n\n\n\n\n1\n1\n4\n6\n7\n\n\n2\n1\n1\n2\n3\n\n\n3\n1\n1\n0\n1\n\n\n4\n1\n13\n11\n12\n\n\n5\n1\n12\n6\n10\n\n\n6\n1\n2\n1\n5\n\n\n\n\n\n\nbeta &lt;- solve(t(X) %*% X) %*% t(X) %*% y\nprint(beta)\n\n              [,1]\n(Intercept) 91.361\ndepression  -1.484\nanxiety      1.049\nstress      -0.973\n\n\nVerifichiamo i calcoli eseguiti usando la funzione lm():\n\nfm &lt;- lm(scs_ts ~ depression + anxiety + stress, data = d_mr)\nprint(coef(fm))\n\n(Intercept)  depression     anxiety      stress \n     91.361      -1.484       1.049      -0.973 \n\n\nI valori predetti sono calcolati come:\n\nyhat &lt;- X %*% beta\ncor(yhat, fm$fitted.values) |&gt; print()\n\n     [,1]\n[1,]    1\n\n\nI residui si ottengono nel modo seguente:\n\ne &lt;- d_mr$scs_ts - yhat\ncor(e, fm$res) |&gt; print()\n\n     [,1]\n[1,]    1\n\n\nLa somma dei quadrati dei residui (Residual Sum of Squares, RSS) è definita nel modo seguente:\n\nRSS &lt;- t(e) %*% e\nprint(RSS)\n\n       [,1]\n[1,] 128700\n\n\nLa stima della varianza dei residui è data da:\n\nvar_e &lt;- RSS / (length(y) - dim(X)[2])\nprint(var_e)\n\n     [,1]\n[1,]  247\n\n\ndove al denominatore abbiamo i gradi di libertà.\nL’errore standard della regressione è dunque dato da:\n\nsqrt(var_e) |&gt; print()\n\n     [,1]\n[1,] 15.7\n\n\nVerifichiamo:\n\nsummary(fm)\n\n\nCall:\nlm(formula = scs_ts ~ depression + anxiety + stress, data = d_mr)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-36.80 -12.00  -0.35  10.74  43.67 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   91.361      1.623   56.29  &lt; 2e-16 ***\ndepression    -1.484      0.238   -6.25  8.7e-10 ***\nanxiety        1.049      0.210    4.99  8.2e-07 ***\nstress        -0.973      0.255   -3.81  0.00015 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.7 on 522 degrees of freedom\nMultiple R-squared:  0.247, Adjusted R-squared:  0.243 \nF-statistic: 57.1 on 3 and 522 DF,  p-value: &lt;2e-16\n\n\nInfine, il coefficiente di determinazione nel modello di regressione multipla ha la stessa definizione di quello incontrato nella regressione bivariata:\n\nR2 &lt;- (sum((yhat - mean(y))^2)) / (sum((y - mean(y))^2)) \nR2\n\n0.246961854370469\n\n\n\n\n45.1.3 Modello di Percorso\nFormuliamo ora il modello di regressione multipla nei termini di un modello SEM.\n\nmod_mr &lt;- \"\n  scs_ts ~ anxiety + depression + stress\n\"\n\nAdattiamo il modello ai dati usando lavaan:\n\nfit_mr &lt;- lavaan::sem(mod_mr, d_mr)\n\nEsaminiamo i parametri ottenuti in questo modo:\n\nparameterEstimates(fit_mr) |&gt; print()\n\n          lhs op        rhs     est     se     z pvalue ci.lower ci.upper\n1      scs_ts  ~    anxiety   1.049  0.209  5.01      0    0.639    1.460\n2      scs_ts  ~ depression  -1.484  0.237 -6.27      0   -1.948   -1.020\n3      scs_ts  ~     stress  -0.973  0.254 -3.83      0   -1.472   -0.475\n4      scs_ts ~~     scs_ts 244.677 15.087 16.22      0  215.106  274.247\n5     anxiety ~~    anxiety  32.082  0.000    NA     NA   32.082   32.082\n6     anxiety ~~ depression  24.546  0.000    NA     NA   24.546   24.546\n7     anxiety ~~     stress  24.538  0.000    NA     NA   24.538   24.538\n8  depression ~~ depression  31.418  0.000    NA     NA   31.418   31.418\n9  depression ~~     stress  25.662  0.000    NA     NA   25.662   25.662\n10     stress ~~     stress  29.714  0.000    NA     NA   29.714   29.714\n\n\nSi noti che i parametri stimati da lavaan sono praticamente identici a quelli trovati con il metodo della massima verosimiglianza.\n\n\n45.1.4 Path Analysis e Scomposizione della Covarianza\nL’obiettivo principale dei modelli SEM (Structural Equation Modeling) è quello di identificare coefficienti di percorso che permettano di ricostruire le covarianze osservate nel modello. Questo viene fatto attraverso la somma degli effetti diretti e indiretti, come specificato dal modello. Per esemplificare, consideriamo la covarianza tra il punteggio totale di self-compassion e il livello di ansia, come indicato dal DASS-21.\nIn questo contesto, l’effetto diretto si riferisce alla relazione diretta tra ansia e auto-compassione. Tuttavia, ci sono anche effetti indiretti che contribuiscono a questa relazione. Uno di questi effetti indiretti proviene dalla covarianza tra ansia e depressione, combinata con l’influenza della depressione sull’auto-compassione. Un altro effetto indiretto deriva dalla covarianza tra ansia e stress, combinata con l’effetto dello stress sull’auto-compassione.\nIn sostanza, nel modello SEM, la covarianza totale tra self-compassion e ansia è quindi una funzione: 1. Dell’effetto diretto dell’ansia sull’auto-compassione. 2. Dell’effetto combinato di ansia e depressione sull’auto-compassione. 3. Dell’effetto combinato di ansia e stress sull’auto-compassione.\nQuesti effetti vengono calcolati e sommati per fornire una stima complessiva della covarianza tra i due costrutti, offrendo una visione più olistica e dettagliata delle dinamiche psicologiche in gioco.\n\nsemPaths(fit_mr,\n    whatLabels = \"est\",\n    sizeMan = 10,\n    edge.label.cex = 1.15,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\n\n# Coefficients from the model\nbeta_anxiety_scs_ts &lt;- 1.0493140 # Coefficient for anxiety predicting scs_ts\nbeta_depression_scs_ts &lt;- -1.4841573 # Coefficient for depression predicting scs_ts\nbeta_stress_scs_ts &lt;- -0.9733368 # Coefficient for stress predicting scs_ts\n\n# Covariances from the model\ncov_anxiety_depression &lt;- 24.5464225\ncov_anxiety_stress &lt;- 24.5381096\ncov_depression_stress &lt;- 25.6615608\n\n# Assuming the variances of anxiety, depression, and stress\nvar_anxiety &lt;- 32.0817418\nvar_depression &lt;- 31.4182365\nvar_stress &lt;- 29.7137880\n\n# Predicted covariance between anxiety and scs_ts\npredicted_cov_anxiety_scs_ts &lt;- \n    beta_anxiety_scs_ts * var_anxiety +\n    beta_depression_scs_ts * cov_anxiety_depression +\n    beta_stress_scs_ts * cov_anxiety_stress\n\n# Output the predicted covariance\nprint(predicted_cov_anxiety_scs_ts)\n\n[1] -26.7\n\n\nVerifichiamo:\n\ncov(d_mr$anxiety, d_mr$scs_ts)\n\n-26.7015390186493\n\n\nLo stesso procedimento si usa per le altre componenti della matrice di varianza/covarianza dei dati.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#errore-di-specificazione",
    "href": "chapters/sem/01_sem_intro.html#errore-di-specificazione",
    "title": "45  Introduzione ai Modelli SEM",
    "section": "45.2 Errore di Specificazione",
    "text": "45.2 Errore di Specificazione\nSpiritosamente chiamato “heartbreak of L.O.V.E.” [Left-Out Variable Error; {cite:t}mauro1990understanding], l’errore di specificazione è una caratteristica fondamentale dei modelli di regressione che deve sempre essere tenuta a mente quando interpretiamo i risultati di questa tecnica di analisi statistica.\nL’errore di specificazione si verifica quando escludiamo dal modello di regressione una variabile che ha due caratteristiche:\n\nè associata con altre variabili inserite nel modello,\nha un effetto diretto sulla \\(y\\).\n\nCome conseguenza dell’errore di specificazione, l’intensità e il segno dei coefficienti parziali di regressione risultano sistematicamente distorti.\nConsideriamo un esempio con dati simulati nei quali immaginiamo che la prestazione sia positivamente associata alla motivazione e negativamente associata all’ansia. Immaginiamo inoltre che vi sia una correlazione positiva tra ansia a motivazione. Ci chiediamo cosa succede al coefficiente parziale della variabile “motivazione” se la variabile “ansia” viene esclusa dal modello di regressione.\n\nset.seed(123)\nn &lt;- 400\nanxiety &lt;- rnorm(n, 10, 1.5)\nmotivation &lt;- 4.0 * anxiety + rnorm(n, 0, 3.5)\ncor(anxiety, motivation)\n\n0.861770572096147\n\n\nCreiamo la variabile performance come una combinazione lineare di motivazione e ansia nella quale la motivazione ha un effetto piccolo, ma positivo, sulla prestazione, e l’ansia ha un grande effetto negativo sulla prestazione:\n\nperformance &lt;-  0.5 * motivation - 5.0 * anxiety + rnorm(n, 0, 3)\n\nSalviamo i dati in un data frame:\n\nsim_dat2 &lt;- tibble(performance, motivation, anxiety)\nsim_dat2 |&gt;\n    head()\n\n\nA tibble: 6 x 3\n\n\nperformance\nmotivation\nanxiety\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n-26.5\n36.4\n9.16\n\n\n-33.0\n34.5\n9.65\n\n\n-35.6\n47.1\n12.34\n\n\n-26.9\n40.3\n10.11\n\n\n-28.6\n43.1\n10.19\n\n\n-40.2\n44.5\n12.57\n\n\n\n\n\nEseguiamo l’analisi di regressione specificando il modello in maniera corretta, ovvero usando come predittori l’ansia e la depressione:\n\nfm1 &lt;- lm(performance ~ motivation + anxiety, sim_dat2)\n\nLe stime dei coefficienti parziali di regressione recuperano correttamente l’intensità e il segno dei coefficienti utilizzati nel modello generatore dei dati:\n\nprint(coef(fm1))\n\n(Intercept)  motivation     anxiety \n      1.371       0.495      -5.105 \n\n\nEseguiamo ora l’analisi di regressione ignorando il predittore anxiety che ha le due caratteristiche di essere associato a motivation e di avere un effetto diretto sulla prestazione:\n\nfm2 &lt;- lm(performance ~ motivation, sim_dat2)\nsummary(fm2) \n\n\nCall:\nlm(formula = performance ~ motivation, data = sim_dat2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.501  -3.409   0.005   3.311  12.616 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -12.3972     1.4459   -8.57  2.2e-16 ***\nmotivation   -0.4372     0.0355  -12.31  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.87 on 398 degrees of freedom\nMultiple R-squared:  0.276, Adjusted R-squared:  0.274 \nF-statistic:  151 on 1 and 398 DF,  p-value: &lt;2e-16\n\n\nSi noti che il risultato prodotto dal modello di regressione è totalmente sbagliato: come conseguenza dell’errore di specificazione, il segno del coefficiente parziale di regressione della variabile “motivazione” è negativo, anche se nel modello generatore dei dati tale coefficiente aveva il segno opposto.\nQuindi, se interpretassimo il coefficiente parziale ottenuto in termini casuali, saremmo portati a concludere che la motivazione fa diminuire la prestazione. Ma in realtà è vero l’opposto.\nÈ facile vedere perché si verifica l’errore di specificazione. Supponiamo che il vero modello sia\n\\[\ny = \\alpha + \\beta_1 X_1 + \\beta_2 X_2 + \\varepsilon\n\\]\nil quale verrebbe stimato da\n\\[\ny = a + b_1 X_1 + b_2 X_2 + e.\n\\]\nSupponiamo però che il ricercatore creda invece che\n\\[\ny = \\alpha^\\prime + \\beta_1^\\prime X_1 + \\varepsilon^\\prime\n\\]\ne quindi stimi\n\\[\ny = a^\\prime + b_1^\\prime X_1 + e^\\prime\n\\]\nomettendo \\(X_2\\) dal modello.\nPer capire che relazione intercorre tra \\(b_1^\\prime\\) e \\(b_1\\), iniziamo a scrivere la formula per \\(b_1^\\prime\\):\n\\[\n\\begin{equation}\nb_1^\\prime = \\frac{Cov(X_1, Y)}{Var(X_1)}.\n\\end{equation}\n\\]\nSviluppando, otteniamo\n\\[\n\\begin{equation}\n\\begin{aligned}\nb_1^\\prime &= \\frac{Cov(X_1, a + b_1 X_1 + b_2 X_2 + e)}{Var(X_1)}\\notag\\\\\n&= \\frac{Cov(X_1, a)+b_1 Cov(X_1, X_1) + b_2 Cov(X_1, X_2) + Cov(X_1, e)}{Var(X_1)}\\notag\\\\\n&= \\frac{0 + b_1 Var(X_1) + b_2 Cov(X_1, X_2) + 0}{Var(X_1)}\\notag\\\\\n&= b_1 + b_2 \\frac{Cov(X_1, X_2)}{Var(X_1)}.\n\\end{aligned}\n\\end{equation}\n\\]\nQuindi, se erroneamente omettiamo \\(X_2\\) dal modello, abbiamo che\n\\[\n\\begin{equation}\n\\mathbb{E}(b_1^\\prime) = \\beta_1 + \\beta_2 \\frac{\\sigma_{12}}{\\sigma_1^2}.\n\\end{equation}\n\\](eq-specific-err)\nVerifichiamo tale conclusione per i dati dell’esempio che stiamo discutendo. Nel caso presente, \\(X_1\\) è motivation e \\(X_2\\) è anxiety. Applicando l’eq. {eq}eq-specific-err otteniamo lo stesso valore per il coefficiente di regressione associato a motivation che era stato ottenuto adattando ai dati il modello performance ~ motivation:\n\nfm1$coef[2] +  fm1$coef[3] * \n  cov(sim_dat2$motivation, sim_dat2$anxiety) / \n  var(sim_dat2$motivation)\n\nmotivation: -0.437167456259783\n\n\nPossiamo dunque concludere che \\(b_1^\\prime\\) è uno stimatore distorto di \\(\\beta_1\\). Si noti che questa distorsione non scompare all’aumentare della numerosità campionaria, il che (in termini statistici) significa che un tale stimatore è inconsistente. Quello che succede in pratica è che alla variabile \\(X_1\\) vengono attribuiti gli effetti delle variabili che sono state omesse dal modello. Si noti che una tale distorsione sistematica di \\(b_1^\\prime\\) può essere evitata solo se si verificano due condizioni:\n\n\\(\\beta_2 = 0\\). Questo è ovvio, dato che, se \\(\\beta_2 = 0\\), ciò significa che il modello non è specificato in modo errato, cioè \\(X_2\\) non appartiene al modello perché non ha un effetto diretto sulla \\(Y\\).\n\\(\\sigma_{12} = 0\\). Cioè, se \\(X_1\\) e \\(X_2\\) sono incorrelate, allora l’omissione di una delle due variabili non comporta stime distorte dell’effetto dell’altra.\n\n\n45.2.1 Soppressione\nLe conseguenze dell’errore di specificazione sono chiamate “soppressione” (suppression). In generale, si ha soppressione quando (1) il valore assoluto del peso beta di un predittore è maggiore di quello della sua correlazione bivariata con il criterio o (2) i due hanno segni opposti.\n\nL’esempio descritto sopra è un caso di soppressione negativa, dove il predittore ha correlazioni bivariate positive con il criterio, ma si riceve un peso beta negativo nell’analisi di regressione multipla.\nUn secondo tipo di soppressione è la soppressione classica, in cui un predittore non è correlato al criterio ma riceve un peso beta diverso da zero.\nC’è anche la soppressione reciproca che può verificarsi quando due variabili sono correlate positivamente con il criterio ma negativamente tra loro.\n\n\n\n45.2.2 Regressione Stepwise\nNel contesto della regressione, è importante comprendere che i predittori non dovrebbero essere selezionati basandosi unicamente sulle loro correlazioni bivariate con la variabile dipendente (il criterio). Queste correlazioni, note come associazioni di “ordine zero”, non tengono conto dell’influenza degli altri predittori. Di conseguenza, i valori delle correlazioni bivariate possono risultare fuorvianti quando si considerano i coefficienti di regressione parziale per le stesse variabili.\nLa significatività statistica delle correlazioni bivariate con la variabile dipendente non è un criterio affidabile per la selezione dei predittori. Questo perché tali correlazioni non considerano gli effetti complessivi degli altri predittori nel modello.\nLe procedure di selezione automatica dei predittori, come quelle impiegate nelle regressioni stepwise, possono essere seducenti per la loro facilità d’uso. Tuttavia, queste procedure sono rischiose. Anche piccole non-linearità o effetti indiretti tra i predittori, che potrebbero non essere immediatamente evidenti, possono distorcere in modo significativo i coefficienti di regressione parziale.\nInvece di affidarsi a metodi automatici, è preferibile selezionare un numero limitato di predittori basandosi su considerazioni teoriche o sui risultati di ricerche precedenti. Questo approccio più ponderato aiuta a evitare le distorsioni che possono emergere dall’uso di procedure di selezione automatica.\nUna volta che sono stati selezionati, i predittori possono essere inseriti nell’equazione di regressione in due modi diversi:\n\ntutti i predittori possono essere inseriti nel modello contemporaneamente;\ni predittori possono essere inseriti nel modello sequenzialmente, mediante una serie di passaggi.\n\nL’ordine di ingresso può essere determinato in base a standard: teorici (razionali) o empirici (statistici). Lo standard razionale corrisponde alla regressione gerarchica, in cui si comunica al computer un ordine fisso per inserire i predittori. Ad esempio, a volte le variabili demografiche vengono inserite nel primo passaggio, quindi nel secondo passaggio viene inserita una variabile psicologica di interesse. Questo ordine non solo controlla le variabili demografiche ma permette anche di valutare il potere predittivo della variabile psicologica, al di là di quello delle semplici variabili demografiche. Quest’ultimo può essere stimato come l’aumento della correlazione multipla al quadrato, o \\(\\Delta R^2\\), da quella della fase 1 con solo predittori demografici a quella della fase 2 con tutti i predittori nell’equazione di regressione.\nUn esempio di standard statistico è la regressione stepwise, in cui il computer seleziona l’inserimento dei predittori in base esclusivamente alla significatività statistica; cioè, viene chiesto: quale predittore, se inserito nell’equazione, avrebbe il valore_\\(p\\) più piccolo per il test del suo coefficiente di regressione parziale? Dopo la selezione, i predittori in una fase successiva possono essere rimossi dall’equazione di regressione in base ai loro valori-\\(p\\) (ad esempio, se \\(p \\geq\\) .05). Il processo stepwise si interrompe quando, aggiungendo più predittori, \\(\\Delta R^2\\) non migliora. Varianti della regressione stepwise includono forward inclusion, in cui i predittori selezionati non vengono successivamente rimossi dal modello, e backward elimination, che inizia con tutti i predittori nel modello per poi rimuoverne alcuni in passi successivi. Per le ragioni descritte nel paragrafo sull’errore di specificazione, i metodi basati sulle procedure di stepwise regression non dovrebbero mai essere usati. Infatti, i problemi relativi a tale procedura sono così gravi che varie riviste non accettano studi che fanno uso di una tale tecnica statistica. I risultati ottenuti con tali metodi, infatti, sono quasi certamente non replicabili in campioni diversi.\nUna considerazione finale riguarda l’idea di rimuovere i predittori “non significativi” dal modello di regressione. Questa è una cattiva idea. Il ricercatore non deve sentirsi in dovere di trascurare quei predittore che non risultano “statisticamente significativi”. In campioni piccoli, la potenza dei test di significatività è bassa e la rimozione di un predittore non significativo può alterare sostanzialmente la soluzione. Se c’è una buona ragione per includere un predittore, allora è meglio lasciarlo nel modello, fino a prova contraria. In termini generali, qualsiasi considerazione basata sulla “significatività statistia” è fuorviante.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#oltre-la-regressione-multipla-luso-dei-modelli-sem",
    "href": "chapters/sem/01_sem_intro.html#oltre-la-regressione-multipla-luso-dei-modelli-sem",
    "title": "45  Introduzione ai Modelli SEM",
    "section": "45.3 Oltre la Regressione Multipla: L’Uso dei Modelli SEM",
    "text": "45.3 Oltre la Regressione Multipla: L’Uso dei Modelli SEM\nAnalizziamo un esempio in cui il modello di Equazioni Strutturali (SEM) viene impiegato per studiare la relazione tra autocompassione e malessere psicologico, utilizzando come indicatori le sotto-scale della DASS-21 (Depressione, Ansia, Stress) e della Self-Compassion Scale. In questo contesto, definiamo due variabili latenti: “malessere psicologico” e “autocompassione”. La variabile latente “malessere psicologico” è composta dalle tre sotto-scale della DASS-21, mentre la variabile “autocompassione” è formata dalle sei sotto-scale della Self-Compassion Scale.\nIl modello strutturale esplora la relazione tra queste due variabili latenti. L’autocompassione è considerata una variabile esogena, ipotizzata come un fattore di protezione che riduce il malessere psicologico, che a sua volta è trattato come variabile endogena. L’ipotesi principale del modello è che esista una relazione di regressione negativa tra autocompassione e malessere psicologico, indicando che livelli più elevati di autocompassione sono associati a minori livelli di malessere psicologico.\nUn elemento chiave dei modelli SEM è la gestione dell’errore di misurazione. Le variabili latenti sono progettate per riflettere il nucleo vero dei costrutti teorici, in questo caso autocompassione e malessere psicologico, isolando gli effetti degli errori di misurazione che possono affliggere gli indicatori osservati. Questo approccio consente di esaminare la “vera” relazione tra i costrutti, eliminando le distorsioni introdotte dagli errori di misurazione nelle misure osservate.\nLa capacità del modello SEM di separare la variabilità attribuibile ai costrutti latenti da quella dovuta agli errori di misurazione aumenta l’accuratezza e l’affidabilità dell’analisi. Questo è particolarmente vantaggioso in campi come la psicologia, dove i costrutti teorici non sono direttamente osservabili e devono essere inferiti attraverso misure potenzialmente errate.\n\nmod_sc &lt;- \"\n  F =~ anxiety + depression + stress\n  SC =~ self_kindness   + common_humanity   + mindfulness   + self_judgment + isolation + over_identification\n  F ~ SC \n\"\n\nAdattiamo il modello ai dati.\n\nfit_sc &lt;- lavaan::sem(mod_sc, dat, std.lv = TRUE)\n\nEsaminiamo la soluzione ottenuta.\n\nstandardizedSolution(fit_sc) |&gt; print()\n\n                   lhs op                 rhs est.std    se      z pvalue\n1                    F =~             anxiety   0.847 0.014  58.51      0\n2                    F =~          depression   0.909 0.011  82.50      0\n3                    F =~              stress   0.929 0.010  91.94      0\n4                   SC =~       self_kindness   0.757 0.022  33.98      0\n5                   SC =~     common_humanity   0.621 0.030  20.70      0\n6                   SC =~         mindfulness   0.689 0.026  26.23      0\n7                   SC =~       self_judgment  -0.770 0.022 -35.80      0\n8                   SC =~           isolation  -0.770 0.022 -35.82      0\n9                   SC =~ over_identification  -0.767 0.022 -35.42      0\n10                   F  ~                  SC  -0.476 0.038 -12.38      0\n11             anxiety ~~             anxiety   0.282 0.025  11.49      0\n12          depression ~~          depression   0.173 0.020   8.63      0\n13              stress ~~              stress   0.136 0.019   7.25      0\n14       self_kindness ~~       self_kindness   0.427 0.034  12.65      0\n15     common_humanity ~~     common_humanity   0.615 0.037  16.53      0\n16         mindfulness ~~         mindfulness   0.525 0.036  14.52      0\n17       self_judgment ~~       self_judgment   0.407 0.033  12.29      0\n18           isolation ~~           isolation   0.407 0.033  12.28      0\n19 over_identification ~~ over_identification   0.411 0.033  12.36      0\n20                   F ~~                   F   0.774 0.037  21.17      0\n21                  SC ~~                  SC   1.000 0.000     NA     NA\n   ci.lower ci.upper\n1     0.819    0.876\n2     0.888    0.931\n3     0.910    0.949\n4     0.713    0.801\n5     0.562    0.679\n6     0.637    0.740\n7    -0.812   -0.728\n8    -0.812   -0.728\n9    -0.810   -0.725\n10   -0.551   -0.400\n11    0.234    0.330\n12    0.134    0.212\n13    0.099    0.173\n14    0.361    0.493\n15    0.542    0.688\n16    0.454    0.596\n17    0.342    0.472\n18    0.342    0.472\n19    0.346    0.476\n20    0.702    0.845\n21    1.000    1.000\n\n\n\nSaturazioni Fattoriali (Loadings) per le Variabili Latenti:\n\nF: Le variabili osservate “anxiety”, “depression”, e “stress” hanno elevate saturazioni fattoriali sulla variabile latente “F”. Questo suggerisce che ciascuna di queste misure è un buon indicatore della variabile latente “F”.\nSC: Le variabili “self_kindness”, “common_humanity”, “mindfulness”, “self_judgment”, “isolation”, e “over_identification” hanno anch’esse significative saturazioni sulla variabile latente “SC”. Si noti che “self_judgment”, “isolation”, e “over_identification” hanno saturazioni negative, indicando che queste variabili sono inversamente associate con “SC”.\n\nRegressione tra Variabili Latenti:\n\nLa relazione di regressione tra “F” e “SC” mostra un coefficiente negativo (-0.48), il che indica una relazione inversa tra queste due variabili latenti. Questo significa che livelli più alti di “SC” sono associati a livelli più bassi di “F”.\n\nVarianza delle Variabili Latenti:\n\nLa varianza di “F” e “SC” indica quanto della variazione nelle variabili latenti è spiegata dai loro rispettivi indicatori. La varianza di “F” (0.77) è relativamente alta, suggerendo che gli indicatori spiegano una buona parte della varianza in “F”. La varianza di “SC” è fissata a 1, un approccio comune per identificare il modello.\n\nVarianze Residue degli Indicatori:\n\nLe varianze residue (ad esempio, “anxiety ~~ anxiety”) rappresentano la varianza non spiegata in ciascun indicatore dalle variabili latenti. Valori più bassi indicano che la variabile latente spiega una maggior parte della varianza dell’indicatore. Ad esempio, “anxiety” ha una varianza residua di 0.28, suggerendo che “F” spiega una buona parte, ma non tutta, della varianza in “anxiety”.\n\n\nGeneriamo una rappresentazione grafica del modello.\n\nsemPaths(fit_sc,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 0.9,\n    style = \"mx\",\n    nCharNodes = 5, nCharEdges = 0, \n    fade=FALSE\n)\n\n\n\n\n\n\n\n\nI coefficienti stimati nel modello SEM appaiono coerenti e in linea con le aspettative, in particolare il coefficiente che descrive l’effetto “causale” del fattore dell’autocompassione sul malessere psicologico, che si attesta a -0.48. Questo valore negativo corrobora l’ipotesi secondo cui l’autocompassione svolge un ruolo di fattore protettivo contro il malessere psicologico. Tuttavia, prima di confermare definitivamente questa conclusione, è cruciale esaminare gli indici di bontà di adattamento del modello. Questi indici ci permetteranno di valutare quanto accuratamente il modello SEM si adatta ai dati osservati, fornendo un quadro più chiaro della validità delle nostre inferenze. In altre parole, sebbene il modello suggerisca una relazione negativa tra autocompassione e malessere psicologico, la conferma finale di questa associazione dipenderà dall’adeguatezza complessiva del modello rispetto ai dati. Questo argomento verrà affrontato nel prossimo capitolo.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#impiego-delle-medie-nei-modelli-sem",
    "href": "chapters/sem/01_sem_intro.html#impiego-delle-medie-nei-modelli-sem",
    "title": "45  Introduzione ai Modelli SEM",
    "section": "45.4 Impiego delle Medie nei Modelli SEM",
    "text": "45.4 Impiego delle Medie nei Modelli SEM\nNei modelli di equazioni strutturali (SEM), come nell’analisi fattoriale, l’accento è posto sull’analisi delle covarianze tra variabili. Tuttavia, a differenza dell’analisi fattoriale, i modelli SEM consentono di includere anche le medie delle variabili osservate e latenti. Questo arricchisce l’analisi, fornendo informazioni preziose in molti contesti, come nei modelli CFA longitudinali, dove le ipotesi centrali riguardano proprio le medie dei costrutti.\n\n45.4.1 Struttura delle Medie nel Modello SEM\nL’equazione generale per la struttura delle medie in un modello SEM è la seguente:\n\\[\nE(y) = \\mu_y = T + \\Lambda A\n\\]\ndove:\n\n\\(y\\) indica i punteggi degli indicatori.\n\\(E(y)\\) rappresenta la media attesa di \\(y\\).\n\\(\\mu_y\\) è il vettore delle medie dei modelli degli indicatori, analogo a \\(\\Sigma\\) nelle strutture di covarianza.\n\\(\\Lambda\\) è la matrice dei carichi fattoriali, che stima le relazioni degli indicatori con i costrutti.\n\\(T\\) è il vettore delle medie degli indicatori.\n\\(A\\) è un vettore delle medie dei costrutti latenti.\n\nIn un diagramma a percorsi, un triangolo contrassegnato con il numero 1 rappresenta la costante di regressione. Questo simbolo indica l’intercetta, che viene utilizzata per stimare la media quando una variabile viene regredita su di essa.\nLa media di ciascun indicatore è stimata nel vettore \\(\\tau\\), mentre \\(\\Lambda\\) rappresenta la matrice di saturazioni fattoriali del modello di misurazione CFA. Questo collegamento tra i carichi e le medie indica che gli indicatori con carichi più elevati hanno un impatto maggiore sulla media del costrutto.\nIl simbolo \\(\\alpha\\) rappresenta le medie stimate dei costrutti latenti. Questa simbologia aiuta a distinguere tra le medie degli indicatori (\\(\\tau\\)) e quelle dei costrutti latenti (\\(\\alpha\\)) nelle equazioni.\n\n\n45.4.2 Vincoli e Scalatura delle Medie\nSimilmente alla gestione delle strutture di covarianza nei modelli SEM, anche per le strutture delle medie è necessario impostare un vincolo per definire la scala. Per le medie, lo zero viene spesso adottato come riferimento. Stabilire questo vincolo aiuta a calcolare le distanze dai punti fissati, che possono essere sia positive sia negative.\n\n\n45.4.3 Stima delle Intercette con lavaan\nPer stimare le intercette in un modello SEM, è essenziale avere accesso ai dati originali o a una matrice di covarianza, oltre alle medie di tutte le variabili coinvolte. L’utilizzo del software lavaan facilita questo processo. Impostando meanstructure = true, si indica a lavaan di integrare automaticamente una costante “1” in tutte le equazioni del modello, facilitando il calcolo delle intercette per le variabili endogene. Questo permette di calcolare con precisione le intercette, che sono cruciali per il modello delle strutture delle medie.\nIn conclusione, la struttura delle medie in un modello SEM è essenziale per ottenere stime accurate delle medie delle variabili, permettendo di confrontare queste stime con le medie osservate nei dati raccolti, proprio come si confrontano le covarianze nel modello con quelle osservate nei dati. Questo approccio arricchisce significativamente l’analisi fornita dai modelli SEM.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#considerazioni-conclusive",
    "href": "chapters/sem/01_sem_intro.html#considerazioni-conclusive",
    "title": "45  Introduzione ai Modelli SEM",
    "section": "45.5 Considerazioni Conclusive",
    "text": "45.5 Considerazioni Conclusive\nIn questo capitolo, abbiamo esplorato i Modelli di Equazioni Strutturali (SEM), evidenziando come questi modelli non si limitino a descrivere le correlazioni tra variabili osservabili, ma permettano anche di analizzare le relazioni tra variabili latenti. La forza dei SEM risiede nella loro capacità di integrare il modello di misurazione, che definisce le relazioni tra gli indicatori e le variabili latenti, con il modello strutturale, che esamina le interazioni tra le stesse variabili latenti.\nNei prossimi capitoli, approfondiremo vari aspetti della modellazione SEM. Esamineremo la bontà di adattamento del modello, un criterio fondamentale per verificare la fedeltà con cui il modello riflette la realtà osservata. Analizzeremo anche il confronto tra modelli alternativi, un passaggio cruciale per identificare il modello che migliora l’interpretazione dei dati.\nUn altro tema importante sarà l’analisi dell’applicabilità dei modelli a gruppi diversi, vitale per valutare la loro generalizzabilità e la pertinenza in contesti specifici. Inoltre, discuteremo le sfide metodologiche legate alla gestione di dati categoriali, all’implementazione di modelli SEM multilivello e alla gestione di dati mancanti. Questi approfondimenti ci permetteranno di comprendere meglio come i modelli SEM possono essere adattati e applicati efficacemente in diversi ambiti di ricerca.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/01_sem_intro.html#session-info",
    "href": "chapters/sem/01_sem_intro.html#session-info",
    "title": "45  Introduzione ai Modelli SEM",
    "section": "45.6 Session Info",
    "text": "45.6 Session Info\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] effectsize_0.8.9   rsvg_2.6.1         DiagrammeRsvg_0.1 \n [4] mvnormalTest_1.0.0 lavaanExtra_0.2.1  MASS_7.3-61       \n [7] viridis_0.6.5      viridisLite_0.4.2  ggpubr_0.6.0      \n[10] ggExtra_0.10.1     gridExtra_2.3      patchwork_1.3.0   \n[13] bayesplot_1.11.1   semTools_0.5-6     semPlot_1.1.6     \n[16] lavaan_0.6-19      psych_2.4.6.26     scales_1.3.0      \n[19] markdown_1.13      knitr_1.49         lubridate_1.9.3   \n[22] forcats_1.0.0      stringr_1.5.1      dplyr_1.1.4       \n[25] purrr_1.0.2        readr_2.1.5        tidyr_1.3.1       \n[28] tibble_3.2.1       ggplot2_3.5.1      tidyverse_2.0.0   \n[31] here_1.0.1        \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.2       later_1.3.2         pbdZMQ_0.3-13      \n  [4] datawizard_0.13.0   XML_3.99-0.17       rpart_4.1.23       \n  [7] lifecycle_1.0.4     rstatix_0.7.2       rprojroot_2.0.4    \n [10] lattice_0.22-6      insight_0.20.5      rockchalk_1.8.157  \n [13] backports_1.5.0     magrittr_2.0.3      openxlsx_4.2.7.1   \n [16] Hmisc_5.2-0         rmarkdown_2.29      httpuv_1.6.15      \n [19] qgraph_1.9.8        zip_2.3.1           pbapply_1.7-2      \n [22] minqa_1.2.8         ADGofTest_0.3       multcomp_1.4-26    \n [25] abind_1.4-8         quadprog_1.5-8      pspline_1.0-20     \n [28] nnet_7.3-19         TH.data_1.1-2       sandwich_3.1-1     \n [31] moments_0.14.1      nortest_1.0-4       openintro_2.5.0    \n [34] arm_1.14-4          airports_0.1.0      codetools_0.2-20   \n [37] tidyselect_1.2.1    farver_2.1.2        lme4_1.1-35.5      \n [40] stats4_4.4.2        base64enc_0.1-3     jsonlite_1.8.9     \n [43] Formula_1.2-5       survival_3.7-0      emmeans_1.10.5     \n [46] tools_4.4.2         Rcpp_1.0.13-1       glue_1.8.0         \n [49] mnormt_2.1.1        xfun_0.49           IRdisplay_1.1      \n [52] withr_3.0.2         numDeriv_2016.8-1.1 fastmap_1.2.0      \n [55] boot_1.3-31         fansi_1.0.6         digest_0.6.37      \n [58] mi_1.1              timechange_0.3.0    R6_2.5.1           \n [61] mime_0.12           estimability_1.5.1  colorspace_2.1-1   \n [64] Cairo_1.6-2         gtools_3.9.5        jpeg_0.1-10        \n [67] copula_1.1-4        utf8_1.2.4          generics_0.1.3     \n [70] data.table_1.16.2   corpcor_1.6.10      usdata_0.3.1       \n [73] htmlwidgets_1.6.4   parameters_0.23.0   pkgconfig_2.0.3    \n [76] sem_3.1-16          gtable_0.3.6        pcaPP_2.0-5        \n [79] htmltools_0.5.8.1   carData_3.0-5       png_0.1-8          \n [82] rstudioapi_0.17.1   tzdb_0.4.0          reshape2_1.4.4     \n [85] uuid_1.2-1          coda_0.19-4.1       checkmate_2.3.2    \n [88] nlme_3.1-166        curl_6.0.0          nloptr_2.1.1       \n [91] repr_1.1.7          zoo_1.8-12          parallel_4.4.2     \n [94] miniUI_0.1.1.1      foreign_0.8-87      pillar_1.9.0       \n [97] grid_4.4.2          vctrs_0.6.5         promises_1.3.0     \n[100] car_3.1-3           OpenMx_2.21.13      xtable_1.8-4       \n[103] cluster_2.1.6       htmlTable_2.4.3     evaluate_1.0.1     \n[106] pbivnorm_0.6.0      mvtnorm_1.3-2       cli_3.6.3          \n[109] kutils_1.73         compiler_4.4.2      rlang_1.1.4        \n[112] crayon_1.5.3        ggsignif_0.6.4      fdrtool_1.2.18     \n[115] plyr_1.8.9          stringi_1.8.4       munsell_0.5.1      \n[118] gsl_2.1-8           lisrelToR_0.3       bayestestR_0.15.0  \n[121] pacman_0.5.1        V8_6.0.0            Matrix_1.7-1       \n[124] IRkernel_1.3.2      hms_1.1.3           stabledist_0.7-2   \n[127] glasso_1.11         shiny_1.9.1         igraph_2.1.1       \n[130] broom_1.0.7         RcppParallel_5.1.9  cherryblossom_0.1.0",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Introduzione ai Modelli SEM</span>"
    ]
  },
  {
    "objectID": "chapters/sem/02_data_preparation.html",
    "href": "chapters/sem/02_data_preparation.html",
    "title": "46  Preparazione dei Dati",
    "section": "",
    "text": "46.1 Introduzione\nQuesto breve capitolo affronta diversi argomenti relativi alla gestione dei dati nell’ambito della modellazione a equazioni strutturali (SEM).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Preparazione dei Dati</span>"
    ]
  },
  {
    "objectID": "chapters/sem/02_data_preparation.html#formati-dei-dati-di-input",
    "href": "chapters/sem/02_data_preparation.html#formati-dei-dati-di-input",
    "title": "46  Preparazione dei Dati",
    "section": "46.2 Formati dei Dati di Input",
    "text": "46.2 Formati dei Dati di Input\nI ricercatori spesso analizzano file di dati grezzi. Tuttavia, alcune analisi SEM possono essere eseguite anche con matrici di covarianze e medie. Se si utilizzano dati grezzi, il software SEM crea una propria matrice di covarianza per l’analisi. Talvolta, è necessario usare dati grezzi, come in casi di distribuzioni non normali, dati mancanti o variabili categoriali.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Preparazione dei Dati</span>"
    ]
  },
  {
    "objectID": "chapters/sem/02_data_preparation.html#definitezza-positiva",
    "href": "chapters/sem/02_data_preparation.html#definitezza-positiva",
    "title": "46  Preparazione dei Dati",
    "section": "46.3 Definitezza Positiva",
    "text": "46.3 Definitezza Positiva\nÈ fondamentale che la matrice di dati, sia quella inizialmente fornita come input che quella calcolata dal computer durante l’analisi, soddisfi i criteri di essere positiva definita. Questo concetto implica diverse proprietà chiave: innanzitutto, la matrice deve avere un inverso, il che significa che non è singolare e può essere invertita matematicamente. Inoltre, è necessario che tutti gli autovalori della matrice siano positivi, indicando che non esistono autovalori negativi che potrebbero causare problemi durante l’analisi. Inoltre, la matrice deve essere priva di correlazioni o covarianze al di fuori limite.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Preparazione dei Dati</span>"
    ]
  },
  {
    "objectID": "chapters/sem/02_data_preparation.html#dati-mancanti",
    "href": "chapters/sem/02_data_preparation.html#dati-mancanti",
    "title": "46  Preparazione dei Dati",
    "section": "46.4 Dati Mancanti",
    "text": "46.4 Dati Mancanti\nQuesto è un argomento complesso che richiede l’uso di metodi statistici moderni e sarà approfondito in un capitolo successivo.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Preparazione dei Dati</span>"
    ]
  },
  {
    "objectID": "chapters/sem/02_data_preparation.html#screening-dei-dati",
    "href": "chapters/sem/02_data_preparation.html#screening-dei-dati",
    "title": "46  Preparazione dei Dati",
    "section": "46.5 Screening dei Dati",
    "text": "46.5 Screening dei Dati\n\nCollinearità Estrema, Valori Anomali e Violazioni delle Assunzioni Distribuzionali: È importante gestire questi problemi per assicurare l’affidabilità dei risultati SEM. La collinearità estrema può essere rilevata tramite il fattore di inflazione della varianza (VIF), mentre i valori anomali e le violazioni delle ipotesi distribuzionali richiedono metodi specifici per essere identificati e gestiti.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Preparazione dei Dati</span>"
    ]
  },
  {
    "objectID": "chapters/sem/02_data_preparation.html#varianze-relative",
    "href": "chapters/sem/02_data_preparation.html#varianze-relative",
    "title": "46  Preparazione dei Dati",
    "section": "46.6 Varianze Relative",
    "text": "46.6 Varianze Relative\n\nGestione delle Varianze: La differenza eccessiva tra le varianze può complicare l’iterazione dei metodi di stima in SEM. Per mitigare questo aspetto, i dati con varianze molto basse o alte possono essere riscalati.\n\nIn sintesi, prima di procedere a qualunque analisi statistica è necessario affrontare diversi problemi relativi alla corretta preparazione e gestione dei dati. Questi aspetti sono fondamentali per assicurare l’accuratezza e l’affidabilità dei risultati delle analisi SEM.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Preparazione dei Dati</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html",
    "href": "chapters/sem/03_gof.html",
    "title": "47  Test del Modello e Indicizzazione",
    "section": "",
    "text": "47.1 Introduzione\nIn questo capitolo, ci concentriamo sulle due principali categorie di statistiche per valutare l’adattamento globale nei modelli SEM: le statistiche di test del modello e gli indici di adattamento approssimativo. Queste due categorie si riferiscono rispettivamente al test di adattamento del modello e alla misurazione continua della sua bontà.\nUn aspetto critico da considerare è che, benché entrambe le categorie di statistiche valutino la corrispondenza media o generale tra modello e dati, possono non rilevare un cattivo adattamento locale. Questo si riferisce a specifiche coppie di variabili osservate per cui il modello potrebbe non spiegare adeguatamente le associazioni osservate. È fondamentale riconoscere che un modello con adattamento locale inadeguato non dovrebbe essere accettato, indipendentemente dalla sua bontà di adattamento globale.\nLa valutazione completa di un modello SEM segue una sequenza metodica: specificazione del modello, stima dei parametri, verifica dell’adattamento e dei parametri, e, se necessario, modifica del modello. Questo processo iterativo prosegue finché si identifica un modello ritenuto accettabile.\nInoltre, questo capitolo esplora due metodi fondamentali per pianificare la dimensione del campione nei modelli SEM: l’analisi della potenza e la stima della precisione dei parametri (precisione nella pianificazione). Questi approcci sono essenziali per assicurare che lo studio sia adeguatamente dimensionato e che i parametri siano stimati con massima precisione. La valutazione degli indici di bontà dell’adattamento, ampiamente utilizzati nella letteratura, rappresenterà un elemento chiave in questo contesto, fornendo una panoramica completa degli strumenti disponibili per giudicare l’efficacia dei modelli SEM.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#introduzione",
    "href": "chapters/sem/03_gof.html#introduzione",
    "title": "47  Test del Modello e Indicizzazione",
    "section": "",
    "text": "Statistiche di Test del Modello: Queste statistiche prevedono una decisione binaria, ossia stabilire se accettare o respingere le ipotesi nulle riguardanti il modello. La decisione si basa sui valori-p derivati dai test di significatività, con l’obiettivo di verificare se l’intero modello si adatti ai dati osservati.\nIndici di Adattamento Approssimativo: A differenza delle statistiche di test, gli indici di adattamento approssimativo forniscono una misura continua che esprime il grado di adattamento del modello ai dati. Questo approccio ricorda la stima dell’effetto quantitativo più che un test dicotomico, fornendo così una valutazione più dettagliata dell’adattamento e superando la semplice accettazione o rifiuto dell’ipotesi nulla.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#valutazione-della-bontà-di-adattamento-nel-modello-sem",
    "href": "chapters/sem/03_gof.html#valutazione-della-bontà-di-adattamento-nel-modello-sem",
    "title": "47  Test del Modello e Indicizzazione",
    "section": "47.2 Valutazione della Bontà di Adattamento nel Modello SEM",
    "text": "47.2 Valutazione della Bontà di Adattamento nel Modello SEM\nNel contesto dei modelli SEM (Structural Equation Modeling), la valutazione dell’adattamento del modello si basa sul confronto tra la matrice di varianze e covarianze stimata dal modello, \\(\\Sigma(\\hat{\\theta})\\), e la matrice di covarianza campionaria, \\(S\\). Il nostro obiettivo è verificare se la discrepanza tra queste due matrici indica possibili inadeguatezze nel modello proposto. Ecco alcuni aspetti rilevanti da considerare:\n\nModelli Saturi vs Modelli Ristretti: Un modello saturo include un numero di parametri in \\(\\theta\\) pari al numero di elementi distinti nella matrice di covarianza. In contrasto, un modello ristretto ha meno parametri rispetto al numero degli elementi distinti nella matrice di covarianza. La differenza tra questi due numeri corrisponde ai gradi di libertà del modello. Per esempio, in un modello saturo, se il numero dei parametri in \\(\\theta\\) e il numero degli elementi distinti nella matrice di covarianza sono entrambi 3, allora il modello ha zero gradi di libertà.\nPerfetto Adattamento dei Modelli Saturi: In un modello saturo, \\(\\Sigma(\\hat{\\theta})\\) coincide sempre con \\(S\\), poiché il modello ha abbastanza parametri per adattarsi perfettamente ai dati del campione. Tuttavia, ciò non implica necessariamente che il modello rappresenti fedelmente la popolazione più ampia. Le stime dei parametri in un modello saturo possono fornire informazioni sui pattern di relazione tra le variabili nel campione specifico, ma è cruciale interpretarle con cautela.\nStima e Identificabilità del Modello: Generalmente, la stima dei parametri non si basa sul semplice risolvere un sistema di equazioni matematiche. Invece, si utilizza una funzione di adattamento o discrepanza tra \\(\\Sigma(\\theta)\\) e \\(S\\), cercando il valore ottimale di \\(\\hat{\\theta}\\) attraverso tecniche di ottimizzazione numerica. Un modello SEM deve essere identificabile, il che significa che deve essere possibile stimare univocamente i parametri del modello. L’identificabilità implica che il numero di unità di informazione, come elementi nella matrice di covarianza, sia maggiore o uguale al numero di parametri da stimare.\n\n\n47.2.1 Gradi di Libertà e Identificabilità del Modello\nI gradi di libertà (dof) in un modello SEM sono calcolati come:\n\\[\ndof = \\# (\\text{unità di informazione}) - \\# (\\text{parametri da stimare})\n\\]\nPer una matrice di covarianza di ordine $ p $, il numero di unità di informazione è $ $. Per garantire l’identificabilità, è necessario soddisfare alcune condizioni:\n\nIn tutti i modelli, l’unità di misura delle variabili latenti deve essere specificata.\nIl numero di unità di informazione deve essere uguale o superiore al numero di parametri da stimare.\nIn modelli ad un fattore, è richiesto un minimo di tre indicatori per una soluzione “appena identificata”.\nIn modelli a più fattori, si raccomanda un minimo di tre indicatori per ogni variabile latente.\n\nUn modello è: - Non identificato se $ dof &lt; 0 $. - Appena identificato o “saturo” se $ dof = 0 $. - Sovra-identificato se $ dof &gt; 0 $.\nÈ importante notare che un’analisi fattoriale con solo due indicatori per un fattore non è possibile, poiché ci sono meno unità di informazione rispetto ai parametri da stimare. Un modello con tre indicatori e un fattore è “appena identificato”, senza gradi di libertà per valutare la bontà dell’adattamento. Per modelli ad un solo fattore comune latente, è quindi necessario disporre di almeno quattro indicatori.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#funzione-di-discrepanza-e-valutazione-della-bontà-di-adattamento",
    "href": "chapters/sem/03_gof.html#funzione-di-discrepanza-e-valutazione-della-bontà-di-adattamento",
    "title": "47  Test del Modello e Indicizzazione",
    "section": "47.3 Funzione di Discrepanza e Valutazione della Bontà di Adattamento",
    "text": "47.3 Funzione di Discrepanza e Valutazione della Bontà di Adattamento\nLa funzione di discrepanza tra \\(S\\) (matrice di covarianza osservata) e \\(\\Sigma(\\theta)\\) (matrice di covarianza stimata dal modello in base ai parametri \\(\\theta\\)) misura l’adeguatezza con cui il modello rappresenta i dati. Derivata dalla log-verosimiglianza per una distribuzione normale multivariata, la funzione confronta le strutture di covarianza teoriche e osservate.\nLa formula per la discrepanza ML (Massima Verosimiglianza) è:\n\\[\nFML(S, \\Sigma(\\theta)) = \\log|\\Sigma(\\theta)| - \\log|S| + \\text{traccia}(S\\Sigma(\\theta)^{-1}) - p,\n\\]\ndove \\(|S|\\) e \\(|\\Sigma(\\theta)|\\) indicano i determinanti di \\(S\\) e \\(\\Sigma(\\theta)\\) rispettivamente, e \\(p\\) è la dimensione delle matrici. Vediamo ogni termine per comprenderne il significato.\n\n47.3.1 Componenti della Formula di Discrepanza\n\nLogaritmo del determinante della matrice stimata, \\(\\log|\\Sigma(\\theta)|\\):\n\nIl termine \\(\\log|\\Sigma(\\theta)|\\) rappresenta una misura della “dimensione” o “scala” della matrice \\(\\Sigma(\\theta)\\). Più precisamente, il determinante di una matrice di covarianza può essere visto come una misura del volume dello spazio descritto dalle variabili nel modello: maggiore è il determinante, più “ampio” è lo spazio che copre la distribuzione del modello. Il logaritmo del determinante di \\(\\Sigma(\\theta)\\) contribuisce quindi a quantificare la scala complessiva del modello.\n\nLogaritmo del determinante della matrice osservata, \\(\\log|S|\\):\n\nSimilmente, \\(\\log|S|\\) rappresenta la scala della matrice di covarianza osservata nei dati. Questo termine funge da riferimento per confrontare la scala dei dati con quella stimata dal modello. In altre parole, \\(|S|\\) ci dice quale sarebbe la “dimensione” dei dati se fossero perfettamente rappresentati solo da \\(S\\), la matrice di covarianza empirica.\n\nTraccia del prodotto \\(S\\Sigma(\\theta)^{-1}\\):\n\nLa traccia, ossia la somma degli elementi diagonali, del prodotto \\(S\\Sigma(\\theta)^{-1}\\) rappresenta la relazione tra \\(S\\) e l’inverso della matrice \\(\\Sigma(\\theta)\\). Se \\(S\\) e \\(\\Sigma(\\theta)\\) fossero perfettamente identiche, questa traccia sarebbe pari a \\(p\\), la dimensione delle matrici, perché il prodotto di una matrice con la propria inversa è la matrice identità, che ha una somma degli elementi diagonali pari alla dimensione. Un valore diverso da \\(p\\) indica discrepanze tra le covarianze osservate e quelle stimate.\n\nTermine di normalizzazione, \\(-p\\):\n\nSottrarre \\(p\\) serve a normalizzare la traccia in modo che, in assenza di discrepanze (ovvero quando \\(S = \\Sigma(\\theta)\\)), il valore complessivo della funzione di discrepanza sia zero. Questo termine fa sì che la discrepanza sia relativa a quanto \\(S\\) differisca da \\(\\Sigma(\\theta)\\) in una forma più bilanciata.\n\n\n\n\n47.3.2 Interpretazione Complessiva\nLa funzione di discrepanza combina queste tre componenti per ottenere una misura della distanza o della differenza tra \\(S\\) e \\(\\Sigma(\\theta)\\). Essa confronta sia la “dimensione” complessiva (tramite i termini log-determinante) sia la “forma” (tramite la traccia) delle due matrici. In sintesi, la funzione di discrepanza \\(FML(S, \\Sigma(\\theta))\\) ci indica quanto il modello con parametri \\(\\theta\\) si discosta dai dati osservati e consente di capire se il modello è una buona rappresentazione delle relazioni di covarianza presenti nei dati.\nSe questa funzione di discrepanza risulta elevata, significa che le covarianze stimate dal modello non rispecchiano adeguatamente quelle osservate, indicando una possibile necessità di migliorare il modello o di rivedere i parametri \\(\\theta\\).\n\n\n47.3.3 Distribuzione e Test di Adattamento\nLa discrepanza calcolata, sotto l’ipotesi di buon adattamento, si distribuisce asintoticamente come una variabile chi-quadrato (χ²), che permette un test statistico. I gradi di libertà sono dati dalla differenza tra il numero di elementi indipendenti nella matrice di covarianza e il numero di parametri del modello.\n\nSe il valore di discrepanza è minore del valore critico χ², l’ipotesi nulla di buon adattamento non viene rifiutata, suggerendo un buon modello.\nSe invece è maggiore, l’ipotesi viene rifiutata, indicando una necessità di revisione del modello.\n\nQuesto test fornisce un’indicazione quantitativa della bontà di adattamento, utile per valutare se le strutture teoriche catturano adeguatamente le relazioni nei dati.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#test-chi2",
    "href": "chapters/sem/03_gof.html#test-chi2",
    "title": "47  Test del Modello e Indicizzazione",
    "section": "47.4 Test \\(\\chi^2\\)",
    "text": "47.4 Test \\(\\chi^2\\)\nIl test del chi quadrato (\\(\\chi^2\\)) è utilizzato per determinare quanto bene un modello teorico si adatta ai dati osservati. La formula per calcolare la statistica \\(\\chi^2\\) è:\n\\[\n\\chi^2 = N \\times F_{\\text{min}},\n\\]\ndove: - \\(N\\) rappresenta la dimensione del campione. - \\(F_{\\text{min}}\\) è il valore minimo della funzione di discrepanza.\nLa funzione di discrepanza, \\(F\\), è una misura di quanto le covarianze (o le varianze) osservate nei dati differiscano da quelle previste dal modello. Durante il processo di stima dei parametri del modello, questa funzione viene minimizzata. Il valore di \\(F\\) al suo minimo, \\(F_{\\text{min}}\\), rappresenta la discrepanza minima tra i dati osservati e quelli previsti dal modello.\nNell’ambito dell’analisi strutturale di covarianza, il valore di \\(F_{\\text{min}}\\) è tipicamente ottenuto attraverso la stima di massima verosimiglianza (Maximum Likelihood, ML). Tuttavia, ci sono due modi comuni per calcolare \\(\\chi^2\\), che possono variare a seconda del software utilizzato:\n\n\\(\\chi^2 = (N - 1) \\times F_{\\text{min}}\\)\n\\(\\chi^2 = N \\times F_{\\text{min}}\\)\n\nLa scelta tra \\(N\\) e \\(N-1\\) dipende da come il software gestisce la normalizzazione e l’adattamento delle strutture di covarianza.\n\n47.4.1 Interpretazione del Test del \\(\\chi^2\\)\n\nIpotesi Nulla $ H_0 $: Il modello si adatta bene ai dati. Ciò significa che non c’è una differenza significativa tra le covarianze osservate e quelle previste dal modello.\nValore p: Un valore p basso (ad esempio, minore di 0.05) suggerisce che dovremmo rifiutare l’ipotesi nulla, indicando che il modello non si adatta bene ai dati.\n\n\n\n47.4.2 Limitazioni\nLa statistica \\(\\chi^2\\) è influenzata dalla dimensione del campione: con campioni ampi, anche lievi discrepanze tra il modello e i dati possono portare a un valore di \\(\\chi^2\\) elevato, risultando in un rifiuto ingiustificato di un modello valido. Inoltre, il test del \\(\\chi^2\\) presenta alcune limitazioni importanti:\n\nNon fornisce indicazioni sulla direzione o sulla natura della discrepanza: il test non specifica dove il modello si discosta dai dati o in che modo le discrepanze si manifestano.\nEfficacia ridotta in modelli complessi: per modelli con molteplici parametri, o in condizioni in cui le ipotesi fondamentali (ad esempio, la normalità multivariata) non sono soddisfatte, il test del \\(\\chi^2\\) potrebbe non essere affidabile.\n\nPer tali ragioni, è comune integrare il test del \\(\\chi^2\\) con altri indici di adattamento, come l’indice di adattamento comparativo (CFI) e la radice dell’errore quadratico medio di approssimazione (RMSEA), per ottenere una valutazione più accurata e robusta dell’adattamento del modello ai dati.\nIl test del \\(\\chi^2\\) resta dunque un utile strumento di valutazione, ma è fondamentale interpretarlo con cautela, tenendo conto delle dimensioni del campione e di altri fattori che possono influire sul risultato. Nonostante le sue limitazioni, la statistica \\(\\chi^2\\) ha un ruolo importante in contesti specifici, come:\n\nConfronto tra modelli nidificati: consente di valutare se aggiunte o modifiche migliorano significativamente l’adattamento del modello.\nCalcolo di altri indici di adattamento: come l’indice di Tucker-Lewis (TLI).\nRapporto tra \\(\\chi^2\\) e gradi di libertà: un rapporto basso è indicativo di un buon adattamento relativo del modello ai dati.\n\nIn conclusione, pur essendo utile, la statistica \\(\\chi^2\\) va affiancata da altri strumenti di valutazione per una comprensione più completa e bilanciata della bontà di adattamento del modello.\n\n\n47.4.3 Test di rapporto di verosimiglianza\nIl test del \\(\\chi^2\\) può essere impiegato come un test di rapporto di verosimiglianza per confrontare due modelli nidificati. In questo contesto, “nidificati” significa che uno dei modelli (considerato il modello più semplice o ristretto) è un caso speciale dell’altro (il modello più complesso), con meno parametri liberi da stimare. Questo tipo di test è particolarmente utile per valutare se l’aggiunta di parametri supplementari (rendendo il modello più complesso) migliora significativamente l’adattamento del modello ai dati.\nIl processo di confronto tra i due modelli avviene nel seguente modo:\n\nSi stima il modello più semplice e si calcola il suo valore di \\(\\chi^2\\).\nSi stima il modello più complesso e si calcola il suo valore di \\(\\chi^2\\).\nSi confrontano i due valori di \\(\\chi^2\\) per determinare se l’aggiunta di parametri aggiuntivi giustifica un miglioramento dell’adattamento del modello ai dati, dati i gradi di libertà aggiuntivi.\n\nSe il valore p associato al \\(\\chi^2\\) del modello più complesso è significativamente più basso rispetto a quello del modello più semplice, questo suggerisce che l’aggiunta dei parametri fornisce un miglioramento significativo nell’adattamento del modello. Al contrario, se non vi è un miglioramento significativo, si può concludere che il modello più semplice è preferibile in termini di parsimonia e adattamento.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#chi-quadrato-normalizzato-nc",
    "href": "chapters/sem/03_gof.html#chi-quadrato-normalizzato-nc",
    "title": "47  Test del Modello e Indicizzazione",
    "section": "47.5 Chi Quadrato Normalizzato (NC)",
    "text": "47.5 Chi Quadrato Normalizzato (NC)\nIl Chi Quadrato Normalizzato (NC) emerge come un tentativo di attenuare l’effetto della dimensione del campione sulla statistica del chi quadrato del modello (\\(\\chi^2\\)). Questa pratica, adottata da alcuni ricercatori, consiste nel dividere \\(\\chi^2\\) per il numero dei gradi di libertà del modello (dfM), risultando nella formula \\(\\frac{\\chi_{ML}}{dfM}\\). Nonostante l’intento di mitigare l’impatto della dimensione del campione (N), l’impiego di NC presenta limitazioni sostanziali:\n\nInfluenza di N sui Modelli Erronei: La statistica \\(\\chi_{ML}\\) è sensibile a N esclusivamente per i modelli non corretti. Questo implica che l’uso di NC per modelli veritieri potrebbe essere fuorviante.\nIndipendenza di dfM da N: I gradi di libertà del modello (dfM) non sono correlati con la dimensione del campione, rendendo la divisione di \\(\\chi_{ML}\\) per dfM arbitraria e priva di fondamento statistico.\nMancanza di Linee Guida: Non esistono criteri consolidati che definiscano i limiti “accettabili” per il valore di NC. Per esempio, non è chiaro se un valore massimo di NC debba essere inferiore a 2.0, 3.0, o altro.\n\nIn conclusione, data la mancanza di una solida giustificazione statistica o logica, Kline (2023) sconsiglia l’utilizzo del Chi Quadrato Normalizzato come strumento di valutazione della bontà di adattamento del modello.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#chi-quadrato-scalato-e-errori-standard-robusti-per-distribuzioni-non-normali",
    "href": "chapters/sem/03_gof.html#chi-quadrato-scalato-e-errori-standard-robusti-per-distribuzioni-non-normali",
    "title": "47  Test del Modello e Indicizzazione",
    "section": "47.6 Chi Quadrato Scalato e Errori Standard Robusti per Distribuzioni Non Normali",
    "text": "47.6 Chi Quadrato Scalato e Errori Standard Robusti per Distribuzioni Non Normali\nNell’ambito dell’analisi di massima verosimiglianza (ML), sia l’approccio ML standard che quello robusto forniscono le stesse stime dei parametri. Tuttavia, il ML robusto differisce nell’introduzione di chi quadrati scalati (\\(\\chi^2\\) scalati) e di errori standard robusti, i quali sono adattati per controbilanciare gli effetti della non normalità dei dati.\nUn metodo pionieristico sviluppato da Satorra e Bentler, che si basa sull’utilizzo di dati completi, calcola il chi quadrato scalato (\\(\\chi_{SB}\\)) applicando un fattore di correzione di scala, indicato con \\(c\\), al valore del chi quadrato non scalato del modello (\\(\\chi_{ML}\\)). Questo fattore di scala \\(c\\) è determinato dalla curtosi multivariata media osservata nei dati grezzi. La formula specifica per il calcolo di \\(\\chi_{SB}\\) è:\n\\[\n\\chi_{SB} = \\frac{\\chi_{ML}}{c}.\n\\]\nQuesta formula evidenzia come il chi quadrato scalato di Satorra-Bentler modifica il chi quadrato tradizionale per tenere conto della curtosi nei dati, fornendo così una misura più affidabile della bontà di adattamento del modello in presenza di distribuzioni non normali.\nQuando si utilizza il Chi Quadrato Scalato di Satorra-Bentler (\\(\\chi_{SB}\\)), è importante comprendere come esso si comporti in campioni casuali. Le distribuzioni di \\(\\chi_{SB}\\) tendono ad avvicinarsi alle distribuzioni chi quadrato centrali, ma con una caratteristica fondamentale: le loro medie sono asintoticamente corrette. Questo significa che, su larga scala, \\(\\chi_{SB}\\) fornisce una stima media accurata della discrepanza tra i dati osservati e quelli previsti dal modello, correggendo per eventuali distorsioni causate dalla non normalità dei dati.\nUn altro tipo di chi quadrato, sviluppato da Asparouhov e Muthén, non si basa sul \\(\\chi_{ML}\\) standard. Invece, nei campioni di grandi dimensioni, il loro chi quadrato scalato corrisponde alla statistica T2* di Yuan e Bentler. Questa versione del chi quadrato è particolarmente adatta per gestire dati non normali o con valori mancanti. I gradi di libertà, sia per \\(\\chi_{SB}\\) che per T2*, sono rappresentati da dfM, indicando la flessibilità del modello in termini di numero di parametri stimabili.\nAl di là di questi, esistono chi quadrati che sono corretti sia per la media che per la varianza. Questi chi quadrati utilizzano fattori di scala diversi e, in genere, seguono distribuzioni chi quadrato centrali con medie e varianze che sono corrette in modo asintotico. Sebbene questi metodi richiedano maggiori risorse computazionali rispetto ai metodi che correggono solo per la media, tendono ad essere più precisi, specialmente in campioni di grandi dimensioni. Questa precisione aggiuntiva è particolarmente utile quando si affrontano set di dati complessi o di ampie dimensioni, permettendo una stima più accurata della bontà di adattamento del modello.\nPer quanto riguarda il software lavaan, ci sono diverse opzioni per il metodo ML robusto:\n\n“MLM” per dati completi, genera un chi quadrato scalato per la media di Satorra-Bentler.\n“MLR” per dati completi o incompleti, produce un chi quadrato corretto per la media basato sulla statistica T2* di Yuan-Bentler.\n“MLMV” per dati completi, calcola un chi quadrato scalato corretto per media e varianza.\n“MLMVS” genera un chi quadrato corretto per media e varianza con una correzione per l’eteroschedasticità di Satterthwaite.\n\nNel contesto dei metodi ML robusti utilizzati nel software lavaan, la “matrice di informazione” è un concetto fondamentale che incide sul calcolo degli errori standard. La matrice di informazione, in statistica, è una matrice che contiene informazioni sulla varianza e covarianza dei parametri stimati di un modello. Viene utilizzata per calcolare gli errori standard delle stime dei parametri, che sono essenziali per testare l’ipotesi statistica e per la costruzione di intervalli di confidenza.\nNel software lavaan, la matrice di informazione può essere di due tipi:\n\nMatrice di Informazione Attesa: Questa è la matrice predefinita utilizzata da lavaan per calcolare gli errori standard nei metodi ML robusti. La matrice di informazione attesa si basa sulle aspettative teoriche della varianza e covarianza dei parametri stimati, derivanti dal modello e dall’insieme dei dati.\nMatrice di Informazione Osservata: Quando si hanno dati mancanti, lavaan passa all’utilizzo della matrice di informazione osservata. Questa matrice utilizza i dati effettivamente osservati per calcolare la varianza e la covarianza dei parametri stimati. Essa può fornire stime più accurate degli errori standard in presenza di dati mancanti.\n\nGli utenti di lavaan hanno la possibilità di specificare se utilizzare la matrice di informazione attesa anche in presenza di dati incompleti, in base alle esigenze specifiche della loro analisi.\nÈ importante per i ricercatori utilizzare questi strumenti in modo etico e metodologicamente corretto. Evitare di selezionare in modo opportunistico le combinazioni di chi quadrati scalati e errori standard robusti che sembrano meglio supportare le ipotesi, e dichiarare chiaramente qualsiasi variabilità nei risultati dovuta alla scelta del metodo di calcolo, è cruciale per mantenere l’integrità e l’affidabilità della ricerca.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#indicizzazione-delladattamento-del-modello",
    "href": "chapters/sem/03_gof.html#indicizzazione-delladattamento-del-modello",
    "title": "47  Test del Modello e Indicizzazione",
    "section": "47.7 Indicizzazione dell’Adattamento del Modello",
    "text": "47.7 Indicizzazione dell’Adattamento del Modello\nL’indicizzazione dell’adattamento del modello si basa sull’uso di indici di adattamento approssimati, i quali si differenziano dai test di significatività tradizionali. Invece di fornire una decisione dicotomica, come il rifiuto o l’accettazione di un’ipotesi nulla, questi indici offrono una misura continua di quanto bene un modello si adatta ai dati osservati. Non essendoci una separazione netta tra i limiti dell’errore di campionamento, gli indici di adattamento forniscono una valutazione più sfumata e graduale della bontà di adattamento.\nQuesti indici possono essere classificati in due categorie principali:\n\nStatistiche di Cattivo Adattamento: In questa categoria, valori più elevati indicano un peggior adattamento del modello ai dati. Un esempio tipico di questa categoria è il chi quadrato del modello, dove valori più alti suggeriscono una maggiore discrepanza tra il modello e i dati.\nStatistiche di Buon Adattamento: Al contrario, per gli indici in questa categoria, valori più alti segnalano un migliore adattamento del modello ai dati. Molti di questi indici sono normalizzati in modo che il loro intervallo varii da 0 a 1.0, dove 1.0 rappresenta l’adattamento ottimale del modello.\n\nA differenza del test del chi quadrato, che si basa su un framework teorico ben definito, l’interpretazione e l’applicazione degli indici di adattamento approssimati non sono guidate da un unico insieme di principi teorici consolidati. Questa situazione fa sì che la valutazione dell’adattamento del modello si allinei maggiormente a ciò che Little (2013) ha descritto come “scuola di modellazione”. Questo approccio contempla l’analisi di modelli statistici complessi in un contesto in cui le regole decisionali sono meno rigide e più soggette a interpretazione.\nLa natura flessibile di questo approccio rispecchia la varietà e la complessità dei modelli statistici, che devono essere personalizzati per rispondere a specifiche domande di ricerca. Questa flessibilità, tuttavia, porta con sé una certa ambiguità nelle regole di valutazione dei modelli statistici. Pur offrendo la possibilità di adattare l’analisi alle particolarità di ogni studio, questa mancanza di rigore teorico uniforme può talvolta non tradursi in pratiche ottimali di modellazione.\nLa questione filosofica relativa all’adattamento esatto dei modelli statistici solleva dubbi sull’idea di perfezione come standard per questi modelli. In effetti, è ampiamente riconosciuto che tutti i modelli statistici sono in qualche misura imperfetti; sono piuttosto strumenti di approssimazione che aiutano i ricercatori a organizzare e interpretare le loro osservazioni sui fenomeni di interesse. Un modello troppo semplificato, che non cattura la complessità del fenomeno, può essere inadeguato e quindi rifiutato. Allo stesso tempo, un modello eccessivamente complesso, che cerca di replicare fedelmente il fenomeno, può risultare di scarsa utilità scientifica a causa della sua complessità eccessiva.\nGeorge Box, nel suo influente lavoro del 1976, avanzò l’idea che nessun modello statistico potesse essere considerato perfettamente “corretto”. Questa visione nasce dalla consapevolezza che tutti i modelli hanno una certa dose di imperfezione intrinseca. Box suggeriva che lo scopo principale di uno scienziato dovrebbe essere la ricerca di una “descrizione economica” dei fenomeni naturali, cercando cioè di formulare modelli che siano semplici, ma al contempo efficaci, nella rappresentazione della realtà. Egli criticava la tendenza a sovraelaborare o sovraparametrizzare i modelli, considerandola un segno di mediocrità nella pratica scientifica. Box enfatizzava l’importanza di concentrarsi sugli errori sostantivi, o “tigri”, piuttosto che su piccole imperfezioni, o “topi”, affermando:\n\n“Since all models are wrong the scientist must be alert to what is importantly wrong. It is inappropriate to be concerned about mice when there are tigers abroad.”\n\nCiò implica che l’obiettivo nella modellazione statistica non dovrebbe essere una perfezione irraggiungibile, ma piuttosto lo sviluppo di modelli che, pur nella loro semplicità, riescano a cogliere gli aspetti fondamentali dei fenomeni analizzati. Questo richiede un equilibrio tra la complessità necessaria per una descrizione accurata e la semplicità che rende un modello pratico e interpretabile.\nHayduk (2014), nel commentare l’affermazione di Box, si focalizza specificatamente sul contesto della modellizzazione SEM (Structural Equation Modeling). Egli identifica le “tigri”, ovvero gli errori gravi nei modelli, come indicatori di una specificazione errata del modello. Hayduk sottolinea l’importanza critica di riconoscere e correggere gli errori significativi piuttosto che disperdere energie su dettagli minori. In sostanza, Hayduk rafforza l’idea che è essenziale distinguere tra errori minori e maggiori, questi ultimi potendo compromettere seriamente la validità e l’utilità di un modello statistico.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#tipologie-di-indici-di-adattamento-approssimati",
    "href": "chapters/sem/03_gof.html#tipologie-di-indici-di-adattamento-approssimati",
    "title": "47  Test del Modello e Indicizzazione",
    "section": "47.8 Tipologie di Indici di Adattamento Approssimati",
    "text": "47.8 Tipologie di Indici di Adattamento Approssimati\nGli indici di adattamento approssimati possono essere classificati in diverse categorie, che riflettono diversi aspetti della bontà di adattamento di un modello statistico ai dati. Sebbene questa classificazione non sia esaustiva né le categorie siano mutualmente esclusive, i tipi principali di indici di adattamento sono i seguenti:\n\nIndici di Adattamento Assoluto: Questi indici, come il GFI (Goodness of Fit Index), misurano quanto bene un modello spiega i dati senza riferimento ad altri modelli. Indicano l’abilità del modello di riprodurre i dati osservati.\nIndici di Adattamento Parsimonioso: Questi indici confrontano i gradi di libertà del modello (dfM) con il numero massimo possibile di gradi di libertà disponibili nei dati. Un esempio è l’AGFI (Adjusted Goodness of Fit Index), che incorpora una penalità per la complessità del modello, benché non sia un indice di adattamento parsimonioso come definito in questa categoria.\nIndici di Adattamento Incrementale (Relativo o Comparativo): Questi indici confrontano l’adattamento del modello del ricercatore con quello di un modello di base, tipicamente un modello di indipendenza che assume covarianze nulle tra le variabili osservate. È possibile scegliere un modello di base diverso, sebbene il calcolo manuale dell’indice possa essere necessario se il modello di base desiderato differisce da quello predefinito nel software.\nIndici di Adattamento Non Centrale: Stimano il grado in cui l’ipotesi di adattamento esatto è falsa, dati il modello e i dati. Questi indici approssimano parametri nelle distribuzioni chi quadrato non centrali, che descrivono anche le distribuzioni campionarie per gli indici di adattamento di questo tipo.\nIndici di Adattamento Predittivo (o basati sulla Teoria dell’Informazione): Derivati dalla teoria dell’informazione, stimano l’adattamento del modello in campioni di replica ipotetici della stessa dimensione, estratti casualmente dalla stessa popolazione del campione originale. Sono utilizzati principalmente per confrontare modelli alternativi basati sulle stesse variabili e adattati agli stessi dati, ma dove i modelli non sono gerarchicamente correlati.\n\nNon tutti gli indici di adattamento approssimati hanno resistito alla prova del tempo. Ad esempio, gli indici di adattamento parsimonioso non hanno mai raggiunto una popolarità significativa tra i ricercatori applicati, restando relativamente oscuri. Altri indici, come il GFI e l’AGFI, sono stati criticati per la loro sensibilità alla dimensione del campione e al numero di indicatori nei modelli di analisi fattoriale.\nI software moderni per la Structural Equation Modeling (SEM) presentano una notevole varietà nel numero di indici di adattamento approssimati forniti nei loro output. Programmi come Amos e LISREL elencano un numero elevato di indici (oltre 12), mentre altri come lavaan e Mplus ne includono un numero più limitato (circa 4-5). Questa abbondanza di indici può portare al rischio di “cherry-picking”, cioè la tendenza a selezionare e riportare solo quegli indici che mostrano risultati favorevoli al modello proposto dal ricercatore. Per mitigare questo rischio, è consigliabile limitarsi a un insieme essenziale di indici e prestare attenzione all’analisi dei residui.\n\n47.8.1 Modello Baseline\nIl modello baseline, noto anche come modello nullo, è un modello in cui tutte le covarianze sono impostate a zero, mentre le varianze sono stimate liberamente. In questo modello, non si stimano i carichi fattoriali; ci si limita invece a stimare le medie e le varianze osservate, eliminando tutte le covarianze tra le variabili.\nÈ utile pensare al modello nullo o baseline come il peggior modello possibile, da confrontare poi con il modello saturato, che rappresenta invece la migliore approssimazione ai dati. Teoricamente, il modello baseline è fondamentale per comprendere come vengono calcolati altri indici di adattamento del modello, in quanto fornisce un punto di riferimento iniziale per la valutazione della bontà di adattamento in un contesto di Modelli di Equazioni Strutturali.\n\n\n47.8.2 Set di Indici di Adattamento Consigliati\nKline (2023) suggerisce un insieme essenziale di soli tre indici di adattamento approssimati, che sono ampiamente utilizzati nei software SEM e frequentemente presenti negli studi pubblicati. Questi indici sono stati selezionati per le seguenti ragioni:\n\nAmpia Presenza nella Letteratura: Sono ampiamente riportati in numerosi studi SEM, rendendoli familiari sia ai ricercatori che ai revisori.\nStandardizzazione: Le scale di questi indici non dipendono dalle variabili osservate o latenti, fornendo così una misura standardizzata di adattamento.\nValidità Statistica Estesa: Almeno uno di questi indici, l’RMSEA, possiede un solido fondamento statistico e un quadro interpretativo più ampio per la stima degli intervalli, i test delle ipotesi e la pianificazione della dimensione del campione.\n\nNonostante la loro utilità, è fondamentale usare questi indici con attenzione. I ricercatori dovrebbero evitare l’uso acritico di soglie o punti di taglio, sia fissi sia variabili, che si suppone differenzino tra modelli con un buon o cattivo adattamento. L’applicazione di queste soglie può essere problematica, poiché non sono valide universalmente per tutti i tipi di modelli e set di dati. L’uso improprio di tali soglie può portare a decisioni errate, in particolare se si trascura l’analisi dei residui.\nIl gruppo principale di tre indici di adattamento approssimati raccomandato comprende:\n\nRoot Mean Square Error of Approximation (RMSEA) di Steiger-Lind (Steiger, 1990), accompagnato dal suo intervallo di confidenza al 90%. L’RMSEA valuta l’adattamento assoluto del modello, penalizzando la complessità del modello, ma non è un indice di adattamento parsimonioso. È un indice di cattivo adattamento dove il valore zero rappresenta l’adattamento ideale, senza un limite massimo teorico.\nComparative Fit Index (CFI) di Bentler (Bentler, 1990). Il CFI è un indice di adattamento incrementale e valuta la bontà di adattamento relativa del modello rispetto a un modello di base. Si estende su una scala da 0 a 1.0, dove 1.0 indica l’adattamento ottimale, e non impone penalità per la complessità del modello.\nStandardized Root Mean Square Residual (SRMR) (Jöreskog & Sörbom; 1981). L’SRMR è un indice di adattamento assoluto che misura la discrepanza tra le correlazioni osservate e quelle previste dal modello. Un valore di zero indica un adattamento perfetto.\n\nSia l’RMSEA sia il CFI incorporano il chi quadrato del modello e i suoi gradi di libertà nelle loro formule. Questo implica che condividono le stesse assunzioni distributive della corrispondente statistica di test. Se tali assunzioni non sono valide, i valori degli indici e della statistica di test (incluso il valore p) potrebbero non essere accurati. Entrambi gli indici sono stati inizialmente definiti per dati continui con distribuzioni normali analizzati tramite ML standard. Tuttavia, in presenza di dati significativamente non normali, i valori di chiML, RMSEA e CFI possono risultare distorti. Alcuni software SEM implementano correzioni ad hoc per la non normalità.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#misure-di-adeguamento-per-il-confronto",
    "href": "chapters/sem/03_gof.html#misure-di-adeguamento-per-il-confronto",
    "title": "47  Test del Modello e Indicizzazione",
    "section": "47.9 Misure di adeguamento per il confronto",
    "text": "47.9 Misure di adeguamento per il confronto\n\n47.9.1 CFI\nGli indici di adattamento comparativo [detti anche indici di adattamento incrementale; ad es. Hu & Bentler (1998)] valutano l’adattamento di una soluzione specificata dall’utente in relazione a un modello di base nidificato più ristretto. Tipicamente, il modello base è un modello “nullo” o “di indipendenza” in cui le covarianze tra tutti gli indicatori di input sono fissate a zero, ma nessun vincolo viene posto sulle varianze degli indicatori. Uno di questi indici, l’indice di adattamento comparativo (comparative fit index, CFI; Bentler, 1990). Il CFI si basa su un confronto relativo, situando il modello di interesse lungo un continuum che va dal modello peggiore (nullo) al modello perfetto (saturo).\nIl CFI valuta la riduzione relativa del parametro di non-centralità (\\(\\lambda\\)) tra il modello di interesse e il modello di riferimento (Bentler, 1990). Il parametro di non-centralità \\(\\lambda_m\\) rappresenta il grado di errore di specificazione del modello \\(m\\) ed è calcolato come:\n\\[\n\\lambda_m = \\chi^2_m - \\text{df}_m,\n\\]\ndove \\(\\chi^2_m\\) è il valore chi-quadro stimato per il modello e \\(\\text{df}_m\\) rappresenta i gradi di libertà. Più alto è \\(\\lambda_m\\), maggiore è la discrepanza tra il modello e i dati osservati. Il valore del CFI si basa sul rapporto tra i parametri di non-centralità del modello di interesse (\\(\\lambda_m\\)) e del modello nullo (\\(\\lambda_b\\)):\n\\[\nCFI(m, b) = 1 - \\frac{\\lambda_m}{\\lambda_b} = 1 - \\frac{\\chi^2_m - \\text{df}_m}{\\chi^2_b - \\text{df}_b}.\n\\]\nIl valore del CFI varia generalmente tra 0 e 1 (anche se in casi particolari può superare 1 o essere negativo), dove un valore vicino a 1 indica un buon adattamento del modello rispetto al modello nullo.\n\n47.9.1.1 Modello nullo come baseline\nIl modello nullo è un modello in cui tutte le variabili osservate sono considerate non correlate. Il CFI misura quindi quanto il modello di interesse riesce a migliorare l’adattamento rispetto a questo modello di riferimento, in modo analogo al concetto di \\(R^2\\) per la regressione lineare.\n\n\n47.9.1.2 Sensibilità ai dati e alle caratteristiche del modello\nIl comportamento del CFI è influenzato da tre fattori principali:\n\nDimensione del campione (\\(n\\)): Campioni più grandi aumentano il parametro di non-centralità del modello nullo (\\(\\lambda_b\\)), migliorando la capacità del CFI di distinguere tra modelli.\nNumero di variabili osservate (\\(p\\)): Un numero elevato di variabili può complicare l’interpretazione del CFI, poiché aumenta i gradi di libertà del modello nullo, riducendo la non-centralità \\(\\lambda_b\\).\nCorrelazione tra variabili (\\(R\\)): Maggiore è la correlazione tra le variabili, più il modello nullo differisce dai dati, e più il CFI può differenziare tra modelli.\n\n\n\n47.9.1.3 Regole empiriche\nValori del CFI superiori a 0.90 erano considerati accettabili in passato (Bentler & Bonett, 1980), mentre valori superiori a 0.95 sono oggi considerati indicativi di un buon adattamento (Hu & Bentler, 1999). Tuttavia, studi di simulazione più recenti, come quelli di Fan e Sivo nel 2005 e di Yuan nel 2005, hanno messo in dubbio l’universalità di un valore soglia specifico per il CFI, evidenziando che l’adeguatezza di tale valore può variare a seconda delle caratteristiche dei modelli e del grado di non normalità nei dati. Di conseguenza, è importante non applicare queste regole in modo meccanico, ma valutare il contesto specifico. Inoltre, Brosseau-Liard e Savalei (2014) hanno descritto delle versioni robuste del CFI adatte per dati non normali. Queste versioni del CFI sono calcolate e fornite dal software lavaan quando si utilizzano metodi di stima Maximum Likelihood (ML) robusti. Questo implica che, quando si lavora con dati che presentano deviazioni dalla normalità, queste versioni robuste del CFI possono offrire una misura più affidabile dell’adattamento del modello.\n\n\n47.9.1.4 Variabilità campionaria\nA livello di popolazione, un modello corretto dovrebbe avere un valore di CFI pari a 1. Tuttavia, la variabilità campionaria può influenzare il parametro di non-centralità \\(\\lambda_m\\) e \\(\\lambda_b\\), causando deviazioni rispetto alle aspettative teoriche. Questo fenomeno è particolarmente rilevante nei campioni piccoli o in presenza di modelli complessi.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#misure-di-adeguamento-parsimonioso",
    "href": "chapters/sem/03_gof.html#misure-di-adeguamento-parsimonioso",
    "title": "47  Test del Modello e Indicizzazione",
    "section": "47.10 Misure di adeguamento parsimonioso",
    "text": "47.10 Misure di adeguamento parsimonioso\n\n47.10.1 TLI\nUn indice che rientra nella degli indici di adeguamento parsimonioso è l’indice Tucker-Lewis (Tucker–Lewis index, TLI, anche chiamato indice di adattamento non normato). Il TLI si pone il problema di penalizzare la complessità del modello, ovvero include una funzione di penalizzazione per l’addizione di parametri che non migliorano in maniera sostanziale l’adattamento del modello. Il TLI è calcolato con la seguente formula:\n\\[\n\\begin{equation}\nTLI = \\frac{(\\chi^2_B / dof_B)–(\\chi^2_T / dof_T)}{(\\chi^2_B / dof_B) – 1},\n\\end{equation}\n\\]\ndove \\(\\chi^2_T\\) è il valore \\(\\chi^2\\) del modello target, \\(dof_T\\) sono i gradi di libertà del modello target, \\(\\chi^2_B\\) è il valore \\(\\chi^2\\) del modello baseline e \\(dof_B\\) sono i gradi di libertà del modello base.\nL’Indice di Tucker-Lewis (TLI) può, in teoria, assumere valori inferiori a zero se il modello di base, ovvero un modello diverso da quello studiato dal ricercatore, mostra un ottimo adattamento ai dati. Tuttavia, questa eventualità è rara nella pratica. Al contrario, il TLI può superare il valore di 1.0 se il modello analizzato dal ricercatore si adatta in modo particolarmente stretto ai dati. Marsh e Balla (1994) hanno evidenziato che la dimensione del campione influenza poco i valori del TLI.\nSecondo quanto osservato da Kenny (2020), si possono trarre due conclusioni importanti:\n\nIl Comparative Fit Index (CFI) e il TLI sono entrambi influenzati dall’entità delle correlazioni tra le variabili misurate. Ciò significa che valori medi di correlazione più elevati risultano in valori più alti sia per il CFI che per il TLI, e il contrario è vero per correlazioni medie più basse.\nI valori del CFI e del TLI mostrano una forte correlazione tra loro. Di conseguenza, è consigliabile riportare solo uno dei due indici per evitare ripetizioni e per mantenere la chiarezza del report. La scelta tra CFI e TLI dovrebbe basarsi su criteri specifici relativi al contesto e agli obiettivi dello studio in questione.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#misure-di-adeguamento-assoluto",
    "href": "chapters/sem/03_gof.html#misure-di-adeguamento-assoluto",
    "title": "47  Test del Modello e Indicizzazione",
    "section": "47.11 Misure di adeguamento assoluto",
    "text": "47.11 Misure di adeguamento assoluto\n\n47.11.1 Root Mean Square Error of Approximation (RMSEA)\nL’Errore Quadratico Medio di Approssimazione (RMSEA) è un indicatore che misura quanto bene un modello statistico si adatta ai dati. A differenza di altri indici come il CFI o il TLI, che confrontano il modello con un modello di base, l’RMSEA valuta l’adattamento del modello in maniera assoluta.\nIl calcolo dell’RMSEA si basa sull’utilizzo del chi quadrato (\\(\\chi^2\\)), che quantifica la discrepanza tra le covarianze osservate nei dati e quelle stimate dal modello. Questa discrepanza, che indichiamo con \\(\\delta\\), è la differenza tra il valore del chi quadrato e i gradi di libertà (df) del modello: \\(\\delta = \\chi^2 - df\\). Un valore di \\(\\delta\\) più alto indica una maggiore discrepanza e quindi un peggior adattamento del modello ai dati.\nLa formula dell’RMSEA è la seguente:\n\\[\nRMSEA = \\sqrt{\\frac{\\max(0, \\delta)}{df \\cdot (n-1)}},\n\\]\ndove \\(n\\) è il numero di osservazioni nel campione. L’RMSEA stima l’errore nell’approssimare la matrice di correlazione (o covarianza) osservata con quella derivante dal modello. Fornisce un’indicazione sulla qualità dell’adattamento del modello alla popolazione, considerando i gradi di libertà e la parsimonia del modello.\nAssumendo che i dati seguano una distribuzione normale multivariata, che il modello sia corretto, e che il campione sia ampio e casuale, il chi quadrato del modello (\\(\\chi^2_{ML}\\)) segue una distribuzione chi quadrato con gradi di libertà \\(dfM\\). Se il modello non è corretto, il chi quadrato segue una distribuzione non centrale \\(\\chi^2(dfM, \\delta)\\), dove \\(\\delta\\) rappresenta il grado di discrepanza tra modello e dati. Se \\(\\delta = 0\\), il modello è perfetto; se \\(\\delta &gt; 0\\), il modello non è adeguato.\nIl parametro di non centralità normalizzato (\\(\\delta_{nor}\\)), meno sensibile alla dimensione del campione, si calcola così:\n\\[\n\\delta_{nor} = \\max(0, \\chi^2_{ML} - dfM).\n\\]\nQuesto parametro fa parte di una funzione che stima gli errori di approssimazione, ossia la discrepanza tra le matrici di covarianza del campione e della popolazione quando il modello è applicato alla matrice di popolazione.\nIl valore finale dell’RMSEA, indicato come \\(eˆ\\) (epsilon minuscolo), si ottiene dalla formula:\n\\[\neˆ = \\sqrt{\\frac{\\delta_{nor}}{dfM (N - 1)}}\n\\]\nSebbene la stima di \\(eˆ\\) non sia imparziale a causa della restrizione \\(eˆ ≥ 0\\), è considerata una buona approssimazione. Browne e Cudeck (1993) suggerirono che un valore di \\(eˆ ≤ 0.05\\) indichi un buon adattamento del modello, ma ricerche successive hanno evidenziato che non esiste una soglia universale precisa. È quindi raccomandato valutare anche il limite superiore dell’intervallo di confidenza di \\(eˆ\\) (indicato come \\(eˆU\\)) per una valutazione più accurata dell’adattamento del modello.\nAltre caratteristiche del RMSEA sono riassunte di seguito:\n\nL’interpretazione di \\(eˆ\\), \\(eˆL\\) e \\(eˆU\\) in relazione a soglie fisse è generalmente appropriata in grandi campioni con modelli ben specificati; in modelli più piccoli o con errori di specificazione significativi, questa interpretazione può richiedere maggiore cautela.\nStudi di simulazione hanno rilevato che l’RMSEA tende a imporre una penalità più severa per la complessità su modelli più piccoli con poche variabili. Ciò è dovuto al fatto che i modelli più piccoli tendono ad avere meno gradi di libertà, mentre i modelli più grandi hanno più “spazio” per valori di dfM più alti.\nStudi di simulazione hanno valutato le prestazioni di forme robuste dell’RMSEA corrette per la non normalità, una delle quali è basata sul chi quadrato scalato di Satorra-Bentler. Questa versione robusta corretta per la popolazione dell’RMSEA generalmente supera la versione non corretta, che tende ad essere sovrastimata in condizioni di non normalità.\nSono stati sviluppati metodi per stimare la potenza statistica delle varie ipotesi nulle basate sull’RMSEA, così come metodi per generare le dimensioni minime del campione richieste per ottenere livelli target di potenza statistica.\n\n\n\n47.11.2 Root Mean Square Residual (RMRS)\nA differenza del chi quadrato del modello e dei gradi di libertà, che valutano la bontà di adattamento di un modello in base a criteri di adattamento globale, l’indice RMRS (Root Mean Square Residual) si concentra esclusivamente sui residui del modello, ovvero le discrepanze tra le correlazioni osservate e quelle previste dal modello.\nLa formula per calcolare l’RMRS è la seguente:\n\\[\nRMRS = \\sqrt{ \\frac{2 \\sum_i\\sum_j(r_{ij} - \\hat{r}_{ij})^2}{p(p+1)}},\n\\]\ndove:\n\n\\(p\\) rappresenta il numero di item (variabili) nel modello,\n\\(r_{ij}\\) è la correlazione osservata tra le variabili \\(i\\) e \\(j\\),\n\\(\\hat{r}_{ij}\\) è la correlazione prevista dal modello tra le variabili \\(i\\) e \\(j\\).\n\nUn valore di RMRS pari a 0 indica un adattamento perfetto del modello, mentre valori crescenti indicano un adattamento meno preciso. In generale, un valore di SRMR inferiore a 0.08 è considerato favorevole (Hu e Bentler, 1999).\nTuttavia, è importante notare che il SRMR è una misura media e può nascondere variazioni significative tra i residui di correlazione individuali. Ad esempio, se il SRMR è 0.03, potrebbe sembrare un buon adattamento. Ma se i residui di correlazione variano da -0.12 a 0.18, con alcuni residui superiori a 0.10, potrebbe indicare problemi di adattamento locali più gravi.\nPertanto, quando si riportano i risultati in un report, per ottenere una comprensione più completa dell’adattamento del modello è consigliabile descrivere i residui di correlazione o, meglio ancora, presentare l’intera matrice dei residui, anziché basarsi esclusivamente su un valore medio come il SRMR.\n\n\n47.11.3 Interpretazione con lavaan\nL’interpretazione degli indici di bontà di adattamento trovati nella CFA o nella modellazione di equazioni strutturali può essere ottenuta usando le funzioni del pacchetto effectsize.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#adattamento-locale",
    "href": "chapters/sem/03_gof.html#adattamento-locale",
    "title": "47  Test del Modello e Indicizzazione",
    "section": "47.12 Adattamento Locale",
    "text": "47.12 Adattamento Locale\nI modelli SEM possono teoricamente superare i test di adattamento globale ma fallire nei test di adattamento locale. Questi dettagli, relativi all’adattamento del modello, sono esaminati direttamente nei test di adattamento locale. L’analisi dei residui (sia standardizzati che di correlazione) è quindi cruciale per una valutazione completa del modello (Maydeu-Olivares e Shi, 2017). Le recenti norme di reportistica per il SEM richiedono agli autori di descrivere sia l’adattamento globale che quello locale (Appelbaum et al., 2018); Greiff e Heene, 2017; Vernon e Eysenck, 2007).\n\n47.12.1 Residui di Covarianza, Residui Standardizzati, Residui Normalizzati\n\nResidui di Covarianza: Sono le differenze tra le covarianze osservate e quelle previste dal modello. Questi residui possono essere difficili da interpretare perché non sono standardizzati, ovvero la loro metrica dipende dalle scale delle variabili coinvolte. Pertanto, residui di covarianza per coppie di variabili diverse non sono direttamente confrontabili a meno che tutte le variabili non siano sulla stessa metrica.\nResidui Standardizzati: Sono versioni standardizzate dei residui di covarianza, interpretati come un test z in campioni grandi. Un residuo standardizzato significativamente diverso da zero indica una discrepanza tra modello e dati. Tuttavia, la significatività di questi residui può dipendere dalla dimensione del campione, con residui vicini allo zero che possono essere significativi in campioni grandi, mentre residui relativamente grandi potrebbero non essere significativi in campioni piccoli.\nResidui Normalizzati: Sono i rapporti tra i residui di covarianza e l’errore standard della covarianza campionaria. Sono generalmente più conservativi dei residui standardizzati in termini di test di significatività. In modelli complessi, quando non è possibile calcolare il denominatore di un residuo standardizzato, il residuo normalizzato fornisce un’alternativa più conservativa.\n\nNel software lavaan ci sono due opzioni principali per calcolare i residui di correlazione:\n\nOpzione cor.bollen: Questa specifica indica al computer di convertire separatamente le matrici di covarianza del campione e quelle implicite dal modello in matrici di correlazione prima di calcolare i residui. Questo processo comporta la standardizzazione di ciascuna matrice in base alle varianze (deviazioni standard quadrate) presenti nella diagonale principale di ciascuna matrice. Le varianze nella matrice di covarianza del campione sono osservate direttamente, mentre le varianze per le variabili endogene nella matrice di covarianza implicata dal modello sono previste dal modello e possono differire dalle varianze osservate corrispondenti.\nOpzione cor.bentler: Questa opzione standardizza sia la matrice di covarianza del campione che quella implicata dal modello basandosi sulle varianze presenti solo nella matrice di covarianza del campione. Poiché non tutti gli elementi della diagonale principale nella matrice di covarianza implicata dal modello sono varianze osservate, alcuni valori dei residui di correlazione del tipo Bentler potrebbero non essere pari a zero. Tuttavia, i valori dei residui fuori diagonale per entrambi i metodi sono generalmente simili.\n\nPer impostazione predefinita, lavaan utilizza il metodo cor.bollen per calcolare i residui di correlazione nelle sue analisi.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#esempio-1",
    "href": "chapters/sem/03_gof.html#esempio-1",
    "title": "47  Test del Modello e Indicizzazione",
    "section": "47.13 Esempio 1",
    "text": "47.13 Esempio 1\nNel capitolo precedente abbiamo formulato un modello SEM nel quale abbiamo definito una variabile latente con le sei sottoscale della Self-Compassion Scale e una seconda variabile latente con le tre sottoscale della DASS-21. Abbiamo ipotizzato che il fattore dell’autocompassione eserciti un effetto (protettivo) nei confronti del disagio psicologico misurato dal fattore definito dalle sottoscale della DASS-21.\n\nd_sc &lt;- read.csv(\"../../data/dass_rosenberg_scs.csv\", header = TRUE)\n\n\nmod_sc &lt;- \"\n  F =~ anxiety + depression + stress\n  SC =~ self_kindness   + common_humanity   + mindfulness   + self_judgment + isolation + over_identification\n  F ~ SC\n\"\n\n\nfit_sc &lt;- lavaan::sem(mod_sc, d_sc)\n\n\nsemPlot::semPaths(fit_sc,\n    what = \"col\", whatLabels = \"std\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\nI coefficienti stimati nel modello SEM appaiono coerenti e in linea con le aspettative, in particolare il coefficiente che descrive l’effetto “causale” del fattore dell’autocompassione sul malessere psicologico, che si attesta a -0.48. Questo valore negativo corrobora l’ipotesi secondo cui l’autocompassione svolge un ruolo di fattore protettivo contro il malessere psicologico. Tuttavia, prima di confermare definitivamente questa conclusione, è cruciale esaminare gli indici di bontà di adattamento del modello. Questi indici ci permetteranno di valutare quanto accuratamente il modello SEM si adatta ai dati osservati, fornendo un quadro più chiaro della validità delle nostre inferenze. In altre parole, sebbene il modello suggerisca una relazione negativa tra autocompassione e malessere psicologico, la conferma finale di questa associazione dipenderà dall’adeguatezza complessiva del modello rispetto ai dati.\n\nfitMeasures(fit_sc) |&gt;\n    print()\n\n                 npar                  fmin                 chisq \n             1.90e+01              4.27e-01              4.49e+02 \n                   df                pvalue        baseline.chisq \n             2.60e+01              0.00e+00              3.13e+03 \n          baseline.df       baseline.pvalue                   cfi \n             3.60e+01              0.00e+00              8.63e-01 \n                  tli                  nnfi                   rfi \n             8.11e-01              8.11e-01              8.01e-01 \n                  nfi                  pnfi                   ifi \n             8.56e-01              6.19e-01              8.64e-01 \n                  rni                  logl     unrestricted.logl \n             8.63e-01             -1.23e+04             -1.21e+04 \n                  aic                   bic                ntotal \n             2.47e+04              2.47e+04              5.26e+02 \n                 bic2                 rmsea        rmsea.ci.lower \n             2.47e+04              1.76e-01              1.62e-01 \n       rmsea.ci.upper        rmsea.ci.level          rmsea.pvalue \n             1.90e-01              9.00e-01              0.00e+00 \n       rmsea.close.h0 rmsea.notclose.pvalue     rmsea.notclose.h0 \n             5.00e-02              1.00e+00              8.00e-02 \n                  rmr            rmr_nomean                  srmr \n             1.20e+00              1.20e+00              7.10e-02 \n         srmr_bentler   srmr_bentler_nomean                  crmr \n             7.10e-02              7.10e-02              7.90e-02 \n          crmr_nomean            srmr_mplus     srmr_mplus_nomean \n             7.90e-02              7.10e-02              7.10e-02 \n                cn_05                 cn_01                   gfi \n             4.65e+01              5.45e+01              8.46e-01 \n                 agfi                  pgfi                   mfi \n             7.33e-01              4.89e-01              6.69e-01 \n                 ecvi \n             9.26e-01 \n\n\nL’analisi degli indici di bontà di adattamento rivela alcune preoccupazioni significative riguardo alla validità del nostro modello SEM. Il rapporto \\(\\chi^2 / df\\) emerge come eccessivamente elevato, segnalando una possibile mancanza di adattamento:\n\n449.141 / 26\n\n17.2746538461538\n\n\nAnalogamente, i valori di CFI e TLI sono inferiori al livello desiderato, suggerendo che il modello non rappresenta adeguatamente la struttura dei dati. In aggiunta, gli indici RMSEA e SRMR superano le soglie accettabili, indicando ulteriormente un’inadeguata aderenza del modello ai dati.\nDi fronte a questi risultati, è imprudente accettare la conclusione precedentemente formulata secondo cui l’autocompassione agisce come un fattore protettivo contro il malessere psicologico. Questa interpretazione, benché teoricamente fondata, non trova un solido supporto empirico nel contesto del modello attuale.\nIn questa situazione, un percorso costruttivo potrebbe essere quello di rivedere e potenzialmente modificare il modello. L’obiettivo sarebbe quello di esplorare alternative che potrebbero risultare in un migliore adattamento ai dati, mantenendo al contempo l’adeguatezza teorica. Ciò potrebbe includere la revisione delle assunzioni del modello, la riconsiderazione delle variabili incluse o la ristrutturazione delle relazioni ipotizzate tra di esse. Solo attraverso un modello che dimostra una bontà di adattamento adeguata possiamo affermare con maggiore sicurezza che i dati empirici sostengono l’ipotesi dell’effetto protettivo dell’autocompassione sul malessere psicologico.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#potere-statistico-e-precisione",
    "href": "chapters/sem/03_gof.html#potere-statistico-e-precisione",
    "title": "47  Test del Modello e Indicizzazione",
    "section": "47.14 Potere Statistico e Precisione",
    "text": "47.14 Potere Statistico e Precisione\nNell’ambito dei modelli di Structural Equation Modeling (SEM), l’analisi della potenza statistica è fondamentale per garantire l’affidabilità e la validità dei risultati. Esistono due approcci principali per quest’analisi: la potenza a priori (prospettica) e la potenza retrospettiva (post hoc, osservata).\n\nPotenza a priori (Prospettica): Questa analisi viene effettuata prima della raccolta dei dati e mira a stimare la probabilità che uno studio identifichi un effetto significativo, se presente nella popolazione. È cruciale nella pianificazione della ricerca per determinare la dimensione del campione necessaria, aumentando così l’efficienza dello studio e prevenendo l’uso di campioni eccessivamente grandi o inadeguati. In SEM, la potenza a priori si stima specificando nel software le caratteristiche del modello di popolazione, ipotesi nulle e alternative, il livello di significatività statistica e la dimensione campionaria prevista.\nPotenza Retrospettiva (Post Hoc, Osservata): A differenza dell’analisi a priori, questa viene condotta dopo la raccolta dei dati. Le statistiche campionarie vengono trattate come parametri reali della popolazione, ma questa pratica presenta limitazioni significative. Le stime possono essere distorte, e una maggiore potenza osservata non implica necessariamente una forte evidenza a favore delle ipotesi nulle non rifiutate. Inoltre, essendo una misura post hoc, non aiuta nella progettazione proattiva della ricerca.\n\nPer l’analisi della potenza in SEM, sono stati sviluppati diversi metodi, tra cui:\n\nIl metodo Satorra–Saris stima la potenza del test del rapporto di verosimiglianza per un singolo parametro.\nIl metodo MacCallum–RMSEA si basa sulla RMSEA di popolazione e sulle distribuzioni chi-quadrato non centrali.\nIl metodo di simulazione Monte Carlo è un’alternativa moderna e flessibile che non presuppone né risultati continui né stima ML predefinita.\n\nCon l’avanzamento degli strumenti informatici, l’analisi della potenza statistica in SEM è diventata più accessibile:\nSoftware SEM con Simulazione Monte Carlo: Software come Mplus e LISREL includono capacità di simulazione Monte Carlo, permettendo di generare dati campionari basati su ipotesi del modello e di valutare la frequenza con cui i risultati significativi vengono ottenuti.\nMetodo Kelley–Lai Precision: Calcola la dimensione campionaria minima necessaria per stimare parametri come l’indice RMSEA entro un margine di errore specificato.\nNel contesto di R, le funzioni semTools::findRMSEApower e semTools::findRMSEAsamplesize del pacchetto semTools facilitano queste analisi:\n\nsemTools::findRMSEApower: Determina la potenza di un test SEM data una dimensione specifica del campione, basandosi sull’RMSEA e altri parametri del test.\nsemTools::findRMSEAsamplesize: Calcola la dimensione del campione necessaria per raggiungere una specifica potenza statistica in un test SEM, considerando l’RMSEA e altri criteri come il livello di significatività e la potenza desiderata.\n\nQuesti strumenti sono importanti per ottimizzare la progettazione della ricerca SEM, garantendo campioni adeguati e potenza statistica sufficiente per rilevare gli effetti di interesse.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#riflessioni-conclusive",
    "href": "chapters/sem/03_gof.html#riflessioni-conclusive",
    "title": "47  Test del Modello e Indicizzazione",
    "section": "47.15 Riflessioni Conclusive",
    "text": "47.15 Riflessioni Conclusive\nNella letteratura SEM sono state sollevate forti argomentazioni contro l’applicazione di RMSEA, CFI e TLI e i loro valori di cutoff convenzionali (si veda, ad esempio, Barrett, 2007). Tuttavia, prima che i ricercatori propongano e accettino alternative migliori, questi indici di bontà dell’adattamento continueranno ad essere applicati nella maggior parte degli studi SEM. Xia & Yang (2019) fanno notare come, in base alla consuetudine corrente, valori RMSEA più grandi e valori CFI e TLI più piccoli indicano un adattamento peggiore. Ciò spinge i ricercatori a modificare i loro modelli per cercare di ottenere indici migliori. Tuttavia, la pratica attuale si è evoluta a tal punto da raggiungere la fase per cui gli indici di adattamento servono come gli unici criteri (in molte situazioni) per determinare se accettare o rifiutare un modello ipotizzato: se i valori degli indici di adattamento raggiungono la soglia “di pubblicabilità” (ad es. RMSEA &lt; .06), allora non si ritiene più necessario migliorare il modello. In realtà, un’affermazione come la seguente non è sufficiente: “poiché i valori RMSEA, CFI e TLI suggeriscono un buon adattamento, questo modello è stato scelto come modello finale”. Il raggiungimento di una serie di soglie desiderate di RMSEA, CFI e TLI è solo uno dei possibili indicatori che devono essere considerati nel processo di selezione di modelli. I ricercatori dovrebbero anche spiegare se esistono altre opzioni per migliorare il modello, perché tali opzioni sono o non sono adottate, e quali sono le conseguenze scientifiche e cliniche che derivano dalla scelta del modello in questione come quello finale.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/03_gof.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/03_gof.html#informazioni-sullambiente-di-sviluppo",
    "title": "47  Test del Modello e Indicizzazione",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] effectsize_0.8.9   rsvg_2.6.1         DiagrammeRsvg_0.1 \n [4] lme4_1.1-35.5      Matrix_1.7-1       mvnormalTest_1.0.0\n [7] lavaanPlot_0.8.1   lavaanExtra_0.2.1  MASS_7.3-61       \n[10] viridis_0.6.5      viridisLite_0.4.2  ggpubr_0.6.0      \n[13] ggExtra_0.10.1     gridExtra_2.3      patchwork_1.3.0   \n[16] bayesplot_1.11.1   semTools_0.5-6     semPlot_1.1.6     \n[19] lavaan_0.6-19      psych_2.4.6.26     scales_1.3.0      \n[22] markdown_1.13      knitr_1.49         lubridate_1.9.3   \n[25] forcats_1.0.0      stringr_1.5.1      dplyr_1.1.4       \n[28] purrr_1.0.2        readr_2.1.5        tidyr_1.3.1       \n[31] tibble_3.2.1       ggplot2_3.5.1      tidyverse_2.0.0   \n[34] here_1.0.1        \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.2       later_1.3.2         pbdZMQ_0.3-13      \n  [4] datawizard_0.13.0   XML_3.99-0.17       rpart_4.1.23       \n  [7] lifecycle_1.0.4     rstatix_0.7.2       rprojroot_2.0.4    \n [10] lattice_0.22-6      insight_0.20.5      rockchalk_1.8.157  \n [13] backports_1.5.0     magrittr_2.0.3      openxlsx_4.2.7.1   \n [16] Hmisc_5.2-0         rmarkdown_2.29      httpuv_1.6.15      \n [19] qgraph_1.9.8        zip_2.3.1           pbapply_1.7-2      \n [22] minqa_1.2.8         RColorBrewer_1.1-3  ADGofTest_0.3      \n [25] multcomp_1.4-26     abind_1.4-8         quadprog_1.5-8     \n [28] pspline_1.0-20      nnet_7.3-19         TH.data_1.1-2      \n [31] sandwich_3.1-1      moments_0.14.1      nortest_1.0-4      \n [34] openintro_2.5.0     arm_1.14-4          airports_0.1.0     \n [37] codetools_0.2-20    tidyselect_1.2.1    farver_2.1.2       \n [40] stats4_4.4.2        base64enc_0.1-3     jsonlite_1.8.9     \n [43] Formula_1.2-5       survival_3.7-0      emmeans_1.10.5     \n [46] tools_4.4.2         Rcpp_1.0.13-1       glue_1.8.0         \n [49] mnormt_2.1.1        xfun_0.49           IRdisplay_1.1      \n [52] numDeriv_2016.8-1.1 withr_3.0.2         fastmap_1.2.0      \n [55] boot_1.3-31         fansi_1.0.6         digest_0.6.37      \n [58] mi_1.1              timechange_0.3.0    R6_2.5.1           \n [61] mime_0.12           estimability_1.5.1  colorspace_2.1-1   \n [64] Cairo_1.6-2         gtools_3.9.5        jpeg_0.1-10        \n [67] copula_1.1-4        DiagrammeR_1.0.11   utf8_1.2.4         \n [70] generics_0.1.3      data.table_1.16.2   corpcor_1.6.10     \n [73] usdata_0.3.1        htmlwidgets_1.6.4   parameters_0.23.0  \n [76] pkgconfig_2.0.3     sem_3.1-16          gtable_0.3.6       \n [79] pcaPP_2.0-5         htmltools_0.5.8.1   carData_3.0-5      \n [82] png_0.1-8           rstudioapi_0.17.1   tzdb_0.4.0         \n [85] reshape2_1.4.4      uuid_1.2-1          curl_6.0.0         \n [88] coda_0.19-4.1       visNetwork_2.1.2    checkmate_2.3.2    \n [91] nlme_3.1-166        nloptr_2.1.1        repr_1.1.7         \n [94] zoo_1.8-12          parallel_4.4.2      miniUI_0.1.1.1     \n [97] foreign_0.8-87      pillar_1.9.0        grid_4.4.2         \n[100] vctrs_0.6.5         promises_1.3.0      car_3.1-3          \n[103] OpenMx_2.21.13      xtable_1.8-4        cluster_2.1.6      \n[106] htmlTable_2.4.3     evaluate_1.0.1      pbivnorm_0.6.0     \n[109] mvtnorm_1.3-2       cli_3.6.3           kutils_1.73        \n[112] compiler_4.4.2      rlang_1.1.4         crayon_1.5.3       \n[115] ggsignif_0.6.4      fdrtool_1.2.18      plyr_1.8.9         \n[118] stringi_1.8.4       munsell_0.5.1       gsl_2.1-8          \n[121] lisrelToR_0.3       bayestestR_0.15.0   V8_6.0.0           \n[124] pacman_0.5.1        IRkernel_1.3.2      hms_1.1.3          \n[127] stabledist_0.7-2    glasso_1.11         shiny_1.9.1        \n[130] igraph_2.1.1        broom_1.0.7         RcppParallel_5.1.9 \n[133] cherryblossom_0.1.0\n\n\n\n\n\n\nBarrett, P. (2007). Structural equation modelling: Adjudging model fit. Personality and Individual Differences, 42(5), 815–824.\n\n\nHayduk, L. A. (2014). Shame for disrespecting evidence: The personal consequences of insufficient respect for structural equation model testing. BMC Medical Research Methodology, 14, 1–10.\n\n\nHu, L., & Bentler, P. M. (1998). Fit indices in covariance structure modeling: Sensitivity to underparameterized model misspecification. Psychological Methods, 3(4), 424--453.\n\n\nKline, R. B. (2023). Principles and practice of structural equation modeling. Guilford publications.\n\n\nXia, Y., & Yang, Y. (2019). RMSEA, CFI, and TLI in structural equation modeling with ordered categorical data: The story they tell depends on the estimation methods. Behavior Research Methods, 51(1), 409–428.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Test del Modello e Indicizzazione</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html",
    "href": "chapters/sem/04_mod_comp.html",
    "title": "48  Confronto tra modelli",
    "section": "",
    "text": "48.1 Introduzione\nI ricercatori spesso confrontano modelli alternativi di equazioni strutturali che includono le stesse variabili e sono adattati agli stessi dati. Il contesto più frequente si verifica quando un singolo modello iniziale viene testato attraverso una serie di passaggi. Ad ogni passaggio, il modello iniziale viene ridefinito aggiungendo uno o più parametri liberi, il che generalmente migliora l’adattamento, oppure eliminando (fissando a zero) uno o più parametri liberi, il che generalmente peggiora l’adattamento. Una coppia di modelli alternativi così specificata viene definita “modelli nidificati”, poiché il modello più semplice dei due, o modello vincolato, è un sottoinsieme proprio del modello più complesso, o modello non vincolato. Un contesto diverso si verifica quando ci sono due o più modelli iniziali tali che (1) ogni modello si basa su una teoria diversa e (2) i modelli alternativi non sono nidificati nella loro relazione l’uno con l’altro. In entrambi i contesti, la scelta tra modelli concorrenti dovrebbe essere guidata tanto da basi concettuali quanto da considerazioni statistiche.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#confrontare-modelli-nel-sem",
    "href": "chapters/sem/04_mod_comp.html#confrontare-modelli-nel-sem",
    "title": "48  Confronto tra modelli",
    "section": "48.2 Confrontare Modelli nel SEM",
    "text": "48.2 Confrontare Modelli nel SEM\nNel contesto dei Modelli di Equazioni Strutturali (SEM), un aspetto critico è il confronto tra diversi modelli per determinare quale sia il più adeguato. Questo confronto si presenta frequentemente nella forma di analisi di modelli nidificati. In tale contesto, si confronta un modello considerato “pieno” o “meno restrittivo” con un altro modello che è “ridotto” o “più restrittivo”.\nIl modello pieno include un insieme più ampio di parametri e ipotesi, offrendo una rappresentazione più complessa delle relazioni tra le variabili. Al contrario, il modello ridotto è una versione più semplificata, con meno parametri e ipotesi, risultando in una struttura più contenuta e potenzialmente più parsimoniosa.\nQuesto tipo di confronto è cruciale per valutare l’adeguatezza dei modelli SEM, permettendo ai ricercatori di decidere se la complessità aggiuntiva del modello pieno sia giustificata rispetto al modello ridotto in termini di adattamento ai dati e coerenza con la teoria sottostante.\n\n48.2.1 Analisi dei Modelli Nidificati\nNell’ambito dei Modelli di Equazioni Strutturali (SEM), i modelli nidificati occupano un ruolo centrale. Due modelli sono definiti come nidificati quando soddisfano specifici criteri gerarchici, delineati come segue:\n\nFormazione del Modello Vincolato:\n\nSi crea un modello vincolato applicando una o più restrizioni a un modello non vincolato esistente. Questo processo aumenta i gradi di libertà del modello vincolato (C) rispetto a quelli del modello non vincolato (U), risultando in \\(\\text{df}_C &gt; \\text{df}_U\\).\n\nDifferenze nei Gradi di Libertà:\n\nLa differenza \\(\\text{df}_C - \\text{df}_U\\) rappresenta il numero di restrizioni imposte al modello non vincolato per creare il modello vincolato, che equivale alla variazione nel numero di parametri liberi tra i due modelli.\n\nParametri Liberi e Vincolati:\n\nI parametri liberi nel modello vincolato costituiscono un sottoinsieme di quelli presenti nel modello non vincolato. Allo stesso modo, i parametri fissi nel modello non vincolato formano un sottoinsieme di quelli nel modello vincolato.\n\nConfronto dei Valori di Chi-Quadro:\n\nTra i due modelli, il valore di \\(\\chi^2\\) è minore o uguale nel modello non vincolato rispetto a quello nel modello vincolato, ovvero \\(\\chi^2_U \\leq \\chi^2_C\\). Questo implica che le distribuzioni di probabilità possibili nel modello vincolato sono comprese anche nel modello non vincolato, che può tuttavia suggerire ulteriori distribuzioni non coerenti con il modello vincolato.\n\n\nQuesto tipo di relazione gerarchica, conosciuta come annidamento dei parametri, permette di valutare l’impatto di specifiche restrizioni o aggiunte di parametri. Ad esempio, un parametro libero in un modello non vincolato può essere fissato a zero, eliminando di conseguenza l’effetto corrispondente nel modello vincolato, oppure può essere sottoposto a un vincolo specificato dal ricercatore, riducendo così il numero di parametri liberi ma mantenendo l’effetto nel modello vincolato.\nConsideriamo, per esempio, un modello di percorso non vincolato U con effetti diretti: \\(X \\rightarrow Y_1 \\rightarrow Y_2\\) e \\(X \\rightarrow Y_2\\). Ridefinendo il percorso \\(X \\rightarrow Y_2 = 0\\), eliminiamo questa connessione dal modello U, generando così il modello vincolato C1, nidificato sotto U. Un’alternativa potrebbe essere imporre un vincolo di uguaglianza tra \\(X\\rightarrow Y_2\\) e \\(Y_1 \\rightarrow Y_2\\), indicando che gli effetti diretti non standardizzati di \\(X\\) e \\(Y_1\\) su \\(Y_2\\) sono identici. Questo produce un modello vincolato, C2, con un parametro libero in meno rispetto a U ma che include tutti i percorsi di U.\nNelle prossime sezioni, esamineremo come testare le ipotesi inerenti a questi modelli nidificati e come valutare la loro adeguatezza nel contesto SEM.\n\n\n48.2.2 Strategie di Costruzione e Potatura nei Modelli SEM\n\n48.2.2.1 Costruzione Progressiva del Modello\nLa costruzione di modelli SEM inizia tipicamente con un modello iniziale semplice e vincolato, che riflette le ipotesi fondamentali basate su teorie sostanziali. Questo approccio, detto anche ricerca in avanti, implica l’aggiunta progressiva di parametri liberi che rappresentano ipotesi precedentemente escluse, in base alla loro importanza. Sebbene questo processo possa generalmente migliorare l’adattamento del modello (riduzione di chiML), un adattamento migliore non è necessariamente indicativo di una maggiore correttezza del modello. La costruzione del modello può teoricamente proseguire fino a quando non si raggiunge un modello perfettamente adatto ai dati (dfM = 0), ma è essenziale valutare la validità teorica e la parsimonia del modello in ogni passaggio.\n\n\n48.2.2.2 Potatura Retrograda del Modello\nIn contrasto, la potatura del modello, o ricerca all’indietro, inizia con un modello più complesso e non vincolato. In questa fase, il ricercatore semplifica il modello eliminando parametri liberi (fissandoli a zero) o imponendo vincoli di stima. Questo processo richiede di dare priorità alle ipotesi in ordine inverso di importanza. Il modello iniziale dovrebbe essere congruente con i dati, altrimenti non ha senso restringerlo ulteriormente. Tipicamente, come si procede con la potatura, l’adattamento complessivo del modello ai dati tende a peggiorare (aumento di chiML). Il criterio per arrestare la potatura si basa sull’adattamento del modello: si ferma quando ulteriori restrizioni peggiorerebbero significativamente l’adattamento ai dati.\n\n\n48.2.2.3 Obiettivi e Considerazioni\nL’obiettivo sia nella costruzione che nella potatura di modelli è identificare un modello con una struttura di covarianza (e, se presente, anche di media) correttamente specificata e teoricamente giustificata. Idealmente, entrambi gli approcci dovrebbero convergere verso lo stesso modello ottimale, benché ciò non sia garantito. È importante evitare il rischio di formulare ipotesi post hoc (HARKing), presentando modelli scoperti in modo esplorativo come se fossero stati ipotizzati a priori. Una soluzione a questo problema è la preregistrazione del piano di analisi.\n\n\n48.2.2.4 Punti di Forza Relativi\n\nCostruzione del Modello: Partire da un modello più semplice può essere vantaggioso, soprattutto per i neofiti del SEM, poiché facilita l’identificazione statistica e riduce il rischio di errori nella specificazione del modello.\nPotatura del Modello: Questo approccio può essere particolarmente efficace per i modelli di misurazione, dove le variabili osservate sono usate come indicatori di un numero limitato di fattori comuni. Un modello di misurazione correttamente specificato inizialmente può rendere la potatura più efficace rispetto alla costruzione.\n\nIn entrambi i casi, è cruciale basare le decisioni su solide basi teoriche oltre che su considerazioni statistiche, per assicurare che il modello finale sia non solo adatto ai dati ma anche coerente con il quadro teorico sottostante.\n\n\n\n48.2.3 Strategie di Ridefinizione dei Modelli SEM: Approcci Teorici ed Empirici\n\n48.2.3.1 Approccio Teorico nella Ridefinizione dei Modelli\nNel processo di costruzione o potatura dei Modelli di Equazioni Strutturali (SEM), l’approccio teorico gioca un ruolo fondamentale. Qui, le modifiche al modello sono guidate da ipotesi teoriche predefinite e specifiche. Ad esempio, considerando un modello di percorso non vincolato U con le relazioni \\(X \\rightarrow Y_1 \\rightarrow Y_2\\) e \\(X \\rightarrow Y_2\\), un ricercatore potrebbe ipotizzare che l’effetto di X su Y2 sia esclusivamente indiretto attraverso Y1. Questa ipotesi può essere testata vincolando il coefficiente di \\(X \\rightarrow Y_2\\) a zero. Se l’adattamento del modello così modificato non è significativamente inferiore rispetto al modello non vincolato, l’ipotesi di un effetto indiretto viene supportata, a patto che la direzionalità delle relazioni sia corretta.\nQuesto approccio enfatizza che le modifiche al modello dovrebbero essere effettuate sulla base di solide basi teoriche e concettuali, piuttosto che su criteri puramente statistici, come evidenziato da Jöreskog (1969): “La decisione di smettere di aggiungere parametri non può basarsi solo su una base statistica; ciò dipende in gran parte dall’interpretazione dei dati da parte del ricercatore, basata su considerazioni teoriche e concettuali sostanziali.”\n\n\n48.2.3.2 Approccio Empirico nella Ridefinizione dei Modelli\nAl contrario, l’approccio empirico nella costruzione o potatura dei modelli SEM si basa su criteri statistici. In questo scenario, i parametri liberi vengono aggiunti o eliminati a seconda della loro significatività statistica o di altri indicatori empirici. Per esempio, se i percorsi sono eliminati solo perché i loro coefficienti non sono statisticamente significativi, la ridefinizione del modello è guidata da considerazioni puramente empiriche. Questo approccio è analogo alla tecnica di eliminazione all’indietro nella regressione multipla, dove il software sceglie quali predittori rimuovere in base a criteri di significatività statistica.\n\n\n48.2.3.3 Implicazioni per l’Interpretazione dei Modelli\nLa scelta tra un approccio teorico o empirico nella ridefinizione dei modelli SEM ha implicazioni significative per come interpretiamo i risultati. Un modello modificato in base a criteri teorici forti offre una maggiore fiducia nella validità delle sue conclusioni, mentre un modello costruito o potato basandosi principalmente su criteri empirici può essere soggetto a errori di Tipo I o II e può non essere replicabile in campioni diversi.\nÈ fondamentale che i ricercatori si avvicinino alla costruzione e alla potatura dei modelli SEM con un equilibrio tra intuizioni teoriche e risultati empirici, per assicurare che i modelli finali siano non solo statisticamente validi ma anche teoricamente giustificati e interpretativamente significativi.\n\n\n\n48.2.4 Test della Differenza Chi-Quadro nel SEM\n\n48.2.4.1 Principi Fondamentali\nIl test della differenza Chi-Quadro (chiD) è una tecnica statistica essenziale nel contesto dei Modelli di Equazioni Strutturali (SEM) per valutare l’effetto delle modifiche ai parametri sui modelli. Questo test viene utilizzato sia nella potatura (restrizione dei parametri) che nella costruzione (aggiunta di parametri) dei modelli. Il valore chiD rappresenta la differenza tra i valori di chi-quadro (chi-quadro massima verosimiglianza, chiML) di due modelli nidificati. I gradi di libertà associati, dfD, sono determinati dalla differenza nei gradi di libertà dei due modelli (\\(\\text{df}_C - \\text{df}_U\\)).\n\n\n48.2.4.2 Applicazione del Test\nPer applicare il test della differenza chi-quadro, si seguono questi passaggi: 1. Definizione dei Modelli: Identificare il modello pieno (con tutti i parametri ritenuti rilevanti) e il modello ridotto (una versione semplificata del modello pieno con alcune restrizioni). 2. Stima dei Modelli: Utilizzare metodi di massima verosimiglianza per stimare entrambi i modelli. 3. Calcolo del Rapporto di Verosimiglianze: Calcolare la differenza tra i logaritmi delle funzioni di verosimiglianza dei due modelli (\\(D = -2(\\ln(L_r) - \\ln(L_f))\\)). 4. Test Statistico: Utilizzare la distribuzione chi-quadrato per determinare il p-value. Un p-value basso indica che il modello ridotto non si adatta ai dati così come il modello pieno.\n\n\n48.2.4.3 Interpretazione dei Risultati\nUn valore piccolo di chiD suggerisce che non c’è una differenza significativa nell’adattamento tra i due modelli, mentre un valore grande indica una differenza significativa. In termini di potatura, un grande chiD implica che il modello è stato eccessivamente vincolato. Nella costruzione, un grande chiD supporta la conservazione del parametro libero aggiunto. Tuttavia, prima di trarre conclusioni definitive, è cruciale considerare l’adattamento complessivo del modello, sia a livello globale che locale.\n\n\n48.2.4.4 Considerazioni nella Stima ML Robusta\nNel caso di stima ML robusta, la differenza tra i chi-quadri scalati non può essere interpretata come un test dell’ipotesi di adattamento uguale a causa delle distribuzioni non centrali in condizioni di non normalità. Sono disponibili metodi specifici per calcolare una statistica di differenza chi-quadro scalata che segue approssimativamente le distribuzioni chi-quadro.\n\n\n48.2.4.5 Implicazioni Teoriche ed Empiriche\nLa decisione di modificare un modello basandosi su un approccio teorico o empirico ha implicazioni significative. Ad esempio, l’eliminazione di percorsi non significativi su base puramente statistica può portare a conclusioni errate se non supportate da una solida base teorica. Inoltre, è importante essere consapevoli dei rischi associati alla capitalizzazione sul caso, come errori di Tipo I e II, e del pericolo di seguire “sentieri che si biforcano” nelle decisioni analitiche, che possono rendere i risultati specifici del campione e difficili da replicare. Per mitigare questi rischi, è consigliabile basare le modifiche del modello più su orientamenti teorici che sui risultati dei test di significatività.\n\n\n\n48.2.5 Test della Differenza Chi-Quadro Scalato\nIl metodo di Satorra e Bentler (2001) permette di calcolare manualmente una statistica di differenza chi-quadro scalata quando si confrontano due modelli gerarchici nella stima ML robusta. Si presume che il modello 1 sia più vincolato rispetto al modello 2 (cioè, dfM1 &gt; dfM2), i chi-quadri non scalati siano chiML e i chi-quadri scalati siano chiSB. La statistica di test Satorra-Bentler è definita come segue:\n\nCalcolare la Statistica di Differenza Chi-Quadro non Scalata e i suoi Gradi di Libertà:\n\n\nchiD = chiML1 - chiML2 e dfD = dfM1 - dfM2.\n\n\nRecuperare il Fattore di Correzione di Scala, c, per Ogni Modello:\n\n\nc1 = chiML1 / chiSB1 e c2 = chiML2 / chiSB2.\n\n\nCalcolare la Statistica di Differenza Chi-Quadro Scalata, chiSD:\n\n\nchiSD = chiD / ((c1 / dfM1 - c2 / dfM2) / dfD).\nLa probabilità per chiSD (dfD) in una distribuzione chi-quadro centrale rappresenta il p-value per il test di differenza chi-quadro scalato.\n\nIn campioni piccoli o quando il modello più vincolato è molto errato, il denominatore di chiSD può essere &lt; 0, invalidando il test. Questo test è implementato nella funzione lavTestLRT() in lavaan (Rosseel et al., 2023).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#esempio",
    "href": "chapters/sem/04_mod_comp.html#esempio",
    "title": "48  Confronto tra modelli",
    "section": "48.3 Esempio",
    "text": "48.3 Esempio\nConsideriamo nuovamente i dati discussi da {cite:t}brown2015confirmatory relativi al modello di misurazione per la depressione maggiore così come è definita nel DSM-IV. Ignoriamo qui le differenze di genere – si veda il Capitolo {ref}factorial-invariance-notebook. Leggiamo i dati in \\(\\mathsf{R}\\):\n\nd_mdd &lt;- readRDS(here::here(\"data\", \"mdd_sex.RDS\"))\n\nConsideriamo il seguente modello:\n\nmodel_mdd &lt;- \"\n  MDD =~ mdd1 + mdd2 + mdd3 + mdd4 + mdd5 + mdd6 + mdd7 + mdd8 + mdd9\n\"\n\nAdattiamo il modello ai dati.\n\nfit_mdd &lt;- cfa(\n    model_mdd,\n    data = d_mdd\n)\n\n\nsemPaths(fit_mdd,\n    whatLabels = \"std\",\n    sizeMan = 8,\n    edge.label.cex = 0.7,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\n\nfitMeasures(fit_mdd) |&gt;\n    print()\n\n                 npar                  fmin                 chisq \n             1.80e+01              7.40e-02              1.10e+02 \n                   df                pvalue        baseline.chisq \n             2.70e+01              0.00e+00              1.30e+03 \n          baseline.df       baseline.pvalue                   cfi \n             3.60e+01              0.00e+00              9.34e-01 \n                  tli                  nnfi                   rfi \n             9.12e-01              9.12e-01              8.87e-01 \n                  nfi                  pnfi                   ifi \n             9.15e-01              6.86e-01              9.34e-01 \n                  rni                  logl     unrestricted.logl \n             9.34e-01             -1.37e+04             -1.37e+04 \n                  aic                   bic                ntotal \n             2.75e+04              2.76e+04              7.50e+02 \n                 bic2                 rmsea        rmsea.ci.lower \n             2.76e+04              6.40e-02              5.20e-02 \n       rmsea.ci.upper        rmsea.ci.level          rmsea.pvalue \n             7.70e-02              9.00e-01              2.90e-02 \n       rmsea.close.h0 rmsea.notclose.pvalue     rmsea.notclose.h0 \n             5.00e-02              1.90e-02              8.00e-02 \n                  rmr            rmr_nomean                  srmr \n             1.91e-01              1.91e-01              4.40e-02 \n         srmr_bentler   srmr_bentler_nomean                  crmr \n             4.40e-02              4.40e-02              5.00e-02 \n          crmr_nomean            srmr_mplus     srmr_mplus_nomean \n             5.00e-02              4.40e-02              4.40e-02 \n                cn_05                 cn_01                   gfi \n             2.74e+02              3.20e+02              9.64e-01 \n                 agfi                  pgfi                   mfi \n             9.40e-01              5.78e-01              9.46e-01 \n                 ecvi \n             1.95e-01 \n\n\nGli indici Comparative Fit Index (CFI) = 0.934 e Tucker-Lewis Index (TLI) = 0.912 sono superiori a 0.9, dunque sono almeno sufficienti per gli standard correnti. L’indice RMSEA = 0.064 è appena superiore alla soglia di 0.06. L’indice SRMR = 0.044 è inferiore alla soglia 0.05. Dunque, complessivamente, il modello sembra adeguato.\nAdattiamo ora il modello con la modifica proposta da {cite:t}brown2015confirmatory, ovvero\n\nmodel2_mdd &lt;- \"\n  MDD =~ mdd1 + mdd2 + mdd3 + mdd4 + mdd5 + mdd6 + mdd7 + mdd8 +  mdd9\n  mdd1 ~~ mdd2\n\"\n\nfit2_mdd &lt;- cfa(\n    model2_mdd,\n    data = d_mdd\n)\n\nEseguiamo il test del rapporto di verosimiglianze:\n\nlavTestLRT(fit_mdd, fit2_mdd)\n\n\nA anova: 2 x 8\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nRMSEA\nDf diff\nPr(&gt;Chisq)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nfit2_mdd\n26\n27490\n27577\n67.6\nNA\nNA\nNA\nNA\n\n\nfit_mdd\n27\n27530\n27614\n110.3\n42.7\n0.236\n1\n6.34e-11\n\n\n\n\n\nIl test indica che il modello alternativo si adatta meglio ai dati del modello originale.\nEsaminiamo gli indici di bontà di adattamento.\n\neffectsize::interpret(fit2_mdd)\n\n\nA data.frame: 10 x 4\n\n\nName\nValue\nThreshold\nInterpretation\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;effctsz_&gt;\n\n\n\n\nGFI\n0.9781\n0.95\nsatisfactory\n\n\nAGFI\n0.9620\n0.90\nsatisfactory\n\n\nNFI\n0.9479\n0.90\nsatisfactory\n\n\nNNFI\n0.9544\n0.90\nsatisfactory\n\n\nCFI\n0.9671\n0.90\nsatisfactory\n\n\nRMSEA\n0.0462\n0.05\nsatisfactory\n\n\nSRMR\n0.0368\n0.08\nsatisfactory\n\n\nRFI\n0.9279\n0.90\nsatisfactory\n\n\nPNFI\n0.6846\n0.50\nsatisfactory\n\n\nIFI\n0.9673\n0.90\nsatisfactory\n\n\n\n\n\nGli indici Comparative Fit Index (CFI) = 0.967 e Tucker-Lewis Index (TLI) = 0.954 sono superiori a 0.95. L’indice RMSEA = 0.046. L’indice SRMR = 0.037.\nIl “costo” che si paga per questo miglioramento dell’adattamento è che indici di adattamento così buoni, probabilmente, non si replicheranno in un altro campione di dati, a meno che venga introdotto un qualche altro aggiustamento che, sicuramente, sarà diverso da quello usato nel campione corrente. Personalmente, non avrei introdotto il “miglioramento” proposto da {cite:t}brown2015confirmatory in quanto, anche senza un tale aggiustamento post-hoc, il modello produce un adattamento accettabile.\n\n48.3.1 Confronto di Modelli Non Annidati\nProseguendo, analizzeremo l’adattamento di due modelli distinti, entrambi costituiti dalle stesse variabili e applicati agli stessi dati. Tuttavia, a differenza di quanto precedentemente delineato, questi modelli non sono collegati gerarchicamente, ma si configurano come modelli non annidati. Questa situazione si presenta comunemente quando i ricercatori mettono a confronto modelli basati su teorie divergenti. È possibile effettuare un confronto informale dei valori del chi-quadrato derivanti da modelli non annidati, ma la loro differenza non va interpretata come una statistica di test valida. In altre parole, i test di differenza del chi-quadrato, sia in forma scalata che non, non sono appropriati in questo contesto. La ragione risiede nel fatto che la differenza tra le statistiche di test di modelli non annidati non segue una distribuzione chi-quadrato centrale. Sebbene siano stati compiuti sforzi per elaborare test di significatività adatti al confronto di modelli non annidati, questi metodi non hanno trovato un ampio utilizzo e spesso portano a complicazioni interpretative (Levy & Hancock, 2007).\nUna soluzione più pragmatica è rappresentata dalla famiglia degli indici di adattamento predittivo, conosciuti anche come criteri teorico-informativi. Questi indici non sono test di significatività, poiché le loro distribuzioni di probabilità variano ampiamente a seconda del tipo di modello e dei dati considerati e, pertanto, rimangono generalmente ignote. Piuttosto, essi riflettono sia la qualità dell’adattamento del modello sia la sua complessità, bilanciando questi due aspetti. Ciò implica l’applicazione di una penalità per la complessità del modello, che consente di regolare l’adattamento in funzione del numero di parametri liberi. Per esempio, nel caso di due modelli non annidati che mostrano un adattamento simile agli stessi dati, verrà privilegiato il modello meno complesso, in quanto considerato più probabile nella generalizzazione su campioni replicati. In questo scenario, il valore del criterio informativo sarà inferiore per il modello più semplice, dato che una penalità maggiore per la complessità viene applicata all’adattamento del modello più complesso. Di conseguenza, il modello con il criterio informativo più basso è da preferire. In questo capitolo, dopo aver introdotto un problema di ricerca, esploreremo due indici di adattamento predittivo ampiamente usati (AIC e BIC).\n\n\n\n```rfbnbjra ../images/romney.png\n\n\n\n\nheight: 550px\n\n\nname: romney-fig\n\n\n\nModelli alternativi non annidati di percorso ricorsivo per l’adattamento dopo un intervento chirurgico cardiaco. (Figura tratta da Kline (2023).)\n\nNella figura sono presentati due modelli di percorso che descrivono il recupero dei pazienti dopo un intervento chirurgico cardiaco (Romney et al., 1992) -- si veda {cite:t}`kline2023principles`, cap. 11. Il modello psicosomatico rappresenta le ipotesi che il morale del paziente trasmetta gli effetti della disfunzione neurologica e dello stato socioeconomico ridotto (SES) su sintomi della malattia e scarse relazioni sociali. Il modello medico rappresenta un diverso schema di relazioni causali tra le stesse variabili. In particolare, sia i sintomi della malattia sia la disfunzione neurologica sono specificati come variabili esogene con effetti diretti sul SES ridotto, basso morale e scarse relazioni. Tra queste tre variabili endogene, si ipotizza che il SES ridotto influenzi indirettamente le scarse relazioni attraverso il suo impatto precedente sul basso morale. Ci sono ulteriori effetti indiretti nel modello medico convenzionale dalle variabili esogene a quelle endogene. I due modelli nella figura non sono annidati, quindi il test della differenza del chi-quadro non può essere utilizzato per confrontarli direttamente. È dunque necessario seguire un altro approccio.\n\n## AIC e BIC\n\nUno degli indici di adattamento predittivo più noti basato sulla stima di massima verosimiglianza (ML) è il Criterio di Informazione di Akaike (AIC), che prende il nome dallo statistico Hirotugu Akaike. La formula per l'AIC di Akaike (1974, p. 719) è:\n\n$$ \\text{AIC} = -2 \\ln L_0 + 2q $$\n\ndove $ L_0 $ è la funzione di verosimiglianza massimizzata nella stima ML per il modello del ricercatore e $ q $ è il numero di parametri liberi del modello. Si noti che la penalità per la complessità nell'equazione precedente, $ 2q $, diventa relativamente più piccola all'aumentare della dimensione del campione (Mulaik, 2009b). \n\nUn diverso indice teorico-informativo che tiene direttamente conto della dimensione del campione è il Criterio di Informazione Bayesiano (BIC) (Raftery, 1993; Schwarz, 1978). La formula è\n\n$$ \\text{BIC} = -2 \\ln L_0 + q \\ln N $$\n\nConfrontato con l'AIC, il BIC impone una penalità relativa maggiore per la complessità del modello. \n\nSupponiamo che il numero di parametri stimati liberamente sia $ q = 10 $ e che $ N = 300 $. La penalità AIC equivale a $ 2(10) $, ovvero 20.000 (Equazione 11.4), ma la penalità BIC per lo stesso modello è $ 10 ($\\ln$ 300) $, ovvero 50.038, più del doppio rispetto all'AIC. I valori relativi delle penalità BIC aumentano più lentamente all'aumentare della dimensione del campione; in altre parole, la sua penalità è asintotica su campioni sempre più grandi (Mulaik, 2009b).\n\nIn sostanza, sia l'AIC che il BIC sono strumenti per bilanciare l'adattamento del modello con la sua complessità, ma differiscono nel modo in cui valutano e penalizzano questa complessità, soprattutto in relazione alla dimensione del campione.\n\nUtilizzando lo script fornito da {cite:t}`kline2023principles`, iniziamo a importare i dati in `R`:\n\n::: {#cell-23 .cell vscode='{\"languageId\":\"r\"}' execution_count=15}\n``` {.r .cell-code}\n# input the correlations in lower diagnonal form\nromneyLower.cor &lt;- \"\n 1.00\n  .53 1.00\n  .15  .18 1.00\n  .52  .29 -.05 1.00\n  .30  .34  .23  .09 1.00 \"\n\n# name the variables and convert to full correlation matrix\nromney.cor &lt;- lavaan::getCov(romneyLower.cor, names = c(\n    \"morale\", \"symptoms\",\n    \"dysfunction\", \"relations\", \"ses\"\n))\n:::\nEsaminiamo le matrici di correlazioni e covarianze:\n\n# display the correlations\nromney.cor\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nmorale\nsymptoms\ndysfunction\nrelations\nses\n\n\n\n\nmorale\n1.00\n0.53\n0.15\n0.52\n0.30\n\n\nsymptoms\n0.53\n1.00\n0.18\n0.29\n0.34\n\n\ndysfunction\n0.15\n0.18\n1.00\n-0.05\n0.23\n\n\nrelations\n0.52\n0.29\n-0.05\n1.00\n0.09\n\n\nses\n0.30\n0.34\n0.23\n0.09\n1.00\n\n\n\n\n\n\n# add the standard deviations and convert to covariances\nromney.cov &lt;- lavaan::cor2cov(romney.cor, sds = c(\n    3.75, 17.00, 19.50,\n    3.50, 24.70\n))\n\n# display the covariances\nromney.cov\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nmorale\nsymptoms\ndysfunction\nrelations\nses\n\n\n\n\nmorale\n14.06\n33.8\n10.97\n6.83\n27.79\n\n\nsymptoms\n33.79\n289.0\n59.67\n17.25\n142.77\n\n\ndysfunction\n10.97\n59.7\n380.25\n-3.41\n110.78\n\n\nrelations\n6.83\n17.3\n-3.41\n12.25\n7.78\n\n\nses\n27.79\n142.8\n110.78\n7.78\n610.09\n\n\n\n\n\nSpecifichiamo il modello psicosomatico.\n\nsomatic.model &lt;- \"\n    # regressions\n    morale ~ ses + dysfunction\n    relations ~ morale\n    symptoms ~ morale\n    # without the zero constraint listed next,\n    # lavaan automatically specifies correlated\n    # disturbances for symptoms and relations,\n    # but their disturbances are independent in\n    # figure 11.1\n    symptoms ~~ 0*relations\n    # unanalyzed association between ses and dysfunction\n    # automatically specified \n\"\n\nSpecifichiamo il modello medico convenzionale.\n\nmedical.model &lt;- \"\n    # regressions\n    ses ~ symptoms + dysfunction\n    morale ~ symptoms + ses\n    relations ~ dysfunction + morale\n    # unanalyzed association between symptoms and dysfunction\n    # automatically specified \n\"\n\nAdattiamo ai dati il modello psicosomatico.\n\nsomatic &lt;- lavaan::sem(somatic.model,\n    sample.cov = romney.cov,\n    sample.nobs = 469, fixed.x = FALSE, sample.cov.rescale = FALSE\n)\n\n\nsemPaths(somatic,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 1.0,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\n\nlavaan::summary(somatic, fit.measures = TRUE, rsquare = TRUE) |&gt; \n    print()\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n  Number of observations                           469\n\nModel Test User Model:\n                                                      \n  Test statistic                                40.488\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               390.816\n  Degrees of freedom                                 9\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.907\n  Tucker-Lewis Index (TLI)                       0.833\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -8572.844\n  Loglikelihood unrestricted model (H1)      -8552.599\n                                                      \n  Akaike (AIC)                               17165.687\n  Bayesian (BIC)                             17207.193\n  Sample-size adjusted Bayesian (SABIC)      17175.455\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.123\n  90 Percent confidence interval - lower         0.090\n  90 Percent confidence interval - upper         0.159\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.982\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.065\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  morale ~                                            \n    ses               0.043    0.007    6.217    0.000\n    dysfunction       0.016    0.009    1.897    0.058\n  relations ~                                         \n    morale            0.485    0.037   13.184    0.000\n  symptoms ~                                          \n    morale            2.403    0.178   13.535    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .relations ~~                                        \n   .symptoms          0.000                           \n  ses ~~                                              \n    dysfunction     110.779   22.821    4.854    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .morale           12.699    0.829   15.313    0.000\n   .relations         8.938    0.584   15.313    0.000\n   .symptoms        207.820   13.571   15.313    0.000\n    ses             610.090   39.840   15.313    0.000\n    dysfunction     380.250   24.831   15.313    0.000\n\nR-Square:\n                   Estimate\n    morale            0.097\n    relations         0.270\n    symptoms          0.281\n\n\n\nOra approfondiamo l’analisi dei residui. Nel contesto di un modello SEM, i residui sono derivati dalle differenze tra la matrice di correlazioni (o covarianze) osservata e quella prevista dal modello. Queste differenze sono elaborate attraverso specifiche funzioni per generare i residui. Utilizzando il pacchetto lavaan in R, possiamo accedere a tre principali tipi di residui: standardized.mplus, normalized, e cor.bollen.\n\nResidui Standardizzati (Standardized.mplus): Questo tipo di residuo è una versione standardizzata dei residui. I residui standardizzati sono ottenuti calcolando la differenza tra i valori osservati e quelli previsti dal modello, e dividendo questa differenza per uno stimatore della deviazione standard del residuo. Questo processo trasforma i residui in una scala in cui hanno una varianza approssimativamente uguale. I residui standardizzati sono utili per identificare punti dati che il modello non riesce a spiegare bene. In lavaan, type = \"standardized.mplus\" si riferisce a una particolare forma di standardizzazione dei residui, simile a quella utilizzata nel software Mplus.\nResidui Normalizzati (Normalized): I residui normalizzati sono un altro tipo di residui standardizzati. Sono calcolati come i residui standardizzati ma poi vengono normalizzati. La normalizzazione qui significa che i residui vengono ulteriormente trasformati in modo che la loro distribuzione si avvicini a una distribuzione normale. Questo è utile per verificare se i residui seguono una distribuzione normale, il che è un’assunzione comune in molti modelli statistici, inclusi quelli SEM.\nCorrelazione dei Residui secondo Bollen (Cor.bollen): Questo tipo di residuo si riferisce alla correlazione tra i residui di due variabili diverse nel modello. Il metodo cor.bollen calcola la correlazione tra i residui dopo che il modello è stato adattato ai dati. Questo tipo di analisi è utile per rilevare se ci sono correlazioni non modellate tra le variabili che potrebbero influenzare la validità del modello.\n\n\nlavaan::residuals(somatic, type = \"standardized.mplus\") |&gt; print()\n\n$type\n[1] \"standardized.mplus\"\n\n$cov\n            morale reltns symptm    ses dysfnc\nmorale       0.000                            \nrelations    0.000  0.000                     \nsymptoms     0.000  0.427  0.000              \nses          0.000 -1.776  4.542  0.000       \ndysfunction  0.000 -3.291  2.549  0.000  0.000\n\n\n\n\nlavaan::residuals(somatic, type = \"normalized\") |&gt; print()\n\n$type\n[1] \"normalized\"\n\n$cov\n            morale reltns symptm   ses dysfnc\nmorale        0.00                           \nrelations     0.00   0.00                    \nsymptoms      0.00   0.30   0.00             \nses           0.00  -1.42   3.71  0.00       \ndysfunction   0.00  -2.77   2.14  0.00   0.00\n\n\n\n\nlavaan::residuals(somatic, type = \"cor.bollen\")\n\n\n    $type\n        'cor.bollen'\n    $cov\n        \n\nA lavaan.matrix.symmetric: 5 x 5 of type dbl\n\n\n\nmorale\nrelations\nsymptoms\nses\ndysfunction\n\n\n\n\nmorale\n0.00e+00\n-1.11e-16\n0.0000\n0.000\n0.000\n\n\nrelations\n-1.11e-16\n-3.33e-16\n0.0144\n-0.066\n-0.128\n\n\nsymptoms\n0.00e+00\n1.44e-02\n0.0000\n0.181\n0.100\n\n\nses\n0.00e+00\n-6.60e-02\n0.1810\n0.000\n0.000\n\n\ndysfunction\n0.00e+00\n-1.28e-01\n0.1005\n0.000\n0.000\n\n\n\n\n\n\n\n\nAdattiamo ai dati il modello medico convenzionale.\n\nmedical &lt;- lavaan::sem(medical.model,\n    sample.cov = romney.cov,\n    sample.nobs = 469, fixed.x = FALSE, sample.cov.rescale = FALSE\n)\n\n\nsemPaths(medical,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 1.15,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\n\nlavaan::summary(medical, fit.measures = TRUE, rsquare = TRUE) |&gt; \n    print()\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n\n  Number of observations                           469\n\nModel Test User Model:\n                                                      \n  Test statistic                                 3.245\n  Degrees of freedom                                 3\n  P-value (Chi-square)                           0.355\n\nModel Test Baseline Model:\n\n  Test statistic                               400.859\n  Degrees of freedom                                 9\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.999\n  Tucker-Lewis Index (TLI)                       0.998\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -8554.222\n  Loglikelihood unrestricted model (H1)      -8552.599\n                                                      \n  Akaike (AIC)                               17132.444\n  Bayesian (BIC)                             17182.251\n  Sample-size adjusted Bayesian (SABIC)      17144.166\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.013\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.080\n  P-value H_0: RMSEA &lt;= 0.050                    0.742\n  P-value H_0: RMSEA &gt;= 0.080                    0.050\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.016\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  ses ~                                               \n    symptoms          0.448    0.063    7.110    0.000\n    dysfunction       0.221    0.055    4.019    0.000\n  morale ~                                            \n    symptoms          0.107    0.009   11.756    0.000\n    ses               0.021    0.006    3.291    0.001\n  relations ~                                         \n    dysfunction      -0.024    0.007   -3.335    0.001\n    morale            0.504    0.037   13.745    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  symptoms ~~                                         \n    dysfunction      59.670   15.553    3.836    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .ses             521.598   34.062   15.313    0.000\n   .morale            9.884    0.645   15.313    0.000\n   .relations         8.732    0.570   15.313    0.000\n    symptoms        289.000   18.872   15.313    0.000\n    dysfunction     380.250   24.831   15.313    0.000\n\nR-Square:\n                   Estimate\n    ses               0.145\n    morale            0.297\n    relations         0.290\n\n\n\n\nlavaan::residuals(medical, type = \"standardized.mplus\") |&gt; print()\n\n$type\n[1] \"standardized.mplus\"\n\n$cov\n               ses morale reltns symptm dysfnc\nses          0.000                            \nmorale       0.000  0.000                     \nrelations   -1.161 -1.818     NA              \nsymptoms     0.000  0.000  0.833  0.000       \ndysfunction  0.000  0.842  0.862  0.000  0.000\n\n\n\n\nlavaan::residuals(medical, type = \"normalized\") |&gt; print()\n\n$type\n[1] \"normalized\"\n\n$cov\n               ses morale reltns symptm dysfnc\nses          0.000                            \nmorale       0.000  0.000                     \nrelations   -0.901 -0.080 -0.069              \nsymptoms     0.000  0.000  0.573  0.000       \ndysfunction  0.000  0.680  0.370  0.000  0.000\n\n\n\n\nlavaan::residuals(medical, type = \"cor.bollen\") |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n               ses morale reltns symptm dysfnc\nses          0.000                            \nmorale       0.000  0.000                     \nrelations   -0.041 -0.003  0.000              \nsymptoms     0.000  0.000  0.028  0.000       \ndysfunction  0.000  0.032  0.017  0.000  0.000\n\n\n\nIn conclusione, i valori i valori degli indici di adattamento predittivo per i due modelli alternativi di Romney et al. (1992) sono stati generati seguendo le istruzioni precedentemente descritte. Non sorprende che l’adattamento globale del modello medico convenzionale, più complesso (con $ dfM = 3 $), sia migliore rispetto a quello del modello psicosomatico, più semplice (con $ dfM = 5 $). Nonostante il modello medico convenzionale sia più complesso e quindi soggetto a una penalità maggiore per il minor numero di gradi di libertà, i valori ottenuti sia nell’AIC che nel BIC sono inferiori rispetto a quelli del modello psicosomatico. Questo indica che il vantaggio in termini di adattamento del modello medico convenzionale è sufficiente a superare la penalità per la sua maggiore complessità. In base a queste analisi, il modello medico convenzionale è considerato più adatto rispetto al modello psicosomatico, come evidenziato dai valori più bassi nei criteri AIC e BIC.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#indici-di-modifica-e-statistiche-correlate-nel-sem",
    "href": "chapters/sem/04_mod_comp.html#indici-di-modifica-e-statistiche-correlate-nel-sem",
    "title": "48  Confronto tra modelli",
    "section": "48.4 Indici di Modifica e Statistiche Correlate nel SEM",
    "text": "48.4 Indici di Modifica e Statistiche Correlate nel SEM\nNell’elaborazione di modelli SEM (Structural Equation Modeling), esiste la possibilità di implementare modifiche attraverso processi automatizzati. Questi interventi, di carattere esplorativo, si basano sull’aggiunta o sulla rimozione di parametri seguendo criteri empirici, come la significatività statistica di un indice di modifica (MI) o di un test di punteggio. Gli indici di modifica, in particolare, sono calcolati per quei parametri che nel modello sono stati inizialmente vincolati e servono a stimare quanto il chi-quadrato del modello di massima verosimiglianza (chiML) si ridurrebbe se un dato parametro vincolato venisse liberato.\nIl meccanismo di modifica automatica opera liberando, ad ogni iterazione, il parametro vincolato che presenta il valore di MI più alto. Questo processo continua finché non si raggiunge un MI che è statisticamente significativo secondo i criteri stabiliti dal ricercatore. È cruciale, tuttavia, riconoscere che l’uso di questa metodologia, specialmente in campioni di piccole dimensioni, può portare alla formulazione di modelli che si basano eccessivamente sul caso. Di conseguenza, questi modelli potrebbero risultare poco robusti e difficilmente replicabili in studi successivi. L’automazione nel processo di ottimizzazione dei modelli SEM richiede dunque un’attenta valutazione del contesto e della dimensione del campione per garantire l’affidabilità e la validità dei risultati ottenuti.\nUn altro strumento di uso frequente nei SEM è il test di Wald, basato sulla statistica W. Questo test è progettato per valutare l’impatto che avrebbe la fissazione a zero di un parametro precedentemente stimato liberamente nel modello. In termini più tecnici, il test di Wald stima l’incremento che si verificherebbe nel chi-quadrato del modello di massima verosimiglianza (chiML) se tale parametro fosse “potato”, ovvero escluso dal modello.\nSimilmente ai processi di modifica automatica, l’efficacia del test di Wald può essere influenzata dalla casualità. Un aspetto cruciale da considerare è la sensibilità di questi test alla dimensione del campione. Infatti, anche piccole modifiche nella bontà di adattamento del modello possono assumere una significatività statistica notevole in campioni di ampie dimensioni.\nDi conseguenza, quando si valutano gli indici di modifica come il MI (Modifica Index), è essenziale che il ricercatore non si limiti a considerarne la sola significatività statistica. È importante anche valutare l’entità del cambiamento che si verificherebbe nel coefficiente del parametro se fosse liberato. Se il cambiamento previsto è minimo, la significatività statistica dell’indice di modifica potrebbe essere più indicativa della dimensione del campione che non della sostanziale rilevanza dell’effetto analizzato. Questa considerazione sottolinea l’importanza di un approccio olistico e critico nell’interpretazione dei risultati dei test diagnostici in SEM, specialmente in contesti dove la dimensione del campione può influenzare significativamente i risultati.\n\nfit_mdd &lt;- cfa(\n    model_mdd,\n    data = d_mdd\n)\n\nCalcoliamo gli indici di modifica.\n\nmodification_indices &lt;- lavInspect(fit_mdd, \"mi\")\nmodification_indices |&gt; print()\n\n    lhs op  rhs     mi    epc sepc.lv sepc.all sepc.nox\n20 mdd1 ~~ mdd2 46.877  0.732   0.732    0.560    0.560\n21 mdd1 ~~ mdd3  4.996 -0.211  -0.211   -0.108   -0.108\n22 mdd1 ~~ mdd4  4.625 -0.215  -0.215   -0.108   -0.108\n23 mdd1 ~~ mdd5  7.797 -0.266  -0.266   -0.137   -0.137\n24 mdd1 ~~ mdd6  6.434 -0.236  -0.236   -0.132   -0.132\n25 mdd1 ~~ mdd7  0.293  0.053   0.053    0.026    0.026\n26 mdd1 ~~ mdd8  5.046 -0.219  -0.219   -0.116   -0.116\n27 mdd1 ~~ mdd9  5.385  0.187   0.187    0.111    0.111\n28 mdd2 ~~ mdd3  1.435 -0.138  -0.138   -0.055   -0.055\n29 mdd2 ~~ mdd4 10.931 -0.402  -0.402   -0.158   -0.158\n30 mdd2 ~~ mdd5  5.835 -0.281  -0.281   -0.113   -0.113\n31 mdd2 ~~ mdd6  2.235 -0.169  -0.169   -0.074   -0.074\n32 mdd2 ~~ mdd7  0.394 -0.076  -0.076   -0.029   -0.029\n33 mdd2 ~~ mdd8  0.027 -0.019  -0.019   -0.008   -0.008\n34 mdd2 ~~ mdd9  2.070 -0.142  -0.142   -0.066   -0.066\n35 mdd3 ~~ mdd4 16.199  0.591   0.591    0.155    0.155\n36 mdd3 ~~ mdd5  3.298  0.258   0.258    0.069    0.069\n37 mdd3 ~~ mdd6  6.361  0.337   0.337    0.098    0.098\n38 mdd3 ~~ mdd7  0.499 -0.106  -0.106   -0.027   -0.027\n39 mdd3 ~~ mdd8  0.014 -0.017  -0.017   -0.005   -0.005\n40 mdd3 ~~ mdd9  2.860 -0.208  -0.208   -0.064   -0.064\n41 mdd4 ~~ mdd5 12.198  0.509   0.509    0.136    0.136\n42 mdd4 ~~ mdd6 17.435  0.573   0.573    0.165    0.165\n43 mdd4 ~~ mdd7  1.272 -0.174  -0.174   -0.043   -0.043\n44 mdd4 ~~ mdd8  0.975  0.143   0.143    0.039    0.039\n45 mdd4 ~~ mdd9  1.879 -0.173  -0.173   -0.053   -0.053\n46 mdd5 ~~ mdd6  7.502  0.364   0.364    0.107    0.107\n47 mdd5 ~~ mdd7  0.096  0.046   0.046    0.012    0.012\n48 mdd5 ~~ mdd8  4.217  0.288   0.288    0.080    0.080\n49 mdd5 ~~ mdd9  0.544 -0.090  -0.090   -0.028   -0.028\n50 mdd6 ~~ mdd7  2.046 -0.201  -0.201   -0.055   -0.055\n51 mdd6 ~~ mdd8  0.877  0.124   0.124    0.037    0.037\n52 mdd6 ~~ mdd9  2.479 -0.180  -0.180   -0.061   -0.061\n53 mdd7 ~~ mdd8  0.188  0.064   0.064    0.017    0.017\n54 mdd7 ~~ mdd9 13.527  0.474   0.474    0.139    0.139\n55 mdd8 ~~ mdd9  0.322  0.069   0.069    0.022    0.022\n\n\nNel modello che stiamo analizzando, l’indice di modifica (MI) più elevato si riferisce alla possibile modifica del parametro che governa la correlazione tra i residui degli indicatori mm1 e mm2. Nel modello model_mdd questa correlazione residua è impostata a zero, indicando l’assenza di una correlazione diretta tra questi residui. Tuttavia, l’indice di modifica suggerisce che se permettessimo a questa correlazione di essere stimata liberamente dal modello (anziché tenerla fissa a zero), si verificherebbe un miglioramento dell’adattamento del modello ai dati osservati. Questa osservazione è in linea con il modello alternativo che è stato proposto per questi dati proposto da {cite:t}brown2015confirmatory.\nIncorporare una correlazione residua tra mm1 e mm2 significa riconoscere che, oltre alla variazione spiegata dalle variabili latenti comuni, esiste una relazione unica tra questi due indicatori che non è catturata dal modello. Tale relazione potrebbe essere dovuta a fattori specifici relativi a questi indicatori o a una misurazione comune non prevista dal modello originale.\nÈ importante sottolineare che ogni modifica al modello basata sugli indici di modifica dovrebbe essere attentamente valutata per assicurarsi che sia supportata sia da giustificazioni teoriche che empiriche. Aggiungere correlazioni residue può migliorare l’adattamento del modello, ma dovrebbe essere fatto con cautela per evitare di creare un modello eccessivamente complesso che potrebbe non essere generalizzabile al di fuori del campione di dati specifico utilizzato.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#riflessioni-conclusive",
    "href": "chapters/sem/04_mod_comp.html#riflessioni-conclusive",
    "title": "48  Confronto tra modelli",
    "section": "48.5 Riflessioni Conclusive",
    "text": "48.5 Riflessioni Conclusive\nNel campo dei modelli SEM, è pratica comune selezionare il modello più appropriato da un insieme di alternative, tutte calibrate sugli stessi dati. È frequente il confronto tra modelli gerarchicamente collegati, in cui il modello più restrittivo è incluso, o annidato, in quello meno restrittivo. In queste situazioni, il test di differenza del chi-quadrato viene impiegato per valutare se i modelli hanno un adattamento equivalente. Utilizzare questo test e le statistiche diagnostiche correlate, come gli indici di modifica, richiede un approccio guidato dalla teoria, non solo da criteri empirici. Un eccessivo affidamento su criteri puramente empirici, come la significatività statistica, può portare a una dipendenza eccessiva dal caso.\nPer confrontare modelli non annidati, il test di differenza del chi-quadrato non è idoneo, ma si possono adottare gli indici di adattamento predittivo, basati sulla teoria dell’informazione, per la loro valutazione. Quando si decide di mantenere un modello, è cruciale prendere in considerazione anche altri modelli alternativi potenzialmente equivalenti. È importante fornire motivazioni solide su perché il modello selezionato dal ricercatore sia da preferire rispetto a queste alternative equivalenti.\nQuesto approccio consente una comprensione più profonda e una scelta più informata del modello, assicurando che la selezione sia fondata su basi teoriche solide e non solamente su risultati statistici.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/04_mod_comp.html#session-info",
    "href": "chapters/sem/04_mod_comp.html#session-info",
    "title": "48  Confronto tra modelli",
    "section": "48.6 Session Info",
    "text": "48.6 Session Info\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] effectsize_0.8.9   rsvg_2.6.1         DiagrammeRsvg_0.1 \n [4] mvnormalTest_1.0.0 lavaanPlot_0.8.1   lavaanExtra_0.2.1 \n [7] MASS_7.3-61        viridis_0.6.5      viridisLite_0.4.2 \n[10] ggpubr_0.6.0       ggExtra_0.10.1     gridExtra_2.3     \n[13] patchwork_1.3.0    bayesplot_1.11.1   semTools_0.5-6    \n[16] semPlot_1.1.6      lavaan_0.6-19      psych_2.4.6.26    \n[19] scales_1.3.0       markdown_1.13      knitr_1.49        \n[22] lubridate_1.9.3    forcats_1.0.0      stringr_1.5.1     \n[25] dplyr_1.1.4        purrr_1.0.2        readr_2.1.5       \n[28] tidyr_1.3.1        tibble_3.2.1       ggplot2_3.5.1     \n[31] tidyverse_2.0.0    here_1.0.1        \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.2       later_1.3.2         pbdZMQ_0.3-13      \n  [4] datawizard_0.13.0   XML_3.99-0.17       rpart_4.1.23       \n  [7] lifecycle_1.0.4     rstatix_0.7.2       rprojroot_2.0.4    \n [10] lattice_0.22-6      insight_0.20.5      rockchalk_1.8.157  \n [13] backports_1.5.0     magrittr_2.0.3      openxlsx_4.2.7.1   \n [16] Hmisc_5.2-0         rmarkdown_2.29      httpuv_1.6.15      \n [19] qgraph_1.9.8        zip_2.3.1           pbapply_1.7-2      \n [22] minqa_1.2.8         RColorBrewer_1.1-3  ADGofTest_0.3      \n [25] multcomp_1.4-26     abind_1.4-8         quadprog_1.5-8     \n [28] pspline_1.0-20      nnet_7.3-19         TH.data_1.1-2      \n [31] sandwich_3.1-1      moments_0.14.1      nortest_1.0-4      \n [34] openintro_2.5.0     arm_1.14-4          performance_0.12.4 \n [37] airports_0.1.0      codetools_0.2-20    tidyselect_1.2.1   \n [40] farver_2.1.2        lme4_1.1-35.5       stats4_4.4.2       \n [43] base64enc_0.1-3     jsonlite_1.8.9      Formula_1.2-5      \n [46] survival_3.7-0      emmeans_1.10.5      tools_4.4.2        \n [49] Rcpp_1.0.13-1       glue_1.8.0          mnormt_2.1.1       \n [52] xfun_0.49           IRdisplay_1.1       numDeriv_2016.8-1.1\n [55] withr_3.0.2         fastmap_1.2.0       boot_1.3-31        \n [58] fansi_1.0.6         digest_0.6.37       mi_1.1             \n [61] timechange_0.3.0    R6_2.5.1            mime_0.12          \n [64] estimability_1.5.1  colorspace_2.1-1    Cairo_1.6-2        \n [67] gtools_3.9.5        jpeg_0.1-10         copula_1.1-4       \n [70] DiagrammeR_1.0.11   utf8_1.2.4          generics_0.1.3     \n [73] data.table_1.16.2   corpcor_1.6.10      usdata_0.3.1       \n [76] htmlwidgets_1.6.4   parameters_0.23.0   pkgconfig_2.0.3    \n [79] sem_3.1-16          gtable_0.3.6        pcaPP_2.0-5        \n [82] htmltools_0.5.8.1   carData_3.0-5       png_0.1-8          \n [85] rstudioapi_0.17.1   tzdb_0.4.0          reshape2_1.4.4     \n [88] uuid_1.2-1          curl_6.0.0          coda_0.19-4.1      \n [91] visNetwork_2.1.2    checkmate_2.3.2     nlme_3.1-166       \n [94] nloptr_2.1.1        repr_1.1.7          zoo_1.8-12         \n [97] parallel_4.4.2      miniUI_0.1.1.1      foreign_0.8-87     \n[100] pillar_1.9.0        grid_4.4.2          vctrs_0.6.5        \n[103] promises_1.3.0      car_3.1-3           OpenMx_2.21.13     \n[106] xtable_1.8-4        cluster_2.1.6       htmlTable_2.4.3    \n[109] evaluate_1.0.1      pbivnorm_0.6.0      mvtnorm_1.3-2      \n[112] cli_3.6.3           kutils_1.73         compiler_4.4.2     \n[115] rlang_1.1.4         crayon_1.5.3        ggsignif_0.6.4     \n[118] fdrtool_1.2.18      plyr_1.8.9          stringi_1.8.4      \n[121] munsell_0.5.1       gsl_2.1-8           lisrelToR_0.3      \n[124] bayestestR_0.15.0   V8_6.0.0            pacman_0.5.1       \n[127] Matrix_1.7-1        IRkernel_1.3.2      hms_1.1.3          \n[130] stabledist_0.7-2    glasso_1.11         shiny_1.9.1        \n[133] igraph_2.1.1        broom_1.0.7         RcppParallel_5.1.9 \n[136] cherryblossom_0.1.0\n\n\n\n\n\n\nKline, R. B. (2023). Principles and practice of structural equation modeling. Guilford publications.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/05_cfa_mod_comp.html",
    "href": "chapters/sem/05_cfa_mod_comp.html",
    "title": "49  CFA: confronto tra modelli",
    "section": "",
    "text": "49.1 Introduzione\nIn un modello CFA, i parametri possono essere stimati senza vincoli, possono essere fissi o possono essre stimati sulla base di alcuni vincoli. Un parametro libero è sconosciuto e il ricercatore consente all’algoritmo di stima di trovare il suo valore ottimale che, insime agli altri parametri del modello, riduce al minimo le differenze tra le matrici di varianze-covarianze osservate e quelle predette dal modello. Un parametro fisso è pre-specificato dal ricercatore ad un valore specifico, più comunemente 1.0 (ad esempio, per definire la metrica di una variabile latente) o 0 (ad esempio, l’assenza di saturazionoi fattoriali o di covarianze di errore). Come per un parametro libero, anche un parametro vincolato è sconosciuto; tuttavia, un tale parametro non può assumere un valore qualsiasi, ma deve rispettare le restrizioni su suoi valori che il ricercatore ha imposto. I vincoli più comuni sono i vincoli di uguaglianza, in cui i parametri non standardizzati devono assumere valori uguali (ad esempio, in diversi gruppi).\nConsideriamo un esempio discusso da Brown (2015). Viene qui esaminato un set di dati in cui le prime tre misure osservate (X1, X2, X3) sono indicatori di un costrutto latente corrispondente alla Memoria uditiva e il secondo insieme di misure (X4, X5, X6) sono indicatori di un altro costrutto latente, Memoria visiva. Le tre misure usate quali indicatori del costrutto di memoria uditiva sono: X1 = memoria logica, X2 = associazione verbale a coppie, X3 = liste di parole; le tre misure usate come indicatori del costrutto di memoria visiva sono: X4 = immagini di facce, X5 = foto di famiglia, X6 = generiche riproduzioni visive. I dati sono i seguenti:\nsds &lt;- '2.610  2.660  2.590  1.940  2.030  2.050'\n\ncors &lt;-'\n  1.000\n  0.661  1.000\n  0.630  0.643  1.000\n  0.270  0.300  0.268  1.000\n  0.297  0.265  0.225  0.805  1.000\n  0.290  0.287  0.248  0.796  0.779  1.000'\n\ncovs &lt;- getCov(cors, sds = sds, names = paste(\"x\", 1:6, sep = \"\"))\nprint(covs)\n\n         x1       x2       x3       x4       x5       x6\nx1 6.812100 4.589059 4.258737 1.367118 1.573595 1.551645\nx2 4.589059 7.075600 4.429884 1.548120 1.430947 1.565011\nx3 4.258737 4.429884 6.708100 1.346593 1.182982 1.316756\nx4 1.367118 1.548120 1.346593 3.763600 3.170251 3.165692\nx5 1.573595 1.430947 1.182982 3.170251 4.120900 3.241808\nx6 1.551645 1.565011 1.316756 3.165692 3.241808 4.202500\nAdattiamo i cinque modelli discussi da Brown (2015).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>CFA: confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/05_cfa_mod_comp.html#modello-congenerico",
    "href": "chapters/sem/05_cfa_mod_comp.html#modello-congenerico",
    "title": "49  CFA: confronto tra modelli",
    "section": "49.2 Modello congenerico",
    "text": "49.2 Modello congenerico\n\nmodel.congeneric &lt;- '\n  auditorymemory =~ x1 + x2 + x3\n  visualmemory   =~ x4 + x5 + x6\n'\n\n\nfit.congeneric &lt;- cfa(\n  model.congeneric, \n  sample.cov = covs, \n  sample.nobs = 200, \n  std.lv = TRUE\n)\n\nL’output si ottiene con:\n\nout = summary(\n  fit.congeneric, \n  fit.measures = TRUE, \n  standardized = TRUE, \n  rsquare = TRUE\n)\nprint(out)\n\nlavaan 0.6-18 ended normally after 21 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n\n  Number of observations                           200\n\nModel Test User Model:\n                                                      \n  Test statistic                                 4.877\n  Degrees of freedom                                 8\n  P-value (Chi-square)                           0.771\n\nModel Test Baseline Model:\n\n  Test statistic                               719.515\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.008\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2337.980\n  Loglikelihood unrestricted model (H1)      -2335.541\n                                                      \n  Akaike (AIC)                                4701.959\n  Bayesian (BIC)                              4744.837\n  Sample-size adjusted Bayesian (SABIC)       4703.652\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.057\n  P-value H_0: RMSEA &lt;= 0.050                    0.929\n  P-value H_0: RMSEA &gt;= 0.080                    0.010\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.012\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  auditorymemory =~                                                      \n    x1                 2.101    0.166   12.663    0.000    2.101    0.807\n    x2                 2.182    0.168   12.976    0.000    2.182    0.823\n    x3                 2.013    0.166   12.124    0.000    2.013    0.779\n  visualmemory =~                                                        \n    x4                 1.756    0.108   16.183    0.000    1.756    0.907\n    x5                 1.795    0.115   15.608    0.000    1.795    0.887\n    x6                 1.796    0.117   15.378    0.000    1.796    0.878\n\nCovariances:\n                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  auditorymemory ~~                                                      \n    visualmemory       0.382    0.070    5.463    0.000    0.382    0.382\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .x1                2.366    0.372    6.365    0.000    2.366    0.349\n   .x2                2.277    0.383    5.940    0.000    2.277    0.323\n   .x3                2.621    0.373    7.027    0.000    2.621    0.393\n   .x4                0.662    0.117    5.668    0.000    0.662    0.177\n   .x5                0.877    0.134    6.554    0.000    0.877    0.214\n   .x6                0.956    0.139    6.866    0.000    0.956    0.229\n    auditorymemory    1.000                               1.000    1.000\n    visualmemory      1.000                               1.000    1.000\n\nR-Square:\n                   Estimate\n    x1                0.651\n    x2                0.677\n    x3                0.607\n    x4                0.823\n    x5                0.786\n    x6                0.771\n\n\n\nIl diagramma di percorso del modello è il seguente.\n\nsemPaths(\n  fit.congeneric,\n  \"std\",\n  posCol = c(\"black\"),\n  edge.label.cex = 1.2,\n  sizeMan = 7,\n  edge.width = 0.4, # Set a fixed width for all arrows\n  fade = FALSE # Disable fading of the arrows\n)",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>CFA: confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/05_cfa_mod_comp.html#modello-tau-equivalente",
    "href": "chapters/sem/05_cfa_mod_comp.html#modello-tau-equivalente",
    "title": "49  CFA: confronto tra modelli",
    "section": "49.3 Modello tau-equivalente",
    "text": "49.3 Modello tau-equivalente\nSolo memoria auditiva:\n\nmodel.tau.a &lt;- '\n  auditorymemory =~ x1 + v1*x1 + v1*x2 + v1*x3\n  visualmemory   =~ x4 + x5 + x6\n'\n\n\nfit.tau.a &lt;- cfa(\n  model.tau.a, \n  sample.cov = covs, \n  sample.nobs = 200, \n  std.lv = TRUE\n)\n\nMemoria auditiva e visiva:\n\nmodel.tau.av &lt;- '\n  auditorymemory =~ NA*x1 + v1*x1 + v1*x2 + v1*x3\n  visualmemory   =~ NA*x4 + v2*x4 + v2*x5 + v2*x6\n'\n\n\nfit.tau.av &lt;- cfa(\n  model.tau.av, \n  sample.cov = covs, \n  sample.nobs = 200, \n  std.lv = TRUE\n)\n\n\nsemPaths(\n  fit.tau.av,\n  \"std\",\n  posCol = c(\"black\"),\n  edge.label.cex = 1.2,\n  sizeMan = 7,\n  edge.width = 0.4, # Set a fixed width for all arrows\n  fade = FALSE # Disable fading of the arrows\n)",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>CFA: confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/05_cfa_mod_comp.html#modello-parallelo",
    "href": "chapters/sem/05_cfa_mod_comp.html#modello-parallelo",
    "title": "49  CFA: confronto tra modelli",
    "section": "49.4 Modello parallelo",
    "text": "49.4 Modello parallelo\nSolo memoria auditiva:\n\nmodel.parallel.a &lt;- '\n  auditorymemory =~ x1 + v1*x1 + v1*x2 + v1*x3\n  visualmemory   =~ x4 + v2*x4 + v2*x5 + v2*x6\n  x1 ~~ v3 * x1\n  x2 ~~ v3 * x2\n  x3 ~~ v3 * x3\n'\n\n\nfit.parallel.a &lt;- cfa(\n  model.parallel.a, \n  sample.cov = covs, \n  sample.nobs = 200, \n  std.lv = TRUE\n)\n\nMemoria auditiva e visiva:\n\nmodel.parallel.av &lt;- '\n  auditorymemory =~ x1 + v1*x1 + v1*x2 + v1*x3\n  visualmemory   =~ x4 + v2*x4 + v2*x5 + v2*x6\n  x1 ~~ v3 * x1\n  x2 ~~ v3 * x2\n  x3 ~~ v3 * x3\n \n  x4 ~~ v4 * x4\n  x5 ~~ v4 * x5\n  x6 ~~ v4 * x6\n'\n\n\nfit.parallel.av &lt;- cfa(\n  model.parallel.av, \n  sample.cov = covs, \n  sample.nobs = 200, \n  std.lv = TRUE\n)\n\n\nsemPaths(\n    fit.parallel.av,\n    \"std\",\n    posCol = c(\"black\"),\n    edge.label.cex = 1.2,\n    sizeMan = 7,\n    edge.width = 0.4, # Set a fixed width for all arrows\n    fade = FALSE # Disable fading of the arrows\n)",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>CFA: confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/05_cfa_mod_comp.html#il-test-del-chi2",
    "href": "chapters/sem/05_cfa_mod_comp.html#il-test-del-chi2",
    "title": "49  CFA: confronto tra modelli",
    "section": "49.5 Il test del \\(\\chi^2\\)",
    "text": "49.5 Il test del \\(\\chi^2\\)\nIl confronto tra modelli nidificati procede attraverso il test \\(\\chi^2\\). Tale test si basa su una proprietà delle variabili casuali distribuite come \\(\\chi^2\\): la differenza tra due v.c. \\(X_1\\) e \\(X_2\\) che seguono la distribuzione \\(\\chi^2\\), rispettivamente con \\(\\nu_1\\) e \\(\\nu_2\\), con \\(\\nu_1 &gt; \\nu_2\\), è una variabile causale che segue la distribuzione \\(\\chi^2\\) con gradi di libertà pari a \\(\\nu_1 - \\nu_2\\).\nUn modello nidificato è un modello che impone dei vincoli sui parametri del modello di partenza. L’imposizione di vincoli sui parametri ha la conseguenza che vi sarà un numero minore di parametri da stimare. Il confronto tra i modelli si esegue valutando in maniera relativa la bontà di adattamento di ciascun modello per mezzo della statistica chi-quadrato. La statistica così calcolata avrà un numero di gradi di libertà uguale alla differenza tra i gradi di libertà dei due modelli.\nNel caso dell’esempio in dicussione, abbiamo\n\nout = anova(\n  fit.congeneric, \n  fit.tau.a, \n  fit.tau.av, \n  fit.parallel.a, \n  fit.parallel.av, \n  test = \"chisq\"\n)\nprint(out)\n\n\nChi-Squared Difference Test\n\n                Df    AIC    BIC  Chisq Chisq diff    RMSEA Df diff Pr(&gt;Chisq)\nfit.congeneric   8 4702.0 4744.8 4.8773                                       \nfit.tau.a       10 4698.7 4735.0 5.6597     0.7823 0.000000       2     0.6763\nfit.tau.av      12 4695.0 4724.6 5.8810     0.2213 0.000000       2     0.8952\nfit.parallel.a  14 4691.1 4714.1 5.9769     0.0959 0.000000       2     0.9532\nfit.parallel.av 16 4690.4 4706.9 9.2772     3.3003 0.057016       2     0.1920\n\n\nI test precedenti indicano come non vi sia una perdita di adattamento passando dal modello congenerico al modello più restrittivo (ovvero, il modello parallelo per entrambi i fattori). Per questi dati, dunque, può essere adottato il modello più semplice, cioè il modello parallelo.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>CFA: confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/05_cfa_mod_comp.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/05_cfa_mod_comp.html#informazioni-sullambiente-di-sviluppo",
    "title": "49  CFA: confronto tra modelli",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>CFA: confronto tra modelli</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html",
    "href": "chapters/sem/06_refine_solution.html",
    "title": "50  La revisione del modello",
    "section": "",
    "text": "50.1 Introduzione\nI passi principali nella CFA e nei modelli SEM comprendono la specificazione del modello, la stima dei parametri, la valutazione del modello e dei parametri e la modificazione del modello. Questa sequenza può essere ripetuta molte volte fino a quando non si trovi un modello considerato accettabile.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#stima-del-modello",
    "href": "chapters/sem/06_refine_solution.html#stima-del-modello",
    "title": "50  La revisione del modello",
    "section": "50.2 Stima del modello",
    "text": "50.2 Stima del modello\nConsideriamo qui un modello SEM con una sola variabile latente identificata da un insieme di indicatori, ovvero un modello CFA. L’obiettivo della CFA è ottenere stime per i parametro del modello (vale a dire, saturazioni fattoriali, varianze e covarianze fattoriali, varianze residue ed eventualmente covarianze degli errori) che sono in grado di produrre una matrice di covarianza prevista (denotata da \\(\\boldsymbol{\\Sigma}\\)) la quale è il più possibile simile alla matrice di covarianze campionarie (denotata da \\(\\boldsymbol{S}\\)). Questo processo di stima è basato sulla minimizzazione di una funzione che descrive la differenza tra \\(\\boldsymbol{\\Sigma}\\) e \\(\\boldsymbol{S}\\). Il metodo di stima più utilizzato nella CFA (e, in generale, nei modelli SEM) è la massima verosimiglianza (ML).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#massima-verosimiglianza",
    "href": "chapters/sem/06_refine_solution.html#massima-verosimiglianza",
    "title": "50  La revisione del modello",
    "section": "50.3 Massima verosimiglianza",
    "text": "50.3 Massima verosimiglianza\nL’equazione fondamentale dell’analisi fattoriale è\n\\[\n\\boldsymbol y = \\boldsymbol \\Lambda  \\boldsymbol x  + \\boldsymbol z,\n\\]\ndove \\(\\boldsymbol{y}\\) è un vettore di \\(p\\) componenti (i punteggi osservati nel del test), \\(\\boldsymbol{x}\\) è un vettore di \\(k &lt; p\\) componenti (i punteggi fattoriali), \\(\\boldsymbol{\\Lambda}\\) è una \\(p \\cdot k\\) matrice (di saturazioni fattoriali), e \\(\\boldsymbol{z}\\) è un vettore di \\(p\\) componenti (la componenti dei punteggi del test non dovute all’effetto causale delle variabili comuni latenti). Per l’item \\(i\\)-esimo, in precedenza abbiamo scritto l’equazione precedente come\n\\[\ny_i = \\lambda_{i1} \\xi_1 + \\dots + \\lambda_{ik} \\xi_k + \\delta_i.\n\\]\nDalle assunzioni del modello fattoriale deriva che\n\\[\n\\boldsymbol{\\Sigma} = \\boldsymbol{\\Lambda}\\boldsymbol{\\Phi}\\boldsymbol{\\Lambda}^\\prime + \\Psi,\n\\]\ndove \\(\\boldsymbol{\\Phi}\\) è la matrice delle inter-correlazioni fattoriali.\nSi assume che il vettore casuale \\(\\boldsymbol{y}\\) abbia una distribuzione normale multivariata con matrice di covarianza \\(\\boldsymbol{\\Sigma}\\) e che da tale distribuzione sia stato estratto un campione casuale di \\(n\\) osservazioni \\(y_l, y_2, \\dots, y_n\\). Il logaritmo della funzione di verosimiglianza per il campione è dato da\n\\[\n\\log L = \\frac{1}{2}n [\\log | \\boldsymbol{\\Sigma}| + tr(\\boldsymbol{\\boldsymbol{S} \\Sigma}^{-1})].\n\\]\nL’equazione precedente viene vista come funzione di \\(\\Lambda\\) e \\(\\Psi\\). Anziché massimizzare \\(\\log L\\), è equivalente e più conveniente minimizzare\n\\[\nF_{k}(\\Lambda, \\Psi) = \\log |\\boldsymbol{\\Sigma}| + tr[\\boldsymbol{S}\\boldsymbol{\\Sigma}^{-1}]  - \\log|\\boldsymbol{S}| – p,\n\\]\ndove \\(|\\boldsymbol{S}|\\) è il determinante della matrice di covarianza tra le variabili osservate, \\(|\\boldsymbol{\\Sigma}|\\) è il determinante della matrice di covarianza prevista e \\(p\\) è il numero di indicatori.\nL’obiettivo della stima di massima verosimiglianza della CFA è trovare le stime dei parametri che rendono più verosimili i dati osservati (o, al contrario, massimizzano la verosimiglianza dei parametri dati i dati). Le stime dei parametri in un modello CFA si ottengono con una procedura iterativa. Cioè, l’algoritmo inizia con una serie iniziale di stime dei parametri (denominate valori iniziali o stime iniziali, che possono essere generate automaticamente dal software o specificate dall’utente) e raffina ripetutamente queste stime nel tentativo di minimizzare la differenza tra \\(\\boldsymbol{\\Sigma}\\) e \\(\\boldsymbol{S}\\). Il programma effettua controlli interni per valutare i suoi progressi nell’ottenere stime dei parametri che al meglio riproducono \\(\\boldsymbol{S}\\). Si raggiunge la convergenza quando l’algoritmo produce una serie di stime dei parametri che non possono essere ulteriormente migliorate per ridurre la differenza tra \\(\\boldsymbol{\\Sigma}\\) e \\(\\boldsymbol{S}\\).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#identificabilità-del-modello",
    "href": "chapters/sem/06_refine_solution.html#identificabilità-del-modello",
    "title": "50  La revisione del modello",
    "section": "50.4 Identificabilità del modello",
    "text": "50.4 Identificabilità del modello\nUn modello CFA deve essere formulato in modo tale da garantire la risolvibilità matematica dello stesso, ovvero deve essere tale da consentire una stima univoca dei parametri del modello. Detto in altre parole, la specificazione del modello ne deve garantire l’dentificabilità.\nIl problema dell’identificazione richiede, innanzitutto, di chiarire il concetto di gradi di libertà (degrees of freedom). Nel presente contesto, per gradi di libertà (\\(dof\\)) intendiamo\n\\[\ndof = \\# (\\text{unità di informazione}) - \\# (\\text{parametri da stimare}).\n\\]\nI dati che vengono analizzati da un modello CFA sono contenuti in una matrice di covarianza. Per una matrice di covarianza di ordine \\(p\\), il numero di unità di informazione è\n\\[\n\\frac{p (p+1)}{2}.\n\\]\nAffinché il modello sia identificabile, devono essere soddisfatte le seguenti condizioni.\n\nIndipendentemente dalla complessità del modello (ad es. modelli ad un fattore rispetto a più fattori), l’unità di misura delle variabili latenti deve essere specificata (di solito fissandola a un valore di 1);\nIndipendentemente dalla complessità del modello, il numero di unità di informazione (es. la matrice di covarianza degli indicatori) deve essere uguale o superiore al numero di parametri da stimare (es. saturazioni fattoriali, specificità, covarianze degli errori dell’indicatore, covarianze tra i fattori);\nNel caso di modelli ad un fattore è richiesto un minimo di tre indicatori. Quando vengono utilizzati tre indicatori, la soluzione a un fattore si dice “appena identificata” (just-identified); in tali condizioni non è possibile valutare la bontà dell’adattamento.\nNel caso di modelli a due o più fattori e due indicatori per costrutto latente, la soluzione è sovraidentificata, a condizione che ogni variabile latente sia correlata con almeno un’altra variabile latente e gli errori tra gli indicatori siano tra loro incorrelati. Tuttavia, poiché tali soluzioni sono suscettibili di scarsa identificazione empirica, viene raccomandato un minimo di tre indicatori per variabile latente.\n\nIn conclusione, una semplice e necessaria condizione per l’identificazione di un modello CFA è che vi siano più unità di informazione che parametri da stimare. Dunque, abbiamo che:\n\nse \\(dof &lt; 0\\), il modello non è identificato e, in questo caso, non è possibile stimare i parametri;\nse \\(dof = 0\\), il modello è appena identificato o “saturo”; in questo caso, la matrice di covarianza riprodotta coincide con la matrice di covarianza delle variabili osservate e, di conseguenza, non esiste un residuo attraverso cui valutare la bontà dell’adattamento del modello;\nse \\(dof &gt; 0\\), il modello è sovra-identificato ed esistono le condizioni per valutare la bontà dell’adattamento.\n\nLe considerazioni precedenti ci fanno capire perché non si può fare un’analisi fattoriale con solo due indicatori e un fattore; in tali circostanze, infatti, ci sono \\((2 \\cdot 3)/2 = 3\\) gradi di libertà, ma 4 parametri da stimare (due saturazioni fattoriali e due specificità). Il caso di tre item e un fattore definisce un modello “appena identificato”, ovvero, il caso in cui ci sono zero gradi di libertà. In tali circostanze è possibile stimare i parametri (ricordiamo il metodo dell’annullamento della tetrade), ma non è possibile un test di bontà dell’adattamento. Questo vuol dire, in pratica, che per un modello SEM ad un solo fattore comune latente è necessario disporre di almeno quattro indicatori.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#un-esempio-concreto",
    "href": "chapters/sem/06_refine_solution.html#un-esempio-concreto",
    "title": "50  La revisione del modello",
    "section": "50.5 Un Esempio Concreto",
    "text": "50.5 Un Esempio Concreto\nNell’approfondire la tematica dei Modelli di Equazioni Strutturali, è utile considerare alcune problematiche comuni che possono emergere nella fase di adattamento del modello ai dati. Facendo riferimento agli esempi discussi da Brown (2015) nel contesto dell’analisi fattoriale confermativa (CFA), possiamo identificare diverse potenziali cause di inadeguato adattamento. Queste cause possono essere di natura sia teorica che tecnica e spesso richiedono un’attenta riflessione e analisi per essere risolte. Esaminiamo alcune delle questioni più rilevanti:\n\nNumero Errato di Fattori Comuni Latenti:\n\nUno degli errori più comuni è ipotizzare un numero di fattori latenti che non riflette adeguatamente la struttura sottostante dei dati. Un numero insufficiente di fattori può portare a un modello semplificato eccessivamente, mentre un numero eccessivo può causare sovra-aggiustamento e complessità non necessaria.\n\nItem che Saturano su Fattori Multipli:\n\nIn alcuni casi, un item può essere erroneamente ipotizzato per saturare su un singolo fattore comune, mentre in realtà ha relazioni significative con più fattori. Questo errore nella specificazione del modello può portare a stime imprecise e a un adattamento inadeguato.\n\nAssegnazione Errata degli Item ai Fattori:\n\nUn’altra possibile causa di inadeguato adattamento riguarda l’errata assegnazione di un item al fattore comune sbagliato. Tale errore può derivare da una comprensione insufficiente delle dimensioni teoriche che si stanno misurando o da una cattiva interpretazione dei dati empirici.\n\nCorrelazioni Residue Non Considerate:\n\nInfine, le correlazioni residue non incorporate nel modello possono giocare un ruolo significativo nell’adattamento del modello. Queste correlazioni possono indicare relazioni non catturate dai fattori comuni, suggerendo la necessità di rivedere l’ipotesi di base del modello o di aggiungere percorsi specifici per accomodare queste correlazioni.\n\n\nIn sintesi, l’adattamento del modello SEM ai dati è un processo complesso che richiede una profonda comprensione sia della teoria sottostante che della natura dei dati. Ogni volta che un modello non si adatta adeguatamente, è essenziale esaminare criticamente questi e altri potenziali fattori per identificare e correggere le cause alla base di tale inadeguatezza. Questo processo non solo migliora l’adattamento del modello, ma può anche fornire intuizioni preziose sulla struttura dei dati e sulla validità delle teorie sottostanti.\nBrown (2015) mostra come il ricercatore possa usare i Modification Indices per valutare le cause del mancato adattamento del modello ai dati. I Modification Indices sono una misura utilizzata per identificare le covariate tra le variabili del modello che potrebbero migliorare l’aderenza del modello ai dati. I modification indices indicano quale sarebbe il miglioramento nell’aderenza del modello, ad esempio, se venisse permessa la correlazione tra due variabili che attualmente non sono considerate correlate. Ciò consente di identificare le relazioni nascoste tra le variabili e può aiutare a migliorare la precisione e l’accuratezza del modello.\nTuttavia, è importante tenere presente che i modification indices da soli non dovrebbero essere usati per prendere decisioni definitive sulle modifiche del modello. Invece, dovrebbero essere considerati insieme ad altre informazioni, come la conoscenza teorica, l’esperienza e altre tecniche di analisi dei dati per determinare se una modifica del modello è giustificata e in che modo.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#un-numero-di-fattori-troppo-piccolo",
    "href": "chapters/sem/06_refine_solution.html#un-numero-di-fattori-troppo-piccolo",
    "title": "50  La revisione del modello",
    "section": "50.6 Un numero di fattori troppo piccolo",
    "text": "50.6 Un numero di fattori troppo piccolo\nUna delle possibili fonti di mancanza di adattamento del modello può dipendere dal fatto che è stato ipotizzato un numero insufficiente di fattori latenti comuni. Brown (2015) discute il caso nel quale si confrontano gli indici di bontà di adattamento di un modello ad un solo fattore comune e un modello a due fattori comuni. L’esempio riguarda i dati già in precedenza discussi e relativi relativi a otto misure di personalità raccolte su un campione di 250 pazienti che hanno concluso un programma di psicoterapia. Le scale sono le seguenti:\n\nanxiety (N1),\nhostility (N2),\ndepression (N3),\nself-consciousness (N4),\nwarmth (E1),\ngregariousness (E2),\nassertiveness (E3),\npositive emotions (E4).\n\nLeggiamo i dati in \\(\\mathsf{R}\\).\n\nvarnames &lt;- c(\"N1\", \"N2\", \"N3\", \"N4\", \"E1\", \"E2\", \"E3\", \"E4\")\n\nsds &lt;- c(5.7,  5.6,  6.4,  5.7,  6.0,  6.2,  5.7,  5.6)\n\ncors &lt;- '\n 1.000\n 0.767  1.000 \n 0.731  0.709  1.000 \n 0.778  0.738  0.762  1.000 \n-0.351  -0.302  -0.356  -0.318  1.000 \n-0.316  -0.280  -0.300  -0.267  0.675  1.000 \n-0.296  -0.289  -0.297  -0.296  0.634  0.651  1.000 \n-0.282  -0.254  -0.292  -0.245  0.534  0.593  0.566  1.000'\n\npsychot_cor_mat &lt;- getCov(cors, names = varnames)\n\nn &lt;- 250\n\nSupponiamo di adattare ai dati il modello “sbagliato” che include un unico fattore comune. Svolgiamo qui l’analisi fattoriale esplorativa usando la funzione sperimentale efa() di lavaan.\n\n# 1-factor model\nf1 &lt;- '\n  efa(\"efa\")*f1 =~ N1 + N2 + N3 + N4 + E1 + E2 + E3 + E4\n'\n\nAdattiamo il modello ai dati.\n\nefa_f1 &lt;-\n  cfa(\n    model = f1,\n    sample.cov = psychot_cor_mat,\n    sample.nobs = 250,\n    rotation = \"oblimin\"\n  )\n\nConsideriamo ora un modello a due fattori.\n\nf2 &lt;- '\n  efa(\"efa\")*f1 +\n  efa(\"efa\")*f2 =~ N1 + N2 + N3 + N4 + E1 + E2 + E3 + E4\n'\n\nAdattiamo il modello ai dati.\n\nefa_f2 &lt;-\n  cfa(\n    model = f2,\n    sample.cov = psychot_cor_mat,\n    sample.nobs = 250,\n    rotation = \"oblimin\"\n  )\n\nEsaminiamo gli indici di bontà di adattamento.\n\n# define the fit measures\nfit_measures_robust &lt;- c(\"chisq\", \"df\", \"pvalue\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")\n\n# collect them for each model\nrbind(\n  fitmeasures(efa_f1, fit_measures_robust),\n  fitmeasures(efa_f2, fit_measures_robust)\n) %&gt;%\n  # wrangle\n  data.frame() %&gt;%\n  mutate(\n    chisq = round(chisq, digits = 0),\n    df = as.integer(df),\n    pvalue = ifelse(pvalue == 0, \"&lt; .001\", pvalue)\n  ) %&gt;%\n  mutate_at(vars(cfi:srmr), ~ round(., digits = 3))\n\n\nA data.frame: 2 x 7\n\n\nchisq\ndf\npvalue\ncfi\ntli\nrmsea\nsrmr\n\n\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n375\n20\n&lt; .001 \n0.71\n0.594\n0.267\n0.187\n\n\n10\n13\n0.709310449320098\n1.00\n1.006\n0.000\n0.010\n\n\n\n\n\n\nprint(effectsize::interpret(efa_f1))\n\n    Name Value Threshold Interpretation\n1    GFI 0.671      0.95           poor\n2   AGFI 0.408      0.90           poor\n3    NFI 0.701      0.90           poor\n4   NNFI 0.594      0.90           poor\n5    CFI 0.710      0.90           poor\n6  RMSEA 0.267      0.05           poor\n7   SRMR 0.187      0.08           poor\n8    RFI 0.581      0.90           poor\n9   PNFI 0.500      0.50   satisfactory\n10   IFI 0.712      0.90           poor\n\n\n\nprint(effectsize::interpret(efa_f2))\n\n    Name   Value Threshold Interpretation\n1    GFI 0.99055      0.95   satisfactory\n2   AGFI 0.97384      0.90   satisfactory\n3    NFI 0.99217      0.90   satisfactory\n4   NNFI 1.00560      0.90   satisfactory\n5    CFI 1.00000      0.90   satisfactory\n6  RMSEA 0.00000      0.05   satisfactory\n7   SRMR 0.00991      0.08   satisfactory\n8    RFI 0.98315      0.90   satisfactory\n9   PNFI 0.46065      0.50           poor\n10   IFI 1.00257      0.90   satisfactory\n\n\nI risultati mostrano come, in un modello EFA, una soluzione a due fattori produca un adattamento adeguato, mentre ciò non si verifica con un modello ad un solo fattore.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#specificazione-errata-delle-relazioni-tra-indicatori-e-fattori-latenti",
    "href": "chapters/sem/06_refine_solution.html#specificazione-errata-delle-relazioni-tra-indicatori-e-fattori-latenti",
    "title": "50  La revisione del modello",
    "section": "50.7 Specificazione errata delle relazioni tra indicatori e fattori latenti",
    "text": "50.7 Specificazione errata delle relazioni tra indicatori e fattori latenti\nUn’altra potenziale fonte di errata specificazione del modello CFA è una designazione errata delle relazioni tra indicatori e fattori latenti.\nIn questo esempio, un ricercatore ha sviluppato un questionario di 12 item (gli item sono valutati su scale da 0 a 8) progettato per valutare le motivazioni dei giovani adulti a consumare bevande alcoliche (Cooper, 1994). La misura aveva lo scopo di valutare tre aspetti di questo costrutto (4 item ciascuno): (1) motivazioni di coping (item 1–4), (2) motivazioni sociali (item 5–8) e (3) motivazioni di miglioramento (item 9 –12). I dati sono i seguenti.\n\nsds &lt;- c(2.06, 1.52, 1.92, 1.41, 1.73, 1.77, 2.49, 2.27, 2.68, 1.75, 2.57, 2.66)\n\ncors &lt;- '\n  1.000 \n  0.300  1.000 \n  0.229  0.261  1.000 \n  0.411  0.406  0.429  1.000 \n  0.172  0.252  0.218  0.481  1.000 \n  0.214  0.268  0.267  0.579  0.484  1.000 \n  0.200  0.214  0.241  0.543  0.426  0.492  1.000 \n  0.185  0.230  0.185  0.545  0.463  0.548  0.522  1.000 \n  0.134  0.146  0.108  0.186  0.122  0.131  0.108  0.151  1.000 \n  0.134  0.099  0.061  0.223  0.133  0.188  0.105  0.170  0.448  1.000 \n  0.160  0.131  0.158  0.161  0.044  0.124  0.066  0.061  0.370  0.350  1.000 \n  0.087  0.088  0.101  0.198  0.077  0.177  0.128  0.112  0.356  0.359  0.507  1.000'\n\ncovs &lt;- getCov(cors, sds = sds, names = paste(\"x\", 1:12, sep = \"\"))\n\nIniziamo con un modello che ipotizza tre fattori comuni latenti correlati, coerentemente con la motivazione che stava alla base della costruzione dello strumento.\n\nmodel1 &lt;- '\n  copingm  =~ x1 + x2 + x3 + x4\n  socialm  =~ x5 + x6 + x7 + x8\n  enhancem =~ x9 + x10 + x11 + x12\n'\n\nAdattiamo il modello ai dati.\n\nfit1 &lt;- cfa(\n  model1, \n  sample.cov = covs, \n  sample.nobs = 500, \n  mimic = \"mplus\"\n)\n\nEsaminando le misure di adattamento potremmo concludere che il modello è adeguato.\n\nprint(effectsize::interpret(fit1))\n\n    Name  Value Threshold Interpretation\n1    GFI 0.9701      0.95   satisfactory\n2   AGFI 0.9472      0.90   satisfactory\n3    NFI 0.9479      0.90   satisfactory\n4   NNFI 0.9710      0.90   satisfactory\n5    CFI 0.9776      0.90   satisfactory\n6  RMSEA 0.0375      0.05   satisfactory\n7   SRMR 0.0344      0.08   satisfactory\n8    RFI 0.9325      0.90   satisfactory\n9   PNFI 0.7324      0.50   satisfactory\n10   IFI 0.9778      0.90   satisfactory\n\n\nTuttavia, un esame più attento mette in evidenza un comportamento anomalo dell’item x4 e alcune caratteristiche anomale del modello in generale.\n\nprint(standardizedSolution(fit1))\n\n        lhs op      rhs est.std    se     z pvalue ci.lower ci.upper\n1   copingm =~       x1   0.432 0.039 11.03   0.00    0.355    0.508\n2   copingm =~       x2   0.436 0.039 11.17   0.00    0.359    0.512\n3   copingm =~       x3   0.451 0.038 11.73   0.00    0.376    0.527\n4   copingm =~       x4   0.953 0.024 38.97   0.00    0.905    1.001\n5   socialm =~       x5   0.633 0.032 20.06   0.00    0.571    0.695\n6   socialm =~       x6   0.748 0.025 29.36   0.00    0.698    0.798\n7   socialm =~       x7   0.690 0.029 24.15   0.00    0.634    0.746\n8   socialm =~       x8   0.729 0.026 27.52   0.00    0.677    0.781\n9  enhancem =~       x9   0.602 0.039 15.58   0.00    0.526    0.678\n10 enhancem =~      x10   0.597 0.039 15.40   0.00    0.521    0.673\n11 enhancem =~      x11   0.661 0.037 17.98   0.00    0.589    0.733\n12 enhancem =~      x12   0.665 0.037 18.17   0.00    0.593    0.737\n13       x1 ~~       x1   0.814 0.034 24.09   0.00    0.747    0.880\n14       x2 ~~       x2   0.810 0.034 23.84   0.00    0.744    0.877\n15       x3 ~~       x3   0.796 0.035 22.94   0.00    0.728    0.864\n16       x4 ~~       x4   0.091 0.047  1.96   0.05    0.000    0.183\n17       x5 ~~       x5   0.599 0.040 14.98   0.00    0.521    0.677\n18       x6 ~~       x6   0.441 0.038 11.57   0.00    0.366    0.515\n19       x7 ~~       x7   0.524 0.039 13.29   0.00    0.447    0.601\n20       x8 ~~       x8   0.469 0.039 12.15   0.00    0.393    0.545\n21       x9 ~~       x9   0.638 0.047 13.71   0.00    0.546    0.729\n22      x10 ~~      x10   0.643 0.046 13.88   0.00    0.552    0.734\n23      x11 ~~      x11   0.563 0.049 11.61   0.00    0.468    0.659\n24      x12 ~~      x12   0.558 0.049 11.45   0.00    0.462    0.653\n25  copingm ~~  copingm   1.000 0.000    NA     NA    1.000    1.000\n26  socialm ~~  socialm   1.000 0.000    NA     NA    1.000    1.000\n27 enhancem ~~ enhancem   1.000 0.000    NA     NA    1.000    1.000\n28  copingm ~~  socialm   0.799 0.031 26.15   0.00    0.739    0.859\n29  copingm ~~ enhancem   0.322 0.051  6.34   0.00    0.222    0.422\n30  socialm ~~ enhancem   0.268 0.056  4.82   0.00    0.159    0.377\n31       x1 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n32       x2 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n33       x3 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n34       x4 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n35       x5 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n36       x6 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n37       x7 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n38       x8 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n39       x9 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n40      x10 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n41      x11 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n42      x12 ~1            0.000 0.045  0.00   1.00   -0.088    0.088\n43  copingm ~1            0.000 0.000    NA     NA    0.000    0.000\n44  socialm ~1            0.000 0.000    NA     NA    0.000    0.000\n45 enhancem ~1            0.000 0.000    NA     NA    0.000    0.000\n\n\nIn particolare, l’item x4 mostra una saturazione molto forte sul fattore Motivi di coping (.955) ed emerge una correlazione molto alta tra i fattori Motivi di coping e Motivi sociali (.798).\nBrown (2015) suggerisce di esaminare i Modification Indices. Tale esame mostra che il MI associato a x4 è molto alto, 18.916.\n\nprint(modindices(fit1))\n\n         lhs op rhs     mi    epc sepc.lv sepc.all sepc.nox\n46   copingm =~  x5  0.030 -0.030  -0.027   -0.015   -0.015\n47   copingm =~  x6  0.484  0.127   0.113    0.064    0.064\n48   copingm =~  x7  0.780  0.220   0.196    0.079    0.079\n49   copingm =~  x8  1.962 -0.323  -0.287   -0.127   -0.127\n50   copingm =~  x9  0.101  0.044   0.039    0.015    0.015\n51   copingm =~ x10  2.016  0.129   0.114    0.065    0.065\n52   copingm =~ x11  1.870 -0.181  -0.161   -0.063   -0.063\n53   copingm =~ x12  0.040 -0.027  -0.024   -0.009   -0.009\n54   socialm =~  x1  6.927 -0.520  -0.569   -0.277   -0.277\n55   socialm =~  x2  0.052 -0.033  -0.036   -0.024   -0.024\n56   socialm =~  x3  2.058 -0.267  -0.292   -0.152   -0.152\n57   socialm =~  x4 18.916  1.300   1.423    1.010    1.010\n58   socialm =~  x9  0.338  0.067   0.073    0.027    0.027\n59   socialm =~ x10  2.884  0.128   0.140    0.080    0.080\n60   socialm =~ x11  4.357 -0.229  -0.251   -0.098   -0.098\n61   socialm =~ x12  0.001  0.004   0.004    0.002    0.002\n62  enhancem =~  x1  1.954  0.093   0.149    0.072    0.072\n63  enhancem =~  x2  0.863  0.045   0.073    0.048    0.048\n64  enhancem =~  x3  0.380  0.038   0.061    0.032    0.032\n65  enhancem =~  x4  3.102 -0.104  -0.168   -0.119   -0.119\n66  enhancem =~  x5  0.596 -0.039  -0.063   -0.036   -0.036\n67  enhancem =~  x6  2.495  0.078   0.125    0.071    0.071\n68  enhancem =~  x7  0.539 -0.052  -0.084   -0.034   -0.034\n69  enhancem =~  x8  0.093 -0.019  -0.031   -0.014   -0.014\n70        x1 ~~  x2 10.299  0.379   0.379    0.149    0.149\n71        x1 ~~  x3  0.986  0.147   0.147    0.046    0.046\n72        x1 ~~  x4  0.016 -0.015  -0.015   -0.019   -0.019\n73        x1 ~~  x5  0.452 -0.080  -0.080   -0.032   -0.032\n74        x1 ~~  x6  0.484 -0.078  -0.078   -0.036   -0.036\n75        x1 ~~  x7  0.290 -0.089  -0.089   -0.027   -0.027\n76        x1 ~~  x8  1.535 -0.181  -0.181   -0.063   -0.063\n77        x1 ~~  x9  0.468  0.133   0.133    0.034    0.034\n78        x1 ~~ x10  0.067  0.033   0.033    0.013    0.013\n79        x1 ~~ x11  4.030  0.364   0.364    0.102    0.102\n80        x1 ~~ x12  1.504 -0.229  -0.229   -0.062   -0.062\n81        x2 ~~  x3  3.508  0.205   0.205    0.088    0.088\n82        x2 ~~  x4  6.780 -0.229  -0.229   -0.393   -0.393\n83        x2 ~~  x5  1.449  0.106   0.106    0.058    0.058\n84        x2 ~~  x6  0.102  0.026   0.026    0.016    0.016\n85        x2 ~~  x7  1.144 -0.130  -0.130   -0.053   -0.053\n86        x2 ~~  x8  0.366 -0.065  -0.065   -0.031   -0.031\n87        x2 ~~  x9  1.877  0.196   0.196    0.067    0.067\n88        x2 ~~ x10  0.434 -0.062  -0.062   -0.032   -0.032\n89        x2 ~~ x11  1.599  0.169   0.169    0.064    0.064\n90        x2 ~~ x12  0.726 -0.117  -0.117   -0.043   -0.043\n91        x3 ~~  x4  0.107 -0.037  -0.037   -0.051   -0.051\n92        x3 ~~  x5  0.024  0.017   0.017    0.008    0.008\n93        x3 ~~  x6  0.211  0.048   0.048    0.024    0.024\n94        x3 ~~  x7  0.009  0.015   0.015    0.005    0.005\n95        x3 ~~  x8  5.281 -0.310  -0.310   -0.117   -0.117\n96        x3 ~~  x9  0.031  0.031   0.031    0.009    0.009\n97        x3 ~~ x10  3.545 -0.221  -0.221   -0.092   -0.092\n98        x3 ~~ x11  5.967  0.408   0.408    0.124    0.124\n99        x3 ~~ x12  0.055 -0.040  -0.040   -0.012   -0.012\n100       x4 ~~  x5  0.063 -0.016  -0.016   -0.028   -0.028\n101       x4 ~~  x6  0.052  0.015   0.015    0.029    0.029\n102       x4 ~~  x7  2.114  0.131   0.131    0.170    0.170\n103       x4 ~~  x8  0.208  0.037   0.037    0.057    0.057\n104       x4 ~~  x9  0.887 -0.091  -0.091   -0.100   -0.100\n105       x4 ~~ x10  1.063  0.065   0.065    0.109    0.109\n106       x4 ~~ x11  2.637 -0.149  -0.149   -0.181   -0.181\n107       x4 ~~ x12  0.169  0.039   0.039    0.046    0.046\n108       x5 ~~  x6  0.370  0.057   0.057    0.036    0.036\n109       x5 ~~  x7  0.292 -0.072  -0.072   -0.030   -0.030\n110       x5 ~~  x8  0.007  0.010   0.010    0.005    0.005\n111       x5 ~~  x9  0.822  0.133   0.133    0.047    0.047\n112       x5 ~~ x10  0.339  0.056   0.056    0.030    0.030\n113       x5 ~~ x11  1.126 -0.145  -0.145   -0.056   -0.056\n114       x5 ~~ x12  1.143 -0.151  -0.151   -0.057   -0.057\n115       x6 ~~  x7  2.528 -0.215  -0.215   -0.101   -0.101\n116       x6 ~~  x8  0.053  0.029   0.029    0.016    0.016\n117       x6 ~~  x9  1.056 -0.141  -0.141   -0.056   -0.056\n118       x6 ~~ x10  0.598  0.069   0.069    0.042    0.042\n119       x6 ~~ x11  0.248  0.064   0.064    0.028    0.028\n120       x6 ~~ x12  1.667  0.170   0.170    0.073    0.073\n121       x7 ~~  x8  1.431  0.206   0.206    0.074    0.074\n122       x7 ~~  x9  0.032 -0.036  -0.036   -0.009   -0.009\n123       x7 ~~ x10  1.521 -0.163  -0.163   -0.065   -0.065\n124       x7 ~~ x11  0.263 -0.097  -0.097   -0.028   -0.028\n125       x7 ~~ x12  0.637  0.156   0.156    0.044    0.044\n126       x8 ~~  x9  1.621  0.227   0.227    0.068    0.068\n127       x8 ~~ x10  1.311  0.134   0.134    0.061    0.061\n128       x8 ~~ x11  2.144 -0.244  -0.244   -0.081   -0.081\n129       x8 ~~ x12  0.591 -0.132  -0.132   -0.043   -0.043\n130       x9 ~~ x10 19.846  0.862   0.862    0.288    0.288\n131       x9 ~~ x11  2.908 -0.518  -0.518   -0.126   -0.126\n132       x9 ~~ x12  7.696 -0.876  -0.876   -0.207   -0.207\n133      x10 ~~ x11  7.331 -0.534  -0.534   -0.197   -0.197\n134      x10 ~~ x12  5.572 -0.484  -0.484   -0.174   -0.174\n135      x11 ~~ x12 26.947  1.711   1.711    0.447    0.447\n\n\nLe considerazioni precedenti, dunque, suggeriscono che il modello potrebbe non avere descritto in maniera adeguata le relazioni tra x4 e i fattori comuni latenti. In base a considerazioni teoriche, supponiamo che abbia senso pensare che x4 saturi non solo sul fattore Motivi di coping ma anche sul fattore di Motivi Sociali. Specifichiamo dunque un nuovo modello nel modo seguente.\n\nmodel2 &lt;- '\n  copingm  =~ x1 + x2 + x3 + x4\n  socialm  =~ x4 + x5 + x6 + x7 + x8\n  enhancem =~ x9 + x10 + x11 + x12\n'\n\nAdattiamo il modello.\n\nfit2 &lt;- cfa(\n  model2, \n  sample.cov = covs, \n  sample.nobs = 500, \n  mimic = \"mplus\"\n)\n\nEsaminiamo gli indici di bontà di adattamento.\n\nprint(effectsize::interpret(fit2))\n\n    Name  Value Threshold Interpretation\n1    GFI 0.9768      0.95   satisfactory\n2   AGFI 0.9583      0.90   satisfactory\n3    NFI 0.9583      0.90   satisfactory\n4   NNFI 0.9839      0.90   satisfactory\n5    CFI 0.9878      0.90   satisfactory\n6  RMSEA 0.0279      0.05   satisfactory\n7   SRMR 0.0289      0.08   satisfactory\n8    RFI 0.9449      0.90   satisfactory\n9   PNFI 0.7260      0.50   satisfactory\n10   IFI 0.9880      0.90   satisfactory\n\n\nLa bontà di adattamento è migliorata.\nEsaminiamo la soluzione standardizzata. Vediamo ora che sono scomparse le due anomalie trovate in precedenza.\n\nprint(standardizedSolution(fit2))\n\n        lhs op      rhs est.std    se     z pvalue ci.lower ci.upper\n1   copingm =~       x1   0.514 0.043 12.03      0    0.430    0.597\n2   copingm =~       x2   0.515 0.043 12.07      0    0.431    0.599\n3   copingm =~       x3   0.516 0.043 12.11      0    0.432    0.600\n4   copingm =~       x4   0.538 0.062  8.66      0    0.416    0.660\n5   socialm =~       x4   0.439 0.061  7.20      0    0.320    0.558\n6   socialm =~       x5   0.632 0.032 20.00      0    0.570    0.694\n7   socialm =~       x6   0.746 0.025 29.28      0    0.696    0.796\n8   socialm =~       x7   0.691 0.028 24.23      0    0.635    0.746\n9   socialm =~       x8   0.731 0.026 27.76      0    0.679    0.782\n10 enhancem =~       x9   0.603 0.039 15.62      0    0.527    0.678\n11 enhancem =~      x10   0.595 0.039 15.31      0    0.519    0.671\n12 enhancem =~      x11   0.665 0.037 18.19      0    0.593    0.737\n13 enhancem =~      x12   0.663 0.037 18.10      0    0.591    0.735\n14       x1 ~~       x1   0.736 0.044 16.79      0    0.650    0.822\n15       x2 ~~       x2   0.735 0.044 16.73      0    0.649    0.821\n16       x3 ~~       x3   0.734 0.044 16.68      0    0.647    0.820\n17       x4 ~~       x4   0.230 0.037  6.29      0    0.158    0.301\n18       x5 ~~       x5   0.601 0.040 15.04      0    0.522    0.679\n19       x6 ~~       x6   0.443 0.038 11.63      0    0.368    0.517\n20       x7 ~~       x7   0.523 0.039 13.29      0    0.446    0.600\n21       x8 ~~       x8   0.466 0.038 12.11      0    0.390    0.541\n22       x9 ~~       x9   0.637 0.046 13.70      0    0.546    0.728\n23      x10 ~~      x10   0.646 0.046 13.99      0    0.556    0.737\n24      x11 ~~      x11   0.558 0.049 11.47      0    0.463    0.653\n25      x12 ~~      x12   0.561 0.049 11.55      0    0.465    0.656\n26  copingm ~~  copingm   1.000 0.000    NA     NA    1.000    1.000\n27  socialm ~~  socialm   1.000 0.000    NA     NA    1.000    1.000\n28 enhancem ~~ enhancem   1.000 0.000    NA     NA    1.000    1.000\n29  copingm ~~  socialm   0.610 0.057 10.74      0    0.498    0.721\n30  copingm ~~ enhancem   0.350 0.059  5.96      0    0.235    0.465\n31  socialm ~~ enhancem   0.265 0.055  4.79      0    0.156    0.373\n32       x1 ~1            0.000 0.045  0.00      1   -0.088    0.088\n33       x2 ~1            0.000 0.045  0.00      1   -0.088    0.088\n34       x3 ~1            0.000 0.045  0.00      1   -0.088    0.088\n35       x4 ~1            0.000 0.045  0.00      1   -0.088    0.088\n36       x5 ~1            0.000 0.045  0.00      1   -0.088    0.088\n37       x6 ~1            0.000 0.045  0.00      1   -0.088    0.088\n38       x7 ~1            0.000 0.045  0.00      1   -0.088    0.088\n39       x8 ~1            0.000 0.045  0.00      1   -0.088    0.088\n40       x9 ~1            0.000 0.045  0.00      1   -0.088    0.088\n41      x10 ~1            0.000 0.045  0.00      1   -0.088    0.088\n42      x11 ~1            0.000 0.045  0.00      1   -0.088    0.088\n43      x12 ~1            0.000 0.045  0.00      1   -0.088    0.088\n44  copingm ~1            0.000 0.000    NA     NA    0.000    0.000\n45  socialm ~1            0.000 0.000    NA     NA    0.000    0.000\n46 enhancem ~1            0.000 0.000    NA     NA    0.000    0.000\n\n\nEsaminando i MI, notiamo che il modello potrebbe migliorare se introduciamo una correlazione tra le specificità x11 e x12.\n\nprint(modindices(fit2))\n\n         lhs op rhs     mi    epc sepc.lv sepc.all sepc.nox\n47   copingm =~  x5  0.076  0.032   0.034    0.020    0.020\n48   copingm =~  x6  1.413  0.143   0.151    0.086    0.086\n49   copingm =~  x7  0.245  0.083   0.088    0.035    0.035\n50   copingm =~  x8  3.668 -0.295  -0.311   -0.137   -0.137\n51   copingm =~  x9  0.243  0.066   0.069    0.026    0.026\n52   copingm =~ x10  0.566  0.065   0.069    0.040    0.040\n53   copingm =~ x11  0.119 -0.044  -0.046   -0.018   -0.018\n54   copingm =~ x12  0.598 -0.102  -0.108   -0.041   -0.041\n55   socialm =~  x1  1.948 -0.396  -0.245   -0.119   -0.119\n56   socialm =~  x2  0.718  0.177   0.110    0.072    0.072\n57   socialm =~  x3  0.298  0.144   0.089    0.047    0.047\n58   socialm =~  x9  0.316  0.114   0.071    0.026    0.026\n59   socialm =~ x10  3.169  0.236   0.146    0.084    0.084\n60   socialm =~ x11  4.927 -0.430  -0.266   -0.104   -0.104\n61   socialm =~ x12  0.017  0.026   0.016    0.006    0.006\n62  enhancem =~  x1  0.314  0.040   0.064    0.031    0.031\n63  enhancem =~  x2  0.003  0.003   0.004    0.003    0.003\n64  enhancem =~  x3  0.037 -0.013  -0.020   -0.011   -0.011\n65  enhancem =~  x4  0.106 -0.013  -0.021   -0.015   -0.015\n66  enhancem =~  x5  0.464 -0.034  -0.055   -0.032   -0.032\n67  enhancem =~  x6  2.703  0.079   0.128    0.072    0.072\n68  enhancem =~  x7  0.467 -0.048  -0.077   -0.031   -0.031\n69  enhancem =~  x8  0.095 -0.019  -0.031   -0.014   -0.014\n70        x1 ~~  x2  1.966  0.187   0.187    0.081    0.081\n71        x1 ~~  x3  2.042 -0.241  -0.241   -0.083   -0.083\n72        x1 ~~  x4  0.775  0.098   0.098    0.082    0.082\n73        x1 ~~  x5  0.238 -0.058  -0.058   -0.024   -0.024\n74        x1 ~~  x6  0.187 -0.048  -0.048   -0.023   -0.023\n75        x1 ~~  x7  0.019 -0.022  -0.022   -0.007   -0.007\n76        x1 ~~  x8  0.366 -0.087  -0.087   -0.032   -0.032\n77        x1 ~~  x9  0.155  0.076   0.076    0.020    0.020\n78        x1 ~~ x10  0.104  0.041   0.041    0.016    0.016\n79        x1 ~~ x11  2.019  0.255   0.255    0.075    0.075\n80        x1 ~~ x12  1.911 -0.257  -0.257   -0.073   -0.073\n81        x2 ~~  x3  0.035 -0.023  -0.023   -0.011   -0.011\n82        x2 ~~  x4  3.029 -0.144  -0.144   -0.163   -0.163\n83        x2 ~~  x5  2.503  0.138   0.138    0.079    0.079\n84        x2 ~~  x6  0.509  0.058   0.058    0.038    0.038\n85        x2 ~~  x7  0.471 -0.082  -0.082   -0.035   -0.035\n86        x2 ~~  x8  0.015  0.013   0.013    0.006    0.006\n87        x2 ~~  x9  1.289  0.161   0.161    0.058    0.058\n88        x2 ~~ x10  0.467 -0.064  -0.064   -0.035   -0.035\n89        x2 ~~ x11  0.338  0.077   0.077    0.031    0.031\n90        x2 ~~ x12  0.970 -0.135  -0.135   -0.052   -0.052\n91        x3 ~~  x4  1.095  0.109   0.109    0.098    0.098\n92        x3 ~~  x5  0.169  0.045   0.045    0.021    0.021\n93        x3 ~~  x6  0.681  0.085   0.085    0.044    0.044\n94        x3 ~~  x7  0.315  0.085   0.085    0.029    0.029\n95        x3 ~~  x8  3.075 -0.235  -0.235   -0.092   -0.092\n96        x3 ~~  x9  0.022 -0.026  -0.026   -0.008   -0.008\n97        x3 ~~ x10  3.825 -0.230  -0.230   -0.100   -0.100\n98        x3 ~~ x11  3.498  0.313   0.313    0.099    0.099\n99        x3 ~~ x12  0.079 -0.049  -0.049   -0.015   -0.015\n100       x4 ~~  x5  0.337 -0.037  -0.037   -0.041   -0.041\n101       x4 ~~  x6  0.033 -0.012  -0.012   -0.015   -0.015\n102       x4 ~~  x7  1.053  0.094   0.094    0.077    0.077\n103       x4 ~~  x8  0.071 -0.022  -0.022   -0.021   -0.021\n104       x4 ~~  x9  0.541 -0.070  -0.070   -0.048   -0.048\n105       x4 ~~ x10  1.128  0.066   0.066    0.070    0.070\n106       x4 ~~ x11  1.313 -0.102  -0.102   -0.079   -0.079\n107       x4 ~~ x12  0.322  0.052   0.052    0.039    0.039\n108       x5 ~~  x6  0.504  0.066   0.066    0.042    0.042\n109       x5 ~~  x7  0.262 -0.068  -0.068   -0.028   -0.028\n110       x5 ~~  x8  0.004  0.008   0.008    0.004    0.004\n111       x5 ~~  x9  0.850  0.135   0.135    0.047    0.047\n112       x5 ~~ x10  0.288  0.052   0.052    0.027    0.027\n113       x5 ~~ x11  1.019 -0.138  -0.138   -0.054   -0.054\n114       x5 ~~ x12  1.224 -0.157  -0.157   -0.059   -0.059\n115       x6 ~~  x7  2.404 -0.209  -0.209   -0.099   -0.099\n116       x6 ~~  x8  0.034  0.023   0.023    0.012    0.012\n117       x6 ~~  x9  0.978 -0.135  -0.135   -0.054   -0.054\n118       x6 ~~ x10  0.524  0.065   0.065    0.039    0.039\n119       x6 ~~ x11  0.341  0.074   0.074    0.033    0.033\n120       x6 ~~ x12  1.520  0.163   0.163    0.069    0.069\n121       x7 ~~  x8  1.171  0.186   0.186    0.067    0.067\n122       x7 ~~  x9  0.020 -0.028  -0.028   -0.007   -0.007\n123       x7 ~~ x10  1.593 -0.167  -0.167   -0.066   -0.066\n124       x7 ~~ x11  0.175 -0.079  -0.079   -0.023   -0.023\n125       x7 ~~ x12  0.586  0.149   0.149    0.042    0.042\n126       x8 ~~  x9  1.808  0.239   0.239    0.072    0.072\n127       x8 ~~ x10  1.267  0.131   0.131    0.060    0.060\n128       x8 ~~ x11  1.791 -0.222  -0.222   -0.075   -0.075\n129       x8 ~~ x12  0.595 -0.132  -0.132   -0.043   -0.043\n130       x9 ~~ x10 20.103  0.864   0.864    0.288    0.288\n131       x9 ~~ x11  3.658 -0.582  -0.582   -0.142   -0.142\n132       x9 ~~ x12  7.229 -0.845  -0.845   -0.199   -0.199\n133      x10 ~~ x11  7.617 -0.543  -0.543   -0.201   -0.201\n134      x10 ~~ x12  4.512 -0.431  -0.431   -0.154   -0.154\n135      x11 ~~ x12 26.071  1.680   1.680    0.440    0.440\n\n\nIl nuovo modello diventa dunque il seguente.\n\nmodel3 &lt;- '\n  copingm  =~ x1 + x2 + x3 + x4\n  socialm  =~ x4 + x5 + x6 + x7 + x8\n  enhancem =~ x9 + x10 + x11 + x12\n  x11 ~~ x12\n'\n\nAdattiamo il modello.\n\nfit3 &lt;- cfa(\n  model3, \n  sample.cov = covs, \n  sample.nobs = 500, \n  mimic = \"mplus\"\n)\n\nUn test basato sul rapporto di verosimiglianze conferma che il miglioramento di adattamento è sostanziale.\n\nprint(lavTestLRT(fit2, fit3))\n\n\nChi-Squared Difference Test\n\n     Df   AIC   BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)    \nfit3 49 23934 24107  45.0                                        \nfit2 50 23957 24125  69.4       24.5 0.217       1    7.5e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nEsaminiamo gli indici di bontà di adattamento.\n\nprint(summary(fit3, fit.measures = TRUE))\n\nlavaan 0.6-19 ended normally after 61 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        41\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                                44.955\n  Degrees of freedom                                49\n  P-value (Chi-square)                           0.638\n\nModel Test Baseline Model:\n\n  Test statistic                              1664.026\n  Degrees of freedom                                66\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.003\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -11926.170\n  Loglikelihood unrestricted model (H1)     -11903.692\n                                                      \n  Akaike (AIC)                               23934.339\n  Bayesian (BIC)                             24107.138\n  Sample-size adjusted Bayesian (SABIC)      23977.002\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.025\n  P-value H_0: RMSEA &lt;= 0.050                    1.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.023\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  copingm =~                                          \n    x1                1.000                           \n    x2                0.740    0.094    7.909    0.000\n    x3                0.933    0.118    7.903    0.000\n    x4                0.719    0.118    6.070    0.000\n  socialm =~                                          \n    x4                1.000                           \n    x5                1.771    0.273    6.485    0.000\n    x6                2.141    0.319    6.703    0.000\n    x7                2.784    0.421    6.611    0.000\n    x8                2.689    0.402    6.681    0.000\n  enhancem =~                                         \n    x9                1.000                           \n    x10               0.648    0.070    9.293    0.000\n    x11               0.776    0.093    8.340    0.000\n    x12               0.802    0.096    8.327    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .x11 ~~                                              \n   .x12               1.460    0.300    4.873    0.000\n  copingm ~~                                          \n    socialm           0.398    0.071    5.603    0.000\n    enhancem          0.669    0.145    4.613    0.000\n  socialm ~~                                          \n    enhancem          0.320    0.084    3.783    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                0.000    0.092    0.000    1.000\n   .x2                0.000    0.068    0.000    1.000\n   .x3                0.000    0.086    0.000    1.000\n   .x4                0.000    0.063    0.000    1.000\n   .x5                0.000    0.077    0.000    1.000\n   .x6                0.000    0.079    0.000    1.000\n   .x7                0.000    0.111    0.000    1.000\n   .x8                0.000    0.101    0.000    1.000\n   .x9                0.000    0.120    0.000    1.000\n   .x10               0.000    0.078    0.000    1.000\n   .x11               0.000    0.115    0.000    1.000\n   .x12               0.000    0.119    0.000    1.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                3.117    0.230   13.546    0.000\n   .x2                1.694    0.125   13.527    0.000\n   .x3                2.705    0.200   13.536    0.000\n   .x4                0.454    0.070    6.502    0.000\n   .x5                1.794    0.130   13.835    0.000\n   .x6                1.384    0.115   12.015    0.000\n   .x7                3.240    0.248   13.089    0.000\n   .x8                2.393    0.194   12.352    0.000\n   .x9                3.958    0.400    9.895    0.000\n   .x10               1.710    0.170   10.063    0.000\n   .x11               4.657    0.371   12.545    0.000\n   .x12               4.997    0.398   12.561    0.000\n    copingm           1.118    0.217    5.158    0.000\n    socialm           0.380    0.110    3.469    0.001\n    enhancem          3.210    0.490    6.550    0.000\n\n\n\nGli indici di fit sono migliorati.\nEsaminiamo la soluzione standardizzata.\n\nprint(standardizedSolution(fit3))\n\n        lhs op      rhs est.std    se     z pvalue ci.lower ci.upper\n1   copingm =~       x1   0.514 0.043 12.02      0    0.430    0.598\n2   copingm =~       x2   0.515 0.043 12.05      0    0.431    0.599\n3   copingm =~       x3   0.514 0.043 12.04      0    0.431    0.598\n4   copingm =~       x4   0.540 0.063  8.61      0    0.417    0.663\n5   socialm =~       x4   0.438 0.061  7.13      0    0.317    0.558\n6   socialm =~       x5   0.632 0.032 20.00      0    0.570    0.694\n7   socialm =~       x6   0.746 0.025 29.29      0    0.697    0.796\n8   socialm =~       x7   0.690 0.029 24.21      0    0.634    0.746\n9   socialm =~       x8   0.731 0.026 27.80      0    0.680    0.783\n10 enhancem =~       x9   0.669 0.041 16.39      0    0.589    0.749\n11 enhancem =~      x10   0.664 0.041 16.24      0    0.584    0.744\n12 enhancem =~      x11   0.542 0.045 12.12      0    0.454    0.629\n13 enhancem =~      x12   0.541 0.045 12.08      0    0.453    0.628\n14      x11 ~~      x12   0.303 0.050  6.10      0    0.205    0.400\n15       x1 ~~       x1   0.736 0.044 16.76      0    0.650    0.822\n16       x2 ~~       x2   0.735 0.044 16.70      0    0.649    0.821\n17       x3 ~~       x3   0.735 0.044 16.73      0    0.649    0.822\n18       x4 ~~       x4   0.229 0.037  6.22      0    0.157    0.301\n19       x5 ~~       x5   0.601 0.040 15.04      0    0.522    0.679\n20       x6 ~~       x6   0.443 0.038 11.64      0    0.368    0.517\n21       x7 ~~       x7   0.524 0.039 13.31      0    0.447    0.601\n22       x8 ~~       x8   0.465 0.038 12.10      0    0.390    0.541\n23       x9 ~~       x9   0.552 0.055 10.10      0    0.445    0.659\n24      x10 ~~      x10   0.559 0.054 10.31      0    0.453    0.666\n25      x11 ~~      x11   0.706 0.048 14.58      0    0.611    0.801\n26      x12 ~~      x12   0.708 0.048 14.62      0    0.613    0.802\n27  copingm ~~  copingm   1.000 0.000    NA     NA    1.000    1.000\n28  socialm ~~  socialm   1.000 0.000    NA     NA    1.000    1.000\n29 enhancem ~~ enhancem   1.000 0.000    NA     NA    1.000    1.000\n30  copingm ~~  socialm   0.610 0.057 10.73      0    0.499    0.721\n31  copingm ~~ enhancem   0.353 0.060  5.84      0    0.235    0.472\n32  socialm ~~ enhancem   0.289 0.056  5.14      0    0.179    0.399\n33       x1 ~1            0.000 0.045  0.00      1   -0.088    0.088\n34       x2 ~1            0.000 0.045  0.00      1   -0.088    0.088\n35       x3 ~1            0.000 0.045  0.00      1   -0.088    0.088\n36       x4 ~1            0.000 0.045  0.00      1   -0.088    0.088\n37       x5 ~1            0.000 0.045  0.00      1   -0.088    0.088\n38       x6 ~1            0.000 0.045  0.00      1   -0.088    0.088\n39       x7 ~1            0.000 0.045  0.00      1   -0.088    0.088\n40       x8 ~1            0.000 0.045  0.00      1   -0.088    0.088\n41       x9 ~1            0.000 0.045  0.00      1   -0.088    0.088\n42      x10 ~1            0.000 0.045  0.00      1   -0.088    0.088\n43      x11 ~1            0.000 0.045  0.00      1   -0.088    0.088\n44      x12 ~1            0.000 0.045  0.00      1   -0.088    0.088\n45  copingm ~1            0.000 0.000    NA     NA    0.000    0.000\n46  socialm ~1            0.000 0.000    NA     NA    0.000    0.000\n47 enhancem ~1            0.000 0.000    NA     NA    0.000    0.000\n\n\nNon ci sono ulteriori motivi di preoccupazione. Brown (2015) conclude che il modello più adeguato sia model3.\nNel caso presente, a mio parare, l’introduzione della correlazione residua tra x11 e x12 si sarebbe anche potuta evitare, dato che il modello model3 (con meno idiosincrasie legate al campione) si era già dimostrato adeguato.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#saturazione-sul-fattore-sbagliato",
    "href": "chapters/sem/06_refine_solution.html#saturazione-sul-fattore-sbagliato",
    "title": "50  La revisione del modello",
    "section": "50.8 Saturazione sul fattore sbagliato",
    "text": "50.8 Saturazione sul fattore sbagliato\nBrown (2015) considera anche il caso opposto, ovvero quello nel quale il ricercatore ipotizza una saturazione spuria. Per i dati in discussione, si può avere la situazione presente.\n\nmodel4 &lt;- '\n  copingm  =~ x1 + x2 + x3 + x4\n  socialm  =~ x4 +x5 + x6 + x7 + x8 + x12\n  enhancem =~ x9 + x10 + x11\n'\n\nAdattiamo il modello ai dati.\n\nfit4 &lt;- cfa(\n  model4, \n  sample.cov = covs, \n  sample.nobs = 500, \n  mimic = \"mplus\"\n)\n\nEsaminiamo la soluzione ottenuta.\n\nprint(summary(fit4, fit.measures = TRUE))\n\nlavaan 0.6-19 ended normally after 59 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        40\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                               212.717\n  Degrees of freedom                                50\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              1664.026\n  Degrees of freedom                                66\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.898\n  Tucker-Lewis Index (TLI)                       0.866\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -12010.051\n  Loglikelihood unrestricted model (H1)     -11903.692\n                                                      \n  Akaike (AIC)                               24100.101\n  Bayesian (BIC)                             24268.685\n  Sample-size adjusted Bayesian (SABIC)      24141.723\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.081\n  90 Percent confidence interval - lower         0.070\n  90 Percent confidence interval - upper         0.092\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.554\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.073\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  copingm =~                                          \n    x1                1.000                           \n    x2                0.741    0.093    7.925    0.000\n    x3                0.932    0.118    7.906    0.000\n    x4                0.699    0.117    5.995    0.000\n  socialm =~                                          \n    x4                1.000                           \n    x5                1.725    0.260    6.634    0.000\n    x6                2.098    0.305    6.879    0.000\n    x7                2.717    0.401    6.775    0.000\n    x8                2.619    0.382    6.848    0.000\n    x12               0.900    0.236    3.818    0.000\n  enhancem =~                                         \n    x9                1.000                           \n    x10               0.638    0.076    8.408    0.000\n    x11               0.767    0.094    8.153    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  copingm ~~                                          \n    socialm           0.410    0.072    5.663    0.000\n    enhancem          0.661    0.148    4.456    0.000\n  socialm ~~                                          \n    enhancem          0.347    0.089    3.902    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                0.000    0.092    0.000    1.000\n   .x2                0.000    0.068    0.000    1.000\n   .x3                0.000    0.086    0.000    1.000\n   .x4                0.000    0.063    0.000    1.000\n   .x5                0.000    0.077    0.000    1.000\n   .x6                0.000    0.079    0.000    1.000\n   .x7                0.000    0.111    0.000    1.000\n   .x8                0.000    0.101    0.000    1.000\n   .x12               0.000    0.119    0.000    1.000\n   .x9                0.000    0.120    0.000    1.000\n   .x10               0.000    0.078    0.000    1.000\n   .x11               0.000    0.115    0.000    1.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                3.106    0.230   13.478    0.000\n   .x2                1.686    0.125   13.449    0.000\n   .x3                2.698    0.200   13.477    0.000\n   .x4                0.463    0.069    6.719    0.000\n   .x5                1.805    0.130   13.886    0.000\n   .x6                1.378    0.115   12.022    0.000\n   .x7                3.255    0.248   13.143    0.000\n   .x8                2.418    0.194   12.455    0.000\n   .x12               6.740    0.430   15.673    0.000\n   .x9                3.891    0.436    8.933    0.000\n   .x10               1.724    0.183    9.435    0.000\n   .x11               4.662    0.371   12.579    0.000\n    copingm           1.129    0.218    5.170    0.000\n    socialm           0.397    0.111    3.566    0.000\n    enhancem          3.277    0.524    6.258    0.000\n\n\n\nÈ chiaro che il modello model4 è inadeguato. Il problema emerge chiaramente anche esaminando i MI.\n\nprint(modindices(fit4))\n\n         lhs op rhs      mi    epc sepc.lv sepc.all sepc.nox\n47   copingm =~  x5   0.090  0.036   0.038    0.022    0.022\n48   copingm =~  x6   0.554  0.090   0.096    0.054    0.054\n49   copingm =~  x7   0.107  0.055   0.059    0.024    0.024\n50   copingm =~  x8   3.919 -0.306  -0.325   -0.143   -0.143\n51   copingm =~ x12   6.109  0.499   0.530    0.199    0.199\n52   copingm =~  x9   0.390 -0.096  -0.102   -0.038   -0.038\n53   copingm =~ x10   0.027 -0.016  -0.017   -0.010   -0.010\n54   copingm =~ x11   0.823  0.123   0.131    0.051    0.051\n55   socialm =~  x1   1.990 -0.398  -0.251   -0.122   -0.122\n56   socialm =~  x2   0.638  0.166   0.105    0.069    0.069\n57   socialm =~  x3   0.372  0.160   0.101    0.053    0.053\n58   socialm =~  x9   0.315 -0.130  -0.082   -0.031   -0.031\n59   socialm =~ x10   1.423  0.179   0.113    0.064    0.064\n60   socialm =~ x11   0.520 -0.150  -0.094   -0.037   -0.037\n61  enhancem =~  x1   1.029  0.067   0.121    0.059    0.059\n62  enhancem =~  x2   0.232  0.023   0.042    0.028    0.028\n63  enhancem =~  x3   0.153 -0.024  -0.043   -0.023   -0.023\n64  enhancem =~  x4   0.745 -0.031  -0.056   -0.040   -0.040\n65  enhancem =~  x5   0.343 -0.028  -0.050   -0.029   -0.029\n66  enhancem =~  x6   0.103  0.015   0.027    0.015    0.015\n67  enhancem =~  x7   2.752 -0.110  -0.198   -0.080   -0.080\n68  enhancem =~  x8   0.129 -0.021  -0.038   -0.017   -0.017\n69  enhancem =~ x12 116.781  0.916   1.658    0.624    0.624\n70        x1 ~~  x2   1.709  0.177   0.177    0.077    0.077\n71        x1 ~~  x3   2.273 -0.257  -0.257   -0.089   -0.089\n72        x1 ~~  x4   0.850  0.103   0.103    0.086    0.086\n73        x1 ~~  x5   0.292 -0.064  -0.064   -0.027   -0.027\n74        x1 ~~  x6   0.188 -0.048  -0.048   -0.023   -0.023\n75        x1 ~~  x7   0.023 -0.025  -0.025   -0.008   -0.008\n76        x1 ~~  x8   0.419 -0.093  -0.093   -0.034   -0.034\n77        x1 ~~ x12   0.025 -0.034  -0.034   -0.007   -0.007\n78        x1 ~~  x9   0.011  0.020   0.020    0.006    0.006\n79        x1 ~~ x10   0.004  0.008   0.008    0.003    0.003\n80        x1 ~~ x11   1.804  0.259   0.259    0.068    0.068\n81        x2 ~~  x3   0.071 -0.034  -0.034   -0.016   -0.016\n82        x2 ~~  x4   2.979 -0.143  -0.143   -0.162   -0.162\n83        x2 ~~  x5   2.403  0.135   0.135    0.077    0.077\n84        x2 ~~  x6   0.551  0.060   0.060    0.040    0.040\n85        x2 ~~  x7   0.457 -0.081  -0.081   -0.035   -0.035\n86        x2 ~~  x8   0.012  0.011   0.011    0.006    0.006\n87        x2 ~~ x12   0.134 -0.058  -0.058   -0.017   -0.017\n88        x2 ~~  x9   1.033  0.145   0.145    0.056    0.056\n89        x2 ~~ x10   1.140 -0.100  -0.100   -0.058   -0.058\n90        x2 ~~ x11   0.323  0.081   0.081    0.029    0.029\n91        x3 ~~  x4   1.472  0.127   0.127    0.113    0.113\n92        x3 ~~  x5   0.140  0.041   0.041    0.019    0.019\n93        x3 ~~  x6   0.717  0.087   0.087    0.045    0.045\n94        x3 ~~  x7   0.317  0.086   0.086    0.029    0.029\n95        x3 ~~  x8   3.121 -0.237  -0.237   -0.093   -0.093\n96        x3 ~~ x12   0.001  0.006   0.006    0.001    0.001\n97        x3 ~~  x9   0.000  0.003   0.003    0.001    0.001\n98        x3 ~~ x10   4.165 -0.241  -0.241   -0.111   -0.111\n99        x3 ~~ x11   3.806  0.350   0.350    0.099    0.099\n100       x4 ~~  x5   0.316 -0.036  -0.036   -0.039   -0.039\n101       x4 ~~  x6   0.052 -0.015  -0.015   -0.019   -0.019\n102       x4 ~~  x7   1.182  0.099   0.099    0.081    0.081\n103       x4 ~~  x8   0.062 -0.021  -0.021   -0.020   -0.020\n104       x4 ~~ x12   0.033  0.020   0.020    0.011    0.011\n105       x4 ~~  x9   1.418 -0.115  -0.115   -0.086   -0.086\n106       x4 ~~ x10   0.914  0.061   0.061    0.068    0.068\n107       x4 ~~ x11   0.517 -0.068  -0.068   -0.047   -0.047\n108       x5 ~~  x6   0.611  0.073   0.073    0.046    0.046\n109       x5 ~~  x7   0.115 -0.045  -0.045   -0.019   -0.019\n110       x5 ~~  x8   0.079  0.034   0.034    0.016    0.016\n111       x5 ~~ x12   3.265 -0.302  -0.302   -0.087   -0.087\n112       x5 ~~  x9   0.203  0.066   0.066    0.025    0.025\n113       x5 ~~ x10   0.000  0.002   0.002    0.001    0.001\n114       x5 ~~ x11   2.312 -0.224  -0.224   -0.077   -0.077\n115       x6 ~~  x7   2.239 -0.200  -0.200   -0.094   -0.094\n116       x6 ~~  x8   0.073  0.033   0.033    0.018    0.018\n117       x6 ~~ x12   0.478  0.109   0.109    0.036    0.036\n118       x6 ~~  x9   1.251 -0.153  -0.153   -0.066   -0.066\n119       x6 ~~ x10   0.784  0.079   0.079    0.051    0.051\n120       x6 ~~ x11   0.370  0.083   0.083    0.033    0.033\n121       x7 ~~  x8   1.644  0.219   0.219    0.078    0.078\n122       x7 ~~ x12   0.433 -0.152  -0.152   -0.032   -0.032\n123       x7 ~~  x9   0.005 -0.015  -0.015   -0.004   -0.004\n124       x7 ~~ x10   1.836 -0.179  -0.179   -0.076   -0.076\n125       x7 ~~ x11   0.348 -0.119  -0.119   -0.031   -0.031\n126       x8 ~~ x12   2.680 -0.335  -0.335   -0.083   -0.083\n127       x8 ~~  x9   0.676  0.147   0.147    0.048    0.048\n128       x8 ~~ x10   0.337  0.068   0.068    0.033    0.033\n129       x8 ~~ x11   3.437 -0.330  -0.330   -0.098   -0.098\n130      x12 ~~  x9   7.051  0.713   0.713    0.139    0.139\n131      x12 ~~ x10   6.960  0.465   0.465    0.136    0.136\n132      x12 ~~ x11  68.717  2.238   2.238    0.399    0.399\n133       x9 ~~ x10   0.081  0.138   0.138    0.053    0.053\n134       x9 ~~ x11   0.166  0.209   0.209    0.049    0.049\n135      x10 ~~ x11   0.423 -0.211  -0.211   -0.075   -0.075\n\n\nIl MI relativo alla saturazione di x12 su enhancem è uguale a 116.781. Chiaramente, in una revisione del modello, questo problema dovrebbe essere affrontato.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#commenti-e-considerazioni-finali",
    "href": "chapters/sem/06_refine_solution.html#commenti-e-considerazioni-finali",
    "title": "50  La revisione del modello",
    "section": "50.9 Commenti e considerazioni finali",
    "text": "50.9 Commenti e considerazioni finali\nGli esempi presentati da Brown (2015) mostrano come l’applicazione dei MI, combinata con l’esame delle soluzioni fattoriali, rappresenti un approccio utile per ottimizzare e perfezionare il modello proposto.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/06_refine_solution.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/06_refine_solution.html#informazioni-sullambiente-di-sviluppo",
    "title": "50  La revisione del modello",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] effectsize_0.8.9  MASS_7.3-61       viridis_0.6.5     viridisLite_0.4.2\n [5] ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0  \n [9] bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19    \n[13] psych_2.4.6.26    scales_1.3.0      markdown_1.13     knitr_1.49       \n[17] lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n[21] purrr_1.0.2       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n[25] ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.17.1   jsonlite_1.8.9      datawizard_0.13.0  \n  [4] magrittr_2.0.3      TH.data_1.1-2       estimability_1.5.1 \n  [7] farver_2.1.2        nloptr_2.1.1        rmarkdown_2.29     \n [10] vctrs_0.6.5         minqa_1.2.8         base64enc_0.1-3    \n [13] rstatix_0.7.2       htmltools_0.5.8.1   broom_1.0.7        \n [16] Formula_1.2-5       htmlwidgets_1.6.4   plyr_1.8.9         \n [19] sandwich_3.1-1      emmeans_1.10.5      zoo_1.8-12         \n [22] uuid_1.2-1          igraph_2.1.1        mime_0.12          \n [25] lifecycle_1.0.4     pkgconfig_2.0.3     Matrix_1.7-1       \n [28] R6_2.5.1            fastmap_1.2.0       shiny_1.9.1        \n [31] numDeriv_2016.8-1.1 digest_0.6.37       OpenMx_2.21.13     \n [34] fdrtool_1.2.18      colorspace_2.1-1    rprojroot_2.0.4    \n [37] Hmisc_5.2-0         fansi_1.0.6         timechange_0.3.0   \n [40] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n [43] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n [46] carData_3.0-5       performance_0.12.4  ggsignif_0.6.4     \n [49] corpcor_1.6.10      gtools_3.9.5        tools_4.4.2        \n [52] pbivnorm_0.6.0      foreign_0.8-87      zip_2.3.1          \n [55] httpuv_1.6.15       nnet_7.3-19         glue_1.8.0         \n [58] quadprog_1.5-8      promises_1.3.0      nlme_3.1-166       \n [61] lisrelToR_0.3       grid_4.4.2          pbdZMQ_0.3-13      \n [64] checkmate_2.3.2     cluster_2.1.6       reshape2_1.4.4     \n [67] generics_0.1.3      gtable_0.3.6        tzdb_0.4.0         \n [70] data.table_1.16.2   hms_1.1.3           car_3.1-3          \n [73] utf8_1.2.4          sem_3.1-16          pillar_1.9.0       \n [76] IRdisplay_1.1       rockchalk_1.8.157   later_1.3.2        \n [79] splines_4.4.2       cherryblossom_0.1.0 lattice_0.22-6     \n [82] survival_3.7-0      kutils_1.73         tidyselect_1.2.1   \n [85] miniUI_0.1.1.1      pbapply_1.7-2       airports_0.1.0     \n [88] stats4_4.4.2        xfun_0.49           qgraph_1.9.8       \n [91] arm_1.14-4          stringi_1.8.4       pacman_0.5.1       \n [94] boot_1.3-31         evaluate_1.0.1      codetools_0.2-20   \n [97] mi_1.1              cli_3.6.3           RcppParallel_5.1.9 \n[100] IRkernel_1.3.2      rpart_4.1.23        parameters_0.23.0  \n[103] xtable_1.8-4        repr_1.1.7          munsell_0.5.1      \n[106] Rcpp_1.0.13-1       coda_0.19-4.1       png_0.1-8          \n[109] XML_3.99-0.17       parallel_4.4.2      usdata_0.3.1       \n[112] bayestestR_0.15.0   jpeg_0.1-10         lme4_1.1-35.5      \n[115] mvtnorm_1.3-2       insight_0.20.5      openxlsx_4.2.7.1   \n[118] crayon_1.5.3        openintro_2.5.0     rlang_1.1.4        \n[121] multcomp_1.4-26     mnormt_2.1.1       \n\n\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>La revisione del modello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html",
    "href": "chapters/sem/07_group_invariance.html",
    "title": "51  Invarianza di misura",
    "section": "",
    "text": "51.1 Introduzione\nGli esempi di modelli SEM che abbiamo esaminato fino ad ora sono stati calcolati all’interno di un singolo gruppo. Questi modelli hanno utilizzato come input una sola matrice di covarianza e hanno portato alla stima dei parametri del modello senza imporre restrizioni specifiche. In questo capitolo, ci occuperemo del concetto di invarianza, estendendo le analisi precedenti ad un contesto multi-gruppo.\nL’invarianza in un’analisi SEM multi-gruppo può riferirsi a diversi aspetti del modello. Un approccio utile è suddividere lo studio dell’invarianza in due componenti:\nIn generale, nella modellazione SEM multi-gruppo si cerca di ottenere la massima parsimonia (semplicità) possibile, desiderando un grado elevato di invarianza sia nelle misure che nelle strutture. Tuttavia, i modelli invarianti sono restrittivi e comportano un compromesso tra adattamento del modello e parsimonia. Pertanto, nella pratica, il ricercatore deve trovare un equilibrio tra questi due aspetti. Di conseguenza, quando si adatta un modello multi-gruppo, si tende a scegliere modelli con un certo grado di invarianza che mantengano un adeguato adattamento del modello. Questo può significare invarianza solo nella misurazione, solo nella struttura, o in un sottoinsieme di parametri in entrambi i componenti.\nNel presente capitolo, per valutare l’invarianza di misura, utilizzeremo l’approccio dell’analisi fattoriale confermativa a gruppi multipli (MG-CFA). Esploreremo se sia sensato considerare la stessa struttura fattoriale in gruppi diversi, ossia se la stessa variabile latente viene misurata in modo equivalente tra i diversi gruppi. Questo aspetto, noto come invarianza di misura, è fondamentale perché permette di effettuare confronti tra le medie dei gruppi. Se un dato strumento di misura valuta dimensioni diverse in gruppi differenti, allora non è possibile fare confronti validi tra questi gruppi utilizzando quella misura. In questo capitolo, affronteremo la questione dell’invarianza di misura considerando prima gli indicatori continui e poi quelli categoriali.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#introduzione",
    "href": "chapters/sem/07_group_invariance.html#introduzione",
    "title": "51  Invarianza di misura",
    "section": "",
    "text": "Invarianza nei modelli di misurazione: si verifica se gli strumenti di misurazione sono invarianti o equivalenti tra diversi gruppi. Questo tipo di analisi testa l’uguaglianza dei parametri associati al modello di misurazione tra i gruppi, inclusi i carichi fattoriali, gli intercetti delle variabili di misura e le varianze e covarianze residue.\nInvarianza nei modelli strutturali: si verifica se le strutture latenti sono invarianti o equivalenti tra diversi gruppi. Questa analisi testa l’uguaglianza dei parametri associati al modello di struttura latente tra i gruppi, inclusi i coefficienti di percorso, le medie e le varianze dei fattori e le covarianze tra i fattori.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#indicatori-continui",
    "href": "chapters/sem/07_group_invariance.html#indicatori-continui",
    "title": "51  Invarianza di misura",
    "section": "51.2 Indicatori continui",
    "text": "51.2 Indicatori continui\n\n51.2.1 Intercette degli item\nIn generale, i modelli di equazioni strutturali vengono utilizzati per modellare unicamente la matrice di covarianza delle variabili osservate in un set di dati. Ricordiamo che, quando abbiamo introdotto il modello dell’analisi fattoriale,\n\\[\ny_i = \\mu + \\lambda_j \\xi_k + \\delta_i,\n\\]\nper semplicità abbiamo ignorato la media \\(\\mu\\) degli indicatori esprimendo i dati osservati nei termini degli scarti dalla media, \\(y_i -\\mu\\), in quanto ciò lascia immutate le covarianze. Tuttavia, in alcune applicazioni (quali, appunto, l’invarianza di misura), è utile considerare anche le medie delle variabili osservate. Per includere nel modello fattoriale le informazioni sulle medie facciamo esplicito riferimento all’intercetta della precedente equazione. Usando la sintassi lavaan, la media di una variabile manifesta viene inserita nel modello specificando l’intercetta dell’equazione precedente come segue\n\nmy_item ~ 1\n\nmy_item ~ 1\n\n\nLa parte sinistra dell’espressione precedente contiene il nome della variabile manifesta a cui si fa riferimento; la parte destra dell’espressione precedente specifica la presenza dell’intercetta.\nPer esempio, nella specificazione di un modello a due fattori comuni, è possibile aggiungere al modello le medie delle variabili manifeste nel modo seguente:\n\nmod1 &lt;- \"\n  # two-factor model\n  f1 =~ x1 + x2 + x3\n  f2 =~ x4 + x5 + x6\n  # intercepts\n  x1 ~ 1\n  x2 ~ 1\n  x3 ~ 1\n  x4 ~ 1\n  x5 ~ 1\n  x6 ~ 1\n\"\n\nTuttavia, è più conveniente omettere le intercette nella specificazione del modello e aggiungere l’argomento meanstructure = TRUE nella funzione cfa().\n\nmod2 &lt;- \"\n  f1 =~ x1 + x2 + x3\n  f2 =~ x4 + x5 + x6\n\"\n\nfit &lt;- cfa(\n  mod2,\n  data = d,\n  meanstructure = TRUE\n)\nSi noti che modelli con o senza meanstructure avranno la stessa statistica chi-quadrato e lo stesso numero di gradi di libertà. Il motivo è che, nel caso di un modello con meanstructure, vengono introdotti \\(p\\) nuovi dati (ovvero, il valore della media per ciascuno dei \\(p\\) indicatori) ma vengono anche stimati ulteriori \\(p\\) parametri (ovvero, un’intercetta per ciascuno dei \\(p\\) indicatori). Il risultato finale è che la bontà dell’adattamento resta immutata. In pratica, l’unico motivo per aggiungere le intercette nella sintassi del modello è quello di introdurre dei vincoli nella stima di tali parametri.\n\n\n51.2.2 Tipologie di Invarianza\nÈ possibile definire diversi livelli di invarianza tra gruppi nel contesto di un’analisi fattoriale confermativa (CFA) multigruppo:\n\nInvarianza Configurale (fit_ef): Questo è il livello più basilare di invarianza. Si verifica quando la stessa struttura fattoriale si adatta a ogni gruppo. Questo significa che gli stessi fattori sono presenti in ogni gruppo e che gli stessi item misurano questi fattori in tutti i gruppi. Non si assumono ancora uguaglianze nelle saturazioni fattoriali o in altri parametri del modello.\nInvarianza Metrica (fit_efl): Oltre all’invarianza configurale, si testa l’uguaglianza delle saturazioni fattoriali (“loadings”) attraverso i gruppi. Questo passo verifica se gli item hanno lo stesso rapporto con il fattore latente in tutti i gruppi. L’invarianza metrica è fondamentale per affermare che la scala del fattore latente è la stessa tra i gruppi.\nInvarianza delle Intercette degli Indicatori (fit_eii): A questo livello, si testa se le intercette degli item sono uguali tra i gruppi, oltre all’uguaglianza delle saturazioni fattoriali. Questo passo è cruciale per sostenere che i gruppi interpretano il significato dei punteggi degli item allo stesso modo.\nInvarianza delle Varianze degli Errori degli Indicatori (fit_eir): Qui si aggiunge il test sull’uguaglianza delle varianze degli errori degli indicatori tra i gruppi. Questo implica che l’affidabilità di ciascun item (in termini di errore di misurazione) è la stessa in tutti i gruppi.\nInvarianza delle Varianze dei Fattori Latenti (fit_fv): In questo modello, si testa l’uguaglianza delle varianze dei fattori latenti tra i gruppi, oltre ai livelli precedenti di invarianza. Questo indica che la variabilità dei fattori latenti è simile tra i gruppi.\nInvarianza delle Medie dei Fattori Latenti (fit_fm): Questo è il livello più stringente di invarianza, dove si testa anche l’uguaglianza delle medie dei fattori latenti tra i gruppi. Questo significa che non solo la struttura e il funzionamento del modello sono simili tra i gruppi, ma anche che i gruppi hanno medie latenti equivalenti.\n\nOgni livello successivo di invarianza include i vincoli del livello precedente. Il passaggio da un livello all’altro richiede un confronto degli indici di adattamento per determinare se l’aggiunta di nuovi vincoli peggiora significativamente l’adattamento del modello. Questa sequenza di modelli aiuta a comprendere in modo rigoroso se e come un costrutto misurato dal modello CFA si manifesti in modo simile o diverso tra i gruppi.\n\n\n51.2.3 Un esempio concreto\nConsideriamo qui un esempio discusso da Brown (2015). Il modello CFA riguarda un modello di misurazione per la depressione maggiore così come è definita nel DSM-IV.\n\nIl campione include 9 indicatori: MDD1, depressed mood; MDD2, loss of interest in usual activities; MDD3, weight/appetite change; MDD4, sleep disturbance; MDD5, psychomotor agitation/retardation; MDD6, fatigue/loss of energy; MDD7, feelings of worthlessness/guilt; MDD8, concentration difficulties; MDD9, thoughts of death/suicidality.\n\nLeggiamo i dati in \\(\\mathsf{R}\\):\n\nd &lt;- readRDS(\n  here::here(\"data\", \"mdd_sex.RDS\")\n)\n\n\nhead(d)\n\n\nA data.frame: 6 x 10\n\n\n\nsex\nmdd1\nmdd2\nmdd3\nmdd4\nmdd5\nmdd6\nmdd7\nmdd8\nmdd9\n\n\n\n&lt;fct&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\nfemale\n5\n4\n1\n6\n5\n6\n5\n4\n2\n\n\n2\nfemale\n5\n5\n5\n5\n4\n5\n4\n5\n4\n\n\n3\nfemale\n4\n5\n4\n2\n6\n6\n0\n0\n0\n\n\n4\nfemale\n5\n5\n3\n3\n5\n5\n6\n4\n0\n\n\n5\nfemale\n5\n5\n0\n5\n0\n4\n6\n0\n0\n\n\n6\nfemale\n6\n6\n4\n6\n4\n6\n5\n6\n2\n\n\n\n\n\nNel caso presente, i gruppi corrispondono al genere. Confrontiamo le distribuzioni di densità empirica degli item tra i due gruppi.\n\nd_long &lt;- d |&gt;\n    pivot_longer(!sex, names_to = \"item\", values_to = \"value\")\n\nd_long |&gt;\n    ggplot(aes(value, col=sex)) +\n    geom_density(linewidth=1.5) +\n    facet_wrap(~item, nrow=3, scales=\"free\") +\n    labs(x=\" \", y=\"Density\")\n\n\n\n\n\n\n\n\nCi poniamo dunque il problema di stabilire l’invarianza fattoriale in funzione del genere. Consideriamo il seguente modello:\n\nmodel_mdd &lt;- \"\n  MDD =~ mdd1 + mdd2 + mdd3 + mdd4 + mdd5 + mdd6 + mdd7 + mdd8 +\n         mdd9\n  mdd1 ~~ mdd2\n\"\n\nSi noti la presenza di una correlazione residua tra gli indicatori mdd1 e mdd2.\nIn precedenza, abbiamo discusso le strutture di media e covarianza dei modelli CFA nel contesto di un’analisi a singolo gruppo. La stima dei parametri del modello, data un campione, può essere effettuata selezionando o individuando il miglior insieme di stime che minimizza una funzione di discrepanza.\nL’estensione di questo metodo di stima per un singolo campione all’analisi CFA multi-gruppo è diretta. Prima di tutto, si definisce una funzione di discrepanza individuale per ogni gruppo o campione. Per stimare tutti i parametri in gruppi indipendenti simultaneamente, si definisce una funzione di discrepanza complessiva come somma ponderata delle funzioni di discrepanza specifiche per gruppo.\nProcediamo quindi con la definizione di modelli che facilitano il confronto tra diversi tipi di invarianza fattoriale. È importante prestare attenzione ai vincoli che vengono progressivamente introdotti man mano che si specificano modelli sempre più restrittivi. Nella sintassi del software lavaan, utilizzato per l’analisi SEM, questi vincoli vengono impostati tramite l’argomento group.equal.\n\n# configural invariance\nfit_ef &lt;- cfa(\n  model_mdd,\n  data = d,\n  group = \"sex\",\n  meanstructure = TRUE\n)\n\n# plus equal factor loadings- metric invariance\nfit_efl &lt;- update(\n  fit_ef,\n  group.equal = c(\"loadings\")\n)\n\n# plus equal indicator intercepts\nfit_eii &lt;- update(\n  fit_efl,\n  group.equal = c(\"loadings\", \"intercepts\")\n)\n\n# plus equal indicator error variances\nfit_eir &lt;- update(\n  fit_eii,\n  group.equal = c(\"loadings\", \"intercepts\", \"residuals\")\n)\n\n# plus equal factor variances\nfit_fv &lt;- update(\n  fit_eir,\n  group.equal = c(\n    \"loadings\", \"intercepts\", \"residuals\",\n    \"lv.variances\"\n  )\n)\n\n# plus equal latent means\nfit_fm &lt;- update(\n  fit_fv,\n  group.equal = c(\n    \"loadings\", \"intercepts\", \"residuals\",\n    \"lv.variances\", \"means\"\n  )\n)\n\nConfrontiamo i modelli mediate il test del rapporto di verosimiglianze:\n\nout &lt;- lavTestLRT(fit_ef, fit_efl, fit_eii, fit_eir, fit_fv, fit_fm)\nprint(out)\n\n\nChi-Squared Difference Test\n\n        Df   AIC   BIC Chisq Chisq diff  RMSEA Df diff Pr(&gt;Chisq)\nfit_ef  52 27526 27785  98.9                                     \nfit_efl 60 27514 27735 102.8       3.93 0.0000       8       0.86\nfit_eii 68 27510 27695 115.3      12.47 0.0386       8       0.13\nfit_eir 77 27502 27645 125.0       9.71 0.0145       9       0.37\nfit_fv  78 27501 27639 125.8       0.79 0.0000       1       0.37\nfit_fm  79 27501 27635 127.7       1.92 0.0495       1       0.17\n\n\nIl confronto tra i precedenti modelli nidificati che introducono vincoli sempre più stringenti sui parametri indica che non vi è una “significativa” perdita di bontà dell’adattamento passando dal modello congenerico al modello che assume l’uguaglianza delle saturazioni fattoriali, delle intercette, delle varianze residue, delle varianze delle variabili latenti e delle medie dei due gruppi. Per i dati discussi da Brown (2015), dunque, possiamo concludere che vi sono forti evidenze di invarianza fattoriale tra maschi e femmine in relazione al costrutto di depressione maggiore. L’invarianza fattoriali giustifica, per questi dati, un confronto tra le medie dei punteggi totali del test calcolate nei due gruppi.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#indicatori-a-livello-di-scala-ordinale",
    "href": "chapters/sem/07_group_invariance.html#indicatori-a-livello-di-scala-ordinale",
    "title": "51  Invarianza di misura",
    "section": "51.3 Indicatori a livello di scala ordinale",
    "text": "51.3 Indicatori a livello di scala ordinale\nNell’analisi multi-gruppo per dati ordinali devono essere affrontati problemi specifici per questo tipo di dati. I dati ordinali si distinguono dalle variabili continue in due modi principali: per il metodo utilizzato nella stima delle saturazioni fattoriali e per l’approccio nell’analisi statistica.\nLe variabili ordinali sono formate da categorie con un ordine logico. Ad esempio, risposte come “fortemente in disaccordo” fino a “fortemente d’accordo”, o “mai”, “a volte”, “spesso”, “sempre” sono sequenze logiche. Sebbene queste categorie siano ordinate, la distanza tra di loro non è necessariamente uniforme o misurabile in termini quantitativi. Pertanto, assegnare valori numerici a queste risposte è un processo arbitrario e non riflette differenze di intensità o grandezza.\nUno dei problemi principali nell’analizzare dati ordinali è come calcolare le correlazioni. Le correlazioni policoriche offrono una soluzione, presupponendo l’esistenza di una variabile latente continua e normalmente distribuita che influisce sulla distribuzione delle risposte. In questo modello, ogni categoria di risposta ordinale corrisponde a un intervallo specifico sulla variabile latente sottostante. I punti di taglio o soglie (\\(\\tau_1, \\tau_2, \\dots, \\tau_k\\)) dividono la distribuzione normale in sezioni che rappresentano le frequenze osservate per ogni categoria.\nNell’ambito dell’invarianza fattoriale per dati ordinali, si pone inizialmente la questione dell’invarianza delle soglie (treshold invariance). Questo concetto presuppone che le soglie che definiscono le correlazioni policoriche rimangano costanti tra i gruppi. La stabilità di queste soglie è fondamentale per assicurare che le relazioni tra le categorie di risposta siano comparabili tra diversi gruppi.\nUn secondo aspetto cruciale è la scelta dello stimatore per la stima delle saturazioni fattoriali. Per i dati ordinali, lo stimatore raccomandato è quello dei minimi quadrati ponderati (Weighted Least Squares, WLS). Questo metodo è preferito perché tiene conto della natura specifica dei dati ordinali, fornendo stime più accurate e affidabili delle saturazioni fattoriali rispetto a metodi pensati per dati continui.\nIn sintesi, l’analisi di invarianza fattoriale con dati ordinali richiede considerazioni e tecniche specifiche, che includono l’utilizzo di correlazioni policoriche e l’impiego di stimatori adeguati, per garantire risultati validi e interpretabili.\n\n51.3.1 Un esempio concreto\nWu & Estabrook (2016) sostengono che la metodologia convenzionale per valutare l’invarianza fattoriale nei dati continui richiede adattamenti significativi quando applicata a indicatori categoriali. Il processo standard inizia con la definizione di un modello di base, seguito dall’applicazione di restrizioni incrementali ai parametri. Tuttavia, secondo Wu & Estabrook (2016), questo approccio risulta subottimale per i dati categoriali, a causa della sua dipendenza critica dalla definizione delle soglie utilizzate per stabilire le correlazioni policoriche con le variabili latenti continue nel modello configurale.\nWu & Estabrook (2016) enfatizzano che, in contesti con dati categoriali, è primario valutare prima l’equivalenza delle soglie tra i gruppi, definendo ciò che viene chiamato un “modello di soglia” (threshold model). Solo dopo questa valutazione preliminare è consigliabile procedere con l’analisi dell’equivalenza delle saturazioni fattoriali tra i gruppi. Questo cambiamento nell’ordine di valutazione è cruciale per garantire che le soglie, che sono fondamentali per la definizione delle correlazioni policoriche nei dati categoriali, siano coerenti e comparabili tra i gruppi prima di procedere con ulteriori analisi sull’invarianza fattoriale. In sostanza, ciò che Wu & Estabrook (2016) propongono è una riconsiderazione della sequenza di passaggi nell’analisi dell’invarianza per dati categoriali, ponendo una maggiore enfasi sull’allineamento delle soglie prima di esplorare altre forme di equivalenza nei modelli multigruppo.\nPer illustrare tale procedura, replichiamo qui il tutorial messo a punto da Svetina et al. (2020).\n\nI dati includono quattro item di una scala sul bullismo, raccolti in tre paesi: Azerbaigian (31 scuole), Austria (40 scuole) e Finlandia (246 scuole). Ogni item è misurato su una scala Likert a 4 punti, da 0 (“mai”) a 3 (“almeno una volta alla settimana”), e chiede ai partecipanti di valutare la frequenza di episodi di bullismo. Un esempio di item è: “mi prendevano in giro o mi insultavano”. I campioni sono composti da 3.808 partecipanti in Azerbaigian, 4.457 in Austria e 4.520 in Finlandia.\n\nLeggiamo in dati in \\(\\textsf{R}\\):\n\ndat &lt;- read.table(\"../../data/BULLY.dat\", header = FALSE)\nnames(dat) &lt;- c(\"IDCNTRY\", \"R09A\", \"R09B\", \"R09C\", \"R09D\")\nhead(dat)\n\n\nA data.frame: 6 x 5\n\n\n\nIDCNTRY\nR09A\nR09B\nR09C\nR09D\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n31\n3\n3\n0\n0\n\n\n2\n31\n0\n0\n0\n0\n\n\n3\n31\n3\n2\n1\n3\n\n\n4\n31\n0\n0\n3\n0\n\n\n5\n31\n0\n0\n0\n0\n\n\n6\n31\n0\n0\n0\n0\n\n\n\n\n\nViene creata la matrice all.results per immagazzinare i risultati dei diversi modelli che verranno confrontati, chiamati baseline (nessun vincolo tra i gruppi), proposition 4 (equivalenza delle soglie tra i gruppi), e proposition 7 (equivalenza delle soglie e delle saturazioni fattoriali tra i gruppi). Gli indici di bontà dell’adattamento che verranno considerati sono: chi-square, df, p, RMSEA, CFI, e TLI.\n\nall.results &lt;- matrix(NA, ncol = 6, nrow = 3)\n\n\n\n51.3.2 Baseline model\nNel baseline model non viene posto alcun vincolo tra i gruppi. È quello dell’invarianza configurale.\n\nmod.cat &lt;- \"F1 =~ R09A + R09B + R09C + R09D\"\n\n\nbaseline &lt;- measEq.syntax(\n  configural.model = mod.cat,\n  data = dat,\n  ordered = c(\"R09A\", \"R09B\", \"R09C\", \"R09D\"),\n  parameterization = \"delta\",\n  ID.fac = \"std.lv\",\n  ID.cat = \"Wu.Estabrook.2016\",\n  group = \"IDCNTRY\",\n  group.equal = \"configural\"\n)\n\nInformazioni sul modello baseline si ottengono nel modo seguente:\n\nout &lt;- summary(baseline)\nprint(out)\n\nThis lavaan model syntax specifies a CFA with 4 manifest indicators (4 of which are ordinal) of 1 common factor(s).\n\nTo identify the location and scale of each common factor, the factor means and variances were fixed to 0 and 1, respectively, unless equality constraints on measurement parameters allow them to be freed.\n\nThe location and scale of each latent item-response underlying 4 ordinal indicators were identified using the \"delta\" parameterization, and the identification constraints recommended by Wu & Estabrook (2016). For details, read:\n\n    https://doi.org/10.1007/s11336-016-9506-0 \n\nPattern matrix indicating num(eric), ord(ered), and lat(ent) indicators per factor:\n\n     F1 \nR09A ord\nR09B ord\nR09C ord\nR09D ord\n\nThe following types of parameter were constrained to equality across groups:\n\n    configural\n\n     F1   \nR09A \"ord\"\nR09B \"ord\"\nR09C \"ord\"\nR09D \"ord\"\n\n\nLe proprietà del modello possono essere esplicitate con la seguente istruzione:\n\ncat(as.character(baseline))\n\n## LOADINGS:\n\nF1 =~ c(NA, NA, NA)*R09A + c(lambda.1_1.g1, lambda.1_1.g2, lambda.1_1.g3)*R09A\nF1 =~ c(NA, NA, NA)*R09B + c(lambda.2_1.g1, lambda.2_1.g2, lambda.2_1.g3)*R09B\nF1 =~ c(NA, NA, NA)*R09C + c(lambda.3_1.g1, lambda.3_1.g2, lambda.3_1.g3)*R09C\nF1 =~ c(NA, NA, NA)*R09D + c(lambda.4_1.g1, lambda.4_1.g2, lambda.4_1.g3)*R09D\n\n## THRESHOLDS:\n\nR09A | c(NA, NA, NA)*t1 + c(R09A.thr1.g1, R09A.thr1.g2, R09A.thr1.g3)*t1\nR09A | c(NA, NA, NA)*t2 + c(R09A.thr2.g1, R09A.thr2.g2, R09A.thr2.g3)*t2\nR09A | c(NA, NA, NA)*t3 + c(R09A.thr3.g1, R09A.thr3.g2, R09A.thr3.g3)*t3\nR09B | c(NA, NA, NA)*t1 + c(R09B.thr1.g1, R09B.thr1.g2, R09B.thr1.g3)*t1\nR09B | c(NA, NA, NA)*t2 + c(R09B.thr2.g1, R09B.thr2.g2, R09B.thr2.g3)*t2\nR09B | c(NA, NA, NA)*t3 + c(R09B.thr3.g1, R09B.thr3.g2, R09B.thr3.g3)*t3\nR09C | c(NA, NA, NA)*t1 + c(R09C.thr1.g1, R09C.thr1.g2, R09C.thr1.g3)*t1\nR09C | c(NA, NA, NA)*t2 + c(R09C.thr2.g1, R09C.thr2.g2, R09C.thr2.g3)*t2\nR09C | c(NA, NA, NA)*t3 + c(R09C.thr3.g1, R09C.thr3.g2, R09C.thr3.g3)*t3\nR09D | c(NA, NA, NA)*t1 + c(R09D.thr1.g1, R09D.thr1.g2, R09D.thr1.g3)*t1\nR09D | c(NA, NA, NA)*t2 + c(R09D.thr2.g1, R09D.thr2.g2, R09D.thr2.g3)*t2\nR09D | c(NA, NA, NA)*t3 + c(R09D.thr3.g1, R09D.thr3.g2, R09D.thr3.g3)*t3\n\n## INTERCEPTS:\n\nR09A ~ c(0, 0, 0)*1 + c(nu.1.g1, nu.1.g2, nu.1.g3)*1\nR09B ~ c(0, 0, 0)*1 + c(nu.2.g1, nu.2.g2, nu.2.g3)*1\nR09C ~ c(0, 0, 0)*1 + c(nu.3.g1, nu.3.g2, nu.3.g3)*1\nR09D ~ c(0, 0, 0)*1 + c(nu.4.g1, nu.4.g2, nu.4.g3)*1\n\n## SCALING FACTORS:\n\nR09A ~*~ c(1, 1, 1)*R09A\nR09B ~*~ c(1, 1, 1)*R09B\nR09C ~*~ c(1, 1, 1)*R09C\nR09D ~*~ c(1, 1, 1)*R09D\n\n\n## LATENT MEANS/INTERCEPTS:\n\nF1 ~ c(0, 0, 0)*1 + c(alpha.1.g1, alpha.1.g2, alpha.1.g3)*1\n\n## COMMON-FACTOR VARIANCES:\n\nF1 ~~ c(1, 1, 1)*F1 + c(psi.1_1.g1, psi.1_1.g2, psi.1_1.g3)*F1\n\n\nPer potere essere passato a lavaan, l’oggetto baseline deve essere in formato char:\n\nmodel.baseline &lt;- as.character(baseline)\n\nAdattiamo il modello ai dati:\n\nfit.baseline &lt;- cfa(\n  model.baseline, \n  data = dat, \n  group = \"IDCNTRY\", \n  ordered = c(\"R09A\", \"R09B\", \"R09C\", \"R09D\")\n)\n\nSalviamo i risultati:\n\nall.results[1, ] &lt;-\n  round(\n    data.matrix(\n      fitmeasures(fit.baseline, fit.measures = c(\n        \"chisq.scaled\", \"df.scaled\", \"pvalue.scaled\",\n        \"rmsea.scaled\", \"cfi.scaled\", \"tli.scaled\"\n      ))\n    ),\n    digits = 3\n  )\n\n\n\n51.3.3 Invarianza delle soglie\nConsideriamo ora il modello threshold invariance (chiamato Proposition 4 da Wu & Estabrook, 2016).\n\nprop4 &lt;- measEq.syntax(\n  configural.model = mod.cat,\n  data = dat,\n  ordered = c(\"R09A\", \"R09B\", \"R09C\", \"R09D\"),\n  parameterization = \"delta\",\n  ID.fac = \"std.lv\",\n  ID.cat = \"Wu.Estabrook.2016\",\n  group = \"IDCNTRY\",\n  group.equal = c(\"thresholds\")\n)\n\nAdattiamo il modello ai dati:\n\nmodel.prop4 &lt;- as.character(prop4)\nfit.prop4 &lt;- cfa(\n  model.prop4,\n  data = dat,\n  group = \"IDCNTRY\",\n  ordered = c(\"R09A\", \"R09B\", \"R09C\", \"R09D\")\n)\n\nSalviamo i risulati\n\n#store model fit information for proposition 4\nall.results[2, ] &lt;-\n  round(data.matrix(\n  fitmeasures(fit.prop4,fit.measures = c(\n    \"chisq.scaled\",\"df.scaled\",\"pvalue.scaled\", \n    \"rmsea.scaled\", \"cfi.scaled\", \"tli.scaled\"))), \n  digits=3\n  )\n\nEseguiamo il confronto tra il modello threshold invariance e il modello baseline:\n\nlavTestLRT(fit.baseline, fit.prop4)\n\n\nA anova: 2 x 7\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nDf diff\nPr(&gt;Chisq)\n\n\n\n&lt;int&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nfit.baseline\n6\nNA\nNA\n26.9\nNA\nNA\nNA\n\n\nfit.prop4\n14\nNA\nNA\n42.2\n61\n8\n2.95e-10\n\n\n\n\n\n\n\n51.3.4 Invarianza delle soglie e delle saturazioni fattoriali\nConsideriamo ora il modello threshold and loading invariance (chiamato Proposition 7 da Wu & Estabrook, 2016).\n\nprop7 &lt;- measEq.syntax(\n  configural.model = mod.cat,\n  data = dat,\n  ordered = c(\"R09A\", \"R09B\", \"R09C\", \"R09D\"),\n  parameterization = \"delta\",\n  ID.fac = \"std.lv\",\n  ID.cat = \"Wu.Estabrook.2016\",\n  group = \"IDCNTRY\",\n  group.equal = c(\"thresholds\", \"loadings\")\n)\n\nAdattiamo il modello ai dati:\n\nmodel.prop7 &lt;- as.character(prop7)\nfit.prop7 &lt;- cfa(\n  model.prop7, \n  data = dat, group = \"IDCNTRY\", \n  ordered = c(\"R09A\", \"R09B\", \"R09C\", \"R09D\")\n  )\n\nSalviamo i risultati:\n\nall.results[3, ] &lt;-\n  round(data.matrix(\n    fitmeasures(fit.prop7, fit.measures = c(\n      \"chisq.scaled\", \"df.scaled\", \"pvalue.scaled\",\n      \"rmsea.scaled\", \"cfi.scaled\", \"tli.scaled\"\n    ))\n  ), digits = 3)\n\ncolumn.names &lt;-\n  c(\n    \"chisq.scaled\", \"df.scaled\", \"pvalue.scaled\", \"rmsea.scaled\",\n    \"cfi.scaled\", \"tli.scaled\"\n  )\n\nrow.names &lt;- c(\"baseline\", \"prop4\", \"prop7\")\n\ncolnames(all.results) &lt;- column.names\nrownames(all.results) &lt;- row.names\n\nEseguiamo i confronti tra modelli:\n\nlavTestLRT(fit.prop4, fit.prop7)\n\n\nA anova: 2 x 7\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nDf diff\nPr(&gt;Chisq)\n\n\n\n&lt;int&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nfit.prop4\n14\nNA\nNA\n42.2\nNA\nNA\nNA\n\n\nfit.prop7\n20\nNA\nNA\n93.1\n73.7\n6\n7.08e-14\n\n\n\n\n\n\nlavTestLRT(fit.prop7, fit.baseline)\n\n\nA anova: 2 x 7\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nDf diff\nPr(&gt;Chisq)\n\n\n\n&lt;int&gt;\n&lt;lgl&gt;\n&lt;lgl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nfit.baseline\n6\nNA\nNA\n26.9\nNA\nNA\nNA\n\n\nfit.prop7\n20\nNA\nNA\n93.1\n136\n14\n4.14e-22\n\n\n\n\n\nUn confronto tra gli indici di bontà di adattamento dei tre modelli è fornito di seguito:\n\nprint(all.results)\n\n         chisq.scaled df.scaled pvalue.scaled rmsea.scaled cfi.scaled\nbaseline         50.9         6             0        0.042      0.997\nprop4           107.8        14             0        0.040      0.994\nprop7           186.5        20             0        0.044      0.989\n         tli.scaled\nbaseline      0.991\nprop4         0.992\nprop7         0.990\n\n\nIn conclusione, nel caso presente, il test del rapporto di verosimiglianza indica che non viene rispettata neppure l’invarianza delle soglie tra gruppi. Gli altri confronti, dunque, sono superflui e sono stati qui presentati solo allo scopo di illustrare la procedura.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#vincoli-inter-gruppi",
    "href": "chapters/sem/07_group_invariance.html#vincoli-inter-gruppi",
    "title": "51  Invarianza di misura",
    "section": "51.4 Vincoli Inter-gruppi",
    "text": "51.4 Vincoli Inter-gruppi\nQuando si adatta un modello di equazioni strutturali ai dati provenienti da più gruppi, è comune imporre vincoli di uguaglianza inter-gruppi su certe stime di parametri non standardizzati. Questi parametri possono riguardare effetti causali, inclusi effetti diretti, indiretti o totali, o per varianze, covarianze, medie o intercette. La scelta tra questi dovrebbe essere correlata a specifiche ipotesi riguardo le differenze tra i gruppi. Se l’adattamento del modello vincolato è molto peggiore rispetto a quello del modello non vincolato—e il modello non vincolato si adatta bene ai dati—possiamo concludere che le popolazioni da cui sono stati selezionati i gruppi possono differire rispetto ai parametri soggetti a vincoli di uguaglianza. Un’alternativa all’analisi multi-gruppo consiste nel rappresentare l’appartenenza al gruppo in un modello a singolo gruppo, adattato ai dati di tutti i gruppi combinati. Alcune variabili endogene nel modello sono regresse sul gruppo e su termini prodotto che coinvolgono il gruppo e altre variabili moderatrici presunte. I coefficienti per i termini prodotto stimano gli effetti interattivi del gruppo con altre variabili causali, e i loro valori possono essere convertiti in stime di effetti indiretti condizionali. Un inconveniente è che i modelli a singolo gruppo generalmente assumono l’omoscedasticità tra i gruppi, e le stime possono essere inesatte se tali assunzioni sono insostenibili. In contrasto, è semplice testare le ipotesi di omogeneità in un approccio multi-gruppo.\nEsaminiamo un esempio discuso da Kline (2023). Lynam e colleghi (1993) hanno misurato lo status socioeconomico familiare (SES), il quoziente intellettivo verbale (QI verbale), la motivazione dell’esaminato durante il test del QI, il rendimento scolastico e i comportamenti delinquenzali in campioni di adolescenti maschi bianchi (n = 181) e neri (n = 214) di età compresa tra i 12 e i 13 anni. Erano studenti di quarta elementare in scuole pubbliche urbane negli Stati Uniti ed erano parte di uno studio longitudinale su individui ad alto rischio di forme precoci di delinquenza.\nLa figura mostra un modello di percorso con una struttura di media (rappresentata con linee tratteggiate) basato sulle variabili dello studio. La figura rappresenta l’ipotesi che SES, motivazione e QI verbale siano cause correlate che influenzano la delinquenza sia direttamente che indirettamente attraverso il rendimento scolastico. Ad esempio, i ragazzi adolescenti con scarsa abilità verbale potrebbero essere più propensi ad abbandonare la scuola, contribuendo così alla delinquenza a causa di prospettive di impiego ridotte o del maggiore tempo non supervisionato per strada.\n\n\n\n\n\n\nFigura 51.1\n\n\n\nLo studio di Lynam et al. (1993) era trasversale, senza precedenza temporale nelle misurazioni, quindi l’unica base per la specificazione della direzionalità è l’argomentazione. La logica appena riassunta è soggetta a critiche. Ad esempio, Block (1995) ha sostenuto che Lynam et al. (1993) hanno trascurato il ruolo potenziale dell’impulsività come mediatore degli effetti dell’abilità verbale sulla delinquenza. Ovvero, Block (1995) ha sostenuto che non è l’abilità verbale compromessa di per sé che è causalmente correlata alla delinquenza; piuttosto, una capacità ridotta di ritardare la gratificazione, pensare prima di agire o rimanere concentrati su un obiettivo sono fattori più importanti nella delinquenza.\n\nSenza vincoli, il modello di percorso nella figura è “appena identificato” (just identified) e si adatterebbe perfettamente ai dati in entrambi i gruppi. Vincoli di uguaglianza inter-gruppi sono stati imposti su parametri chiave in questo esempio per testare l’ipotesi che rimanere a scuola sia un fattore protettivo relativamente più forte contro la delinquenza tra i giovani maschi neri rispetto ai giovani maschi bianchi, specialmente in aree urbane a basso SES con relativamente più famiglie monogenitoriali dove i giovani neri sono rappresentati sproporzionatamente (Lynam et al., 1993). La strategia di analisi è delineata di seguito: per il modello 1, tutti i 7 effetti diretti non standardizzati sono vincolati all’uguaglianza nei due gruppi. Ci si aspettava che il modello 1 fosse incoerente con i dati, se l’effetto del rendimento scolastico sull’illegalità è diverso tra i gruppi.\nPer il modello 2, il vincolo di uguaglianza per l’effetto diretto del rendimento sulla delinquenza è rilasciato, ciò che ci si aspettava migliorasse notevolmente l’adattamento rispetto a quello del modello 1.\nAssumendo un adattamento globale e locale soddisfacente del modello 2 in entrambi i gruppi, vengono testati due modelli aggiuntivi. Per il modello 3, l’intercetta per la regressione del rendimento su SES, sforzo nel test e QI verbale è vincolata all’uguaglianza\nModello 4: l’intercetta per la regressione della delinquenza su SES, sforzo nel test, QI verbale e rendimento è vincolata all’uguaglianza. Il confronto degli adattamenti relativi dei modelli 3 e 4 con il modello 2 testa l’uguaglianza delle intercette di regressione per le variabili endogene tra i gruppi.\n\nSpecifichiamo i dati.\n\n# black, n = 214\n# white, n = 181\n# input the correlations in lower diagnonal form\n\nblackLower.cor &lt;- \"\n 1.00\n  .08 1.00\n  .28  .30 1.00\n  .05  .21  .50 1.00\n -.11 -.17 -.26 -.33 1.00 \"\n\nwhiteLower.cor &lt;- \"\n 1.00\n  .25 1.00\n  .37  .40 1.00\n  .27  .28  .61 1.00\n -.11 -.20 -.31 -.21 1.00 \"\n\n# name the variables and convert to full correlation matrix\nblack.cor &lt;- lavaan::getCov(blackLower.cor, names = c(\n    \"ses\", \"effort\", \"viq\",\n    \"achieve\", \"delinq\"\n))\nwhite.cor &lt;- lavaan::getCov(whiteLower.cor, names = c(\n    \"ses\", \"effort\", \"viq\",\n    \"achieve\", \"delinq\"\n))\n\n# display the correlations\nblack.cor\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nses\neffort\nviq\nachieve\ndelinq\n\n\n\n\nses\n1.00\n0.08\n0.28\n0.05\n-0.11\n\n\neffort\n0.08\n1.00\n0.30\n0.21\n-0.17\n\n\nviq\n0.28\n0.30\n1.00\n0.50\n-0.26\n\n\nachieve\n0.05\n0.21\n0.50\n1.00\n-0.33\n\n\ndelinq\n-0.11\n-0.17\n-0.26\n-0.33\n1.00\n\n\n\n\n\n\nwhite.cor\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nses\neffort\nviq\nachieve\ndelinq\n\n\n\n\nses\n1.00\n0.25\n0.37\n0.27\n-0.11\n\n\neffort\n0.25\n1.00\n0.40\n0.28\n-0.20\n\n\nviq\n0.37\n0.40\n1.00\n0.61\n-0.31\n\n\nachieve\n0.27\n0.28\n0.61\n1.00\n-0.21\n\n\ndelinq\n-0.11\n-0.20\n-0.31\n-0.21\n1.00\n\n\n\n\n\n\n# add the standard deviations and convert to covariances\nblack.cov &lt;- lavaan::cor2cov(black.cor, sds = c(10.58, 1.35, 13.62, .79, 1.63))\nwhite.cov &lt;- lavaan::cor2cov(white.cor, sds = c(11.53, 1.32, 16.32, .96, 1.45))\n\n# input group means\nblack.mean &lt;- c(31.96, -.01, 93.76, 2.51, 1.40)\nwhite.mean &lt;- c(34.64, .05, 104.18, 2.88, 1.22)\n\n# display the covariances and means\nblack.cov\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nses\neffort\nviq\nachieve\ndelinq\n\n\n\n\nses\n111.936400\n1.142640\n40.347888\n0.417910\n-1.896994\n\n\neffort\n1.142640\n1.822500\n5.516100\n0.223965\n-0.374085\n\n\nviq\n40.347888\n5.516100\n185.504400\n5.379900\n-5.772156\n\n\nachieve\n0.417910\n0.223965\n5.379900\n0.624100\n-0.424941\n\n\ndelinq\n-1.896994\n-0.374085\n-5.772156\n-0.424941\n2.656900\n\n\n\n\n\n\nblack.mean\n\n\n31.96-0.0193.762.511.4\n\n\n\nwhite.cov\n\n\nA matrix: 5 x 5 of type dbl\n\n\n\nses\neffort\nviq\nachieve\ndelinq\n\n\n\n\nses\n132.940900\n3.804900\n69.622752\n2.988576\n-1.839035\n\n\neffort\n3.804900\n1.742400\n8.616960\n0.354816\n-0.382800\n\n\nviq\n69.622752\n8.616960\n266.342400\n9.556992\n-7.335840\n\n\nachieve\n2.988576\n0.354816\n9.556992\n0.921600\n-0.292320\n\n\ndelinq\n-1.839035\n-0.382800\n-7.335840\n-0.292320\n2.102500\n\n\n\n\n\n\nwhite.mean\n\n\n34.640.05104.182.881.22\n\n\n\n# specify fit statistics for abbreviated output\nfit.stats &lt;- c(\n    \"chisq\", \"df\", \"pvalue\",\n    \"rmsea\", \"rmsea.ci.lower\", \"rmsea.ci.upper\", \"cfi\", \"srmr\"\n)\n\n# combine covariances matrices, mean vectors, and group sizes\n# into single list objects\ncombined.cov &lt;- list(black = black.cov, white = white.cov)\ncombined.mean &lt;- list(black = black.mean, white = white.mean)\ncombined.n &lt;- list(black = 214, white = 181)\n\nAdattiamo i quattro modelli ai dati:\n\n# specify basic covariance model\nlynam.model &lt;- \"\n achieve ~ ses + effort + viq\n delinq ~ achieve + ses + effort + viq \"\n\n\n# model 1\n# all 7 direct effect constrained to equality\n# over groups\nlynam1 &lt;- lavaan::sem(\n    lynam.model,\n    sample.cov = combined.cov,\n    sample.mean = combined.mean, sample.nobs = combined.n,\n    group.equal = c(\"regressions\"), fixed.x = FALSE, meanstructure = TRUE\n)\n\n\n# model 2\n# direct effects constrained to equality over groups\n# but the constraint on achievement -&gt; delinguency is\n# released in syntax for \"group.partial\"\n# so 6 direct effects are constrained over groups\n\nlynam2 &lt;- lavaan::sem(\n    lynam.model,\n    sample.cov = combined.cov,\n    sample.mean = combined.mean, sample.nobs = combined.n,\n    group.equal = c(\"regressions\"), group.partial = c(\"delinq ~ achieve\"),\n    fixed.x = FALSE, meanstructure = TRUE\n)\n\n\n# model 3\n# retained model\n# 6 direct effects constrained to equality over groups\n# achievement -&gt; delinguency is a free parameter in both groups\n# intercept for achievement constrained to equality\n\nlynam3 &lt;- lavaan::sem(lynam.model,\n    sample.cov = combined.cov,\n    sample.mean = combined.mean, sample.nobs = combined.n,\n    group.equal = c(\"regressions\", \"intercepts\"),\n    group.partial = c(\"delinq ~ achieve\", \"delinq ~ 1\"),\n    fixed.x = FALSE, meanstructure = TRUE\n)\n\n\n# model 4\n# 6 direct effects constrained to equality over groups\n# achievement -&gt; delinguency is a free parameter in both groups\n# intercept for deleinquency constrained to equality\n\nlynam4 &lt;- lavaan::sem(lynam.model,\n    sample.cov = combined.cov,\n    sample.mean = combined.mean, sample.nobs = combined.n,\n    group.equal = c(\"regressions\", \"intercepts\"),\n    group.partial = c(\"delinq ~ achieve\", \"achieve ~ 1\"),\n    fixed.x = FALSE, meanstructure = TRUE\n)\n\nEsaminiamo la bontà di adattamento.\n\n# global fit statistics\nlavaan::fitMeasures(lynam1, fit.stats) |&gt; print()\n\n         chisq             df         pvalue          rmsea rmsea.ci.lower \n        11.736          7.000          0.110          0.059          0.000 \nrmsea.ci.upper            cfi           srmr \n         0.115          0.975          0.036 \n\n\n\nlavaan::fitMeasures(lynam2, fit.stats) |&gt; print()\n\n         chisq             df         pvalue          rmsea rmsea.ci.lower \n         6.107          6.000          0.411          0.010          0.000 \nrmsea.ci.upper            cfi           srmr \n         0.093          0.999          0.029 \n\n\n\nlavaan::fitMeasures(lynam3, fit.stats) |&gt; print()\n\n         chisq             df         pvalue          rmsea rmsea.ci.lower \n         6.409          7.000          0.493          0.000          0.000 \nrmsea.ci.upper            cfi           srmr \n         0.083          1.000          0.030 \n\n\n\nlavaan::fitMeasures(lynam4, fit.stats) |&gt; print()\n\n         chisq             df         pvalue          rmsea rmsea.ci.lower \n        10.237          7.000          0.176          0.048          0.000 \nrmsea.ci.upper            cfi           srmr \n         0.107          0.983          0.033 \n\n\nEseguiamo il test del rapporto tra verosimiglianze.\n\n# chi square difference tests\nlavaan::anova(lynam1, lynam2)\n\n\nA anova: 2 x 8\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nRMSEA\nDf diff\nPr(&gt;Chisq)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nlynam2\n6\n9850.196\n9985.478\n6.107287\nNA\nNA\nNA\nNA\n\n\nlynam1\n7\n9853.825\n9985.128\n11.735711\n5.628424\n0.1530851\n1\n0.01767152\n\n\n\n\n\n\nlavaan::anova(lynam2, lynam3)\n\n\nA anova: 2 x 8\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nRMSEA\nDf diff\nPr(&gt;Chisq)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nlynam2\n6\n9850.196\n9985.478\n6.107287\nNA\nNA\nNA\nNA\n\n\nlynam3\n7\n9848.498\n9979.801\n6.409026\n0.3017387\n0\n1\n0.5827945\n\n\n\n\n\n\nlavaan::anova(lynam2, lynam4)\n\n\nA anova: 2 x 8\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nRMSEA\nDf diff\nPr(&gt;Chisq)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nlynam2\n6\n9850.196\n9985.478\n6.107287\nNA\nNA\nNA\nNA\n\n\nlynam4\n7\n9852.326\n9983.629\n10.236768\n4.129481\n0.1258788\n1\n0.04214226\n\n\n\n\n\nI risultati finora descritti sono coerenti con l’ipotesi che l’effetto diretto non standardizzato del rendimento sulla delinquenza vari in base all’appartenenza al gruppo. Tuttavia, prima di stabilire una scelta definitiva per questo esempio, consideriamo due modelli aggiuntivi. Queste analisi sono più esplorative poiché i modelli di percorso di Lynam et al. (1993) non avevano strutture di media. Il modello 3, rispetto al modello 2, presenta intercette vincolate all’uguaglianza per il rendimento. Si prevedeva che vincolare questo parametro avrebbe leggermente degradato l’adattamento globale rispetto al modello 2, in cui la stessa intercetta è liberamente stimata in entrambi i gruppi. Questo perché il modello 2 con effetti diretti uguali per le cause del rendimento - SES, sforzo nel test e QI verbale - nei due gruppi è generalmente coerente con i dati. I risultati del modello 3 supportano questa previsione.\nPoiché l’intercetta per il rendimento è ugualmente vincolata nel modello 3, la sua intera struttura di media non è più solo identificata; specificamente, ha gradi di libertà (df) = 1, quindi non tutte le medie previste per le variabili endogene, rendimento e delinquenza, corrisponderanno esattamente alle loro controparti osservate. I residui delle medie per le variabili esogene - SES, sforzo nel test e QI verbale - saranno tutti uguali a zero poiché nessun vincolo ha influenzato le loro medie previste. Il modello 3 prevede quindi accuratamente covarianze e medie in entrambi i gruppi, quindi viene mantenuto.\nNel modello 4, l’intercetta per la regressione della delinquenza su SES, sforzo nel test, QI verbale e rendimento è vincolata all’uguaglianza nei gruppi più tutti gli effetti diretti eccetto quello del rendimento sulla delinquenza, che è liberamente stimato in entrambi i campioni. Poiché l’effetto diretto appena menzionato contribuisce all’intercetta per la delinquenza, si prevedeva che vincolare l’intercetta appena menzionata all’uguaglianza avrebbe peggiorato notevolmente l’adattamento rispetto al modello 2. Questa previsione è coerente con i risultati; in particolare, sebbene il modello 4 superi il test chi-quadrato, il suo adattamento è relativamente peggiore rispetto a quello del modello 2. Anche i residui per il modello 4 sono simili a quelli del modello 1; cioè, insoddisfacenti. Pertanto, il modello 4 viene rifiutato.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/07_group_invariance.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/07_group_invariance.html#informazioni-sullambiente-di-sviluppo",
    "title": "51  Invarianza di misura",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] MASS_7.3-61       viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [5] ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1 \n [9] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19     psych_2.4.6.26   \n[13] scales_1.3.0      markdown_1.13     knitr_1.49        lubridate_1.9.3  \n[17] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[21] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n[25] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.17.1   jsonlite_1.8.9      magrittr_2.0.3     \n  [4] TH.data_1.1-2       estimability_1.5.1  farver_2.1.2       \n  [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n [10] Cairo_1.6-2         minqa_1.2.8         base64enc_0.1-3    \n [13] rstatix_0.7.2       htmltools_0.5.8.1   broom_1.0.7        \n [16] Formula_1.2-5       htmlwidgets_1.6.4   plyr_1.8.9         \n [19] sandwich_3.1-1      emmeans_1.10.5      zoo_1.8-12         \n [22] uuid_1.2-1          igraph_2.1.1        mime_0.12          \n [25] lifecycle_1.0.4     pkgconfig_2.0.3     Matrix_1.7-1       \n [28] R6_2.5.1            fastmap_1.2.0       shiny_1.9.1        \n [31] digest_0.6.37       OpenMx_2.21.13      fdrtool_1.2.18     \n [34] colorspace_2.1-1    rprojroot_2.0.4     Hmisc_5.2-0        \n [37] labeling_0.4.3      fansi_1.0.6         timechange_0.3.0   \n [40] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n [43] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n [46] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n [49] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n [52] foreign_0.8-87      zip_2.3.1           httpuv_1.6.15      \n [55] nnet_7.3-19         glue_1.8.0          quadprog_1.5-8     \n [58] promises_1.3.0      nlme_3.1-166        lisrelToR_0.3      \n [61] grid_4.4.2          pbdZMQ_0.3-13       checkmate_2.3.2    \n [64] cluster_2.1.6       reshape2_1.4.4      generics_0.1.3     \n [67] gtable_0.3.6        tzdb_0.4.0          data.table_1.16.2  \n [70] hms_1.1.3           car_3.1-3           utf8_1.2.4         \n [73] sem_3.1-16          pillar_1.9.0        IRdisplay_1.1      \n [76] rockchalk_1.8.157   later_1.3.2         splines_4.4.2      \n [79] cherryblossom_0.1.0 lattice_0.22-6      survival_3.7-0     \n [82] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n [85] pbapply_1.7-2       airports_0.1.0      stats4_4.4.2       \n [88] xfun_0.49           qgraph_1.9.8        arm_1.14-4         \n [91] stringi_1.8.4       pacman_0.5.1        boot_1.3-31        \n [94] evaluate_1.0.1      codetools_0.2-20    mi_1.1             \n [97] cli_3.6.3           RcppParallel_5.1.9  IRkernel_1.3.2     \n[100] rpart_4.1.23        xtable_1.8-4        repr_1.1.7         \n[103] munsell_0.5.1       Rcpp_1.0.13-1       coda_0.19-4.1      \n[106] png_0.1-8           XML_3.99-0.17       parallel_4.4.2     \n[109] usdata_0.3.1        jpeg_0.1-10         lme4_1.1-35.5      \n[112] mvtnorm_1.3-2       openxlsx_4.2.7.1    crayon_1.5.3       \n[115] openintro_2.5.0     rlang_1.1.4         multcomp_1.4-26    \n[118] mnormt_2.1.1       \n\n\n\n\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied research. Guilford publications.\n\n\nKline, R. B. (2023). Principles and practice of structural equation modeling. Guilford publications.\n\n\nSvetina, D., Rutkowski, L., & Rutkowski, D. (2020). Multiple-group invariance with categorical outcomes using updated guidelines: an illustration using M plus and the lavaan/semtools packages. Structural Equation Modeling: A Multidisciplinary Journal, 27(1), 111–130.\n\n\nWu, H., & Estabrook, R. (2016). Identification of confirmatory factor analysis models of different levels of invariance for ordered categorical outcomes. Psychometrika, 81(4), 1014–1045.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Invarianza di misura</span>"
    ]
  },
  {
    "objectID": "chapters/sem/08_multilevel_sem.html",
    "href": "chapters/sem/08_multilevel_sem.html",
    "title": "52  Modelli di Equazioni Strutturali Multilivello",
    "section": "",
    "text": "52.1 Introduzione\nNella ricerca psicologica e nelle scienze sociali, i dati raccolti spesso mostrano strutture complesse a più livelli, in cui le informazioni sono organizzate in gruppi o cluster, e le osservazioni all’interno di ogni cluster tendono a essere correlate tra loro. Questo fenomeno rappresenta un aspetto cruciale poiché, in molte situazioni, i modelli classici di equazioni strutturali (SEM) non sono adatti per analizzare dati di questo tipo. La principale limitazione dei modelli SEM tradizionali risiede nella loro incapacità di tenere conto delle correlazioni intrinseche che caratterizzano i dati strutturati su più livelli, il che può portare a stime distorte e inefficaci.\nDi conseguenza, per analizzare i dati a struttura multilivello, è necessario estendere l’approccio SEM classico integrando una modellazione adatta a tale struttura. La modellazione delle equazioni strutturali multilivello (multilevel SEM) introduce variabili latenti progettate per catturare sia la variazione tra i cluster sia quella all’interno dei cluster. In questo modo, le variabili osservate sono influenzate da fattori latenti che operano sia a livello individuale che a livello di gruppo.\nQuesta strategia consente di distinguere tra la variazione sistematica tra i gruppi (variazione tra i cluster) e la variazione individuale all’interno dei gruppi (variazione intra-cluster), permettendo un’analisi più precisa e rappresentativa dei dati complessi tipici delle scienze sociali e psicologiche. Adottando questo approccio, si ottiene una visione più completa e informativa delle dinamiche che sottostanno ai fenomeni studiati, in quanto il modello multilevel SEM è in grado di cogliere sia le differenze tra gruppi sia le specificità individuali all’interno dei gruppi.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Modelli di Equazioni Strutturali Multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/08_multilevel_sem.html#concetto-generale-per-la-modellazione-a-equazioni-strutturali-multilivello",
    "href": "chapters/sem/08_multilevel_sem.html#concetto-generale-per-la-modellazione-a-equazioni-strutturali-multilivello",
    "title": "52  Modelli di Equazioni Strutturali Multilivello",
    "section": "52.2 Concetto Generale per la Modellazione a Equazioni Strutturali Multilivello",
    "text": "52.2 Concetto Generale per la Modellazione a Equazioni Strutturali Multilivello\nLa tipica implementazione della modellazione a equazioni strutturali multilivello (SEM) prevede la scomposizione dell’outcome osservato in due componenti, una per descrivere la varianza a livello “within” (all’interno) e l’altra a livello “between” (tra gruppi), come segue:\n\\[\n\\bar{y}_{ij} - \\bar{y}_{..} = (\\bar{y}_{ij} - \\bar{y}_{.j}) + (\\bar{y}_{.j} - \\bar{y}_{..}),\n\\]\ndove \\(j\\) è l’indicatore del cluster \\(j\\)-esimo (ad esempio, una scuola, come nell’esempio sopra), con \\(j = 1, \\dots, J\\) e \\(i\\) rappresenta l’indicatore dell’individuo \\(i\\)-esimo all’interno del cluster, con \\(i = 1, \\dots, n_j\\). Il termine \\(\\bar{y}_{.j}\\) indica la media a livello di cluster per il cluster \\(j\\), mentre \\(\\bar{y}_{..}\\) rappresenta la media complessiva.\nQuesta equazione corrisponde alla scomposizione della matrice di covarianza della popolazione in componenti “within” e “between”:\n\\[\n\\text{Cov}(y) = \\Sigma_T = \\Sigma_W + \\Sigma_B.\n\\]\nBasandosi su questa scomposizione, è possibile costruire una funzione di verosimiglianza per stimare i parametri associati ai pesi fattoriali, ai coefficienti di percorso e alle varianze residue nei modelli di equazioni strutturali. Gli errori standard e gli intervalli di credibilità si possono ottenere grazie alla teoria della stima di massima verosimiglianza per l’inferenza statistica. Questa stima viene implementata in lavaan basandosi sui metodi descritti da McDonald e Goldstein (1989).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Modelli di Equazioni Strutturali Multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/08_multilevel_sem.html#un-esempio-pratico",
    "href": "chapters/sem/08_multilevel_sem.html#un-esempio-pratico",
    "title": "52  Modelli di Equazioni Strutturali Multilivello",
    "section": "52.3 Un Esempio Pratico",
    "text": "52.3 Un Esempio Pratico\nIn questo capitolo introdurremo l’implementazione in lavaan per l’analisi SEM multilivello seguendo il tutorial fornito sul sito di lavaan.org.\n\nUtilizzeremo un set di dati artificiali ricavato dal sito di MPlus.\n\n\ndat &lt;- read.table(\"http://statmodel.com/usersguide/chap9/ex9.6.dat\")\nnames(dat) &lt;- c(\"y1\", \"y2\", \"y3\", \"y4\", \"x1\", \"x2\", \"w\", \"clus\")\nhead(dat)\n\n\nA data.frame: 6 x 8\n\n\n\ny1\ny2\ny3\ny4\nx1\nx2\nw\nclus\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n\n\n\n\n1\n2.2033\n1.859\n1.7385\n2.245\n1.143\n-0.797\n-0.150\n1\n\n\n2\n1.9349\n2.128\n0.0831\n2.509\n1.949\n-0.123\n-0.150\n1\n\n\n3\n0.3220\n0.977\n-0.8354\n0.558\n-0.716\n-0.767\n-0.150\n1\n\n\n4\n0.0732\n-1.743\n-2.3103\n-1.514\n-2.649\n0.638\n-0.150\n1\n\n\n5\n-1.2149\n0.453\n0.3726\n-1.790\n-0.263\n0.303\n-0.150\n1\n\n\n6\n0.2983\n-1.820\n0.5613\n-2.091\n-0.945\n1.363\n0.319\n2\n\n\n\n\n\nIl data frame è costituito da 1000 righe:\n\ndim(dat) |&gt; print()\n\n[1] 1000    8\n\n\n\nCi sono 110 cluster (clus), il che significa che ci sono misure ripetute per ciascun cluster (possiamo immaginare che i cluster corrispondano ai soggetti):\n\nlength(unique(dat$clus))\n\n110\n\n\n\nAnalizzeremo questi dati mediante un modello di equazioni strutturali multilivello. Iniziamo a definire il modello SEM appropriato per questi dati.\n\nmodel &lt;- \"\n    level: 1\n        fw =~ y1 + y2 + y3 + y4\n        fw ~ x1 + x2\n\n    level: 2\n        fb =~ y1 + y2 + y3 + y4\n\n    # optional\n    y1 ~~ 0*y1\n    y2 ~~ 0*y2\n    y3 ~~ 0*y3\n    y4 ~~ 0*y4\n    fb ~ w\n\"\n\nQuesta sintassi del modello è strutturata su due livelli, uno per il livello 1 (intra-cluster) e uno per il livello 2 (inter-cluster). All’interno di ciascun livello, è possibile specificare un modello come nel caso a livello singolo, ma con una distinzione importante: ogni livello rappresenta fonti di variabilità differenti, in questo caso tra le misurazioni individuali (livello 1) e le differenze tra gruppi o cluster (livello 2).\n\n52.3.1 Spiegazione dei livelli\n\nLivello 1 (Intra-cluster):\n\nFattore latente fw: Al livello individuale, fw è un fattore latente che riflette la variazione tra le quattro variabili osservate y1, y2, y3 e y4. La sintassi fw =~ y1 + y2 + y3 + y4 indica che fw è il costrutto latente che sottende questi indicatori osservabili.\nEffetto dei predittori x1 e x2: Al livello individuale, fw è modellato in funzione dei predittori x1 e x2 (fw ~ x1 + x2), che rappresentano variabili a livello intra-cluster che possono influenzare il fattore latente fw. Questo permette di catturare come i predittori influenzano la variabilità nelle risposte individuali.\n\nLivello 2 (Inter-cluster):\n\nFattore latente fb: Al livello del cluster, fb rappresenta un secondo fattore latente che viene definito anch’esso dalle variabili y1, y2, y3 e y4, ma con una prospettiva inter-cluster. Questo livello considera quindi la variazione nei punteggi medi del cluster piuttosto che nelle risposte individuali.\nEffetto del predittore w: Al livello di cluster, fb è modellato come funzione del predittore w (fb ~ w), che rappresenta una variabile esplicativa per le differenze tra cluster.\n\n\n\n\n52.3.2 Parte opzionale\nLa sezione opzionale, che include espressioni come y1 ~~ 0*y1, specifica la varianza residua delle variabili osservate y1, y2, y3 e y4 al livello di cluster. L’uso di 0*y1, 0*y2, etc., indica che la varianza residua a livello inter-cluster viene fissata a zero, assumendo che tutta la varianza tra cluster sia spiegata da fb.\nIn sintesi, questo modello permette di distinguere come i fattori latenti (fw e fb) siano influenzati rispettivamente da variabili a livello individuale e cluster, consentendo di modellare simultaneamente la variabilità intra- e inter-cluster.\n\n\n52.3.3 Coefficiente di Correlazione Intraclasse (ICC)\nL’analisi SEM Multilivello permette di calcolare il Coefficiente di Correlazione Intraclasse (ICC), una misura statistica utile in studi dove i dati sono raggruppati in cluster o gruppi (come ad esempio soggetti all’interno di classi o pazienti all’interno di ospedali). L’ICC valuta il grado di somiglianza o omogeneità delle misurazioni all’interno di ciascun gruppo rispetto alla variazione totale nei dati.\nPiù precisamente, l’ICC quantifica la proporzione della varianza totale che può essere attribuita alle differenze tra i gruppi piuttosto che a quelle all’interno dei gruppi. Quando l’ICC è elevato, significa che una parte rilevante della varianza osservata nei dati deriva dalle differenze tra i gruppi. In questo caso, le misurazioni all’interno dello stesso gruppo tendono a essere più simili tra loro rispetto a quelle di gruppi differenti. Al contrario, un ICC basso indica che la varianza è in gran parte dovuta alle differenze individuali all’interno dei gruppi, suggerendo una scarsa influenza del raggruppamento.\nIn sintesi, l’ICC è un indice di quanto “forte” sia l’effetto del raggruppamento sulle misurazioni, informando sulla necessità di considerare la struttura multilivello dei dati nell’analisi.\n\n\n52.3.4 ICC nei Modelli SEM Multilivello\nIl calcolo dell’ICC (Intra-Class Correlation) per ciascuna variabile osservata in un modello SEM multilivello è essenziale per comprendere la struttura gerarchica dei dati e la variabilità tra i gruppi. L’ICC quantifica la proporzione della varianza totale di una variabile che può essere attribuita a differenze tra gruppi piuttosto che a variazioni individuali all’interno dei gruppi. Un ICC elevato per una variabile suggerisce che l’appartenenza a un determinato gruppo (come una scuola, una famiglia o un ospedale) ha un’influenza rilevante su quella variabile, indicando che una parte consistente della variazione osservata è dovuta alle differenze tra gruppi piuttosto che alle differenze tra individui all’interno di ciascun gruppo.\nIn pratica, l’ICC è un criterio utile per determinare l’adeguatezza di un modello multilivello. Un ICC basso suggerisce che la variabilità tra i gruppi è limitata e che potrebbe non essere necessario utilizzare un modello multilivello complesso, in quanto le differenze individuali rappresentano la maggior parte della variabilità. Al contrario, un ICC alto indica che la struttura a cluster dei dati è rilevante e che ignorarla potrebbe portare a stime distorte e a conclusioni potenzialmente errate.\nUtilizzando l’ICC come guida, i ricercatori possono decidere se e in che misura adottare un approccio multilivello per rappresentare in modo accurato e affidabile la varianza attribuibile a fattori tra e intra-gruppo, ottenendo così una comprensione più precisa dei fenomeni studiati.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Modelli di Equazioni Strutturali Multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/08_multilevel_sem.html#calcolo-dellicc-con-lmer",
    "href": "chapters/sem/08_multilevel_sem.html#calcolo-dellicc-con-lmer",
    "title": "52  Modelli di Equazioni Strutturali Multilivello",
    "section": "52.4 Calcolo dell’ICC con lmer",
    "text": "52.4 Calcolo dell’ICC con lmer\nPer iniziare, poniamoci il problema di calcolare l’ICC mediante un modello lineare multilivello. Svolgeremo questi calcoli con lmer. Il modello lmer considera ogni variabile osservata separatamente, fornendo un’analisi indipendente per ciascuna. Per y1, per esempio, abbiamo:\n\nmodel_lmer &lt;- lmer(y1 ~ 1 + (1 | clus), data = dat)\n\nCalcoliamo l’ICC:\n\n varianza_cluster &lt;- VarCorr(model_lmer)$clus[1]\n varianza_residua &lt;- attr(VarCorr(model_lmer), \"sc\")^2\n ICC &lt;- varianza_cluster / (varianza_cluster + varianza_residua)\n ICC\n\n0.129536196128802\n\n\n\nIl Coefficiente di Correlazione Intraclass (ICC) di 0.129 per la variabile y1 significa che circa il 12.9% della variazione totale in y1 è ascrivibile alle differenze tra gli studenti, considerati come unità separate o cluster individuali. Questa percentuale relativamente modesta della variazione totale suggerisce che le caratteristiche o i comportamenti individuali degli studenti spiegano solo una piccola parte della variazione osservata in y1.\nUn ICC di questo livello, che si può considerare relativamente basso, implica che la maggior parte della variazione nella variabile non è legata in modo sostanziale alle differenze tra gli studenti. Questo può essere indicativo del fatto che altri fattori, esterni alle caratteristiche individuali degli studenti, giocano un ruolo più determinante. In un contesto educativo, ad esempio, questo potrebbe suggerire che elementi come l’ambiente scolastico, le metodologie didattiche impiegate, o le specificità del programma di studi, hanno un impatto maggiore sulla variazione di y1 rispetto alle differenze individuali tra gli studenti.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Modelli di Equazioni Strutturali Multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/08_multilevel_sem.html#calcolo-dellicc-con-sem",
    "href": "chapters/sem/08_multilevel_sem.html#calcolo-dellicc-con-sem",
    "title": "52  Modelli di Equazioni Strutturali Multilivello",
    "section": "52.5 Calcolo dell’ICC con sem",
    "text": "52.5 Calcolo dell’ICC con sem\nOra calcoliamo l’ICC utilizzando un modello SEM multilivello. Per adattare questo modello, è necessario aggiungere l’argomento cluster= nella chiamata alla funzione sem di lavaan. Questo argomento specifica la variabile di raggruppamento, permettendo al modello di tenere conto della struttura a cluster dei dati e di stimare l’ICC per ciascuna variabile osservata.\n\n fit &lt;- sem(model,\n     data = dat,\n     cluster = \"clus\",\n     fixed.x = FALSE\n )\n\nL’argomento fixed.x controlla come vengono trattate le variabili predittive (o esogene) all’interno del modello.\nQuando fixed.x = FALSE, si sta indicando a lavaan di trattare le variabili esogene non come valori fissi, ma come variabili aleatorie con una propria varianza e covarianza da stimare nel modello. Questo significa che le variabili esogene non sono considerate “date” o senza errore, ma il modello tiene conto della loro variabilità.\n\nfixed.x = TRUE (impostazione predefinita in lavaan):\n\nLe variabili esogene sono considerate senza errore (cioè come dati “fissi”).\nNon si stima la varianza delle variabili esogene.\nQuesto approccio è tipico nei modelli di regressione classici, dove le variabili predittive sono trattate come note e prive di errore.\n\nfixed.x = FALSE:\n\nLe variabili esogene sono considerate come variabili aleatorie con errori di misurazione, quindi la loro varianza e covarianza vengono stimate.\nQuesto approccio è più realistico in molti contesti psicologici e sociali, dove è ragionevole assumere che anche le variabili esogene possano contenere errori.\n\n\nIn molte situazioni di ricerca, le variabili esogene (come i punteggi dei questionari o le misure di osservazione) non sono perfettamente accurate e possono contenere errore. Impostare fixed.x = FALSE consente di modellare questa incertezza, offrendo una rappresentazione più realistica dei dati.\n\nsummary(fit) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 27 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        26\n\n  Number of observations                          1000\n  Number of clusters [clus]                        110\n\nModel Test User Model:\n                                                      \n  Test statistic                                 3.863\n  Degrees of freedom                                17\n  P-value (Chi-square)                           1.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nLevel 1 [within]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  fw =~                                               \n    y1                1.000                           \n    y2                0.999    0.033   30.735    0.000\n    y3                0.995    0.033   29.804    0.000\n    y4                1.017    0.033   30.364    0.000\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  fw ~                                                \n    x1                0.973    0.042   23.287    0.000\n    x2                0.510    0.038   13.422    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  x1 ~~                                               \n    x2                0.032    0.032    1.014    0.311\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    x1                0.007    0.031    0.222    0.825\n    x2                0.014    0.032    0.440    0.660\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .y1                0.981    0.057   17.151    0.000\n   .y2                0.948    0.056   17.015    0.000\n   .y3                1.070    0.060   17.700    0.000\n   .y4                1.014    0.059   17.182    0.000\n   .fw                0.980    0.071   13.888    0.000\n    x1                0.985    0.044   22.361    0.000\n    x2                1.017    0.045   22.361    0.000\n\n\nLevel 2 [clus]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  fb =~                                               \n    y1                1.000                           \n    y2                0.960    0.073   13.078    0.000\n    y3                0.924    0.074   12.452    0.000\n    y4                0.949    0.075   12.631    0.000\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  fb ~                                                \n    w                 0.344    0.078    4.429    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .y1               -0.083    0.074   -1.128    0.259\n   .y2               -0.077    0.071   -1.081    0.280\n   .y3               -0.045    0.071   -0.637    0.524\n   .y4               -0.030    0.072   -0.418    0.676\n    w                 0.006    0.086    0.070    0.944\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .y1                0.000                           \n   .y2                0.000                           \n   .y3                0.000                           \n   .y4                0.000                           \n   .fb                0.361    0.078    4.643    0.000\n    w                 0.815    0.110    7.416    0.000\n\n\n\n\nfitMeasures(fit) |&gt;\n    print()\n\n                 npar                  fmin                 chisq \n               26.000                 3.913                 3.863 \n                   df                pvalue        baseline.chisq \n               17.000                 1.000              3283.563 \n          baseline.df       baseline.pvalue                   cfi \n               24.000                 0.000                 1.000 \n                  tli                  nnfi                   rfi \n                1.006                 1.006                 0.998 \n                  nfi                  pnfi                   ifi \n                0.999                 0.707                 1.004 \n                  rni                  logl     unrestricted.logl \n                1.004             -9527.429             -9525.497 \n                  aic                   bic                ntotal \n            19106.857             19234.459              1000.000 \n                 bic2                 rmsea        rmsea.ci.lower \n            19151.882                 0.000                 0.000 \n       rmsea.ci.upper        rmsea.ci.level          rmsea.pvalue \n                0.000                 0.900                 1.000 \n       rmsea.close.h0 rmsea.notclose.pvalue     rmsea.notclose.h0 \n                0.050                 0.000                 0.080 \n                 srmr           srmr_within          srmr_between \n                0.022                 0.004                 0.017 \n\n\nQuando calcoliamo l’ICC utilizzando lavaan, otteniamo valori distinti per ciascuna variabile osservata all’interno del modello multilivello. Questi valori rappresentano la proporzione di varianza in ogni variabile che è attribuibile alle differenze tra i gruppi, considerando le relazioni specificate nel modello SEM. In altre parole, l’ICC di ciascun item riflette quanto le differenze tra i gruppi influenzano quella particolare variabile, nel contesto delle dipendenze definite dal modello.\nPer ottenere l’ICC per ciascuno dei quattro item, è possibile utilizzare il comando:\n\nlavInspect(fit, \"icc\") |&gt;\n    print()\n\n   y1    y2    y3    y4    x1    x2 \n0.125 0.121 0.106 0.115 0.000 0.000 \n\n\n\nNel caso di y1, la stima di ICC fornita dal modello SEM multilivello è molto simile al risultato ottenuto con lmer.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Modelli di Equazioni Strutturali Multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/08_multilevel_sem.html#riflessioni-conclusive",
    "href": "chapters/sem/08_multilevel_sem.html#riflessioni-conclusive",
    "title": "52  Modelli di Equazioni Strutturali Multilivello",
    "section": "52.6 Riflessioni Conclusive",
    "text": "52.6 Riflessioni Conclusive\nIn questo capitolo, abbiamo illustrato il modello di equazioni strutturali multilivello utilizzando lavaan. Come evidenziato dall’esempio, l’implementazione in lavaan è molto diretta, richiedendo solo l’inclusione dell’opzione cluster nella funzione sem. È importante sottolineare che, al momento, lavaan supporta solo modelli SEM a due livelli.\nNell’ambito dei modelli SEM multilivello, abbiamo visto come l’interpretazione dei coefficienti di correlazione intra-classe (ICC) possa fornire intuizioni significative sulla variazione dei dati all’interno di gruppi o cluster. Un ICC basso, come quello osservato nell’esempio (0.129), indica che una porzione minore della variazione totale è attribuibile alle differenze tra i cluster. Nel contesto specifico dei nostri dati, dove ogni studente è considerato un cluster individuale, ciò suggerisce che fattori esterni agli studenti stessi potrebbero giocare un ruolo più significativo nella variazione osservata rispetto alle caratteristiche individuali degli studenti.\nIn conclusione, la modellazione di equazioni strutturali multilivello è uno strumento potente e flessibile nell’analisi di dati strutturati gerarchicamente. lavaan, sebbene limitato ai modelli a due livelli, fornisce un approccio accessibile e diretto per questi tipi di analisi. Per modelli più complessi e a più livelli, Mplus offre soluzioni alternative che possono gestire una gamma più ampia di esigenze analitiche.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Modelli di Equazioni Strutturali Multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/08_multilevel_sem.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/08_multilevel_sem.html#informazioni-sullambiente-di-sviluppo",
    "title": "52  Modelli di Equazioni Strutturali Multilivello",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lme4_1.1-35.5     Matrix_1.7-1      MASS_7.3-61       viridis_0.6.5    \n [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n[13] lavaan_0.6-19     psych_2.4.6.26    scales_1.3.0      markdown_1.13    \n[17] knitr_1.49        lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n[21] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n[25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.17.1   jsonlite_1.8.9      magrittr_2.0.3     \n  [4] TH.data_1.1-2       estimability_1.5.1  farver_2.1.2       \n  [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n [19] emmeans_1.10.5      zoo_1.8-12          uuid_1.2-1         \n [22] igraph_2.1.1        mime_0.12           lifecycle_1.0.4    \n [25] pkgconfig_2.0.3     R6_2.5.1            fastmap_1.2.0      \n [28] shiny_1.9.1         digest_0.6.37       OpenMx_2.21.13     \n [31] fdrtool_1.2.18      colorspace_2.1-1    rprojroot_2.0.4    \n [34] Hmisc_5.2-0         fansi_1.0.6         timechange_0.3.0   \n [37] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n [40] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n [43] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n [46] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n [49] foreign_0.8-87      zip_2.3.1           httpuv_1.6.15      \n [52] nnet_7.3-19         glue_1.8.0          quadprog_1.5-8     \n [55] promises_1.3.0      nlme_3.1-166        lisrelToR_0.3      \n [58] grid_4.4.2          pbdZMQ_0.3-13       checkmate_2.3.2    \n [61] cluster_2.1.6       reshape2_1.4.4      generics_0.1.3     \n [64] gtable_0.3.6        tzdb_0.4.0          data.table_1.16.2  \n [67] hms_1.1.3           car_3.1-3           utf8_1.2.4         \n [70] sem_3.1-16          pillar_1.9.0        IRdisplay_1.1      \n [73] rockchalk_1.8.157   later_1.3.2         splines_4.4.2      \n [76] cherryblossom_0.1.0 lattice_0.22-6      survival_3.7-0     \n [79] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n [82] pbapply_1.7-2       airports_0.1.0      stats4_4.4.2       \n [85] xfun_0.49           qgraph_1.9.8        arm_1.14-4         \n [88] stringi_1.8.4       pacman_0.5.1        boot_1.3-31        \n [91] evaluate_1.0.1      codetools_0.2-20    mi_1.1             \n [94] cli_3.6.3           RcppParallel_5.1.9  IRkernel_1.3.2     \n [97] rpart_4.1.23        xtable_1.8-4        repr_1.1.7         \n[100] munsell_0.5.1       Rcpp_1.0.13-1       coda_0.19-4.1      \n[103] png_0.1-8           XML_3.99-0.17       parallel_4.4.2     \n[106] usdata_0.3.1        jpeg_0.1-10         mvtnorm_1.3-2      \n[109] openxlsx_4.2.7.1    crayon_1.5.3        openintro_2.5.0    \n[112] rlang_1.1.4         multcomp_1.4-26     mnormt_2.1.1",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Modelli di Equazioni Strutturali Multilivello</span>"
    ]
  },
  {
    "objectID": "chapters/sem/09_structural_regr.html",
    "href": "chapters/sem/09_structural_regr.html",
    "title": "53  Modelli di Regressione Strutturale",
    "section": "",
    "text": "53.1 Introduzione\nUn comune modello nell’analisi delle equazioni strutturali (SEM) è il modello di regressione strutturale (Sr), noto anche come modello di percorso con variabili latenti o modello LISREL completo. La parte “strutturale” di un modello Sr rappresenta ipotesi sugli effetti diretti o indiretti tra variabili osservate o fattori comuni, mentre la parte di “misurazione” rappresenta la corrispondenza tra i fattori comuni e i loro indicatori.\nLa specificazione di modelli Sr con indicatori continui e i requisiti per la loro identificazione saranno considerati per primi in questo capitolo. Successivamente, vengono delineate due diverse strategie per analizzare i modelli Sr completi in cui ogni variabile nel modello strutturale è un fattore comune e ciascuno ha molteplici indicatori. Queste strategie affrontano il problema di come identificare la fonte o le fonti di errore di specificazione, separando la valutazione della parte di misurazione del modello dall’analisi della sua parte strutturale.\nInoltre, in questo capitolo vengono discussi anche i modelli Sr parziali con indicatori singoli per alcune, ma non tutte, le variabili nella parte strutturale del modello. Viene inoltre spiegato un metodo per specificare il modello che controlla esplicitamente la misurazione negli indicatori singoli senza influenzare l’adattamento del modello.\nNella {numref}kline-15-1-fig (a), è rappresentato un modello di percorso con variabili manifeste. Viene assunto che la variabile esogena X1 sia misurata senza errore, sebbene questa assunzione sia spesso violata nella pratica. Le variabili endogene nel modello, come Y1 e Y4, possono avere errori casuali che si manifestano nelle loro perturbazioni.\nEsempi di un modello di percorso con variabili manifeste (a) e di un corrispondente modello di regressione strutturale completo con indicatori multipli per ogni fattore comune nella parte strutturale (b). (Figura tratta da {cite:t}kline2023principles)\nPer affrontare questa sfida, la modellazione bifase, suggerita da Anderson e Gerbing (1988), propone un’euristica in due fasi per l’identificazione dei modelli SR completi:\nLa modellazione bifase presenta diversi problemi. Primo, richiede di considerare tutte le opzioni possibili di riformulazione sia del modello CFA che del modello strutturale. Secondo, il processo può essere paragonato a un “giardino dei sentieri che si biforcano”, dove ogni decisione può portare a diverse possibili modifiche del modello originale. Terzo, esiste il rischio di capitalizzare sulle variazioni casuali quando si testano e si riformulano modelli correlati usando gli stessi dati.\nUn’altra sfida è rappresentata dai modelli equivalenti. Ad esempio, un modello CFA al Primo Passaggio e un modello SR corrispondente con una parte strutturale appena identificata al Secondo Passaggio potrebbero risultare equivalenti. La preferenza per il modello SR dovrebbe quindi basarsi su ragionamenti logici o essere supportata dal design dello studio.\nSe un modello di misurazione viene mantenuto al Primo Passaggio, esistono due approcci principali per testare il modello strutturale nel Secondo Passaggio: il Model building (ricerca in avanti) e il Model trimming (ricerca all’indietro). Chou e Bentler (2002) hanno osservato che la probabilità di identificare il modello SR corretto dipende significativamente dal modello inizialmente specificato. Inoltre, hanno trovato che il modello corretto veniva identificato circa il 60% delle volte nel Model trimming.\nÈ importante notare che se i carichi fattoriali variano notevolmente con diversi modelli strutturali, ciò indica che il modello di misurazione non è invariante e aumenta il rischio di confondimento interpretativo.\nInfine, alcuni studi hanno mostrato che le statistiche globali di adattamento standard possono essere più influenzate dalla componente di misurazione dei modelli SR rispetto alla parte strutturale. Tuttavia, i risultati delle simulazioni al computer indicano che non esiste un insieme universale di soglie per gli indici di adattamento approssimato che siano validi per tutti i tipi di modelli e dati.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Modelli di Regressione Strutturale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/09_structural_regr.html#introduzione",
    "href": "chapters/sem/09_structural_regr.html#introduzione",
    "title": "53  Modelli di Regressione Strutturale",
    "section": "",
    "text": "```mmwujudd ../images/kline_15_1.png\n\n\n\n\nheight: 500px\n\n\nname: kline-15-1-fig\n\n\n\n\n\nLa Figura {numref}`kline-15-1-fig` (b) illustra un modello di Regressione Strutturale (SR) completo, che integra sia componenti strutturali sia di misurazione. In questo modello SR, a differenza del modello di percorso, ciascun indicatore (X1, Y1, Y4) è definito come uno tra numerosi indicatori associati a un fattore comune. Di conseguenza, tutte le variabili osservabili in questa figura includono termini di errore.\n\nNella parte strutturale del modello, presentata nella Figura {numref}`kline-15-1-fig` (b), si osserva la rappresentazione degli stessi schemi di effetti causali diretti e indiretti trovati nel modello di percorso (mostrato nella Figura {numref}`kline-15-1-fig` (a)), ma applicati ai fattori comuni. Tale modello strutturale è di tipo ricorsivo, sebbene sia anche fattibile la progettazione di un modello SR con componenti strutturali non ricorsive. In questa figura, ogni fattore endogeno possiede una perturbazione specifica ($D_B$, $D_C$), che, a differenza dei modelli di percorso con variabili manifeste, riflettono unicamente cause non considerate e non errori di misurazione degli indicatori del fattore.\n\nPer quanto riguarda l'analisi delle medie, le osservazioni e i parametri nei modelli SR sono trattati allo stesso modo di quelli nei modelli di percorso e nei modelli di Analisi Fattoriale Confermativa (CFA), conformemente alle regole precedentemente stabilite. L'identificazione di un modello SR completo avviene quando sia la sua componente di misurazione, riformulata come un modello CFA, sia la parte strutturale risultano identificate. La regola di identificazione in due fasi implica che, per determinare se un modello SR completo sia identificato, è necessario esaminare separatamente ciascuna delle sue parti, ovvero quelle di misurazione e strutturale.\n\n## Modellazione in Due Fasi\n\nImmaginiamo che un ricercatore abbia definito un modello di Regressione Strutturale (SR) completo, come mostrato nella {numref}`kline-15-2-fig`(a). Dopo aver raccolto i dati, il ricercatore adotta un approccio monofase per analizzare il modello, eseguendo una stima simultanea delle componenti di misurazione e strutturali. Tuttavia, i risultati rivelano che il modello non si adatta bene ai dati. Ciò solleva interrogativi sulla localizzazione del problema: è nella parte di misurazione, nella parte strutturale, o in entrambe? Identificare la fonte del problema con precisione può essere complesso usando un approccio monofase.\n\n```{figure} ../images/kline_15_2.png\n---\nheight: 600px\nname: kline-15-2-fig\n---\nValutazione della regola in due fasi per l'identificazione di un modello di regressione strutturale completo presentato con simbolismo grafico compatto per i termini di errore degli indicatori nella parte di misurazione e le perturbazioni nella parte strutturale. (Figura tratta da {cite:t}`kline2023principles`)\n\n\nPrimo Passaggio: Il modello SR originale viene trasformato in un modello di misurazione CFA. Questo modello riformulato viene quindi analizzato per valutare l’adattamento ai dati. Un cattivo adattamento del modello CFA potrebbe indicare errori nelle ipotesi di misurazione del ricercatore e suggerire che l’adattamento del modello SR originale potrebbe essere compromesso, specialmente se la sua parte strutturale è sovraidentificata.\nSecondo Passaggio: Con un modello di misurazione CFA ritenuto valido dal primo passaggio, il secondo confronta l’adattamento del modello SR originale con quello di diversi modelli strutturali alternativi, utilizzando il test di differenza del chi-quadro.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Modelli di Regressione Strutturale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/09_structural_regr.html#una-applicazione-concreta",
    "href": "chapters/sem/09_structural_regr.html#una-applicazione-concreta",
    "title": "53  Modelli di Regressione Strutturale",
    "section": "53.2 Una Applicazione Concreta",
    "text": "53.2 Una Applicazione Concreta\nLa {numref}kline-15-3-fig illustra un modello SR (Regressione Strutturale) iniziale che esplora il rendimento scolastico e l’adattamento in aula di studenti di età media corrispondente ai gradi 7-8. Il modello considera l’influenza dell’abilità cognitiva generale e del livello di rischio di disturbi psicopatologici. Uno degli indicatori di rischio deriva dalla diagnosi di disturbi psichiatrici maggiori nei genitori, mentre il secondo è basato sul livello socio-economico (SES) della famiglia, con punteggi più alti che indicano un SES inferiore. Le abilità cognitive sono valutate tramite i punteggi in ragionamento verbale, analisi visivo-spaziale e memoria, ottenuti da un test di QI somministrato individualmente.\n\n\n\n```mmwujudd ../images/kline_15_3.png\n\n\n\n\nheight: 600px\n\n\nname: kline-15-3-fig\n\n\n\nModello iniziale completo di regressione strutturale del rendimento scolastico e dell’adattamento in classe come funzione dell’abilità cognitiva e del rischio di psicopatologia. (Figura tratta da {cite:t}kline2023principles)\n\nIl modello comprende due fattori endogeni: il rendimento scolastico, valutato attraverso test standardizzati di lettura, aritmetica e ortografia, e l'adattamento in classe, misurato con tre indicatori forniti dagli insegnanti riguardo alla motivazione, stabilità emotiva e qualità delle relazioni sociali degli studenti. In questo modello strutturale, sia il rendimento scolastico sia l'adattamento in classe sono influenzati dall'abilità cognitiva e dal rischio, ma non vi è un effetto diretto o una covarianza delle perturbazioni tra questi due fattori endogeni, indicando che eventuali associazioni tra di essi sono attribuibili alle loro cause comuni, i fattori esogeni.\n\n::: {#cell-7 .cell vscode='{\"languageId\":\"r\"}' execution_count=4}\n``` {.r .cell-code}\n# input the correlations in lower diagnonal form\nworlandLower.cor &lt;- \"\n1.00\n .70 1.00\n .65  .60 1.00\n .55  .50  .45 1.00\n .50  .45  .40  .70 1.00\n .35  .35  .30  .55  .50 1.00\n .30  .30  .30  .50  .45  .44 1.00\n .25  .20  .22  .41  .28  .34  .40 1.00\n .35  .32  .32  .48  .45  .42  .60  .45 1.00\n-.25 -.24 -.22 -.21 -.18 -.15 -.15 -.12 -.17 1.00\n-.22 -.26 -.30 -.25 -.22 -.18 -.17 -.14 -.20  .42 1.00 \"\n\n# name the variables and convert to full correlation matrix\nworland.cor &lt;- lavaan::getCov(worlandLower.cor, names = c(\n    \"verbal\", \"visual\",\n    \"memory\", \"read\", \"math\", \"spell\", \"motive\", \"harmony\", \"stable\", \"parent\", \"ses\"\n))\n\n# add the standard deviations and convert to covariances\nworland.cov &lt;- lavaan::cor2cov(worland.cor,\n    sds = c(\n        13.75, 14.80, 12.60, 14.90, 15.25, 13.85, 9.50, 11.10, 8.70,\n        12.00, 8.50\n    )\n)\n:::\nPrimo Passaggio nella modellazione in due fasi per un modello SR completo di rendimento scolastico e adattamento in classe\n\n# 4-factor CFA\nworlandCFA.model &lt;- \"\n    Cognitive =~ verbal + visual + memory\n    Achieve =~ read + math + spell\n    Adjust =~ motive + harmony + stable\n    Risk =~ parent + ses \n \"\n\n\nworlandCFA &lt;- lavaan::cfa(worlandCFA.model,\n    sample.cov = worland.cov,\n    sample.nobs = 158\n)\n\n\nsemPlot::semPaths(worlandCFA,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\n\nfitMeasures(worlandCFA, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n\n chisq     df    cfi    tli  rmsea   srmr \n16.212 38.000  1.000  1.049  0.000  0.023 \n\n\n\n# predicted correlation matrix for factors\nlavaan::lavInspect(worlandCFA, \"cor.lv\") |&gt; print()\n\n          Cogntv Achiev Adjust   Risk\nCognitive  1.000                     \nAchieve    0.703  1.000              \nAdjust     0.500  0.751  1.000       \nRisk      -0.459 -0.401 -0.349  1.000\n\n\n\nlavaan::residuals(worlandCFA, type = \"standardized.mplus\") |&gt; print()\n\n$type\n[1] \"standardized.mplus\"\n\n$cov\n        verbal visual memory   read   math  spell motive harmny stable parent\nverbal   0.000                                                               \nvisual   0.000  0.000                                                        \nmemory   0.123 -0.130  0.000                                                 \nread     0.598  0.113 -0.285  0.000                                          \nmath     0.505  0.038 -0.377  0.597  0.000                                   \nspell   -0.952 -0.255 -0.704 -0.667 -0.206  0.000                            \nmotive  -0.821 -0.157  0.310 -0.042 -0.078  1.484  0.000                     \nharmony  0.117 -0.453  0.171  0.942 -1.179  1.196 -1.010  0.000              \nstable   0.285  0.112  0.566 -1.893 -0.467  0.942  0.422  0.436  0.000       \nparent  -0.092 -0.260 -0.157  0.234  0.379  0.157  0.301  0.020 -0.022  0.000\nses      1.890 -0.247 -1.417 -0.370 -0.114 -0.112  0.213 -0.125 -0.362  0.000\n           ses\nverbal        \nvisual        \nmemory        \nread          \nmath          \nspell         \nmotive        \nharmony       \nstable        \nparent        \nses      0.000\n\n\n\n\nlavaan::residuals(worlandCFA, type = \"cor.bollen\") |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n        verbal visual memory   read   math  spell motive harmny stable parent\nverbal   0.000                                                               \nvisual   0.000  0.000                                                        \nmemory   0.001 -0.002  0.000                                                 \nread     0.015  0.003 -0.010  0.000                                          \nmath     0.017  0.001 -0.016  0.007  0.000                                   \nspell   -0.040 -0.012 -0.035 -0.010 -0.006  0.000                            \nmotive  -0.031 -0.007  0.016 -0.001 -0.003  0.075  0.000                     \nharmony  0.006 -0.026  0.010  0.041 -0.053  0.071 -0.027  0.000              \nstable   0.011  0.005  0.028 -0.034 -0.015  0.045  0.005  0.012  0.000       \nparent  -0.003 -0.010 -0.007  0.008  0.017  0.009  0.015  0.001 -0.001  0.000\nses      0.050 -0.010 -0.068 -0.012 -0.005 -0.006  0.010 -0.008 -0.016  0.000\n           ses\nverbal        \nvisual        \nmemory        \nread          \nmath          \nspell         \nmotive        \nharmony       \nstable        \nparent        \nses      0.000\n\n\n\n\n# calculate factor reliability coefficients (semTools)\nsemTools::reliability(worlandCFA) |&gt; print()\n\n       Cognitive   Achieve    Adjust      Risk\nalpha  0.8463118 0.8087788 0.7249416 0.5675488\nomega  0.8513712 0.8207835 0.7296141 0.5770344\nomega2 0.8513712 0.8207835 0.7296141 0.5770344\nomega3 0.8515872 0.8227890 0.7327431 0.5770344\navevar 0.6588592 0.6101369 0.4744677 0.4094519\n\n\nSecondo Passaggio nella modellazione in due fasi per un modello SR completo di rendimento scolastico e adattamento in classe\nI risultati del Passaggio 1 del metodo in due fasi, che si concentrava sul modello di misurazione, consentono di procedere all’analisi del modello SR originale, che prevede cinque percorsi nella {numref}kline-15-3-fig, nel Passaggio 2 del metodo. Anche questa seconda analisi ha portato a una soluzione ammissibile.\n\n# step 2a\n# 4-factor SR model with 5 paths among factors\n\n# by default, lavaan frees the disturbance covariance\n# between a pair of outcomes in a structural model\n# when there is no direct effect between them\n# thus, this parameter is explicitly fixed to zero\n# in this analysis\n\nworlandSRa_model &lt;- \"\n    # measurement part\n    Cognitive =~ verbal + visual + memory\n    Achieve =~ read + math + spell\n    Adjust =~ motive + harmony + stable\n    Risk =~ parent + ses\n    # structural part (5 paths)\n    Achieve ~ Cognitive + Risk\n    Adjust ~ Cognitive + Risk\n    # constrain disturbance covariance to zero\n    Adjust ~~ 0*Achieve \n\"\n\n\nworlandSRa &lt;- lavaan::sem(worlandSRa_model,\n    sample.cov = worland.cov,\n    sample.nobs = 158\n)\n\n\nsemPaths(\n    worlandSRa,\n    \"std\",\n    posCol = c(\"black\"),\n    edge.label.cex = 1.2,\n    sizeMan = 7,\n    edge.width = 0.4, # Set a fixed width for all arrows\n    fade = FALSE # Disable fading of the arrows\n)\n\n\n\n\n\n\n\n\n\nfitMeasures(worlandSRa, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n\n chisq     df    cfi    tli  rmsea   srmr \n37.320 39.000  1.000  1.004  0.000  0.045 \n\n\n\nlavaan::residuals(worlandSRa, type = \"standardized.mplus\") |&gt; print()\n\n$type\n[1] \"standardized.mplus\"\n\n$cov\n        verbal visual memory   read   math  spell motive harmny stable parent\nverbal   0.002                                                               \nvisual   0.209  0.002                                                        \nmemory   0.204 -0.221  0.002                                                 \nread     0.671  0.115 -0.298  0.004                                          \nmath     0.562  0.043 -0.384  0.596  0.002                                   \nspell   -0.902 -0.252 -0.709 -0.670 -0.201  0.000                            \nmotive  -0.777 -0.158  0.304  0.030 -0.026  1.506  0.000                     \nharmony  0.132 -0.456  0.165  0.962 -1.151  1.208 -0.997  0.000              \nstable   0.307  0.102  0.554 -1.797 -0.427  0.959  0.421  0.426  0.000       \nparent   0.229  0.003  0.057  0.544  0.602  0.308  0.446  0.118  0.158     NA\nses      2.082  0.015 -1.204  0.009  0.133  0.043  0.362 -0.024 -0.143  4.056\n           ses\nverbal        \nvisual        \nmemory        \nread          \nmath          \nspell         \nmotive        \nharmony       \nstable        \nparent        \nses      0.003\n\n\n\n\nlavaan::residuals(worlandSRa, type = \"cor.bollen\") |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n        verbal visual memory   read   math  spell motive harmny stable parent\nverbal   0.000                                                               \nvisual   0.002  0.000                                                        \nmemory   0.003 -0.004  0.000                                                 \nread     0.018  0.003 -0.010  0.000                                          \nmath     0.019  0.002 -0.016  0.007  0.000                                   \nspell   -0.038 -0.012 -0.036 -0.010 -0.006  0.000                            \nmotive  -0.029 -0.007  0.015  0.001 -0.001  0.076  0.000                     \nharmony  0.007 -0.026  0.010  0.042 -0.052  0.072 -0.026  0.000              \nstable   0.012  0.004  0.027 -0.033 -0.013  0.046  0.005  0.012  0.000       \nparent   0.007  0.000  0.003  0.021  0.028  0.018  0.023  0.008  0.008  0.000\nses      0.059  0.001 -0.058  0.000  0.006  0.003  0.018 -0.002 -0.007  0.312\n           ses\nverbal        \nvisual        \nmemory        \nread          \nmath          \nspell         \nmotive        \nharmony       \nstable        \nparent        \nses      0.000\n\n\n\nSebbene gli indici di fit siano buoni, l’adattamento locale del modello con cinque percorsi tra i fattori è scarso. Ad esempio, i residui standardizzati per diverse coppie di indicatori dei fattori di rendimento e adattamento hanno spesso un valore maggiore di 2:\n\nLettura, Motivazione, 3.466\nOrtografia, Motivazione, 3.348\nLettura, Armonia, 2.903\n\nBasandosi su tutti questi risultati relativi all’adattamento globale e locale, il modello SR iniziale nella {numref}kline-15-3-fig con cinque percorsi tra i fattori è rifiutato.\nEsaminiamo i modification indices.\n\nmodificationIndices(worlandSRa, sort = TRUE, minimum.value = 5)\n\n\nA lavaan.data.frame: 3 x 8\n\n\n\nlhs\nop\nrhs\nmi\nepc\nsepc.lv\nsepc.all\nsepc.nox\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n125\nAchieve\n~\nAdjust\n19.96338\n115.19070\n63.74565\n63.7456489\n63.7456489\n\n\n120\nparent\n~~\nses\n19.96208\n32.61953\n32.61953\n0.3608746\n0.3608746\n\n\n126\nAdjust\n~\nAchieve\n19.95923\n26.12006\n47.19990\n47.1999001\n47.1999001\n\n\n\n\n\nI risultati dei modification indices mostrano che l’assenza di un percorso tra i fattori di rendimento e adattamento nella {numref}kline-15-3-fig è chiaramente incoerente con i dati. Per aggiungere una covariazione tra i fattori di rendimento e adattamento abbiamo due opzioni: o aggiungere un effetto diretto tra i fattori o permettere alle loro perturbazioni di covariare. Ma sarebbe difficile giustificare un effetto diretto rispetto all’altro: scarse abilità scolastiche potrebbero peggiorare l’adattamento in classe tanto quanto i problemi comportamentali a scuola potrebbero influire negativamente sul rendimento. La specificazione di una causalità reciproca tra Rendimento e Adattamento renderebbe il modello strutturale non ricorsivo, ma il modello non sarebbe identificato senza imporre vincoli irrealistici. Riformuliamo dunque il modello della {numref}kline-15-3-fig permettendo alle perturbazioni tra i fattori di rendimento e adattamento di covariare.\n\n# step 2b\n# 4-factor SR model with 6 paths among factors\n# this model is equivalent to the basic 4-factor\n# CFA measurement model analyzed in step 1\n\nworlandSRb_model &lt;- \"\n    # measurement part\n    Cognitive =~ verbal + visual + memory\n    Achieve =~ read + math + spell\n    Adjust =~ motive + harmony + stable\n    Risk =~ parent + ses\n    # structural part (6 paths)\n    Achieve ~ Cognitive + Risk\n    Adjust ~ Cognitive + Risk\n    Adjust ~~ Achieve \n\"\n\n\nworlandSRb &lt;- lavaan::sem(worlandSRb_model,\n    sample.cov = worland.cov,\n    sample.nobs = 158\n)\n\n\nsemPlot::semPaths(worlandSRb,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\n\nparameterEstimates(worlandSRb) |&gt; print()\n\n         lhs op       rhs     est     se      z pvalue ci.lower ci.upper\n1  Cognitive =~    verbal   1.000  0.000     NA     NA    1.000    1.000\n2  Cognitive =~    visual   1.000  0.090 11.144  0.000    0.824    1.175\n3  Cognitive =~    memory   0.788  0.077 10.217  0.000    0.637    0.940\n4    Achieve =~      read   1.000  0.000     NA     NA    1.000    1.000\n5    Achieve =~      math   0.925  0.083 11.100  0.000    0.761    1.088\n6    Achieve =~     spell   0.678  0.080  8.480  0.000    0.521    0.835\n7     Adjust =~    motive   1.000  0.000     NA     NA    1.000    1.000\n8     Adjust =~   harmony   0.861  0.136  6.311  0.000    0.593    1.128\n9     Adjust =~    stable   0.940  0.114  8.231  0.000    0.716    1.164\n10      Risk =~    parent   1.000  0.000     NA     NA    1.000    1.000\n11      Risk =~       ses   0.773  0.224  3.445  0.001    0.333    1.212\n12   Achieve  ~ Cognitive   0.719  0.109  6.574  0.000    0.504    0.933\n13   Achieve  ~      Risk  -0.175  0.190 -0.922  0.357   -0.548    0.198\n14    Adjust  ~ Cognitive   0.261  0.070  3.743  0.000    0.124    0.398\n15    Adjust  ~      Risk  -0.146  0.127 -1.153  0.249   -0.395    0.102\n16   Achieve ~~    Adjust  36.374  7.967  4.565  0.000   20.758   51.989\n17    verbal ~~    verbal  46.253  9.782  4.728  0.000   27.080   65.426\n18    visual ~~    visual  76.171 12.153  6.268  0.000   52.351   99.991\n19    memory ~~    memory  69.732  9.717  7.177  0.000   50.688   88.776\n20      read ~~      read  51.273 11.238  4.562  0.000   29.246   73.299\n21      math ~~      math  86.341 13.049  6.616  0.000   60.764  111.917\n22     spell ~~     spell 112.797 14.078  8.012  0.000   85.205  140.389\n23    motive ~~    motive  37.737  6.463  5.839  0.000   25.071   50.404\n24   harmony ~~   harmony  83.953 10.590  7.927  0.000   63.197  104.709\n25    stable ~~    stable  29.306  5.386  5.441  0.000   18.749   39.863\n26    parent ~~    parent  88.005 18.343  4.798  0.000   52.053  123.958\n27       ses ~~       ses  38.896 10.208  3.810  0.000   18.889   58.904\n28 Cognitive ~~ Cognitive 141.617 22.098  6.409  0.000   98.307  184.928\n29   Achieve ~~   Achieve  84.317 16.324  5.165  0.000   52.322  116.312\n30    Adjust ~~    Adjust  38.008  8.188  4.642  0.000   21.960   54.055\n31      Risk ~~      Risk  55.079 19.989  2.755  0.006   15.902   94.257\n32 Cognitive ~~      Risk -40.517 12.448 -3.255  0.001  -64.914  -16.119\n\n\n\nfitMeasures(worlandSRb, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n\n chisq     df    cfi    tli  rmsea   srmr \n16.212 38.000  1.000  1.049  0.000  0.023 \n\n\n\n# predicted correlation matrix for factors\nlavaan::lavInspect(worlandSRb, \"cor.lv\") |&gt; print()\n\n          Cogntv Achiev Adjust   Risk\nCognitive  1.000                     \nAchieve    0.703  1.000              \nAdjust     0.500  0.751  1.000       \nRisk      -0.459 -0.401 -0.349  1.000\n\n\n\nlavaan::residuals(worlandSRb, type = \"standardized.mplus\") |&gt; print()\n\n$type\n[1] \"standardized.mplus\"\n\n$cov\n        verbal visual memory   read   math  spell motive harmny stable parent\nverbal      NA                                                               \nvisual  -0.002     NA                                                        \nmemory   0.122 -0.131     NA                                                 \nread     0.597  0.113 -0.285     NA                                          \nmath     0.504  0.038 -0.377  0.597     NA                                   \nspell   -0.952 -0.255 -0.704 -0.667 -0.206     NA                            \nmotive  -0.821 -0.157  0.309 -0.042 -0.078  1.484     NA                     \nharmony  0.117 -0.453  0.171  0.942 -1.179  1.196 -1.010     NA              \nstable   0.284  0.112  0.566 -1.893 -0.467  0.942  0.422  0.436     NA       \nparent  -0.093 -0.261 -0.157  0.234  0.378  0.157  0.300  0.019 -0.022  0.034\nses      1.891 -0.247 -1.417 -0.370 -0.114 -0.112  0.213 -0.125 -0.362  0.043\n           ses\nverbal        \nvisual        \nmemory        \nread          \nmath          \nspell         \nmotive        \nharmony       \nstable        \nparent        \nses      0.017\n\n\n\n\nlavaan::residuals(worlandSRb, type = \"cor.bollen\") |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n        verbal visual memory   read   math  spell motive harmny stable parent\nverbal   0.000                                                               \nvisual   0.000  0.000                                                        \nmemory   0.001 -0.002  0.000                                                 \nread     0.015  0.003 -0.010  0.000                                          \nmath     0.017  0.001 -0.016  0.007  0.000                                   \nspell   -0.040 -0.012 -0.036 -0.010 -0.006  0.000                            \nmotive  -0.031 -0.007  0.016 -0.001 -0.003  0.075  0.000                     \nharmony  0.006 -0.026  0.010  0.041 -0.053  0.071 -0.027  0.000              \nstable   0.011  0.005  0.028 -0.034 -0.015  0.045  0.005  0.012  0.000       \nparent  -0.003 -0.011 -0.007  0.008  0.017  0.009  0.015  0.001 -0.001  0.000\nses      0.050 -0.010 -0.068 -0.012 -0.005 -0.006  0.010 -0.008 -0.016  0.000\n           ses\nverbal        \nvisual        \nmemory        \nread          \nmath          \nspell         \nmotive        \nharmony       \nstable        \nparent        \nses      0.000\n\n\n\nConfrontiamo i due modelli con il test del rapporto di verosimiglianza.\n\nlavTestLRT(worlandSRa, worlandSRb)\n\n\nA anova: 2 x 8\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nRMSEA\nDf diff\nPr(&gt;Chisq)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nworlandSRb\n38\n12938.16\n13023.92\n16.21231\nNA\nNA\nNA\nNA\n\n\nworlandSRa\n39\n12957.27\n13039.96\n37.32037\n21.10805\n0.3567438\n1\n4.340999e-06\n\n\n\n\n\nL’adattamento del modello SR con 5 percorsi tra i fattori è significativamente peggiore rispetto a quello del modello CFA con 6 percorsi. Gli indici di fit del modello con 6 percorsi sono buoni così come il suo adattamento locale.\n\n# standardized estimates with standard errors\nlavaan::standardizedSolution(worlandSRb) |&gt; print()\n\n         lhs op       rhs est.std    se      z pvalue ci.lower ci.upper\n1  Cognitive =~    verbal   0.868 0.032 27.075  0.000    0.805    0.931\n2  Cognitive =~    visual   0.806 0.037 21.720  0.000    0.733    0.879\n3  Cognitive =~    memory   0.747 0.043 17.470  0.000    0.663    0.831\n4    Achieve =~      read   0.876 0.031 28.212  0.000    0.815    0.937\n5    Achieve =~      math   0.791 0.038 20.777  0.000    0.717    0.866\n6    Achieve =~     spell   0.639 0.053 11.984  0.000    0.534    0.743\n7     Adjust =~    motive   0.761 0.049 15.552  0.000    0.665    0.857\n8     Adjust =~   harmony   0.561 0.065  8.657  0.000    0.434    0.688\n9     Adjust =~    stable   0.781 0.048 16.381  0.000    0.688    0.875\n10      Risk =~    parent   0.620 0.100  6.217  0.000    0.425    0.816\n11      Risk =~       ses   0.677 0.104  6.495  0.000    0.473    0.881\n12   Achieve  ~ Cognitive   0.657 0.079  8.357  0.000    0.503    0.811\n13   Achieve  ~      Risk  -0.100 0.107 -0.935  0.350   -0.310    0.110\n14    Adjust  ~ Cognitive   0.431 0.103  4.179  0.000    0.229    0.633\n15    Adjust  ~      Risk  -0.151 0.127 -1.186  0.236   -0.400    0.098\n16   Achieve ~~    Adjust   0.643 0.085  7.592  0.000    0.477    0.808\n17    verbal ~~    verbal   0.246 0.056  4.421  0.000    0.137    0.355\n18    visual ~~    visual   0.350 0.060  5.847  0.000    0.233    0.467\n19    memory ~~    memory   0.442 0.064  6.920  0.000    0.317    0.567\n20      read ~~      read   0.232 0.054  4.271  0.000    0.126    0.339\n21      math ~~      math   0.374 0.060  6.197  0.000    0.255    0.492\n22     spell ~~     spell   0.592 0.068  8.686  0.000    0.458    0.725\n23    motive ~~    motive   0.421 0.074  5.649  0.000    0.275    0.567\n24   harmony ~~   harmony   0.686 0.073  9.445  0.000    0.543    0.828\n25    stable ~~    stable   0.390 0.075  5.229  0.000    0.244    0.536\n26    parent ~~    parent   0.615 0.124  4.967  0.000    0.372    0.858\n27       ses ~~       ses   0.542 0.141  3.840  0.000    0.265    0.818\n28 Cognitive ~~ Cognitive   1.000 0.000     NA     NA    1.000    1.000\n29   Achieve ~~   Achieve   0.498 0.077  6.470  0.000    0.347    0.649\n30    Adjust ~~    Adjust   0.732 0.080  9.108  0.000    0.574    0.889\n31      Risk ~~      Risk   1.000 0.000     NA     NA    1.000    1.000\n32 Cognitive ~~      Risk  -0.459 0.098 -4.686  0.000   -0.651   -0.267\n\n\nLa correlazione stimata tra i fattori esogeni abilità cognitiva e rischio, –.459, è sensata: è negativa (più alto il rischio, minore l’abilità cognitiva). Questa correlazione non è prossima a -1.0, il che suggerisce che i due fattori sono distinti e non quasi identici, confermando così l’ipotesi di validità discriminante.\nAnalizzando gli impatti specifici, un incremento di un punto nel fattore cognitivo (misurato come varianza comune del ragionamento verbale) prevede un aumento di .719 punti nel rendimento scolastico (misurato come varianza comune dell’abilità di lettura), tenendo conto del fattore di rischio. In termini standardizzati, un aumento di una deviazione standard nell’abilità cognitiva si traduce in un aumento di .657 deviazioni standard nel rendimento scolastico, sempre controllando per il rischio.\nL’influenza del rischio sul rendimento scolastico è meno marcata: un incremento di un punto nel rischio (misurato come varianza comune del disturbo genitoriale) prevede una diminuzione di .175 punti nel rendimento scolastico. Standardizzando, un aumento di una deviazione standard nel rischio si associa a una diminuzione di .100 deviazioni standard nel rendimento, controllando per l’abilità cognitiva.\nLa correlazione di perturbazione di .643 misura la relazione tra il rendimento scolastico e l’adattamento in classe, dopo aver escluso l’influenza di altri fattori noti, in questo caso l’abilità cognitiva e il rischio di psicopatologia. In termini più semplici, la correlazione di perturbazione ci dice quanto sono correlati il rendimento scolastico e l’adattamento in classe quando si tiene conto (o si “controlla”) dell’effetto dell’abilità cognitiva e del rischio. Un valore di .643 indica una correlazione moderatamente forte, suggerendo che quando il rendimento scolastico di uno studente migliora (o peggiora), anche il suo adattamento in classe tende a migliorare (o peggiorare) in modo simile, indipendentemente dal suo livello di abilità cognitiva o dal grado di rischio di psicopatologia.\nLa presenza di questa correlazione parziale sostanziale implica che ci sono fattori non misurati nel modello che influenzano sia il rendimento scolastico sia l’adattamento in classe. Questi fattori non misurati potrebbero includere variabili come il sostegno familiare, la qualità dell’insegnamento, fattori ambientali o personalità dello studente. Importante è che questi fattori non misurati sono distinti sia dall’abilità cognitiva dello studente sia dal suo rischio di psicopatologia. In conclusione, il valore di .643 non solo mette in luce l’interdipendenza tra rendimento scolastico e adattamento in classe, ma suggerisce anche l’esistenza di altre variabili influenti che non sono state direttamente misurate o incluse nel modello. Questa informazione può essere preziosa per indirizzare ulteriori ricerche o interventi educativi.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Modelli di Regressione Strutturale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/09_structural_regr.html#strategie-avanzate-di-modellazione-per-i-modelli-di-regressione-strutturale",
    "href": "chapters/sem/09_structural_regr.html#strategie-avanzate-di-modellazione-per-i-modelli-di-regressione-strutturale",
    "title": "53  Modelli di Regressione Strutturale",
    "section": "53.3 Strategie Avanzate di Modellazione per i Modelli di Regressione Strutturale",
    "text": "53.3 Strategie Avanzate di Modellazione per i Modelli di Regressione Strutturale\nOltre all’approccio tradizionale di modellazione in due fasi, esiste un metodo più complesso a quattro fasi per analizzare i modelli SR completi. Questa strategia, introdotta da Mulaik e Millsap nel 2000, amplia la modellazione bifase aggiungendo ulteriori analisi esplorative che possono portare a conclusioni più definitive in una serie più estesa di studi. Questo metodo prevede che ogni fattore comune abbia almeno quattro indicatori, numero ritenuto sufficiente per testare l’unidimensionalità con il test dell’annullamento della tetrade. I quattro indicatori rappresentano anche il numero minimo perché un modello CFA a singolo fattore sia considerato sovraidentificato. Il ricercatore testa quindi una serie di almeno quattro modelli gerarchicamente correlati, seguendo questi passaggi:\n\nPrimo Passaggio: Il modello iniziale meno restrittivo è un modello EFA, dove ogni indicatore satura su tutti i fattori. Il numero di fattori è lo stesso dei modelli analizzati nei passaggi successivi. Questo modello viene analizzato con lo stesso metodo di stima utilizzato nei passaggi successivi, ad esempio il metodo ML per indicatori continui e normalmente distribuiti. Alternativamente, si possono usare tecniche come ESEM o E/CFA al posto dell’EFA. Questo passaggio serve a testare la correttezza provvisoria delle ipotesi riguardo al numero di fattori.\nSecondo Passaggio: Corrisponde al Primo Passaggio della modellazione bifase. Qui, si specifica un modello CFA con alcuni carichi incrociati fissati a zero, identificando gli indicatori che non dipendono da certi fattori comuni. Se l’adattamento del modello CFA è ragionevole, si può procedere al test del modello SR; in caso contrario, il modello di misurazione va rivisto.\nTerzo Passaggio: Si specifica il modello SR target con lo stesso schema di carichi incrociati fissati a zero del modello CFA del Secondo Passaggio. Tipicamente, la parte strutturale del modello SR include meno effetti diretti rispetto al totale delle covarianze tra fattori nel modello CFA. Se la parte strutturale del modello SR ha tanti percorsi quanti il modello CFA, i due modelli saranno equivalenti e questo passaggio può essere omesso.\nQuarto Passaggio: Coinvolge test su ipotesi specifiche sui parametri definiti dall’inizio del processo. Questi test possono comportare l’applicazione di vincoli zero o altri, aumentando di uno dfM. I Passaggi 3 e 4 della modellazione a quattro fasi rappresentano una precisazione delle attività generali del Secondo Passaggio della modellazione bifase.\n\nUna delle critiche alla modellazione a quattro fasi riguarda la necessità di avere almeno quattro indicatori per fattore, condizione non sempre pratica o desiderabile, specialmente quando pochi indicatori, o anche un singolo indicatore ottimale, presentano migliori caratteristiche psicometriche rispetto a quattro. Tuttavia, Mulaik e Millsap hanno osservato che avere almeno quattro indicatori può compensare, in parte, le limitazioni di un campione più piccolo incrementando dfM.\nEntrambi gli approcci, bifase e quattro fasi, sfruttano la variazione casuale quando i modelli vengono testati e riformulati utilizzando gli stessi dati, e sono considerati migliori della modellazione monofase, dove non esiste una distinzione tra questioni di misurazione e struttura.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Modelli di Regressione Strutturale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/09_structural_regr.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/09_structural_regr.html#informazioni-sullambiente-di-sviluppo",
    "title": "53  Modelli di Regressione Strutturale",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lme4_1.1-35.5     Matrix_1.7-0      ggokabeito_0.1.0  viridis_0.6.5    \n [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n[13] lavaan_0.6-18     psych_2.4.6.26    scales_1.3.0      markdown_1.13    \n[17] knitr_1.48        lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n[21] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n[25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.16.0  jsonlite_1.8.9     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5.1 farver_2.1.2      \n  [7] nloptr_2.1.1       rmarkdown_2.28     vctrs_0.6.5       \n [10] minqa_1.2.8        base64enc_0.1-3    rstatix_0.7.2     \n [13] htmltools_0.5.8.1  broom_1.0.6        Formula_1.2-5     \n [16] htmlwidgets_1.6.4  plyr_1.8.9         sandwich_3.1-1    \n [19] emmeans_1.10.4     zoo_1.8-12         uuid_1.2-1        \n [22] igraph_2.0.3       mime_0.12          lifecycle_1.0.4   \n [25] pkgconfig_2.0.3    R6_2.5.1           fastmap_1.2.0     \n [28] shiny_1.9.1        digest_0.6.37      OpenMx_2.21.12    \n [31] fdrtool_1.2.18     colorspace_2.1-1   rprojroot_2.0.4   \n [34] Hmisc_5.1-3        fansi_1.0.6        timechange_0.3.0  \n [37] abind_1.4-8        compiler_4.4.1     withr_3.0.1       \n [40] glasso_1.11        htmlTable_2.4.3    backports_1.5.0   \n [43] carData_3.0-5      ggsignif_0.6.4     MASS_7.3-61       \n [46] corpcor_1.6.10     gtools_3.9.5       tools_4.4.1       \n [49] pbivnorm_0.6.0     foreign_0.8-87     zip_2.3.1         \n [52] httpuv_1.6.15      nnet_7.3-19        glue_1.7.0        \n [55] quadprog_1.5-8     promises_1.3.0     nlme_3.1-166      \n [58] lisrelToR_0.3      grid_4.4.1         pbdZMQ_0.3-13     \n [61] checkmate_2.3.2    cluster_2.1.6      reshape2_1.4.4    \n [64] generics_0.1.3     gtable_0.3.5       tzdb_0.4.0        \n [67] data.table_1.16.0  hms_1.1.3          car_3.1-2         \n [70] utf8_1.2.4         sem_3.1-16         pillar_1.9.0      \n [73] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [76] splines_4.4.1      lattice_0.22-6     survival_3.7-0    \n [79] kutils_1.73        tidyselect_1.2.1   miniUI_0.1.1.1    \n [82] pbapply_1.7-2      stats4_4.4.1       xfun_0.47         \n [85] qgraph_1.9.8       arm_1.14-4         stringi_1.8.4     \n [88] pacman_0.5.1       boot_1.3-31        evaluate_1.0.0    \n [91] codetools_0.2-20   mi_1.1             cli_3.6.3         \n [94] RcppParallel_5.1.9 IRkernel_1.3.2     rpart_4.1.23      \n [97] xtable_1.8-4       repr_1.1.7         munsell_0.5.1     \n[100] Rcpp_1.0.13        coda_0.19-4.1      png_0.1-8         \n[103] XML_3.99-0.17      parallel_4.4.1     jpeg_0.1-10       \n[106] mvtnorm_1.3-1      openxlsx_4.2.7.1   crayon_1.5.3      \n[109] rlang_1.1.4        multcomp_1.4-26    mnormt_2.1.1",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Modelli di Regressione Strutturale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/10_missing_data.html",
    "href": "chapters/sem/10_missing_data.html",
    "title": "54  Dati mancanti",
    "section": "",
    "text": "54.1 Introduzione\nRaramente un ricercatore si trova nella situazione fortunata nella quale un’analisi statistica (di tipo CFA/SEM o altro) può essere condotta utilizzando un set di dati in cui tutte le variabili sono state osservate su tutte le unità statistiche: nella pratica ricerca i dati mancanti sono la norma piuttosto che l’eccezione.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Dati mancanti</span>"
    ]
  },
  {
    "objectID": "chapters/sem/10_missing_data.html#tipologie-di-dati-mancanti",
    "href": "chapters/sem/10_missing_data.html#tipologie-di-dati-mancanti",
    "title": "54  Dati mancanti",
    "section": "54.2 Tipologie di dati mancanti",
    "text": "54.2 Tipologie di dati mancanti\nCi sono molti motivi che possono stare alla base dei dati mancanti. Ad esempio, i dati possono mancare per disegno dello studio (“mancanza pianificata”), come ad esempio nei progetti di ricerca in cui i partecipanti al campione vengono selezionati casualmente per completare sottoinsiemi diversi della batteria di valutazione (una scelta di questo tipo viene motivata, ad esempio, a causa di considerazioni pratiche come i vincoli di tempo). In tali condizioni, si presume che i dati mancanti si distribuiscano in un modo completamente casuale rispetto a tutte le altre variabili nello studio.\nIn generale, i meccanismi che determinano la presenza di dati mancanti possono essere classificati in tre categorie:\n\nvalori mancanti completamente casuali (Missing Completely At Random, MCAR). La probabilità di dati mancanti su una variabile non è collegata né al valore mancante sulla variabile, né al valore di ogni altra variabile presente nella matrice dati che si sta analizzando;\nvalori mancanti casuali (Missing At Random, MAR). I valori mancanti sono indipendenti dal valore che viene a mancare, ma dipendono da altre variabili, cioè i dati sulla variabile sono mancanti per categorie di partecipanti che potrebbero essere identificati dai valori assunti dalle altre variabili presenti nello studio;\nvalori mancanti non ignorabili (Missing Not At Random, MNAR). La mancanza di un dato può dipendere sia dal valore del dato stesso che dalle altre variabili. Per esempio, se si studia la salute mentale e le persone depresse riferiscono meno volentieri informazioni riguardanti il loro stato di salute, allora i dati non sono mancanti per caso.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Dati mancanti</span>"
    ]
  },
  {
    "objectID": "chapters/sem/10_missing_data.html#la-gestione-dei-dati-mancanti",
    "href": "chapters/sem/10_missing_data.html#la-gestione-dei-dati-mancanti",
    "title": "54  Dati mancanti",
    "section": "54.3 La gestione dei dati mancanti",
    "text": "54.3 La gestione dei dati mancanti\nIl passo successivo dopo la definizione dei meccanismi è quello della gestione dei dati mancanti. Sostanzialmente le scelte possibili sono due: l’eliminazione dei casi o la sostituzione dei dati mancanti. Un metodo semplice, indicato solo nel caso in cui l’ammontare dei dati mancanti è limitato e questi sono mancanti completamente a caso (MCAR), è quello di rimuovere i casi con dati mancanti (case deletion).\nCi sono due metodi per eliminare le osservazioni con valori mancanti: listwise deletion e pairwise deletion. Nel primo caso si elimina dal campione ogni osservazione che contiene dati mancanti. Le analisi avverranno quindi solo sui casi che hanno valori validi su tutte le variabili in esame. In questo modo si ottiene una maggiore semplicità di trattazione nell’analisi statistica, tuttavia non si utilizza tutta l’informazione osservata (si riduce la numerosità campionaria e, quindi, l’informazione). Il secondo metodo è la pairwise deletion, che utilizza tutti i casi che hanno i dati validi su due variabili volta per volta. In questo modo si riesce a massimizzare la numerosità del campione da utilizzare, ma si tratta comunque di un metodo che presenta dei problemi, per esempio il fatto che con questo approccio i parametri del modello saranno basati su differenti insiemi di dati, con differenti numerosità campionarie e differenti errori standard.\nQuando i dati non sono MNAR è opportuno sostituirli con appropriate funzioni dei dati effettivamente osservati. Questa procedura è chiamata imputazione (imputation). Di seguito sono indicati alcuni metodi.\n\nMean Imputation. Il dato mancante viene sostituito con la media della variabile. Questo metodo, utilizzato troppo spesso per la sua semplicità, riducendo la variabilità dei dati, ha effetti importanti su molte analisi dei dati e, in generale, dovrebbe essere evitato.\nRegression Imputation. Si tratta di un approccio basato sulle informazioni disponibili sulle altre variabili. Si stima una equazione di regressione lineare per ogni variabile utilizzando le altre variabili come predittori. Questo metodo offre il vantaggio di poter utilizzare i rapporti esistenti tra le variabili per effettuare le valutazioni dei dati mancanti; tuttavia esso è usato raramente, in quanto amplifica le correlazioni tra le variabili; quindi, se le analisi si basano su regressioni o modelli SEM, questo metodo è sconsigliato.\nMultiple Imputation. La tecnica di multiple imputation, applicabile in caso di MAR, prevede che un dato mancante su una variabile sia sostituito, sulla base dei dati esistenti sulle altre variabili, con un valore che però comprende anche una componente di errore ricavata dalla distribuzione dei residui della variabile.\nExpectation-Maximization. Un altro approccio moderno del trattamento dei dati mancanti è l’applicazione dell’algoritmo Expectation Maximization (EM). La tecnica è quella di stimare i parametri sulla base dei dati osservati, e di stimare poi i dati mancanti sulla base di questi parametri (fase E). Poi i parametri vengono nuovamente stimati sulla base della nuova matrice di dati (fase M), e così via. Questo processo viene iterato fino a quando i valori stimati convergono. Tuttavia, una limitazione fondamentale dell’utilizzo dell’algoritmo EM per calcolare le matrici di input per le analisi CFA/SEM è che gli errori standard risultanti delle stime dei parametri non sono consistenti. Pertanto, gli intervalli di confidenza e i test di significatività possono risultare compromessi.\n\n\n54.3.1 Metodo Direct ML\nBenché i metodi precedenti vengano spesso usati, nella pratica concreta è preferibile usare il metodo Direct ML, conosciuto anche come “raw ML” o “full information ML” (FIML), in quanto è generalmente considerano come il metodo migliore per gestire i dati mancanti nella maggior parte delle applicazioni CFA e SEM. Il metodo full information ML è esente dai problemi associati all’utilizzo dell’algoritmo EM e produce stime consistenti sotto l’ipotesi di normalità multivariata per dati mancanti MAR.\nIntuitivamente, l’approccio utilizza la relazione tra le variabili per dedurre quali siano i valori mancanti con maggiore probabilità. Ad esempio, se due variabili, \\(X\\) e \\(Y\\), sono correlate positivamente, allora se, per alcune osservazioni \\(i\\), \\(X_i\\) è il valore più alto nella variabile, è probabile che anche il valore mancante \\(Y_i\\) sia un valore alto. FIML utilizza queste informazioni senza procedere all’imputazione dei valori mancanti, ma invece basandosi sulle stime più verosimili dei parametri della popolazione, ovvero massimizzando direttamente la verosimiglianza del modello specificato. Sotto l’assunzione di normalità multivariata, la funzione di verosimiglianza diventa\n\\[\nL(\\mu, \\Sigma) = \\prod_i f(y_i \\mid \\mu_i, \\Sigma_i),\n\\]\ndove \\(y_i\\) sono i dati, \\(\\mu_i\\) e \\(\\Sigma_i\\) sono i parametri della popolazione se gli elementi mancanti in \\(y_i\\) vengono rimossi. Si cercano i valori \\(\\mu\\) e \\(\\Sigma\\) che massimizzano la verosimiglianza.\nIn lavaan l’applicazione di tale metodo si ottiene specificando l’argomento missing = \"ml\".\n\n\n54.3.2 Un esempio concreto\nPer applicare il metodo direct ML, {cite:t}brown2015confirmatory prende in esame i dati reali di un questionario (un singolo fattore, quattro item, una covarianza di errore) caratterizzato dalla presenza di dati mancanti. Importiamo i dati in R:\n\nd &lt;- rio::import(here::here(\"data\", \"brown_table_9_1.csv\"))\nhead(d)\n\n\nA data.frame: 6 x 5\n\n\n\nsubject\ns1\ns2\ns3\ns4\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n5760\n2\n0\n1\nNA\n\n\n2\n5761\n3\n3\n3\nNA\n\n\n3\n5763\n2\n4\n4\nNA\n\n\n4\n5761\n2\n0\n0\nNA\n\n\n5\n5769\n2\n1\n1\nNA\n\n\n6\n5771\n4\n3\n3\nNA\n\n\n\n\n\nAbbiamo 650 osservazioni:\n\ndim(d)\n\n\n6505\n\n\nLe frequenze di dati mancanti vengono ottentute mediante la funzione summary()\n\nsummary(d)\n\n    subject           s1              s2              s3              s4       \n Min.   :5756   Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   :0.000  \n 1st Qu.:5934   1st Qu.:2.000   1st Qu.:2.000   1st Qu.:1.000   1st Qu.:2.000  \n Median :6102   Median :3.000   Median :3.000   Median :2.000   Median :3.000  \n Mean   :6104   Mean   :2.926   Mean   :2.563   Mean   :2.208   Mean   :2.404  \n 3rd Qu.:6275   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:3.000  \n Max.   :6451   Max.   :4.000   Max.   :4.000   Max.   :4.000   Max.   :4.000  \n                NA's   :25      NA's   :25      NA's   :25      NA's   :190    \n\n\nIl modello viene specificato come segue {cite:p}brown2015confirmatory:\n\nmodel &lt;- '\n  esteem =~ s1 + s2 + s3 + s4\n  s2 ~~ s4\n'\n\nAdattiamo il modello ai dati specificanto l’utilizzo del metodo full information ML per la gestione dei dati mancanti:\n\nfit &lt;- cfa(model, data = d, missing = \"fiml\")\n\nÈ possibile identificare le configurazioni di risposte agli item che contengono dati mancanti:\n\nfit@Data@Mp[[1]]$npatterns\n\n5\n\n\n\npats &lt;- fit@Data@Mp[[1]]$pat * 1L\ncolnames(pats) &lt;- fit@Data@ov.names[[1]]\nprint(pats)\n\n     s1 s2 s3 s4\n[1,]  1  1  1  1\n[2,]  1  1  1  0\n[3,]  0  1  1  1\n[4,]  1  0  1  1\n[5,]  1  1  0  1\n\n\nPossiamo esaminare la proporzione di dati disponibili per ciascun indicatore e per ciascuna coppia di indicatori:\n\ncoverage &lt;- fit@Data@Mp[[1]]$coverage\ncolnames(coverage) &lt;- rownames(coverage) &lt;- fit@Data@ov.names[[1]]\nprint(coverage)\n\n          s1        s2        s3        s4\ns1 0.9615385 0.9230769 0.9230769 0.6692308\ns2 0.9230769 0.9615385 0.9230769 0.6692308\ns3 0.9230769 0.9230769 0.9615385 0.6692308\ns4 0.6692308 0.6692308 0.6692308 0.7076923\n\n\nAd esempio, consideriamo l’item s1; se moltiplichiamo la copertura di questo elemento per la numerosità campionaria possiamo concludere che questa variabile contiene 25 osservazioni mancanti; e così via.\n\n650 * 0.9615385\n\n625.000025\n\n\nProcediamo poi come sempre per esaminare la soluzione ottenuta.\n\neffectsize::interpret(fit)\n\n\nA data.frame: 10 x 4\n\n\nName\nValue\nThreshold\nInterpretation\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;effctsz_&gt;\n\n\n\n\nGFI\n0.999449432\n0.95\nsatisfactory\n\n\nAGFI\n0.992292047\n0.90\nsatisfactory\n\n\nNFI\n0.999192581\n0.90\nsatisfactory\n\n\nNNFI\n0.998977535\n0.90\nsatisfactory\n\n\nCFI\n0.999829589\n0.90\nsatisfactory\n\n\nRMSEA\n0.020237880\n0.05\nsatisfactory\n\n\nSRMR\n0.004853126\n0.08\nsatisfactory\n\n\nRFI\n0.995155487\n0.90\nsatisfactory\n\n\nPNFI\n0.166532097\n0.50\npoor\n\n\nIFI\n0.999830133\n0.90\nsatisfactory\n\n\n\n\n\n\nstandardizedSolution(fit)\n\n\nA lavaan.data.frame: 15 x 9\n\n\nlhs\nop\nrhs\nest.std\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nesteem\n=~\ns1\n0.7374475\n0.01988465\n37.086269\n0.000000e+00\n0.6984743\n0.7764207\n\n\nesteem\n=~\ns2\n0.9204499\n0.01340775\n68.650574\n0.000000e+00\n0.8941712\n0.9467286\n\n\nesteem\n=~\ns3\n0.8804116\n0.01325286\n66.431820\n0.000000e+00\n0.8544365\n0.9063867\n\n\nesteem\n=~\ns4\n0.9045975\n0.01632859\n55.399610\n0.000000e+00\n0.8725941\n0.9366010\n\n\ns2\n~~\ns4\n-0.8859917\n0.21560986\n-4.109236\n3.969709e-05\n-1.3085793\n-0.4634042\n\n\ns1\n~~\ns1\n0.4561711\n0.02932777\n15.554237\n0.000000e+00\n0.3986898\n0.5136525\n\n\ns2\n~~\ns2\n0.1527720\n0.02468233\n6.189531\n6.034349e-10\n0.1043956\n0.2011485\n\n\ns3\n~~\ns3\n0.2248754\n0.02333594\n9.636441\n0.000000e+00\n0.1791378\n0.2706130\n\n\ns4\n~~\ns4\n0.1817033\n0.02954160\n6.150760\n7.711243e-10\n0.1238028\n0.2396038\n\n\nesteem\n~~\nesteem\n1.0000000\n0.00000000\nNA\nNA\n1.0000000\n1.0000000\n\n\ns1\n~1\n\n2.3753651\n0.07760125\n30.609882\n0.000000e+00\n2.2232695\n2.5274608\n\n\ns2\n~1\n\n1.8809244\n0.06578470\n28.592125\n0.000000e+00\n1.7519887\n2.0098600\n\n\ns3\n~1\n\n1.5838240\n0.05914055\n26.780676\n0.000000e+00\n1.4679106\n1.6997373\n\n\ns4\n~1\n\n1.8496616\n0.07101084\n26.047595\n0.000000e+00\n1.7104829\n1.9888403\n\n\nesteem\n~1\n\n0.0000000\n0.00000000\nNA\nNA\n0.0000000\n0.0000000",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Dati mancanti</span>"
    ]
  },
  {
    "objectID": "chapters/sem/10_missing_data.html#dati-mancanti-in-r",
    "href": "chapters/sem/10_missing_data.html#dati-mancanti-in-r",
    "title": "54  Dati mancanti",
    "section": "54.4 Dati mancanti in R",
    "text": "54.4 Dati mancanti in R\nConcludiamo il capitolo con qualche breve accenno alla gestione dei dati mancanti in R.\nIn R, i valori mancanti vengono indicati dal codice NA, che significa not available — non disponibile.\nSe una variabile contiene valori mancanti, R non è in grado di applicare ad essa alcune funzioni, come ad esempio la media. Per questa ragione, la gran parte delle funzioni di R prevedono modi specifici per trattare i valori mancanti.\nCi sono diversi tipi di dati “mancanti” in R;\n\nNA - generico dato mancante;\nNaN - il codice NaN (Not a Number) indica i valori numerici impossibili, quali ad esempio un valore 0/0;\nInf e -Inf - Infinity, si verifca, ad esempio, quando si divide un numero per 0.\n\nLa funzione is.na() ritorna un output che indica con TRUE le celle che contengono NA o NaN.\nSi noti che\n\nse is.na(x) è TRUE, allora !is.na(x) è FALSE;\nall(!is.na(x)) ritorna TRUE se tutti i valori x sono NOT NA;\nany(is.na(x)) risponde alla domanda: c’è qualche valore NA (almeno uno) in x?;\ncomplete.cases(x) ritorna TRUE se ciascun elemento di x è is NOT NA; ritorna FALSE se almeno un elemento di x è NA;\n\nLe funzioni R is.nan() e is.infinite() si applicano ai tipi di dati NaN e Inf.\nPer esempio, consideriamo il seguente data.frame:\n\nd &lt;- tibble(\n  w = c(1, 2, NA, 3, NA), \n  x = 1:5, \n  y = 1, \n  z = x ^ 2 + y,\n  q = c(3, NA, 5, 1, 4)\n)\nd\n\n\nA tibble: 5 x 5\n\n\nw\nx\ny\nz\nq\n\n\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1\n1\n2\n3\n\n\n2\n2\n1\n5\nNA\n\n\nNA\n3\n1\n10\n5\n\n\n3\n4\n1\n17\n1\n\n\nNA\n5\n1\n26\n4\n\n\n\n\n\n\nis.na(d$w)\nis.na(d$x)\n\n\nFALSEFALSETRUEFALSETRUE\n\n\n\nFALSEFALSEFALSEFALSEFALSE\n\n\nPer creare un nuovo Dataframe senza valori mancanti:\n\nd_clean &lt;- d[complete.cases(d), ]\nd_clean\n\n\nA tibble: 2 x 5\n\n\nw\nx\ny\nz\nq\n\n\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1\n1\n2\n3\n\n\n3\n4\n1\n17\n1\n\n\n\n\n\nOppure, se vogliamo eliminare le righe con NA solo in una variabile:\n\nd1 &lt;- d[!is.na(d$q), ]\nd1\n\n\nA tibble: 4 x 5\n\n\nw\nx\ny\nz\nq\n\n\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1\n1\n2\n3\n\n\nNA\n3\n1\n10\n5\n\n\n3\n4\n1\n17\n1\n\n\nNA\n5\n1\n26\n4\n\n\n\n\n\nSe vogliamo esaminare le righe con i dati mancanti in qualunque colonna:\n\nd_na &lt;- d[!complete.cases(d), ]\nd_na\n\n\nA tibble: 3 x 5\n\n\nw\nx\ny\nz\nq\n\n\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n2\n2\n1\n5\nNA\n\n\nNA\n3\n1\n10\n5\n\n\nNA\n5\n1\n26\n4",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Dati mancanti</span>"
    ]
  },
  {
    "objectID": "chapters/sem/10_missing_data.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/10_missing_data.html#informazioni-sullambiente-di-sviluppo",
    "title": "54  Dati mancanti",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] rio_1.2.2         ggokabeito_0.1.0  viridis_0.6.5     viridisLite_0.4.2\n [5] ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0  \n [9] bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-18    \n[13] psych_2.4.6.26    scales_1.3.0      markdown_1.13     knitr_1.48       \n[17] lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n[21] purrr_1.0.2       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n[25] ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.16.0  jsonlite_1.8.9     datawizard_0.12.3 \n  [4] magrittr_2.0.3     TH.data_1.1-2      estimability_1.5.1\n  [7] farver_2.1.2       nloptr_2.1.1       rmarkdown_2.28    \n [10] vctrs_0.6.5        minqa_1.2.8        effectsize_0.8.9  \n [13] base64enc_0.1-3    rstatix_0.7.2      htmltools_0.5.8.1 \n [16] broom_1.0.6        Formula_1.2-5      htmlwidgets_1.6.4 \n [19] plyr_1.8.9         sandwich_3.1-1     emmeans_1.10.4    \n [22] zoo_1.8-12         uuid_1.2-1         igraph_2.0.3      \n [25] mime_0.12          lifecycle_1.0.4    pkgconfig_2.0.3   \n [28] Matrix_1.7-0       R6_2.5.1           fastmap_1.2.0     \n [31] shiny_1.9.1        digest_0.6.37      OpenMx_2.21.12    \n [34] fdrtool_1.2.18     colorspace_2.1-1   rprojroot_2.0.4   \n [37] Hmisc_5.1-3        fansi_1.0.6        timechange_0.3.0  \n [40] abind_1.4-8        compiler_4.4.1     withr_3.0.1       \n [43] glasso_1.11        htmlTable_2.4.3    backports_1.5.0   \n [46] carData_3.0-5      performance_0.12.3 R.utils_2.12.3    \n [49] ggsignif_0.6.4     MASS_7.3-61        corpcor_1.6.10    \n [52] gtools_3.9.5       tools_4.4.1        pbivnorm_0.6.0    \n [55] foreign_0.8-87     zip_2.3.1          httpuv_1.6.15     \n [58] nnet_7.3-19        R.oo_1.26.0        glue_1.7.0        \n [61] quadprog_1.5-8     promises_1.3.0     nlme_3.1-166      \n [64] lisrelToR_0.3      grid_4.4.1         pbdZMQ_0.3-13     \n [67] checkmate_2.3.2    cluster_2.1.6      reshape2_1.4.4    \n [70] generics_0.1.3     gtable_0.3.5       tzdb_0.4.0        \n [73] R.methodsS3_1.8.2  data.table_1.16.0  hms_1.1.3         \n [76] car_3.1-2          utf8_1.2.4         sem_3.1-16        \n [79] pillar_1.9.0       IRdisplay_1.1      rockchalk_1.8.157 \n [82] later_1.3.2        splines_4.4.1      lattice_0.22-6    \n [85] survival_3.7-0     kutils_1.73        tidyselect_1.2.1  \n [88] miniUI_0.1.1.1     pbapply_1.7-2      stats4_4.4.1      \n [91] xfun_0.47          qgraph_1.9.8       arm_1.14-4        \n [94] stringi_1.8.4      pacman_0.5.1       boot_1.3-31       \n [97] evaluate_1.0.0     codetools_0.2-20   mi_1.1            \n[100] cli_3.6.3          RcppParallel_5.1.9 IRkernel_1.3.2    \n[103] rpart_4.1.23       parameters_0.22.2  xtable_1.8-4      \n[106] repr_1.1.7         munsell_0.5.1      Rcpp_1.0.13       \n[109] coda_0.19-4.1      png_0.1-8          XML_3.99-0.17     \n[112] parallel_4.4.1     bayestestR_0.14.0  jpeg_0.1-10       \n[115] lme4_1.1-35.5      mvtnorm_1.3-1      insight_0.20.4    \n[118] openxlsx_4.2.7.1   crayon_1.5.3       rlang_1.1.4       \n[121] multcomp_1.4-26    mnormt_2.1.1",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Dati mancanti</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html",
    "href": "chapters/sem/11_small_samples.html",
    "title": "55  Modellizzazione SEM in Piccoli Campioni",
    "section": "",
    "text": "55.1 Introduzione\nQuesto capitolo si concentra sull’applicazione dei modelli SEM in contesti caratterizzati da campioni di piccole dimensioni e sintetizza l’esposizione di Rosseel (2020) su questo argomento, mentre si avvale di esempi numerici tratti dai principi di Kline (2023).\nÈ risaputo che la maggior parte dei metodi di stima e inferenza in SEM si basa su presupposti asintotici, presupponendo la presenza di campioni casuali di grandi dimensioni. Tuttavia, nei casi di campioni più limitati, come quelli con N &lt; 200, emergono specifiche problematiche: i metodi iterativi possono non convergere, si possono verificare soluzioni non valide a causa dei casi di Heywood o di altri risultati anomali difficili da interpretare, e le stime dei parametri possono risultare fortemente distorte.\nInnanzitutto, i modelli strutturali possono diventare molto complessi, coinvolgendo numerose variabili (sia osservate che latenti), rendendo necessaria l’analisi di numerosi parametri e richiedendo un adeguato volume di dati per ottenere stime accurate. Inoltre, il framework statistico alla base della SEM tradizionale si fonda sulla teoria dei grandi campioni, suggerendo che una buona precisione nelle stime dei parametri e nell’inferenza sia garantita solo con campioni di dimensioni considerevoli. Alcuni studi di simulazione hanno addirittura suggerito che dimensioni del campione enormi siano necessarie per risultati affidabili, sebbene tali conclusioni siano rilevanti solo in specifici contesti e abbiano contribuito alla convinzione generalizzata che la SEM sia applicabile solo con campioni di dimensioni considerevoli (ad es. n &gt; 500) o addirittura molto grandi (n &gt; 2000).\nTuttavia, la realtà delle dimensioni ridotte dei campioni è una situazione comune per molte ragioni. In tali casi, molti ricercatori esitano ad utilizzare la SEM e si affidano a metodologie subottimali, come l’analisi di regressione o l’analisi di percorso basate su punteggi sommati. Tuttavia, è importante notare che il bias associato alle dimensioni ridotte del campione può essere ancora più accentuato in tecniche come la regressione multipla o l’analisi di percorso con variabili manifeste, soprattutto in assenza di considerazioni sull’errore di misurazione. Una strategia più efficace potrebbe essere quella di adottare l’approccio della SEM, pur cercando soluzioni per affrontare le sfide poste dalle dimensioni ridotte del campione.\nQuesto capitolo si propone di esplorare diverse strategie per superare tali sfide nell’utilizzo della SEM con campioni di piccole dimensioni. Sarà organizzato in tre sezioni: innanzitutto, verranno esaminate le problematiche comuni associate alle dimensioni ridotte del campione nella SEM. Successivamente, saranno presentati quattro approcci alternativi di stima che possono essere impiegati quando le dimensioni del campione sono limitate, anziché ricorrere alla SEM tradizionale. Infine, saranno discussi alcuni possibili correttivi per le statistiche di test e gli errori standard nelle situazioni di piccoli campioni. L’efficacia di alcune di queste tecniche sarà illustrata tramite l’analisi di un modello di fattore comune applicato a un campione di dimensioni ridotte.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html#problemi-con-le-piccole-dimensioni-del-campione-nella-sem",
    "href": "chapters/sem/11_small_samples.html#problemi-con-le-piccole-dimensioni-del-campione-nella-sem",
    "title": "55  Modellizzazione SEM in Piccoli Campioni",
    "section": "55.2 Problemi con le Piccole Dimensioni del Campione nella SEM",
    "text": "55.2 Problemi con le Piccole Dimensioni del Campione nella SEM\nConsideriamo un modello SEM con almeno 4 indicatori continui per ciascuna variabile latente. Se tutte le variabili osservate sono continue, lo stimatore di default nella maggior parte (se non in tutti) dei pacchetti software SEM è il metodo della massima verosimiglianza. Generalmente, lo stimatore della massima verosimiglianza è una buona scelta perché presenta molte proprietà statistiche desiderabili. Inoltre, l’approccio della massima verosimiglianza può essere adattato per gestire dati mancanti (sotto l’assunzione che i dati siano mancanti casualmente) e sono stati sviluppati errori standard e statistiche di test “robusti” per trattare dati non normali e modelli mal specificati.\nTuttavia, se la dimensione del campione è relativamente piccola (ad esempio, n &lt; 200), possono sorgere diversi problemi. Innanzitutto, il modello potrebbe non convergere, il che significa che l’ottimizzatore (l’algoritmo che cerca di trovare i valori dei parametri del modello che massimizzano la verosimiglianza dei dati) non è riuscito a trovare una soluzione che soddisfi uno o più criteri di convergenza. In rare occasioni, l’ottimizzatore potrebbe semplicemente sbagliare. In questo caso, modificare i criteri di convergenza, passare a un altro algoritmo di ottimizzazione o fornire valori iniziali migliori potrebbe risolvere il problema. Ma se la dimensione del campione è piccola, potrebbe benissimo essere che il set di dati non contenga informazioni sufficienti per trovare una soluzione unica per il modello.\nUn secondo problema potrebbe essere che il modello ottenga la convergenza ma produca una soluzione non ammissibile. Ciò significa che alcuni parametri assumono valori inamissibili. L’esempio più comune è una varianza negativa. Un altro esempio è un valore di correlazione che supera 1 (in valore assoluto). È importante rendersi conto che alcuni approcci di stima (sia frequentisti che bayesiani) potrebbero, per progettazione, non produrre mai soluzioni fuori gamma. Sebbene ciò possa sembrare una caratteristica desiderabile, maschera potenziali problemi con il modello o i dati. È importante che gli utenti notino varianze negative (o altri parametri fuori gamma). Le varianze negative sono spesso innocue, ma possono essere un sintomo di una cattiva specificazione strutturale.\nUn terzo problema riguarda il fatto che la massima verosimiglianza è una tecnica per grandi campioni. Questo implica che lavorare con piccole dimensioni del campione può portare a stime puntuali distorte, errori standard troppo piccoli, intervalli di confidenza non sufficientemente ampi e p-valori per test di ipotesi non affidabili.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html#soluzioni-possibili-per-la-stima-dei-parametri",
    "href": "chapters/sem/11_small_samples.html#soluzioni-possibili-per-la-stima-dei-parametri",
    "title": "55  Modellizzazione SEM in Piccoli Campioni",
    "section": "55.3 Soluzioni Possibili per la Stima dei Parametri",
    "text": "55.3 Soluzioni Possibili per la Stima dei Parametri\nIn questa sezione, consideriamo brevemente quattro approcci alternativi per stimare i parametri in un contesto SEM con dimensioni di campione piccole. Ci limitiamo ai metodi frequentisti e alle soluzioni disponibili in software gratuiti e open-source.\n\n55.3.1 Stima di Verosimiglianza Penalizzata\nI metodi di stima di verosimiglianza penalizzata (o metodi di regolarizzazione) sono stati sviluppati nella letteratura del machine learning statistico e sono particolarmente utili quando la dimensione del campione è piccola rispetto al numero di variabili nel modello. Questi metodi sono simili ai metodi di verosimiglianza ordinari (come la massima verosimiglianza) ma includono un termine di penalità aggiuntivo per controllare la complessità del modello. Il termine di penalità può essere formulato per incorporare conoscenze pregresse sui parametri o per scoraggiare valori dei parametri meno realistici (ad esempio, lontani da zero). Due termini di penalità popolari sono la penalità ridge e la penalità lasso (least absolute shrinkage and selection operator).\nPer illustrare come funziona questa penalizzazione, immaginate un modello di regressione univariata con un gran numero di predittori. Senza penalizzazione, tutti i coefficienti di regressione sono calcolati nel modo usuale. Tuttavia, il termine di penalità ridge ridurrà tutti i coefficienti verso zero, mentre la penalità lasso ridurrà ulteriormente i piccoli coefficienti fino a zero. In quest’ultimo approccio, sopravvivono solo i predittori “forti” (per i quali c’è un forte supporto nei dati), mentre i predittori “deboli” che possono essere difficilmente distinti dal rumore vengono eliminati. In generale, l’aggiunta di termini di penalità porta a modelli meno complessi, il che è particolarmente vantaggioso se la dimensione del campione è piccola.\nSebbene queste approcci di penalizzazione siano esistiti da alcuni decenni, sono stati applicati solo di recente alla SEM. Due esempi nel software R sono il pacchetto regsem (Jacobucci, Grimm, Brandmaier, Serang e Kievit, 2018) e il pacchetto lslx (Huang e Hu, 2018).\nUno svantaggio di questi metodi di penalizzazione è che l’utente deve indicare quali parametri richiedono la penalizzazione e in che misura. In un’analisi esplorativa, può essere utile e persino vantaggioso penalizzare i parametri verso zero se nel dati non si trova un forte supporto per essi. Tuttavia, la SEM è di solito un approccio confermativo, e l’utente deve assicurarsi che tutti i parametri inizialmente postulati nel modello non vengano rimossi dalla penalizzazione.\n\n\n55.3.2 Model-implied instrumental variables\nBollen (1996) ha proposto un approccio alternativo di stima per i modelli SEM basato sull’utilizzo di variabili strumentali implicite nel modello in combinazione con il metodo dei minimi quadrati a due stadi (MIIV-2SLS). In questo approccio, il modello viene tradotto in un insieme di equazioni (di regressione). Successivamente, ogni variabile latente in queste equazioni viene sostituita con il suo indicatore principale (solitamente il primo indicatore, dove il carico fattoriale è fissato a uno e l’intercetta a zero) meno il suo termine di errore residuo. Le equazioni risultanti non contengono più variabili latenti ma hanno una struttura dell’errore più complessa. È importante notare che la stima dei minimi quadrati ordinari non è più adatta per risolvere queste equazioni poiché alcuni predittori sono ora correlati con il termine di errore nell’equazione. Qui entrano in gioco le variabili strumentali (anche chiamate strumenti). Per ogni equazione, è necessario trovare un insieme di variabili strumentali. Una variabile strumentale deve essere non correlata con il termine di errore dell’equazione ma fortemente correlata con il predittore problematico. Di solito, le variabili strumentali sono ricercate al di fuori del modello, ma nell’approccio di Bollen, le variabili strumentali sono selezionate tra le variabili osservate che fanno parte del modello. Sono stati sviluppati diversi procedimenti automatizzati per trovare queste variabili strumentali all’interno del modello. Una volta selezionati gli strumenti, è necessaria una procedura di stima per stimare tutti i coefficienti delle equazioni, come il metodo dei minimi quadrati a due stadi (2SLS).\nUna motivazione principale per MIIV-2SLS è che è robusto: non si basa sulla normalità ed è meno probabile che diffonda il bias (che può derivare da errate specificazioni strutturali) in una parte del modello ad altre parti del modello. Un’altra caratteristica attraente di MIIV-2SLS è che non è iterativo. Ciò significa che non possono esserci problemi di convergenza e MIIV-2SLS può fornire una soluzione ragionevole per modelli in cui il massimo verosimigliante fallisce nella convergenza.\nSono necessarie ulteriori ricerche per valutare le prestazioni di questo stimatore in contesti in cui la dimensione del campione è (molto) piccola. L’approccio MIIV-2SLS è disponibile nel pacchetto R MIIVsem (Fisher, Bollen, Gates, & Rönkkö, 2017).\n\n\n55.3.3 Stima a Due Fasi\nNel metodo di stima a due fasi, si effettua una distinzione tra la parte di misurazione e la parte strutturale (di regressione) del modello, e la stima avviene in due passaggi distinti. Nel primo passo, vengono adattati uno per uno tutti i modelli di misurazione. Nel secondo passo, viene adattato il modello completo, inclusa la parte strutturale, ma i parametri dei modelli di misurazione vengono mantenuti fissi ai valori trovati nel primo passo. La principale motivazione per l’approccio a due fasi è quella di separare il modello (o i modelli) di misurazione dalla parte strutturale durante la stima in modo che non possano influenzarsi reciprocamente. Nel tradizionale framework della massima verosimiglianza, invece, tutti i parametri vengono adattati simultaneamente. Di conseguenza, errori nella specificazione del modello strutturale possono influenzare i pesi fattoriali stimati di uno o più modelli di misurazione, e ciò può causare problemi di interpretazione per le variabili latenti.\nL’approccio a due fasi è stato recentemente implementato nel pacchetto R lavaan (Rosseel, 2012).\n\n\n55.3.4 Regressione del Punteggio dei Fattori\nL’idea fondamentale della regressione del punteggio dei fattori è quella di sostituire tutte le variabili latenti con i loro punteggi. Questo processo è simile al metodo in due fasi, dove ciascun modello di misurazione viene adattato individualmente. Successivamente, si calcolano i punteggi dei fattori per tutte le variabili latenti nel modo consueto. Una volta che le variabili latenti sono sostituite dai loro punteggi, tutte le variabili diventano osservabili. In un passaggio finale, si stima la parte strutturale del modello. Questa stima può consistere in un’analisi di regressione o in un’analisi dei percorsi. Il termine “regressione del punteggio dei fattori” si riferisce a entrambi gli scenari.\nSe usata in modo ingenuo, questa regressione potrebbe portare a un notevole bias nelle stime dei parametri della parte strutturale, anche con campioni di grandi dimensioni. Questo si verifica perché i punteggi dei fattori vengono trattati come se fossero osservati senza errore di misurazione. Esistono però diversi metodi per correggere questo bias. Ad esempio, il metodo di Croon (2002) procede come segue: prima, si calcola la matrice di varianza-covarianza dei punteggi dei fattori. Poi, sulla base delle informazioni dei modelli di misurazione, gli elementi di questa matrice vengono corretti per approssimare le varianze e covarianze implicite dal modello delle variabili latenti. Questa matrice di varianza-covarianza corretta diventa poi l’input per un’analisi di regressione o dei percorsi regolare.\nSimile al metodo in due fasi, la regressione del punteggio dei fattori (combinata con la correzione di Croon) può essere un’alternativa utile per modelli piuttosto grandi in combinazione con una dimensione campionaria relativamente piccola. Inoltre, è possibile adattare i modelli di misurazione utilizzando un stimatore non iterativo, evitando problemi di convergenza. Tuttavia, la correzione di Croon può produrre una matrice di varianza-covarianza (per le variabili appartenenti alla parte strutturale) che non è definita positiva, specialmente se l’errore di misurazione è sostanziale. Pertanto, la correzione di Croon non è esente da problemi di stima. In questo caso, l’unica soluzione potrebbe essere quella di creare un punteggio somma per ogni variabile latente e stimare un modello in cui ogni variabile latente ha un unico indicatore (il punteggio somma) con la sua affidabilità fissata a un valore realistico fornito dall’utente.\n\n\n55.3.5 Discussione\nTutti i metodi descritti in questa sezione hanno vantaggi e svantaggi. L’approccio della verosimiglianza penalizzata è forse l’unico metodo specificamente progettato per gestire campioni (molto) piccoli. Gli altri tre metodi utilizzano un approccio di “divide et impera”; scompongono il modello completo in parti più piccole e stimano i parametri di ciascuna parte in successione. Oltre a ridurre la complessità e a essere meno vulnerabili a problemi di convergenza, gli ultimi tre metodi hanno il vantaggio di essere efficaci nel localizzare le parti problematiche all’interno di un modello ampio.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html#inferenze-per-modelli-sem-in-piccoli-campioni",
    "href": "chapters/sem/11_small_samples.html#inferenze-per-modelli-sem-in-piccoli-campioni",
    "title": "55  Modellizzazione SEM in Piccoli Campioni",
    "section": "55.4 Inferenze per Modelli SEM in Piccoli Campioni",
    "text": "55.4 Inferenze per Modelli SEM in Piccoli Campioni\nMolti autori hanno documentato che quando la dimensione del campione è piccola, il test del chi-quadrato porta ad un inflezione degli errori di Tipo I anche nelle circostanze ideali (cioè, modello correttamente specificato, dati normali). Allo stesso modo, gli errori standard sono spesso attenuati (troppo piccoli) e gli intervalli di confidenza non sono sufficientemente ampi.\nNelle due sottosezioni successive, {cite:t}rosseel2020small discute brevemente alcuni tentativi per affrontare questi problemi di inferenza su campioni piccoli nella modellizzazione SEM.\n\n55.4.1 Migliorare la statistica test del chi-quadrato\nSono state suggerite diverse correzioni per migliorare le prestazioni della statistica del test del chi-quadrato, come la correzione di Bartlett. I risultati di studi di simulazione su questo tema, però, non sono coerenti e, secondo {cite:t}rosseel2020small, per valutare i modelli quando la dimensione del campione è piccola, potrebbe essere opportuno abbandonare del tutto il test del chi-quadrato e esplorare approcci alternativi.\nUn approccio è quello di considerare gli intervalli di confidenza e i test di aderenza basati sull’indice SRMR (standardized root mean square residuals; Maydeu-Olivares, Shi, & Rosseel, 2018). Questi test sembrano funzionare bene anche quando n = 100 (la dimensione del campione più piccola considerata in Maydeu-Olivares et al., 2018) e il modello non è troppo grande. Questi test sono stati implementati come parte della funzione lavResiduals() del pacchetto lavaan.\n\n\n55.4.2 Una migliore stima degli Errori Standard e degli Intervalli di Confidenza\nIn generale, è ben noto che se si utilizza la teoria dei grandi campioni per costruire espressioni analitiche al fine di calcolare gli errori standard, questi possono avere prestazioni scadenti in campioni di piccole dimensioni.\nQuando le assunzioni alla base degli errori standard analitici non sono soddisfatte, spesso si suggerisce di utilizzare un approccio di resampling. Un metodo popolare è il bootstrap (Efron & Tibshirani, 1993): viene generato un campione bootstrap (o campione di replica) e si stima un nuovo set di parametri per questo campione bootstrap. Questo processo viene ripetuto un gran numero di volte (ad esempio, 1.000), e la deviazione standard di un parametro su tutti i campioni bootstrap replicati viene utilizzata come stima dell’errore standard per quel parametro. Purtroppo, nonostante molti altri vantaggi, sembra che il bootstrap non sia una soluzione affidabile quando la dimensione del campione è (molto) piccola (Yung & Bentler, 1996).\nIn alternativa, sono state sviluppate correzioni per dimensioni di campione piccole sugli errori standard (robusti) e sono state recentemente adattate al contesto SEM. Tuttavia, questa tecnologia non è ancora disponibile nei software SEM.\n\n\n55.4.3 Strategie Alternative\nOltre ai metodi precedenti suggeriti da {cite:t}rosseel2020small, altre strategie possibili sono stati indicate da {cite:t}kline2023principles.\n\nSelezione di Indicatori con Elevate Caratteristiche Psicometriche: {cite:t}kline2023principles consiglia di utilizzare indicatori che mostrino eccellenti proprietà psicometriche, idealmente con carichi standardizzati superiori a .70 per gli indicatori continui. Questa pratica riduce il rischio di incorrere in casi di Heywood.\nApplicazione di Restrizioni di Uguaglianza sui Carichi Non Standardizzati: Imponendo vincoli di uguaglianza sulle saturazioni degli indicatori relativi allo stesso fattore si possono evitare soluzioni inammissibili. Questo approccio è particolarmente valido quando gli indicatori sono sulla stessa scala. Un’alternativa valida consiste nel fissare le saturazioni degli indicatori dello stesso fattore a valori costanti non nulli, che riflettano le variazioni nelle loro deviazioni standard.\nSEM Basato su Compositi per Modelli Complessi in Campioni Piccoli: Questo approccio implica l’utilizzo di metodi basati su compositi, che sono combinazioni lineari di variabili osservate. In pratica, le variabili osservate vengono combinate in modi specifici per formare indicatori composti che rappresentano i costrutti latenti nel modello. Questi indicatori composti vengono quindi utilizzati per stimare i parametri del modello SEM. L’obiettivo è ottenere risultati per modelli complessi che richiederebbero campioni molto più ampi con il tradizionale approccio SEM. Tuttavia, è importante notare che i risultati del SEM basato su compositi possono essere soggetti a distorsioni anche nei campioni di piccole dimensioni.\nParceling: Questa strategia coinvolge la creazione di “parcel” o aggregati di due o più indicatori a livello di item attraverso la media dei singoli item. In altre parole, gli indicatori originali sono raggruppati in insiemi più piccoli e i loro valori sono combinati per creare nuovi indicatori aggregati. Questi nuovi indicatori vengono quindi utilizzati per rappresentare i costrutti latenti nel modello SEM. Sebbene il parceling possa ridurre la varianza dell’errore e migliorare la stabilità dei modelli, è importante considerare le sue limitazioni, tra cui la possibile perdita di informazioni e la sensibilità alle scelte fatte durante il processo di parceling. I risultati ottenuti con il parceling possono variare notevolmente in base alle decisioni prese dal ricercatore durante l’analisi.\n\n\n55.4.3.1 Parceling\nApprofondiamo brevemente la strategia del parceling. Il parceling è una strategia che comporta la suddivisione di un set di item in gruppi più piccoli, o “parcel”, per semplificare i modelli e migliorare la loro stima e adattamento.\nUn esempio, citato in {cite:t}kline2023principles, illustra come il parceling possa essere utilizzato in una CFA per ridurre il numero di indicatori e semplificare il modello. {cite:t}kline2023principles considera un questionario di 120 item diviso in tre gruppi distinti di 40 item ciascuno, ognuno mirato a misurare un dominio specifico di un costrutto. In un campione di 150 partecipanti, un’analisi fattoriale confermativa (CFA) con tre fattori e 40 indicatori per fattore, con 120 indicatori totali, può presentare sfide notevoli nella stima del modello a causa della ridotta dimensione del campione. Per affrontare questi problemi, il ricercatore può suddividere ogni gruppo di 40 item in 4 gruppi minori (o “parcel”) di 10 item ciascuno, sommando i punteggi all’interno di ogni “parcel”. Questi punteggi aggregati sostituiscono poi gli item singoli come indicatori in un modello CFA a 3 fattori che avrà quindi solo 12 indicatori in totale (4 indicatori parcellizzati per fattore). Se gli indicatori parcellizzati hanno una distribuzione normale, per la stima si può ricorrere al metodo dei minimi quadrati (ML); altrimenti, si può utilizzare un estimatore ML robusto.\nQuesto metodo è particolarmente utile in situazioni dove si hanno molti item e campioni di dimensioni ridotte, e offre diversi benefici, tra cui:\n\nMaggiore Affidabilità: Il parceling può aumentare l’affidabilità di una scala psicometrica, poiché gli item aggregati tendono ad avere maggior coerenza interna rispetto agli item singoli.\nRapporto Varianza Comune/Varianza Unica: Utilizzando il parceling, si può ottenere un rapporto più favorevole tra la varianza comune (quella spiegata dai fattori comuni) e la varianza unica (quella non spiegata).\nMinore Probabilità di Violazioni Distribuzionali: La pratica del parceling riduce la probabilità che le assunzioni distribuzionali siano violate, il che è importante per l’applicazione di certe tecniche statistiche.\n\nNonostante i suoi benefici, il parceling ha anche limitazioni. La metodologia utilizzata per formare i parcel può influenzare i risultati. Inoltre, il parceling non è consigliabile quando gli item all’interno di un parcel non sono unidimensionali, poiché ciò può distorcere i risultati. È fondamentale verificare l’unidimensionalità prima di procedere con il parceling.\nIn studi con campioni di piccole dimensioni, il parceling ha dimostrato diversi vantaggi, come mostrato nello studio di simulazione di Orçan e Yanyun (2016). Questi includono una riduzione della complessità del modello, tassi di errore di Tipo I più ragionevoli e tassi di errore di Tipo I più bassi quando si utilizza il metodo di stima della massima verosimiglianza con errori standard robusti (MLR) a livello di parcel.\nIn conclusione, il parceling è una tecnica utile che può migliorare l’affidabilità e la validità dei modelli psicometrici, specialmente in presenza di grandi set di item e campioni di piccole dimensioni. Tuttavia, è essenziale valutare attentamente la sua applicabilità e procedere con cautela, specialmente per quanto riguarda l’unidimensionalità dei parcel.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html#sem-in-un-piccolo-campione",
    "href": "chapters/sem/11_small_samples.html#sem-in-un-piccolo-campione",
    "title": "55  Modellizzazione SEM in Piccoli Campioni",
    "section": "55.5 SEM in un Piccolo Campione",
    "text": "55.5 SEM in un Piccolo Campione\n{cite:t}kline2023principles discute uno studio in cui è stato applicato un modello CFA a due fattori ad un campione di 103 donne, le quali hanno compilato questionari su esperienze di origine familiare e adattamento coniugale {cite:p}sabatelli2003family.\n\n# input the correlations in lower diagnonal form\nsabatelliLower.cor &lt;- \"\n 1.000\n  .740 1.000\n  .265  .422 1.000\n  .305  .401  .791 1.000\n  .315  .351  .662  .587 1.000 \"\n\n# name the variables and convert to full correlation matrix\nsabatelli.cor &lt;- lavaan::getCov(sabatelliLower.cor, names = c(\n    \"problems\", \"intimacy\", \"father\", \"mother\", \"both\"\n    )\n)\n\n# add the standard deviations and convert to covariances\nsabatelli.cov &lt;- lavaan::cor2cov(sabatelli.cor, sds = c(\n    32.936, 22.749, 13.390, 13.679, 14.382\n    )\n)\n\nIl modello proposto dagli autori è specificato di seguito:\n\nsabatelli_model &lt;- \"\n    # common factors\n    # loading for intimacy constrained to equal .691\n    Marital =~ problems + intimacy\n    FOE =~ father + mother + both\n\"\n\nIn riferimento al modello specificato sopra, la soluzione fornita da lavaan risulta inammissibile a causa di un caso di Heywood, evidenziato da una varianza d’errore negativa per la variabile “intimità”.\n\noriginal &lt;- lavaan::sem(sabatelli_model,\n    sample.cov = sabatelli.cov,\n    sample.nobs = 103\n)\n\n\nlavaan::summary(original,\n    fit.measures = TRUE, standardized = TRUE,\n    rsquare = TRUE\n) |&gt; print()\n\nlavaan 0.6.17 ended normally after 141 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        11\n\n  Number of observations                           103\n\nModel Test User Model:\n                                                      \n  Test statistic                                 4.688\n  Degrees of freedom                                 4\n  P-value (Chi-square)                           0.321\n\nModel Test Baseline Model:\n\n  Test statistic                               271.302\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.997\n  Tucker-Lewis Index (TLI)                       0.993\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2087.964\n  Loglikelihood unrestricted model (H1)      -2085.620\n                                                      \n  Akaike (AIC)                                4197.928\n  Bayesian (BIC)                              4226.910\n  Sample-size adjusted Bayesian (SABIC)       4192.163\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.041\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.159\n  P-value H_0: RMSEA &lt;= 0.050                    0.448\n  P-value H_0: RMSEA &gt;= 0.080                    0.387\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.028\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Marital =~                                                            \n    problems          1.000                              23.362    0.713\n    intimacy          1.006    0.221    4.547    0.000   23.503    1.038\n  FOE =~                                                                \n    father            1.000                              12.488    0.937\n    mother            0.919    0.089   10.320    0.000   11.480    0.843\n    both              0.808    0.098    8.206    0.000   10.088    0.705\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Marital ~~                                                            \n    FOE             129.409   44.216    2.927    0.003    0.444    0.444\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .problems        528.472  130.514    4.049    0.000  528.472    0.492\n   .intimacy        -39.892  109.200   -0.365    0.715  -39.892   -0.078\n   .father           21.613   10.983    1.968    0.049   21.613    0.122\n   .mother           53.509   11.710    4.570    0.000   53.509    0.289\n   .both            103.075   16.168    6.375    0.000  103.075    0.503\n    Marital         545.776  169.103    3.227    0.001    1.000    1.000\n    FOE             155.939   26.732    5.833    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    problems          0.508\n    intimacy             NA\n    father            0.878\n    mother            0.711\n    both              0.497\n\n\n\nPer ovviare ad un tale problema, in una nuova analisi del modello di adattamento coniugale è stato applicato un vincolo specifico ai carichi non standardizzati degli indicatori. A causa delle differenze sostanziali nelle metriche tra le due variabili, ovvero “intimacy” (con una deviazione standard di 22.749) e “problems” (con una deviazione standard di 32.936), sono state fissate le seguenti saturazioni fattoriali: il carico per la variabile “problemi” è stato fissato a 1, mentre il carico per la variabile “intimità” è stato fissato a 0.691. Questi valori sono stati calcolati in modo da riflettere proporzionalmente la differenza nelle deviazioni standard tra le due variabili.\n\n# analysis with constrained loadings for indicators of marital adjustment\n# model df = 5\n\n# standard deviations for both indicators\n# of the marital factor are listed next\n# intimacy, sd = 22.749\n# problems, sd = 32.936\n# ratio = 22.749/32.936 = .691\n\n# specify model with constrained loadings for problems, intimacy\n\nproportional.model &lt;- \"\n    # common factors\n    # loading for intimacy constrained to equal .691\n    Marital =~ 1*problems + .691*intimacy\n    FOE =~ father + mother + both \n\"\n\n\nproportional &lt;- lavaan::sem(proportional.model,\n    sample.cov = sabatelli.cov,\n    sample.nobs = 103\n)\n\n\nsemPlot::semPaths(proportional,\n    what = \"col\", whatLabels = \"par\", style = \"mx\", \n    layout = \"tree2\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 8, sizeMan2 = 5\n)\n\n\n\n\n\n\n\n\n\nlavaan::summary(proportional,\n    fit.measures = TRUE, standardized = TRUE,\n    rsquare = TRUE\n) |&gt; print()\n\nlavaan 0.6.17 ended normally after 110 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n  Number of observations                           103\n\nModel Test User Model:\n                                                      \n  Test statistic                                 8.449\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.133\n\nModel Test Baseline Model:\n\n  Test statistic                               271.302\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.987\n  Tucker-Lewis Index (TLI)                       0.974\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2089.845\n  Loglikelihood unrestricted model (H1)      -2085.620\n                                                      \n  Akaike (AIC)                                4199.690\n  Bayesian (BIC)                              4226.037\n  Sample-size adjusted Bayesian (SABIC)       4194.449\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.082\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.174\n  P-value H_0: RMSEA &lt;= 0.050                    0.242\n  P-value H_0: RMSEA &gt;= 0.080                    0.584\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.045\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Marital =~                                                            \n    problems          1.000                              28.391    0.840\n    intimacy          0.691                              19.618    0.885\n  FOE =~                                                                \n    father            1.000                              12.373    0.929\n    mother            0.935    0.091   10.279    0.000   11.568    0.850\n    both              0.821    0.100    8.235    0.000   10.155    0.710\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Marital ~~                                                            \n    FOE             164.822   42.788    3.852    0.000    0.469    0.469\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .problems        335.923   80.080    4.195    0.000  335.923    0.294\n   .intimacy        106.214   34.373    3.090    0.002  106.214    0.216\n   .father           24.457   11.060    2.211    0.027   24.457    0.138\n   .mother           51.489   11.730    4.389    0.000   51.489    0.278\n   .both            101.712   16.070    6.329    0.000  101.712    0.497\n    Marital         806.040  132.946    6.063    0.000    1.000    1.000\n    FOE             153.095   26.669    5.741    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    problems          0.706\n    intimacy          0.784\n    father            0.862\n    mother            0.722\n    both              0.503\n\n\n\n\nfitMeasures(proportional, c(\"chisq\", \"df\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt;\n    print()\n\nchisq    df   cfi   tli rmsea  srmr \n8.449 5.000 0.987 0.974 0.082 0.045 \n\n\n\nlavaan::parameterEstimates(proportional) |&gt; print()\n\n        lhs op      rhs     est      se      z pvalue ci.lower ci.upper\n1   Marital =~ problems   1.000   0.000     NA     NA    1.000    1.000\n2   Marital =~ intimacy   0.691   0.000     NA     NA    0.691    0.691\n3       FOE =~   father   1.000   0.000     NA     NA    1.000    1.000\n4       FOE =~   mother   0.935   0.091 10.279  0.000    0.757    1.113\n5       FOE =~     both   0.821   0.100  8.235  0.000    0.625    1.016\n6  problems ~~ problems 335.923  80.080  4.195  0.000  178.970  492.877\n7  intimacy ~~ intimacy 106.214  34.373  3.090  0.002   38.843  173.584\n8    father ~~   father  24.457  11.060  2.211  0.027    2.779   46.134\n9    mother ~~   mother  51.489  11.730  4.389  0.000   28.498   74.481\n10     both ~~     both 101.712  16.070  6.329  0.000   70.216  133.208\n11  Marital ~~  Marital 806.040 132.946  6.063  0.000  545.471 1066.608\n12      FOE ~~      FOE 153.095  26.669  5.741  0.000  100.825  205.365\n13  Marital ~~      FOE 164.822  42.788  3.852  0.000   80.959  248.684\n\n\n\nlavaan::standardizedSolution(proportional) |&gt; print()\n\n        lhs op      rhs est.std    se      z pvalue ci.lower ci.upper\n1   Marital =~ problems   0.840 0.036 23.439  0.000    0.770    0.910\n2   Marital =~ intimacy   0.885 0.037 24.040  0.000    0.813    0.957\n3       FOE =~   father   0.929 0.035 26.779  0.000    0.861    0.997\n4       FOE =~   mother   0.850 0.040 21.126  0.000    0.771    0.929\n5       FOE =~     both   0.710 0.055 12.800  0.000    0.601    0.818\n6  problems ~~ problems   0.294 0.060  4.884  0.000    0.176    0.412\n7  intimacy ~~ intimacy   0.216 0.065  3.317  0.001    0.088    0.344\n8    father ~~   father   0.138 0.064  2.139  0.032    0.012    0.264\n9    mother ~~   mother   0.278 0.068  4.065  0.000    0.144    0.412\n10     both ~~     both   0.497 0.079  6.313  0.000    0.342    0.651\n11  Marital ~~  Marital   1.000 0.000     NA     NA    1.000    1.000\n12      FOE ~~      FOE   1.000 0.000     NA     NA    1.000    1.000\n13  Marital ~~      FOE   0.469 0.091  5.177  0.000    0.292    0.647\n\n\nNonostante gli indici di bontà di adattamento siano eccellenti, la potenza di questa analisi statistica risulta estremamente limitata. Per valutare questa limitazione, è possibile utilizzare la funzione semTools::findRMSEAsamplesize(). Questa funzione calcola la dimensione del campione necessaria per rilevare una differenza significativa tra RMSEA_0 e RMSEA_A, considerando un modello con df gradi di libertà.\nPer esempio, se desideriamo distinguere tra RMSEA_0=0.05 e RMSEA_A=0.10 utilizzando il modello attuale con 5 gradi di libertà, la funzione ci indica che sono necessarie 561 osservazioni per ottenere una potenza statistica di 0.8:\n\nsemTools::findRMSEAsamplesize(0.05, .10, 5, .80, .05, 1)\n\n561\n\n\nPer creare un grafico che rappresenti la potenza statistica per rilevare la differenza tra RMSEA_0=0.05 e RMSEA_A=0.10 (utilizzati qui come esempio) al variare della dimensione del campione, è possibile seguire la seguente procedura:\n\nsemTools::plotRMSEApower(rmsea0 = .05, rmseaA = .10, df = 5, 50, 1000)\n\n\n\n\n\n\n\n\nQuesta analisi di potenza indica che la dimensione del campione utilizzato (\\(n\\) = 103) è del tutto inadeguata.\nPer migliorare il nostro giudizio sull’adattamento del modello consideriamo l’analisi dei residui.\n\nlavaan::residuals(proportional, type = \"standardized.mplus\") |&gt; print()\n\n$type\n[1] \"standardized.mplus\"\n\n$cov\n         prblms intmcy father mother   both\nproblems     NA                            \nintimacy     NA  0.918                     \nfather   -3.994  1.039  0.002              \nmother   -0.871  1.049  0.328  0.000       \nboth      0.407  0.930  0.352 -2.776     NA\n\n\n\n\nlavaan::lavResiduals(proportional, type = \"cor.bollen\", summary = TRUE) |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n         prblms intmcy father mother   both\nproblems  0.000                            \nintimacy -0.004  0.000                     \nfather   -0.101  0.036  0.000              \nmother   -0.030  0.048  0.002  0.000       \nboth      0.035  0.056  0.003 -0.016  0.000\n\n$cov.z\n         prblms intmcy father mother   both\nproblems  0.000                            \nintimacy -2.524  0.000                     \nfather   -2.386  1.245  0.000              \nmother   -0.586  1.134  0.881  0.000       \nboth      0.523  0.936  0.507 -1.143  0.000\n\n$summary\n                           cov\ncrmr                     0.044\ncrmr.se                  0.015\ncrmr.exactfit.z          0.504\ncrmr.exactfit.pvalue     0.307\nucrmr                    0.023\nucrmr.se                 0.029\nucrmr.ci.lower          -0.024\nucrmr.cilupper           0.071\nucrmr.closefit.h0.value  0.050\nucrmr.closefit.z        -0.928\nucrmr.closefit.pvalue    0.823\n\n\n\nQuesti sono risultati relativamente scarsi per un modello così piccolo. Il computer non è stato in grado di calcolare tutti i residui standardizzati possibili, il che non è sorprendente in un campione così ridotto.\n\n55.5.1 Stimatore MIIV-2SLS\nUna seconda analisi viene condotta utilizzando lo stimatore MIIV-2SLS. Il pacchetto MIIVsem non calcola statistiche globali di bontà di adattamento. Al contrario, calcola il test di Sargan per ciascun indicatore previsto dal modello. Le statistiche del test di Sargan approssimano distribuzioni chi-quadro centrali con gradi di libertà equivalenti al numero di item meno uno, quindi df = 2. L’ipotesi nulla è che ogni insieme di strumenti multipli sia incorrelato con il termine di errore per l’equazione. Il mancato rifiuto dell’ipotesi nulla per il test di Sargan suggerisce una buona corrispondenza del modello con i dati.\n\nMIIVsem::miivs(sabatelli_model)\n\nModel Equation Information \n\n LHS        RHS        MIIVs                     \n intimacy   problems   father, mother, both      \n mother     father     problems, intimacy, both  \n both       father     problems, intimacy, mother\n\n\n\n\nsabatelli &lt;- MIIVsem::miive(sabatelli_model,\n    sample.cov = sabatelli.cov,\n    sample.nobs = 103, var.cov = TRUE\n)\n\nlavaan::summary(sabatelli, rsquare = TRUE) |&gt; print()\n\nMIIVsem (0.5.8) results \n\nNumber of observations                                                    103\nNumber of equations                                                         3\nEstimator                                                           MIIV-2SLS\nStandard Errors                                                      standard\nMissing                                                              listwise\n\n\nParameter Estimates:\n\n\nSTRUCTURAL COEFFICIENTS:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Sargan   df   P(Chi)\n  FOE =~                                                                     \n    father            1.000                                                  \n    mother            0.899    0.089   10.149    0.000    1.763    2    0.414\n    both              0.787    0.099    7.935    0.000    3.590    2    0.166\n  Marital =~                                                                 \n    problems          1.000                                                  \n    intimacy          0.805    0.155    5.195    0.000    4.980    2    0.083\n\nINTERCEPTS:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   \n    both              0.000                              \n    father            0.000                              \n    intimacy          0.000                              \n    mother            0.000                              \n    problems          0.000                              \n\nVARIANCES:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   \n    FOE             158.501                              \n    Marital         702.393                              \n    both            103.301                              \n    father           20.856                              \n    intimacy         50.871                              \n    mother           54.195                              \n    problems        422.427                              \n\nCOVARIANCES:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   \n  Marital ~~                                             \n    FOE             157.495                              \n\nR-SQUARE:\n                   Estimate\n    problems          0.624\n    intimacy          0.900\n    father            0.884\n    mother            0.703\n    both              0.487\nNULL\n\n\nSi noti che le stime standardizzate non sono calcolate nella versione del pacchetto MIIVsem utilizzata in questa analisi. I valori non standardizzati delle saturazioni fattoriali sono simili a quelli ottenuti in precedenza.\nIl pacchetto MIIVsem non fornisce né le correlazioni previste dal modello per gli indicatori né i residui di correlazione. Per ottenere i residui di correlazione per l’estimatore 2SLS, è possibile utilizzare il pacchetto lavaan per specificare nuovamente il modello precedentemente adattato, ma con l’importante modifica di fissare tutti i parametri non standardizzati in modo che siano identici alle loro controparti 2SLS. Successivamente, è possibile adattare nuovamente il modello con questi parametri fissati alla matrice di covarianza. La matrice di correlazione prevista in questa analisi si basa sulle stime dei parametri 2SLS, consentendo così di ottenere i residui di correlazione desiderati.\n\nsabatelliFixed.model &lt;- \"\n    # common factors\n    Marital =~ 1.0*problems + .805*intimacy\n    FOE =~ 1.0*father + .899*mother + .787*both\n    # factor variances, covariances\n    FOE ~~ 158.501*FOE\n    Marital ~~ 157.495*FOE\n    Marital ~~ 702.393*Marital\n    # indicator error variances\n    father ~~ 20.856*father\n    mother ~~ 54.195*mother\n    both ~~ 103.301*both\n    problems ~~ 422.427*problems\n    intimacy ~~ 50.781*intimacy \n \"\n\n\nsabatelliFixed &lt;- lavaan::sem(sabatelliFixed.model,\n    sample.cov = sabatelli.cov,\n    sample.nobs = 103\n)\n\n\n# standardized parameter \"estimates\" listed\n# next are fixed to nonzero constants, and\n# standard errors are undefined\nlavaan::parameterEstimates(sabatelliFixed)\n\n\nA lavaan.data.frame: 13 x 9\n\n\nlhs\nop\nrhs\nest\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nMarital\n=~\nproblems\n1.000\n0\nNA\nNA\n1.000\n1.000\n\n\nMarital\n=~\nintimacy\n0.805\n0\nNA\nNA\n0.805\n0.805\n\n\nFOE\n=~\nfather\n1.000\n0\nNA\nNA\n1.000\n1.000\n\n\nFOE\n=~\nmother\n0.899\n0\nNA\nNA\n0.899\n0.899\n\n\nFOE\n=~\nboth\n0.787\n0\nNA\nNA\n0.787\n0.787\n\n\nFOE\n~~\nFOE\n158.501\n0\nNA\nNA\n158.501\n158.501\n\n\nMarital\n~~\nFOE\n157.495\n0\nNA\nNA\n157.495\n157.495\n\n\nMarital\n~~\nMarital\n702.393\n0\nNA\nNA\n702.393\n702.393\n\n\nfather\n~~\nfather\n20.856\n0\nNA\nNA\n20.856\n20.856\n\n\nmother\n~~\nmother\n54.195\n0\nNA\nNA\n54.195\n54.195\n\n\nboth\n~~\nboth\n103.301\n0\nNA\nNA\n103.301\n103.301\n\n\nproblems\n~~\nproblems\n422.427\n0\nNA\nNA\n422.427\n422.427\n\n\nintimacy\n~~\nintimacy\n50.781\n0\nNA\nNA\n50.781\n50.781\n\n\n\n\n\n\nlavaan::residuals(sabatelliFixed, type = \"standardized.mplus\") |&gt; print()\n\n$type\n[1] \"standardized.mplus\"\n\n$cov\n         prblms intmcy father mother   both\nproblems -0.338                            \nintimacy -0.180  0.092                     \nfather   -0.938  0.016 -0.073              \nmother   -0.120  0.293  0.043  0.116       \nboth      0.491  0.412  0.067  0.100  0.118\n\n\n\n\nlavaan::residuals(sabatelliFixed, type = \"cor.bollen\") |&gt; print()\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n         prblms intmcy father mother   both\nproblems  0.000                            \nintimacy -0.010  0.000                     \nfather   -0.086  0.001  0.000              \nmother   -0.008  0.026  0.003  0.000       \nboth      0.055  0.038  0.006  0.002  0.000\n\n\n\nSi noti che nessuno dei residui di correlazione assoluti basati sui risultati 2SLS supera lo 0.10, compreso il residuo per la coppia di indicatori “problems” e “father”. In termini di adattamento locale, dunque, in questo esempio i risultati dello stimatore 2SLS sono da preferire rispetto a quelli dello stimatore ML.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html#considerazioni-conclusive",
    "href": "chapters/sem/11_small_samples.html#considerazioni-conclusive",
    "title": "55  Modellizzazione SEM in Piccoli Campioni",
    "section": "55.6 Considerazioni Conclusive",
    "text": "55.6 Considerazioni Conclusive\nIn questo capitolo abbiamo discusso diversi problemi nel contesto della SEM quando le dimensioni del campione sono ridotte e vengono utilizzati metodi di stima standard (massima verosimiglianza), come la mancata convergenza, le soluzioni non ammissibili, il bias, le statistiche di test poco performanti e gli intervalli di confidenza e gli errori standard inaccurati. Come possibili soluzioni per ottenere stime puntuali migliori, {cite:t}rosseel2020small presenta quattro approcci alternativi alla stima: la stima della verosimiglianza penalizzata, le variabili strumentali derivanti dal modello, la stima a due fasi e la regressione dei punteggi fattoriali. Solo il primo metodo è stato specificamente progettato per gestire campioni ridotti. Gli altri approcci sono stati sviluppati con altre preoccupazioni in mente, ma potrebbero essere alternative valide per la stima quando le dimensioni del campione sono ridotte.\nPer quanto riguarda l’inferenza, {cite:t}rosseel2020small discute vari tentativi per migliorare le prestazioni della statistica del chi-quadro per valutare l’adattamento globale in presenza di campioni ridotti. Per quanto riguarda gli errori standard, sottolinea che il bootstrapping potrebbe non essere la soluzione che stiamo cercando. Per ottenere errori standard (e intervalli di confidenza) migliori nel contesto di campioni ridotti, {cite:t}rosseel2020small ritiene che sia necessario aspettare fino a quando nuove tecnologie saranno disponibili. Altri suggerimenti sono stati forniti da {cite:t}kline2023principles. La tecnica del “parceling” è stata presentata in relazione alla discussione fornita da {cite:t}rioux2020item.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/11_small_samples.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/11_small_samples.html#informazioni-sullambiente-di-sviluppo",
    "title": "55  Modellizzazione SEM in Piccoli Campioni",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] MIIVsem_0.5.8     lme4_1.1-35.5     Matrix_1.7-0      ggokabeito_0.1.0 \n [5] viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1   \n [9] gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6   \n[13] semPlot_1.1.6     lavaan_0.6-18     psych_2.4.6.26    scales_1.3.0     \n[17] markdown_1.13     knitr_1.48        lubridate_1.9.3   forcats_1.0.0    \n[21] stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2       readr_2.1.5      \n[25] tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n[29] here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.16.0  jsonlite_1.8.9     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5.1 farver_2.1.2      \n  [7] nloptr_2.1.1       rmarkdown_2.28     vctrs_0.6.5       \n [10] minqa_1.2.8        base64enc_0.1-3    rstatix_0.7.2     \n [13] htmltools_0.5.8.1  broom_1.0.6        Formula_1.2-5     \n [16] htmlwidgets_1.6.4  plyr_1.8.9         sandwich_3.1-1    \n [19] emmeans_1.10.4     zoo_1.8-12         uuid_1.2-1        \n [22] igraph_2.0.3       mime_0.12          lifecycle_1.0.4   \n [25] pkgconfig_2.0.3    R6_2.5.1           fastmap_1.2.0     \n [28] shiny_1.9.1        digest_0.6.37      OpenMx_2.21.12    \n [31] fdrtool_1.2.18     colorspace_2.1-1   rprojroot_2.0.4   \n [34] Hmisc_5.1-3        fansi_1.0.6        timechange_0.3.0  \n [37] abind_1.4-8        compiler_4.4.1     withr_3.0.1       \n [40] glasso_1.11        htmlTable_2.4.3    backports_1.5.0   \n [43] carData_3.0-5      ggsignif_0.6.4     MASS_7.3-61       \n [46] corpcor_1.6.10     gtools_3.9.5       tools_4.4.1       \n [49] pbivnorm_0.6.0     foreign_0.8-87     zip_2.3.1         \n [52] httpuv_1.6.15      nnet_7.3-19        glue_1.7.0        \n [55] quadprog_1.5-8     promises_1.3.0     nlme_3.1-166      \n [58] lisrelToR_0.3      grid_4.4.1         pbdZMQ_0.3-13     \n [61] checkmate_2.3.2    cluster_2.1.6      reshape2_1.4.4    \n [64] generics_0.1.3     gtable_0.3.5       tzdb_0.4.0        \n [67] data.table_1.16.0  hms_1.1.3          car_3.1-2         \n [70] utf8_1.2.4         sem_3.1-16         pillar_1.9.0      \n [73] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [76] splines_4.4.1      lattice_0.22-6     survival_3.7-0    \n [79] kutils_1.73        tidyselect_1.2.1   miniUI_0.1.1.1    \n [82] pbapply_1.7-2      stats4_4.4.1       xfun_0.47         \n [85] qgraph_1.9.8       arm_1.14-4         stringi_1.8.4     \n [88] pacman_0.5.1       boot_1.3-31        evaluate_1.0.0    \n [91] codetools_0.2-20   mi_1.1             cli_3.6.3         \n [94] RcppParallel_5.1.9 IRkernel_1.3.2     rpart_4.1.23      \n [97] xtable_1.8-4       repr_1.1.7         munsell_0.5.1     \n[100] Rcpp_1.0.13        coda_0.19-4.1      png_0.1-8         \n[103] XML_3.99-0.17      parallel_4.4.1     jpeg_0.1-10       \n[106] mvtnorm_1.3-1      openxlsx_4.2.7.1   crayon_1.5.3      \n[109] rlang_1.1.4        multcomp_1.4-26    mnormt_2.1.1      \n\n\n\n\n\n\nKline, R. B. (2023). Principles and practice of structural equation modeling. Guilford publications.\n\n\nRosseel, Y. (2020). Small sample solutions for structural equation modeling. In Small sample size solutions: A guide for applied researchers and practitioners (pp. 226–238). Routledge.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Modellizzazione SEM in Piccoli Campioni</span>"
    ]
  },
  {
    "objectID": "chapters/sem/12_temp_reliability.html",
    "href": "chapters/sem/12_temp_reliability.html",
    "title": "56  Affidabilità longitudinale",
    "section": "",
    "text": "56.1 Introduzione\nNel capitolo (interrater-reliability?), abbiamo discusso il calcolo dell’affidabilità delle misure in disegni longitudinali utilizzando la teoria della generalizzabilità. In questo capitolo, affronteremo lo stesso problema attraverso i modelli di equazioni strutturali (SEM).\nNegli ultimi anni, i progressi tecnologici hanno trasformato i metodi di raccolta dei dati longitudinali intensivi, consentendo la raccolta di informazioni in modo meno invasivo e riducendo le difficoltà per i partecipanti. Tradizionalmente, i dati longitudinali venivano raccolti con un numero limitato di misurazioni ripetute e intervalli di tempo lunghi tra una misurazione e l’altra. Oggi, invece, è possibile ottenere dati con un numero elevato di misurazioni ravvicinate nel tempo, grazie all’uso di strumenti come applicazioni per smartphone e tablet. Questi dati longitudinali intensivi permettono di esaminare la dinamica di processi psicologici che variano nel tempo, come i cambiamenti giornalieri negli stati psicologici.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Affidabilità longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/12_temp_reliability.html#la-struttura-annidata-dei-dati-longitudinali",
    "href": "chapters/sem/12_temp_reliability.html#la-struttura-annidata-dei-dati-longitudinali",
    "title": "56  Affidabilità longitudinale",
    "section": "56.2 La Struttura Annidata dei Dati Longitudinali",
    "text": "56.2 La Struttura Annidata dei Dati Longitudinali\nI dati raccolti con misure quotidiane presentano una struttura annidata, poiché le varie occasioni di misurazione sono raggruppate all’interno dello stesso individuo. Per analizzare l’affidabilità in questo contesto, si ricorre comunemente a due approcci: la teoria della generalizzabilità e l’approccio fattoriale.\n\nTeoria della Generalizzabilità: Questo approccio scompone la varianza totale in componenti di tempo, item e persona, permettendo di valutare l’affidabilità dei cambiamenti nel tempo a livello individuale. Tuttavia, la teoria della generalizzabilità si basa su alcune assunzioni che possono non adattarsi completamente ai dati raccolti.\nApproccio Fattoriale: Questo metodo è più flessibile e consente di modellare le associazioni tra gli item e il punteggio vero, oltre a gestire le varianze degli errori. Nelle sezioni precedenti abbiamo esaminato come l’analisi fattoriale confermativa multilivello (MCFA) possa stimare l’ICC delle singole variabili in contesti con dati annidati (cioè con misurazioni multiple per lo stesso partecipante). Ora, utilizzeremo la MCFA per determinare l’affidabilità sia a livello intra-individuale che inter-individuale nei casi di misure ripetute nel tempo.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Affidabilità longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/12_temp_reliability.html#studio-di-caso-affidabilità-con-dati-longitudinali-intensivi",
    "href": "chapters/sem/12_temp_reliability.html#studio-di-caso-affidabilità-con-dati-longitudinali-intensivi",
    "title": "56  Affidabilità longitudinale",
    "section": "56.3 Studio di Caso: Affidabilità con Dati Longitudinali Intensivi",
    "text": "56.3 Studio di Caso: Affidabilità con Dati Longitudinali Intensivi\nPer guidare la discussione, ci baseremo sull’articolo di Alphen et al. (2022), che offre un tutorial per valutare l’affidabilità dei dati longitudinali intensivi raccolti quotidianamente. In questo studio, gli autori utilizzano dati empirici relativi al livello di stress lavorativo giornaliero tra insegnanti di scuola secondaria, mostrando come calcolare l’affidabilità tramite MCFA e confrontandola con gli indici di affidabilità derivati dalla teoria della generalizzabilità.\nGrazie a questa comparazione, è possibile comprendere i vantaggi e le differenze tra i due approcci, evidenziando come l’analisi fattoriale multilivello consenta una rappresentazione più dettagliata delle variazioni intra-individuali e inter-individuali nei dati longitudinali.\n\n56.3.1 Affidabilità nei Modelli Fattoriali a Livello Singolo\nIn psicologia, la Confermatory Factor Analysis (CFA) è ormai lo standard per valutare la dimensionalità e l’affidabilità dei punteggi. Quando si lavora con dati a livello singolo, l’affidabilità può essere misurata attraverso diversi indici. Tra questi, l’indice \\(\\omega\\) offre un’alternativa al tradizionale coefficiente di consistenza interna \\(\\alpha\\), poiché non richiede che i carichi fattoriali degli item contribuiscano in egual misura al costrutto latente.\nI valori di \\(\\omega\\) spaziano tra zero e uno, con valori prossimi a uno che indicano una maggiore affidabilità della scala. Questo indice rappresenta la proporzione di varianza nei punteggi della scala spiegata dal fattore latente comune a tutti gli indicatori.\n\n\n56.3.2 Definizione dell’Affidabilità Composita \\(\\omega\\)\nL’affidabilità composita \\(\\omega\\) per un costrutto misurato con \\(p\\) item è calcolata come segue:\n\\[\n\\omega = \\frac{\\sum_{i=1}^{p} \\lambda_i^2 \\Phi}{\\sum_{i=1}^{p} \\lambda_i^2 \\Phi + \\sum_{i=1}^{p} \\theta_i},\n\\]\ndove:\n\n\\(i\\) indica ciascun item,\n\\(\\lambda\\) rappresenta il carico fattoriale dell’item sul costrutto latente,\n\\(\\Phi\\) è la varianza del fattore latente,\n\\(\\theta\\) è la varianza residua dell’item.\n\nQuesto indice considera i diversi contributi degli item al costrutto latente, fornendo una stima dell’affidabilità che riflette meglio la struttura fattoriale del punteggio rispetto agli approcci che assumono contributi uniformi.\nPer chiarire, Alphen et al. (2022) propongono un esempio concreto. Supponiamo di avere un modello a singolo fattore, in cui la varianza del fattore è fissata a 1 per l’identificazione del modello, e i carichi fattoriali sui tre indicatori sono pari a 0.7, 0.8 e 0.9. Di conseguenza, le specificità degli item saranno rispettivamente 0.51, 0.36 e 0.19.\nInserendo questi valori nell’equazione per \\(\\omega\\), otteniamo un’affidabilità composita della scala pari a 0.84:\n\\[\n\\omega = \\frac{\\left(0.70 + 0.80 + 0.90\\right)^{2} \\cdot 1}{\\left(0.70 + 0.80 + 0.90\\right)^{2} \\cdot 1 + \\left(0.51 + 0.36 + 0.19\\right)} = 0.84.\n\\]\nQuesto risultato indica che l’84% della varianza totale nei punteggi della scala è attribuibile al fattore comune, il che riflette un’elevata affidabilità della misura.\n\n\n56.3.3 Affidabilità nei Modelli Fattoriali Multilivello\nIn psicologia, i dati spesso presentano una struttura annidata, in cui le unità a un livello inferiore sono raggruppate in unità di livello superiore. Ad esempio, gli studenti sono annidati nelle classi, e i pazienti negli ospedali. Con misurazioni ripetute sugli stessi individui, come nei dati raccolti giornalmente, le occasioni di misurazione sono annidate negli individui. Nel caso illustrato qui, i dati empirici provengono da misurazioni ripetute su insegnanti durante 15 occasioni di raccolta, creando una struttura annidata in cui le occasioni sono raggruppate per ogni insegnante.\nL’analisi fattoriale multilivello consente di rappresentare varianze e covarianze distinte per le differenze intra-individuali e inter-individuali (Muthén, 1994). Nell’esempio discusso, Alphen et al. (2022) si focalizzano su strutture a due livelli: le occasioni di misurazione (Livello 1, o livello intra-individuale) e gli individui (Livello 2, o livello inter-individuale).\nLa Figura 56.1 illustra un modello fattoriale multilivello. In una confermatory factor analysis (CFA) a due livelli, i punteggi degli item vengono suddivisi in componenti latenti intra- e inter-individuali. La componente inter-individuale modella la struttura di covarianza tra individui, spiegando le differenze tra di essi e fornendo un’interpretazione simile a quella di una CFA a livello singolo. La componente intra-individuale modella la covarianza tra le misurazioni ripetute per ciascun individuo, riflettendo le variazioni all’interno degli individui nei diversi momenti temporali. In questo contesto, il livello intra-individuale rappresenta caratteristiche di stato (condizioni fluttuanti nel tempo), mentre il livello inter-individuale riflette caratteristiche di tratto, offrendo una misura aggregata più stabile nel tempo, simile a un’indicazione della personalità.\n\n\n\n\n\n\nFigura 56.1: Un modello configurale multilivello con i carichi fattoriali, varianze residuali e varianza del fattoriali dell’esempio discusso da Alphen et al. (2022). (Figura tratta da Alphen et al., 2022)\n\n\n\nGeldhof et al. (2014) hanno ampliato il metodo per calcolare \\(\\omega\\) adattandolo ai modelli a due livelli, ottenendo così indici di affidabilità distinti per il livello intra-individuale (\\(\\omega_w\\)) e inter-individuale (\\(\\omega_b\\)). Questo approccio è stato successivamente sviluppato ulteriormente da Lai (2021).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Affidabilità longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/12_temp_reliability.html#calcolo-dellaffidabilità-della-scala-con-dati-giornalieri",
    "href": "chapters/sem/12_temp_reliability.html#calcolo-dellaffidabilità-della-scala-con-dati-giornalieri",
    "title": "56  Affidabilità longitudinale",
    "section": "56.4 Calcolo dell’Affidabilità della Scala con Dati Giornalieri",
    "text": "56.4 Calcolo dell’Affidabilità della Scala con Dati Giornalieri\nAlphen et al. (2022) illustrano l’analisi dell’affidabilità utilizzando un dataset con misure longitudinali intensive giornaliere sullo stress degli insegnanti. In questo contesto, il fattore comune a livello esterno può essere interpretato come la componente stabile dello stress, mentre il fattore a livello interno rappresenta la variabilità dello stress nel tempo. Quando si modellano componenti interne ed esterne dello stesso fattore, il modello fattoriale multilivello prende il nome di modello configurale (Stapleton et al., 2016).\nNel modello configurale multilivello, i fattori a livello interno ed esterno riflettono componenti diverse della stessa variabile latente, con una struttura fattoriale identica per entrambi i livelli e carichi fattoriali uguali (Asparouhov & Muthen, 2012). Lai (2021) ha fornito le formule per calcolare gli indici di affidabilità a livello intra-individuale (\\(\\omega_w\\)) e inter-individuale (\\(\\omega_b\\)) in questi modelli configurali. Alphen et al. (2022) illustrano come calcolare questi indici di affidabilità.\n\n56.4.1 Affidabilità a Livello Intra-Individuale\nPer determinare l’affidabilità a livello intra-individuale, utilizziamo la seguente formula:\n\\[\n\\omega_w = \\frac{\\sum (\\lambda_i^2 \\Phi_w)}{\\sum (\\lambda_i^2 \\Phi_w) + \\sum (\\theta_w)},\n\\]\ndove il pedice \\(w\\) si riferisce al livello intra-individuale. I carichi fattoriali (\\(\\lambda\\)) non hanno un pedice di livello specifico poiché sono vincolati ad essere identici a entrambi i livelli. In questo contesto, \\(\\Phi_w\\) rappresenta la varianza del fattore a livello intra-individuale e \\(\\theta_w\\) rappresenta la varianza residua al livello interno.\nInserendo i valori di esempio dalla Figura 56.1, otteniamo un’affidabilità intra-individuale di 0.84:\n\\[\n\\omega_w = \\frac{(0.70 + 0.80 + 0.90)^2}{(0.70 + 0.80 + 0.90)^2 + (0.51 + 0.36 + 0.19)} = 0.84.\n\\]\nQuesto valore indica che il fattore comune a livello interno spiega l’84% della varianza nelle deviazioni a livello intra-individuale nei punteggi della scala.\n\n\n56.4.2 Affidabilità a Livello Inter-Individuale\nPer il calcolo dell’affidabilità a livello inter-individuale, l’equazione è la seguente:\n\\[\n\\omega_b = \\frac{\\sum (\\lambda_i^2 \\Phi_b)}{\\sum (\\lambda_i^2 (\\Phi_b + \\Phi_w/n)) + \\sum (\\theta_b + \\theta_w/n)},\n\\]\ndove \\(n\\) è il numero di occasioni di misurazione (in questo caso, 15). La presenza di \\(\\Phi_w/n\\) e \\(\\theta_w/n\\) tiene conto della varianza dell’errore di campionamento delle medie osservate a livello di persona.\nInserendo i valori di esempio di Alphen et al. (2022) e impostando \\(n = 15\\), otteniamo un’affidabilità inter-individuale di 0.90:\n\\[\n\\omega_b = \\frac{(0.70 + 0.80 + 0.90)^2}{\n    (0.70 + 0.80 + 0.90)^2(0.90 + \\frac{1}{15}) + (0.05 + 0.05 + 0.05) \\\\\n    + \\frac{(0.51 + 0.36 + 0.19)}{15}\n} = 0.90\n\\]\nQuesto valore indica che il fattore comune a livello esterno spiega il 90% della varianza nelle medie osservate a livello di persona.\n\n\n56.4.3 Interpretazione degli Indici di Affidabilità\nQueste formule permettono di calcolare l’affidabilità delle componenti intra- e inter-individuali di una scala in studi longitudinali intensivi. I risultati consentono ai ricercatori di distinguere tra variazioni stabili (livello inter-individuale) e temporanee (livello intra-individuale), offrendo una visione dettagliata della dinamica dei fenomeni psicologici misurati.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Affidabilità longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/12_temp_reliability.html#confronto-con-la-teoria-della-generalizzabilità",
    "href": "chapters/sem/12_temp_reliability.html#confronto-con-la-teoria-della-generalizzabilità",
    "title": "56  Affidabilità longitudinale",
    "section": "56.5 Confronto con la Teoria della Generalizzabilità",
    "text": "56.5 Confronto con la Teoria della Generalizzabilità\nAlphen et al. (2022) hanno anche derivato le componenti di varianza per il calcolo del punteggio di affidabilità a livello interno e a livello esterno utilizzando la teoria della generalizzabilità. Per questi dati, Alphen et al. (2022) trovano che la stima dell’affidabilità a livello interno è .87, molto simile alla stima ottenuta con l’approccio CFA multilivello. Tuttavia, la stima a livello esterno ottenuta con il metodo della generalizzabilità è 0.99, che è .11 più alta rispetto all’approccio analitico fattoriale. Questa differenza potrebbe essere causata dalle assunzioni più rigide fatte dal metodo della teoria della generalizzabilità. Tuttavia, Alphen et al. (2022) notano che questi risultati sono specifici al dataset utilizzato e sarebbe necessario uno studio di simulazione per valutare, in generale, quali sono le differenze sistematiche tra le stime di affidabilità ottenute con i due diversi metodi.\nQui sotto viene presentato il metodo SEM per il calcolo dell’affidabilità inter- e intra-persona usando gli script R forniti da Alphen et al. (2022).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Affidabilità longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/12_temp_reliability.html#metodo-a-cinque-passi",
    "href": "chapters/sem/12_temp_reliability.html#metodo-a-cinque-passi",
    "title": "56  Affidabilità longitudinale",
    "section": "56.6 Metodo a Cinque Passi",
    "text": "56.6 Metodo a Cinque Passi\nPer studiare l’affidabilità delle misure longitudinali intensive, Alphen et al. (2022) propongono una procedura in cinque passi. Questi passaggi sono stati ideati per evitare bias e problemi di specificazione del modello e comprendono:\n\nIspezione delle Correlazioni Intraclasse: Questo primo passo verifica la proporzione di varianza attribuibile alle differenze tra i gruppi, utile per comprendere la struttura annidata dei dati.\nVerifica della Varianza e Covarianza al Livello tra-Persone: Si testa la presenza di varianza e covarianza significative tra le persone per valutare la necessità di un approccio multilivello, dove le differenze tra persone giocano un ruolo importante.\nSpecifica Progressiva del Modello di Misura a ciascun Livello: Il modello di misura viene definito gradualmente per ciascun livello, assicurando che la struttura del modello rappresenti adeguatamente le relazioni tra variabili.\nVerifica della Invarianza di Misura tra Livelli: Si testa l’invarianza della misura per assicurarsi che la struttura del modello sia simile nei vari livelli, il che è fondamentale per confrontare interpretazioni tra livelli.\nCalcolo degli Indici di Affidabilità a Livello Intra-Personale (ωw) e Inter-Personale (ωb): Infine, si calcolano gli indici di affidabilità a livello intra-personale (ωw) e inter-personale (ωb) per quantificare la stabilità delle misure rispettivamente entro e tra persone.\n\nQuesta procedura strutturata permette di valutare l’affidabilità delle misure in studi longitudinali intensivi, garantendo che il modello rispetti le caratteristiche dei dati e fornisca stime affidabili di variabilità e stabilità a livello intra- e inter-personale.\nIniziamo ad importare i dati dell’esempio di Alphen et al. (2022).\n\nvan_alphen &lt;- read.table(\"../../data/van_alphen.dat\", na.strings = \"9999\")\ncolnames(van_alphen) &lt;- c(\"day\", \"school\", \"ID\", \"str1\", \"str2\", \"str3\", \"str4\")\nvan_alphen |&gt; head()\n\n\nA data.frame: 6 x 7\n\n\n\nday\nschool\nID\nstr1\nstr2\nstr3\nstr4\n\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n1\n1\n1\n26\nNA\n24\n78\n\n\n2\n1\n1\n30\nNA\n24\n24\n50\n\n\n3\n1\n1\n55\nNA\n36\n70\n72\n\n\n4\n1\n1\n92\nNA\n37\n34\n41\n\n\n5\n1\n2\n20\n24\nNA\n24\n36\n\n\n6\n1\n2\n22\n12\nNA\n18\n39\n\n\n\n\n\n\nvan_alphen |&gt; tail()\n\n\nA data.frame: 6 x 7\n\n\n\nday\nschool\nID\nstr1\nstr2\nstr3\nstr4\n\n\n\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1264\n15\n6\n62\nNA\n66\n57\n61\n\n\n1265\n15\n6\n87\nNA\n42\n59\n83\n\n\n1266\n15\n6\n115\n53\nNA\n53\n45\n\n\n1267\n15\n6\n118\nNA\n16\n32\n16\n\n\n1268\n15\n6\n123\n23\n22\n23\nNA\n\n\n1269\n15\n6\n143\n64\n64\n65\nNA\n\n\n\n\n\nEseguiamo una procedura di imputazione multipla per gestire il problema dei dati mancanti. In Mplus, questo passaggio può essere eseguito direttamente durante la procedura di fit, ma in R non è possibile. Pertanto, utilizziamo missRanger per imputare i dati prima di adattare il modello.\n\nimp &lt;- missRanger(van_alphen, num.trees = 100)\n\n\nVariables to impute:        str3, str4, str1, str2\nVariables used to impute:   day, school, ID, str1, str2, str3, str4\n\niter 1 \n  |============================================================| 100%\niter 2 \n  |============================================================| 100%\niter 3 \n  |============================================================| 100%\niter 4 \n  |============================================================| 100%\niter 5 \n  |============================================================| 100%\niter 6 \n  |============================================================| 100%\n\n\n\nimp |&gt; summary()\n\n     day                school            ID              str1       \n Length:1269        Min.   :1.000   Min.   :  1.00   Min.   :  0.00  \n Class :character   1st Qu.:2.000   1st Qu.: 43.00   1st Qu.:  7.00  \n Mode  :character   Median :4.000   Median : 81.00   Median : 28.09  \n                    Mean   :3.779   Mean   : 78.53   Mean   : 33.63  \n                    3rd Qu.:5.000   3rd Qu.:115.00   3rd Qu.: 57.05  \n                    Max.   :6.000   Max.   :151.00   Max.   :100.00  \n      str2               str3             str4       \n Min.   :  0.0000   Min.   :  0.00   Min.   :  0.00  \n 1st Qu.:  0.7818   1st Qu.:  7.00   1st Qu.:  7.00  \n Median : 21.0000   Median : 30.00   Median : 34.00  \n Mean   : 26.6310   Mean   : 34.71   Mean   : 34.99  \n 3rd Qu.: 45.0000   3rd Qu.: 61.00   3rd Qu.: 59.00  \n Max.   :100.0000   Max.   :100.00   Max.   :100.00  \n\n\n\n56.6.1 Passo 1: Ispezione delle Correlazioni Intraclasse\nIl modello multilivello è appropriato se una quota rilevante della varianza può essere attribuita al livello tra-persone. Il coefficiente di correlazione intraclasse (ICC) di una variabile permette di quantificare l’entità di questa proporzione (Snijders & Bosker, 2012). Pertanto, il primo passo consiste nel verificare se è presente una varianza significativa al livello tra-persone attraverso l’ispezione dell’ICC, calcolato come:\n\\[\n\\text{ICC} = \\frac{\\sigma_b}{\\sigma_b + \\sigma_w},\n\\]\ndove \\(\\sigma_b\\) e \\(\\sigma_w\\) rappresentano, rispettivamente, la varianza dell’indicatore al livello tra-persone e al livello entro-persona, ottenute adattando modelli saturi a entrambi i livelli.\n\nmodel1 &lt;- \"\n        level: 1\n            str1 ~~ str2 + str3 + str4\n            str2 ~~ str3 + str4\n            str3 ~~ str4\n\n        level: 2\n            str1 ~~ str2 + str3 + str4\n            str2 ~~ str3 + str4\n            str3 ~~ str4\n\n          # means\n            str1 + str2 + str3 + str4  ~ 1\n    \"\n\n\nfit.step1 &lt;- lavaan(\n    model = model1, data = imp, cluster = \"ID\",\n    auto.var = TRUE\n)\n\nsummary(fit.step1) |&gt; print()\n\nlavaan 0.6-19 ended normally after 363 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        24\n\n  Number of observations                          1269\n  Number of clusters [ID]                          151\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nLevel 1 [within]:\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  str1 ~~                                             \n    str2            342.261   16.268   21.039    0.000\n    str3            406.537   19.297   21.067    0.000\n    str4            359.195   17.888   20.081    0.000\n  str2 ~~                                             \n    str3            305.064   16.314   18.699    0.000\n    str4            307.366   15.847   19.396    0.000\n  str3 ~~                                             \n    str4            347.263   18.496   18.775    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1            471.690   19.872   23.737    0.000\n    str2            383.080   16.154   23.714    0.000\n    str3            538.863   22.705   23.733    0.000\n    str4            489.494   20.666   23.686    0.000\n\n\nLevel 2 [ID]:\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  str1 ~~                                             \n    str2            275.295   38.963    7.066    0.000\n    str3            278.127   40.489    6.869    0.000\n    str4            292.286   41.776    6.996    0.000\n  str2 ~~                                             \n    str3            251.097   37.105    6.767    0.000\n    str4            271.965   39.062    6.962    0.000\n  str3 ~~                                             \n    str4            282.024   40.920    6.892    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1             34.296    1.579   21.727    0.000\n    str2             27.161    1.492   18.207    0.000\n    str3             35.551    1.561   22.779    0.000\n    str4             35.744    1.611   22.186    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1            298.145   42.327    7.044    0.000\n    str2            270.910   37.919    7.144    0.000\n    str3            281.366   41.412    6.794    0.000\n    str4            310.574   44.373    6.999    0.000\n\n\n\n\n# see ICC\nlavInspect(fit.step1, \"icc\") |&gt; print()\n\n str1  str2  str3  str4 \n0.387 0.414 0.343 0.388 \n\n\n\n\n56.6.2 Passo 2: Verifica della Varianza e Covarianza al Livello tra-Persone\nIn questo secondo passaggio, verifichiamo se: a) la varianza al livello tra-persone è significativa e b) se esistono covarianze significative a questo livello (Hox, Moerbeek, & van der Schoot, 2017). Il test delle covarianze ci permette di valutare se esistono effettivamente relazioni che potrebbero essere modellate con un fattore comune al livello tra-persone.\nPer prima cosa, per testare la significatività della varianza al livello tra-persone (passo 2a), adattiamo ai dati un modello nullo per questo livello. Un modello nullo è un modello in cui tutte le varianze (e le covarianze) sono fissate a zero. Al livello entro-persona specifichiamo invece un modello saturo, in cui tutti gli item sono correlati, assicurando così una perfetta aderenza ai dati. In questo modo, ogni eventuale discrepanza nel fit del modello deriva esclusivamente dal livello tra-persone, dove le varianze sono fissate a zero.\nQuando si testa il fit del modello, un test χ² significativo indica che il modello differisce in modo significativo rispetto a un modello che si adatterebbe perfettamente ai dati (Kline, 2011). Pertanto, se il test χ² respinge il modello nullo, possiamo concludere che è presente una varianza significativa al livello tra-persone.\nSuccessivamente, per verificare se esiste una covarianza significativa al livello tra-persone (passo 2b), rilasciamo il vincolo sulle varianze in modo da stimarle liberamente, mantenendo però le covarianze fissate a zero. La specificazione del modello saturo al livello entro-persona rimane invariata. Anche in questo caso, un test χ² significativo di fit del modello indica che questo modello differisce in modo significativo rispetto a un modello che si adatterebbe perfettamente ai dati. Poiché non abbiamo modellato alcuna relazione tra gli item a livello tra-persone, un test χ² significativo indica che le covarianze dovrebbero essere prese in considerazione. In altre parole, un test χ² significativo suggerisce la presenza di covarianze significative, che potrebbero essere spiegate con un modello fattoriale nei passaggi successivi.\n\nmodel2a &lt;- \"\n        level: 1\n            str1 ~~ str2 + str3 + str4\n            str2 ~~ str3 + str4\n            str3 ~~ str4\n\n        level: 2\n            str1 ~~ 0*str2 + 0*str3 + 0*str4\n            str2 ~~ 0*str3 + 0*str4\n            str3 ~~ 0*str4\n\n            str1 ~~ 0*str1\n            str2 ~~ 0*str2\n            str3 ~~ 0*str3\n            str4 ~~ 0*str4\n\n          # means\n            str1 + str2 + str3 + str4  ~ 1\n    \"\n\n\nfit.step2a &lt;- lavaan(\n    model = model2a, data = imp, cluster = \"ID\",\n    auto.var = TRUE\n)\n\nsummary(fit.step2a) |&gt; print()\n\nlavaan 0.6-19 ended normally after 118 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        14\n\n  Number of observations                          1269\n  Number of clusters [ID]                          151\n\nModel Test User Model:\n                                                      \n  Test statistic                               565.401\n  Degrees of freedom                                10\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nLevel 1 [within]:\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  str1 ~~                                             \n    str2            623.313   26.624   23.412    0.000\n    str3            685.932   29.343   23.376    0.000\n    str4            659.716   29.075   22.690    0.000\n  str2 ~~                                             \n    str3            558.874   25.832   21.635    0.000\n    str4            588.081   26.523   22.172    0.000\n  str3 ~~                                             \n    str4            633.186   28.998   21.835    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1            774.610   30.685   25.244    0.000\n    str2            660.240   26.257   25.145    0.000\n    str3            817.829   32.239   25.368    0.000\n    str4            809.702   32.482   24.928    0.000\n\n\nLevel 2 [ID]:\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  str1 ~~                                             \n    str2              0.000                           \n    str3              0.000                           \n    str4              0.000                           \n  str2 ~~                                             \n    str3              0.000                           \n    str4              0.000                           \n  str3 ~~                                             \n    str4              0.000                           \n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1             33.560    0.781   42.954    0.000\n    str2             26.659    0.721   36.960    0.000\n    str3             34.732    0.803   43.264    0.000\n    str4             34.988    0.799   43.802    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1              0.000                           \n    str2              0.000                           \n    str3              0.000                           \n    str4              0.000                           \n\n\n\n\nmodel2b &lt;- \"\n        level: 1\n            str1 ~~ str2 + str3 + str4\n            str2 ~~ str3 + str4\n            str3 ~~ str4\n\n        level: 2\n            str1 ~~ 0*str2 + 0*str3 + 0*str4\n            str2 ~~ 0*str3 + 0*str4\n            str3 ~~ 0*str4\n\n          # means\n            str1 + str2 + str3 + str4  ~ 1\n    \"\n\n\nfit.step2b &lt;- lavaan(\n    model = model2b, data = imp, cluster = \"ID\",\n    auto.var = TRUE\n)\n\nsummary(fit.step2b) |&gt; print()\n\nlavaan 0.6-19 ended normally after 240 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        18\n\n  Number of observations                          1269\n  Number of clusters [ID]                          151\n\nModel Test User Model:\n                                                      \n  Test statistic                               425.971\n  Degrees of freedom                                 6\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nLevel 1 [within]:\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  str1 ~~                                             \n    str2            583.494   26.975   21.631    0.000\n    str3            671.583   30.182   22.251    0.000\n    str4            623.698   29.159   21.390    0.000\n  str2 ~~                                             \n    str3            532.195   25.884   20.561    0.000\n    str4            534.900   26.062   20.524    0.000\n  str3 ~~                                             \n    str4            598.950   29.051   20.617    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1            750.987   32.247   23.288    0.000\n    str2            589.951   26.541   22.228    0.000\n    str3            790.466   33.639   23.498    0.000\n    str4            738.518   32.940   22.420    0.000\n\n\nLevel 2 [ID]:\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  str1 ~~                                             \n    str2              0.000                           \n    str3              0.000                           \n    str4              0.000                           \n  str2 ~~                                             \n    str3              0.000                           \n    str4              0.000                           \n  str3 ~~                                             \n    str4              0.000                           \n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1             33.600    0.803   41.857    0.000\n    str2             26.507    0.803   32.993    0.000\n    str3             34.830    0.874   39.851    0.000\n    str4             35.056    0.878   39.911    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    str1              6.520    3.333    1.956    0.050\n    str2             23.657    5.371    4.405    0.000\n    str3             17.924    5.397    3.321    0.001\n    str4             24.375    6.655    3.663    0.000\n\n\n\n\nanova(fit.step2a, fit.step2b)\n\n\nA anova: 2 x 8\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nRMSEA\nDf diff\nPr(&gt;Chisq)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nfit.step2b\n6\n42786.09\n42878.72\n425.9705\nNA\nNA\nNA\nNA\n\n\nfit.step2a\n10\n42917.52\n42989.56\n565.4010\n139.4305\n0.1633418\n4\n3.737341e-29\n\n\n\n\n\n\n\n56.6.3 Passo 3: Definizione di un Modello di Misura al Livello Entro-Persona\nIn questo terzo passo, esaminiamo se gli item possono essere rappresentati da un unico fattore al livello entro-persona. Per farlo, definiamo un modello di misura per il livello entro-persona, mantenendo invece un modello saturo a livello tra-persone. La bontà di adattamento di questo modello, e dei modelli successivi, può essere valutata utilizzando il test χ². Se il test χ² risulta significativo, dobbiamo rifiutare l’adattamento perfetto del modello. Con campioni di grandi dimensioni, anche piccole discrepanze nel modello possono portare a rifiutarlo (Marsh, Balla, & McDonald, 1988).\nPer questo motivo, oltre al test χ², consideriamo anche indici di adattamento approssimato: un RMSEA inferiore a 0.05 e un CFI superiore a 0.95 indicano un buon adattamento (Browne & Cudeck, 1992), mentre un RMSEA inferiore a 0.08 e un CFI superiore a 0.90 indicano un adattamento accettabile (Hu & Bentler, 1999).\nSe il modello non si adatta adeguatamente ai dati, possono essere intrapresi passi aggiuntivi per affrontare le cause di tale discrepanza prima di procedere. In questi casi, l’ispezione degli indici di modifica o dei residui di correlazione può fornire informazioni preziose su eventuali discrepanze locali del modello. È importante che le modifiche al modello siano sempre guidate da considerazioni teoriche, poiché seguire esclusivamente i risultati statistici può portare a modelli che non si generalizzano ad altri campioni (MacCallum, 1986).\nSolo quando il modello si adatta adeguatamente ai dati e ha senso teorico, è opportuno passare al passaggio successivo.\n\nmodel3 &lt;- \"\n        level: 1\n            stress =~ str1 + str2 + str3 + str4\n            stress ~~ 1*stress\n\n        level: 2\n            str1 ~~ str2 + str3 + str4\n            str2 ~~ str3 + str4\n            str3 ~~ str4\n\n          # means\n            str1 + str2 + str3 + str4  ~ 1\n    \"\n\n\nfit.step3 &lt;- lavaan(\n    model = model3, data = imp, cluster = \"ID\",\n    auto.var = TRUE\n)\n\n\nfitMeasures(fit.step3) |&gt; print()\n\n                 npar                  fmin                 chisq \n               22.000                13.036                50.423 \n                   df                pvalue        baseline.chisq \n                2.000                 0.000              4286.653 \n          baseline.df       baseline.pvalue                   cfi \n               12.000                 0.000                 0.989 \n                  tli                  nnfi                   rfi \n                0.932                 0.932                 0.929 \n                  nfi                  pnfi                   ifi \n                0.988                 0.165                 0.989 \n                  rni                  logl     unrestricted.logl \n                0.989            -21207.330            -21182.119 \n                  aic                   bic                ntotal \n            42458.661             42571.872              1269.000 \n                 bic2                 rmsea        rmsea.ci.lower \n            42501.990                 0.138                 0.107 \n       rmsea.ci.upper        rmsea.ci.level          rmsea.pvalue \n                0.172                 0.900                 0.000 \n       rmsea.close.h0 rmsea.notclose.pvalue     rmsea.notclose.h0 \n                0.050                 0.999                 0.080 \n                 srmr           srmr_within          srmr_between \n                0.023                 0.019                 0.003 \n\n\n\n\n56.6.4 Passo 4: Adattamento di un Modello a Due Livelli con Vincoli tra Livelli\nNel modello configurale, desideriamo che il costrutto abbia un significato comparabile a entrambi i livelli. Ad esempio, vorremmo che la “nervosità” rappresenti il sentimento generale degli individui come indicatore di stress al livello tra-persone e che, al livello entro-persona, esprima le variazioni quotidiane di quella stessa emozione per indicare le fluttuazioni giornaliere dello stress. Per permettere questa interpretazione, è necessario vincolare i carichi fattoriali affinché siano uguali tra il livello entro-persona e tra-persona.\nIn questo modello, la varianza del fattore al livello tra-persone deve essere stimata liberamente, poiché il vincolo sui carichi fattoriali già identifica la scala del fattore a livello tra-persona quando la varianza del fattore al livello entro-persona è fissata (Jak et al., 2014).\n\nmodel4 &lt;- \"\n        level: 1\n            stress =~ L1*str1 + L2*str2 + L3*str3 + L4*str4\n            stress ~~ 1*stress\n\n        level: 2\n            stress =~ L1*str1 + L2*str2 + L3*str3 + L4*str4\n            stress ~~ fb*stress\n\n          # means\n            str1 + str2 + str3 + str4  ~ 1\n    \"\n\n\nfit.step4 &lt;- lavaan(\n    model = model4, data = imp, cluster = \"ID\",\n    auto.var = TRUE\n)\n\n\nfitMeasures(fit.step4) |&gt; print()\n\n                 npar                  fmin                 chisq \n               17.000                13.049                83.701 \n                   df                pvalue        baseline.chisq \n                7.000                 0.000              4286.653 \n          baseline.df       baseline.pvalue                   cfi \n               12.000                 0.000                 0.982 \n                  tli                  nnfi                   rfi \n                0.969                 0.969                 0.967 \n                  nfi                  pnfi                   ifi \n                0.980                 0.572                 0.982 \n                  rni                  logl     unrestricted.logl \n                0.982            -21223.969            -21182.119 \n                  aic                   bic                ntotal \n            42481.938             42569.420              1269.000 \n                 bic2                 rmsea        rmsea.ci.lower \n            42515.419                 0.093                 0.076 \n       rmsea.ci.upper        rmsea.ci.level          rmsea.pvalue \n                0.111                 0.900                 0.000 \n       rmsea.close.h0 rmsea.notclose.pvalue     rmsea.notclose.h0 \n                0.050                 0.894                 0.080 \n                 srmr           srmr_within          srmr_between \n                0.032                 0.018                 0.014 \n\n\n\n\n56.6.5 Passo 5: Calcolo degli Indici di Affidabilità\nSe il modello ottenuto al Passo 4 presenta un buon adattamento, l’ultimo passo consiste nel calcolare gli indici di affidabilità. Utilizziamo le stime dei parametri ottenute per calcolare ωb (affidabilità a livello tra-persone) e ωw (affidabilità a livello entro-persona) seguendo le formule presentate.\nQuesti indici quantificano la stabilità e la coerenza delle misure rispettivamente tra e entro persone, offrendo una valutazione completa dell’affidabilità delle misure a ciascun livello del modello.\n\nmodel5 &lt;- \"\n        level: 1\n            stress =~ L1*str1 + L2*str2 + L3*str3 + L4*str4\n            stress ~~ 1*stress\n\n            str1 ~~ tw1*str1\n            str2 ~~ tw2*str2\n            str3 ~~ tw3*str3\n            str4 ~~ tw4*str4\n\n        level: 2\n            stress =~ L1*str1 + L2*str2 + L3*str3 + L4*str4\n            stress ~~ fb*stress\n\n            str1 ~~ tb1*str1\n            str2 ~~ tb2*str2\n            str3 ~~ tb3*str3\n            str4 ~~ tb4*str4\n\n          # means\n            str1 + str2 + str3 + str4  ~ 1\n\n          # reliability calculations\n          lambda := L1 + L2 + L3 + L4\n          thetaw := tw1 + tw2 + tw3 + tw4\n          thetab := tb1 + tb2 + tb3 + tb4\n          omega_w := lambda^2 / (lambda^2 + thetaw)\n          omega_b := (lambda^2 * fb) / (lambda^2 * (1/15 + fb) + fb + thetaw/15)\n    \"\n\n\nfit.step5 &lt;- lavaan(\n    model = model5, data = imp, cluster = \"ID\",\n    auto.var = TRUE\n)\n\n\nsummary(fit.step5) |&gt; print()\n\nlavaan 0.6-19 ended normally after 93 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n  Number of equality constraints                     4\n\n  Number of observations                          1269\n  Number of clusters [ID]                          151\n\nModel Test User Model:\n                                                      \n  Test statistic                                83.701\n  Degrees of freedom                                 7\n  P-value (Chi-square)                           0.000\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nLevel 1 [within]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  stress =~                                           \n    str1      (L1)   20.206    0.482   41.904    0.000\n    str2      (L2)   17.076    0.458   37.289    0.000\n    str3      (L3)   19.223    0.526   36.513    0.000\n    str4      (L4)   18.338    0.520   35.246    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    stress            1.000                           \n   .str1     (tw1)   53.776    5.062   10.623    0.000\n   .str2     (tw2)  110.015    5.823   18.895    0.000\n   .str3     (tw3)  160.900    8.107   19.847    0.000\n   .str4     (tw4)  172.111    8.652   19.894    0.000\n\n\nLevel 2 [ID]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  stress =~                                           \n    str1      (L1)   20.206    0.482   41.904    0.000\n    str2      (L2)   17.076    0.458   37.289    0.000\n    str3      (L3)   19.223    0.526   36.513    0.000\n    str4      (L4)   18.338    0.520   35.246    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .str1             34.417    1.589   21.659    0.000\n   .str2             27.133    1.415   19.179    0.000\n   .str3             35.533    1.591   22.335    0.000\n   .str4             35.740    1.537   23.251    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    stress    (fb)    0.745    0.112    6.644    0.000\n   .str1     (tb1)   -0.800    2.973   -0.269    0.788\n   .str2     (tb2)   18.555    4.380    4.236    0.000\n   .str3     (tb3)   20.302    5.575    3.641    0.000\n   .str4     (tb4)   23.640    6.723    3.516    0.000\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    lambda           74.843    1.708   43.824    0.000\n    thetaw          496.802   12.485   39.790    0.000\n    thetab           61.697    9.488    6.502    0.000\n    omega_w           0.919    0.004  232.932    0.000\n    omega_b           0.911    0.012   75.486    0.000\n\n\n\nI risultati sono simili a quelli riportati da Alphen et al. (2022), sebbene nel loro caso sia stata utilizzata una diversa procedura di imputazione e il modello sia stato adattato con Mplus.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Affidabilità longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/12_temp_reliability.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/12_temp_reliability.html#informazioni-sullambiente-di-sviluppo",
    "title": "56  Affidabilità longitudinale",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lme4_1.1-35.5     Matrix_1.7-0      ggokabeito_0.1.0  viridis_0.6.5    \n [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n[13] lavaan_0.6-18     psych_2.4.6.26    scales_1.3.0      markdown_1.13    \n[17] knitr_1.48        lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n[21] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n[25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.16.0  jsonlite_1.8.9     magrittr_2.0.3    \n  [4] TH.data_1.1-2      estimability_1.5.1 farver_2.1.2      \n  [7] nloptr_2.1.1       rmarkdown_2.28     vctrs_0.6.5       \n [10] minqa_1.2.8        base64enc_0.1-3    rstatix_0.7.2     \n [13] htmltools_0.5.8.1  broom_1.0.6        Formula_1.2-5     \n [16] htmlwidgets_1.6.4  plyr_1.8.9         sandwich_3.1-1    \n [19] emmeans_1.10.4     zoo_1.8-12         uuid_1.2-1        \n [22] igraph_2.0.3       mime_0.12          lifecycle_1.0.4   \n [25] pkgconfig_2.0.3    R6_2.5.1           fastmap_1.2.0     \n [28] shiny_1.9.1        digest_0.6.37      OpenMx_2.21.12    \n [31] fdrtool_1.2.18     colorspace_2.1-1   rprojroot_2.0.4   \n [34] Hmisc_5.1-3        fansi_1.0.6        timechange_0.3.0  \n [37] abind_1.4-8        compiler_4.4.1     withr_3.0.1       \n [40] glasso_1.11        htmlTable_2.4.3    backports_1.5.0   \n [43] carData_3.0-5      ggsignif_0.6.4     MASS_7.3-61       \n [46] corpcor_1.6.10     gtools_3.9.5       tools_4.4.1       \n [49] pbivnorm_0.6.0     foreign_0.8-87     zip_2.3.1         \n [52] httpuv_1.6.15      nnet_7.3-19        glue_1.7.0        \n [55] quadprog_1.5-8     promises_1.3.0     nlme_3.1-166      \n [58] lisrelToR_0.3      grid_4.4.1         pbdZMQ_0.3-13     \n [61] checkmate_2.3.2    cluster_2.1.6      reshape2_1.4.4    \n [64] generics_0.1.3     gtable_0.3.5       tzdb_0.4.0        \n [67] data.table_1.16.0  hms_1.1.3          car_3.1-2         \n [70] utf8_1.2.4         sem_3.1-16         pillar_1.9.0      \n [73] IRdisplay_1.1      rockchalk_1.8.157  later_1.3.2       \n [76] splines_4.4.1      lattice_0.22-6     survival_3.7-0    \n [79] kutils_1.73        tidyselect_1.2.1   miniUI_0.1.1.1    \n [82] pbapply_1.7-2      stats4_4.4.1       xfun_0.47         \n [85] qgraph_1.9.8       arm_1.14-4         stringi_1.8.4     \n [88] pacman_0.5.1       boot_1.3-31        evaluate_1.0.0    \n [91] codetools_0.2-20   mi_1.1             cli_3.6.3         \n [94] RcppParallel_5.1.9 IRkernel_1.3.2     rpart_4.1.23      \n [97] xtable_1.8-4       repr_1.1.7         munsell_0.5.1     \n[100] Rcpp_1.0.13        coda_0.19-4.1      png_0.1-8         \n[103] XML_3.99-0.17      parallel_4.4.1     jpeg_0.1-10       \n[106] mvtnorm_1.3-1      openxlsx_4.2.7.1   crayon_1.5.3      \n[109] rlang_1.1.4        multcomp_1.4-26    mnormt_2.1.1      \n\n\n\n\n\n\nAlphen, T. van, Jak, S., Jansen in de Wal, J., Schuitema, J., & Peetsma, T. (2022). Determining reliability of daily measures: An illustration with data on teacher stress. Applied Measurement in Education, 35(1), 63–79.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Affidabilità longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/sem/13_esem.html",
    "href": "chapters/sem/13_esem.html",
    "title": "57  Exploratory structural equation modelling",
    "section": "",
    "text": "57.1 Introduzione\nNumerosi ricercatori hanno riportato che l’ESEM supera il CFA in diversi campi della psicologia, tra cui la psicologia clinica, della salute, industriale (Marsh et al., 2014), e quella educativa (Alamer, Al Khateeb, & Jeno, 2023; Alamer & Marsh, 2022; Alamer, Morin, et al., 2023; Guay et al., 2015; Kruk et al., 2023). Tuttavia, in alcuni contesti empirici, è necessario introdurre restrizioni al modello ESEM completamente libero. Questo ha portato allo sviluppo del set-ESEM (Marsh et al., 2020), che rappresenta un equilibrio tra ESEM e CFA in un unico quadro analitico. In questo capitolo introdurremo la tecnica della Exploratory structural equation modelling (ESEM) seguendo il tutorial proposto da Marsh & Alamer (2024).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Exploratory structural equation modelling</span>"
    ]
  },
  {
    "objectID": "chapters/sem/13_esem.html#efa-cfa-esem-e-set-esem",
    "href": "chapters/sem/13_esem.html#efa-cfa-esem-e-set-esem",
    "title": "57  Exploratory structural equation modelling",
    "section": "57.2 EFA , CFA , ESEM e SET-­ESEM",
    "text": "57.2 EFA , CFA , ESEM e SET-­ESEM\nL’analisi fattoriale esplorativa (EFA) e l’analisi fattoriale confermativa (CFA) rappresentano i due principali approcci per studiare le strutture latenti. L’EFA risale ai lavori di Spearman (1904) e Thurstone (1935, 1947), che inizialmente la chiamavano semplicemente “analisi fattoriale.” Solo con l’arrivo della CFA si iniziò a distinguere tra EFA (esplorativa) e CFA (confermativa). La CFA, infatti, è divenuta popolare perché consente di valutare l’adattamento del modello, trattare i dati mancanti con metodi avanzati e confrontare modelli teorici alternativi. Tuttavia, uno dei limiti principali della CFA è la rigida ipotesi che ogni item carichi solo su un singolo fattore, escludendo possibili carichi incrociati.\nQuesto limite della CFA ha portato allo sviluppo dell’ESEM (Exploratory Structural Equation Modeling) da parte di Asparouhov e Muthén (2009), che combina i vantaggi della SEM con la flessibilità dell’EFA. L’ESEM consente di integrare SEM con carichi incrociati tra i fattori, adattandosi così meglio ai dati psicometrici complessi. Questo approccio si è rivelato superiore alla CFA in numerosi studi per la capacità di migliorare l’adattamento del modello e sostenere la validità discriminante tra i fattori. Recentemente, Gegenfurtner (2022) ha confermato con una meta-analisi su 158 studi che l’ESEM supera la CFA per la bontà di adattamento e la validità discriminante.\nTuttavia, in alcune situazioni, l’ESEM può risultare meno parsimonioso della CFA, e per questo è stato sviluppato il set-ESEM (Marsh et al., 2020). Il set-ESEM consente di bilanciare la flessibilità dell’ESEM con la struttura più rigorosa della CFA, utilizzando tecniche di rotazione come la rotazione geomin o il target rotation per limitare i carichi incrociati non essenziali.\nL’ESEM è molto diffuso nella psicometria corrente, dove ha mostrato risultati superiori rispetto alla CFA per comprendere le strutture fattoriali complesse. L’ESEM consente una rappresentazione accurata delle correlazioni e delle regressioni tra fattori, utilizzando tutti i dati disponibili a livello degli indicatori.\n\n\n\n\n\n\nFigura 57.1",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Exploratory structural equation modelling</span>"
    ]
  },
  {
    "objectID": "chapters/sem/13_esem.html#situazioni-dove-set-esem-è-preferibile-a-esem",
    "href": "chapters/sem/13_esem.html#situazioni-dove-set-esem-è-preferibile-a-esem",
    "title": "57  Exploratory structural equation modelling",
    "section": "57.3 Situazioni dove Set-ESEM è Preferibile a ESEM",
    "text": "57.3 Situazioni dove Set-ESEM è Preferibile a ESEM\nEsistono comunque situazioni in cui l’ESEM completo potrebbe non essere la scelta migliore per un’analisi. Questo accade quando un insieme di fattori e item dovrebbe allinearsi e distinguersi da altri insiemi non pertinenti. In questi casi, è possibile creare mini-set di ESEM all’interno di un modello più ampio, un approccio noto come set-ESEM (Marsh et al., 2020). Il set-ESEM rappresenta uno sviluppo recente nell’analisi fattoriale, ideato per raggiungere un compromesso ottimale tra CFA ed ESEM completo in termini di indici di adattamento del modello, parsimonia, struttura confermativa e un modello di misurazione ben definito. In alcune situazioni, il set-ESEM può risultare più adeguato dell’ESEM completo. Qui descriviamo due esempi con dati reali e analisi empiriche.\nIl primo riguarda il caso in cui un ricercatore raccoglie dati su item relativi a costrutti concettualmente distinti o appartenenti a teorie diverse. Ad esempio, consideriamo un dataset contenente item che misurano autonomia, competenza e relazionalità delle tre necessità psicologiche di base (Ryan & Deci, 2017) tramite la scala BPN-L2 (Alamer, 2022), e due costrutti, perseveranza nello sforzo e coerenza dell’interesse, derivati dalla teoria del grit (Duckworth et al., 2007) e misurati con la scala L2-grit (Alamer, 2021b). In questo caso, risulterebbe inappropriato stimare carichi incrociati tra item del grit e quelli delle tre necessità psicologiche, poiché ogni teoria attribuisce funzioni differenti ai propri costrutti e item: ad esempio, i fattori delle necessità psicologiche di base sono teorizzati come influenzati dal contesto sociale, mentre il grit è considerato un tratto stabile della personalità. In queste circostanze, il ricercatore potrebbe preferire due blocchi (o set) ESEM: uno per i tre fattori delle necessità psicologiche, con carichi incrociati tra loro ma non con gli item del grit, e un secondo set per i due fattori del grit senza carichi incrociati con le necessità psicologiche. Questo approccio consente un modello più parsimonioso, bilanciando esame teorico e test empirico.\nLa seconda situazione in cui l’uso del set-ESEM è consigliato riguarda il caso in cui i dati provengano da costrutti rilevanti raccolti in più momenti temporali. Qui, i carichi incrociati dovrebbero essere stimati solo tra i costrutti nidificati nello stesso momento temporale. Per esempio, consideriamo un dataset che contiene la passione armoniosa, la passione ossessiva e l’autonomia misurate in due tempi distinti. Questi costrutti sono concettualmente correlati, per cui i carichi incrociati sono ragionevoli all’interno dello stesso tempo. Tuttavia, permettere che i carichi incrociati tra item del primo tempo influenzino quelli del secondo sarebbe teoricamente inappropriato e tecnicamente poco desiderabile, rischiando di creare effetti di confondimento. Inoltre, è consuetudine nelle analisi longitudinali SEM correlare i residui degli stessi item nel tempo (Marsh & Hau, 1996). L’applicazione del set-ESEM in tali casi consente di mantenere la flessibilità dell’ESEM con la rigore e parsimonia della CFA.\nNelle sezioni successive, riportiamo due studi per illustrare questi scenari, dimostrando come il set-ESEM possa rappresentare un’alternativa preferibile al sistema restrittivo della CFA. Per quanto riguarda gli indici di affidabilità, si raccomanda che i ricercatori riportino il coefficiente omega (ω) per la composita affidabilità del modello. Essendo questa una dimostrazione metodologica, ci focalizziamo più sulle analisi che sulla discussione teorica.\nNei due esempi discussi da Marsh & Alamer (2024), i partecipanti erano studenti sauditi che imparano l’inglese come L2 in un’università pubblica saudita e hanno partecipato tramite un questionario online. Nel primo studio hanno partecipato 269 studenti, mentre nel secondo 389. Tutti gli studenti, di età compresa tra i 18 e i 20 anni (M = 18.5), parlavano l’arabo come lingua madre.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Exploratory structural equation modelling</span>"
    ]
  },
  {
    "objectID": "chapters/sem/13_esem.html#studio-1",
    "href": "chapters/sem/13_esem.html#studio-1",
    "title": "57  Exploratory structural equation modelling",
    "section": "57.4 Studio 1",
    "text": "57.4 Studio 1\nLo Studio 1 affronta il primo scenario in cui un ricercatore potrebbe avere costrutti derivati da teorie o livelli concettuali differenti. Nel nostro caso, abbiamo due blocchi teorici.\n\nIl primo include tre costrutti derivati dalla teoria delle necessità psicologiche di base (BPN): autonomia, competenza e relazionalità (vedi Noels, 2023; Ryan & Deci, 2017). In particolare, la misura è costituita dalla percezione degli studenti sull’insegnante di inglese come promotore di questi tre fattori BPN.\nIl secondo blocco rappresenta diversi risultati associati alla teoria BPN, come senso di significato, senso di sicurezza e motivazione intrinseca. La ricerca suggerisce che, quando gli studenti percepiscono il proprio insegnante come un promotore delle BPN, si osserva una maggiore motivazione intrinseca, senso di significato e sicurezza (Alamer, 2022; Alamer & Al Khateeb, 2023; Alamer, Al Khateeb, & Jeno, 2023; Guay et al., 2015; Noels, 2023).\n\nIn questo studio, ai partecipanti è stato chiesto di indicare la propria percezione dell’insegnante come promotore delle tre BPN, mentre per il senso di significato, sicurezza e motivazione intrinseca i partecipanti hanno risposto sulla propria percezione di sé. Questa distinzione giustifica l’assenza di carichi incrociati tra variabili che misurano aspetti di “self-sense” (ovvero, senso di significato, sicurezza e motivazione intrinseca), poiché ogni blocco ha un significato concettuale distinto. È difficile giustificare concettualmente carichi incrociati tra costrutti che riguardano l’insegnante e quelli legati alla percezione di sé. Infine, l’intenzione di abbandonare il corso viene usata come variabile di esito nel modello. I due modelli alternativi sono presentati nella Figura seguente.\n\n\n\n\n\n\nFigura 57.2\n\n\n\nStrumenti di Misura\nÈ stata utilizzata la scala BPN-L2 (Alamer, 2022), con tre item per ciascun costrutto. Esempi di item includono: “Il mio insegnante ci permette di scegliere i compiti di apprendimento linguistico” (autonomia; ω = .75), “Il mio insegnante ci dice che siamo capaci di imparare l’inglese” (competenza; ω = .75) e “Il mio insegnante di inglese è amichevole e cordiale con noi” (relazionalità; ω = .91).\nPer la motivazione intrinseca, sono stati adottati tre item dalla scala SDT-L2 (Alamer, 2022), ad esempio: “Imparo l’inglese perché mi piace” (ω = .91). Tre item per il senso di sicurezza e tre per il significato sono stati presi da Dörnyei e Ushioda (2021) e da Dörnyei e Ryan (2015). Un esempio di item per il senso di sicurezza è “Credo nelle mie capacità di fare bene nel corso” (ω = .74) e per il significato “So perché mi sono iscritto a questo corso” (ω = .91).\nPer misurare l’intenzione di abbandonare il corso sono stati adottati cinque item di Lounsbury et al. (2004), come: “Non ho intenzione di continuare a studiare in questo settore” (ω = .90). Le misure sono basate su una scala Likert a cinque punti, da 1 (fortemente in disaccordo) a 5 (fortemente d’accordo).\nCodice lavaan per il modello ESEM\nImportiamo i dati e esaminiamo le variabili.\n\nstudy1_dat &lt;- rio::import(\n    here::here(\n        \"data\", \"marsh_alamer\", \"Study_1_data.csv\"\n    )\n)\n\nglimpse(study1_dat)\n\nRows: 269\nColumns: 23\n$ Intent_to_withdraw1 &lt;int&gt; 2, 2, 4, 4, 5, 1, 4, 2, 1, 5, 2, 4, 5, 5, 3, 5~\n$ Intent_to_withdraw2 &lt;int&gt; 2, 3, 4, 5, 5, 1, 3, 4, 1, 5, 2, 5, 5, 5, 4, 5~\n$ Intent_to_withdraw3 &lt;int&gt; 1, 2, 1, 4, 5, 1, 3, 1, 1, 5, 2, 3, 4, 4, 3, 4~\n$ Intent_to_withdraw4 &lt;int&gt; 2, 2, 3, 5, 5, 1, 3, 2, 2, 4, 2, 4, 4, 4, 5, 4~\n$ Intent_to_withdraw5 &lt;int&gt; 3, 3, 4, 4, 4, 1, 4, 2, 1, 5, 2, 4, 4, 4, 4, 4~\n$ T_relatedness1      &lt;int&gt; 4, 4, 2, 2, 2, 5, 1, 4, 4, 4, 4, 4, 1, 1, 1, 1~\n$ T_relatedness2      &lt;int&gt; 3, 4, 2, 2, 1, 5, 1, 4, 4, 2, 4, 3, 1, 1, 1, 1~\n$ T_relatedness3      &lt;int&gt; 3, 5, 2, 2, 2, 5, 1, 4, 4, 4, 4, 4, 1, 1, 3, 1~\n$ T_competence1       &lt;int&gt; 4, 5, 2, 4, 3, 5, 5, 4, 2, 4, 3, 4, 2, 2, 3, 2~\n$ T_competence2       &lt;int&gt; 4, 4, 2, 3, 3, 5, 5, 5, 2, 4, 4, 4, 2, 2, 4, 2~\n$ T_competence3       &lt;int&gt; 4, 5, 1, 4, 3, 5, 4, 4, 2, 4, 3, 3, 2, 2, 4, 2~\n$ T_autonomy1         &lt;int&gt; 4, 5, 2, 2, 4, 5, 3, 4, 4, 4, 4, 4, 2, 2, 2, 2~\n$ T_autonomy2         &lt;int&gt; 3, 5, 2, 3, 3, 5, 1, 4, 4, 4, 4, 3, 3, 3, 2, 3~\n$ T_autonomy3         &lt;int&gt; 3, 5, 2, 2, 3, 5, 1, 4, 4, 3, 3, 3, 1, 1, 4, 1~\n$ S_meaning1          &lt;int&gt; 4, 5, 4, 1, 2, 5, 3, 4, 4, 3, 5, 2, 5, 5, 2, 5~\n$ S_meaning2          &lt;int&gt; 3, 5, 4, 2, 2, 5, 1, 4, 4, 4, 5, 2, 5, 5, 1, 5~\n$ S_meaning3          &lt;int&gt; 5, 4, 4, 2, 2, 5, 1, 4, 4, 4, 5, 2, 5, 5, 2, 5~\n$ S_confidence1       &lt;int&gt; 4, 5, 5, 4, 4, 5, 5, 4, 2, 5, 5, 3, 5, 5, 4, 5~\n$ S_confidence2       &lt;int&gt; 4, 5, 5, 4, 4, 5, 5, 4, 2, 5, 5, 4, 5, 5, 4, 5~\n$ S_confidence3       &lt;int&gt; 4, 5, 5, 5, 5, 5, 5, 4, 1, 5, 5, 3, 5, 5, 4, 5~\n$ S_Intrinsic1        &lt;int&gt; 4, 4, 2, 2, 2, 5, 3, 4, 2, 5, 3, 3, 2, 2, 1, 2~\n$ S_Intrinsic2        &lt;int&gt; 4, 5, 2, 3, 3, 5, 4, 4, 2, 5, 5, 4, 1, 1, 2, 1~\n$ S_Intrinsic3        &lt;int&gt; 4, 5, 1, 2, 3, 5, 4, 4, 2, 5, 4, 4, 1, 1, 2, 1~\n\n\nDefiniamo il modello ESEM.\n\nesem1 &lt;- '\n\n  # the long format (more flexible) each factor is defined separately\n  efa(\"teacher\")*Teacher_autonomy =~ T_autonomy1 + T_autonomy2 + T_autonomy3 + T_competence1 + T_competence2 + T_competence3 + T_relatedness1 + T_relatedness2 + T_relatedness3\n  efa(\"teacher\")*Teacher_competence =~ T_autonomy1 + T_autonomy2 + T_autonomy3 + T_competence1 + T_competence2 + T_competence3 + T_relatedness1 + T_relatedness2 + T_relatedness3\n  efa(\"teacher\")*Teacher_relatedness =~ T_autonomy1 + T_autonomy2 + T_autonomy3 + T_competence1 + T_competence2 + T_competence3 + T_relatedness1 + T_relatedness2 + T_relatedness3\n\n  # the short format (less flexible) all factors defined in one instance (remove ”##” if you want to use this)\n  # efa(\"teacher\")*Teacher_autonomy +\n  # efa(\"teacher\")*Teacher_competence +\n  # efa(\"teacher\")*Teacher_relatedness =~ T_autonomy1 + T_autonomy2 + T_autonomy3 + T_competence1 + T_competence2 + T_competence3 + T_relatedness1 + T_relatedness2 + T_relatedness3\n\n  # defining the second ESEM block\n  efa(\"self\")*Self_Meaning =~ S_meaning1 + S_meaning2 + S_meaning3 + S_confidence1 + S_confidence2 + S_confidence3 + S_Intrinsic1 + S_Intrinsic2+S_Intrinsic3\n  efa(\"self\")*Self_Confidence =~ S_meaning1 + S_meaning2 + S_meaning3 + S_confidence1 + S_confidence2 + S_confidence3 + S_Intrinsic1 + S_Intrinsic2+S_Intrinsic3\n  efa(\"self\")*Intrinsic_Motivation =~ S_meaning1 + S_meaning2 + S_meaning3 + S_confidence1 + S_confidence2 + S_confidence3 + S_Intrinsic1 + S_Intrinsic2+S_Intrinsic3\n\n  # defining the outcome variable\n  Intent_to_Quit =~ Intent_to_withdraw1 + Intent_to_withdraw2 + Intent_to_withdraw3 + Intent_to_withdraw4 + Intent_to_withdraw5\n\n  # defining the structural part\n  Self_Meaning ~ Teacher_autonomy + Teacher_competence + Teacher_relatedness\n  Self_Confidence ~ Teacher_autonomy + Teacher_competence + Teacher_relatedness\n  Intrinsic_Motivation ~ Teacher_autonomy + Teacher_competence + Teacher_relatedness\n  Intent_to_Quit ~ Self_Meaning + Self_Confidence + Intrinsic_Motivation +\n                   Teacher_autonomy + Teacher_competence + Teacher_relatedness\n'\n\n\nout1 &lt;- sem(\n    model = esem1,\n    data = study1_dat,\n    estimator = \"MLR\", # verbose = TRUE, test = \"yuan.bentler\",\n    rotation = \"geomin\",\n    rotation.args = list(geomin.epsilon = 0.005)\n)\n\n\nsemPlot::semPaths(\n    out1,\n    what = \"col\", whatLabels = \"no\", style = \"mx\",\n    layout = \"tree\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 5, sizeMan2 = 4\n)\n\n\n\n\n\n\n\n\n\nsummary(out1, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 67 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                       103\n  Row rank of the constraints matrix                12\n\n  Rotation method                       GEOMIN OBLIQUE\n  Geomin epsilon                                 0.005\n  Rotation algorithm (rstarts)                GPA (30)\n  Standardized metric                             TRUE\n  Row weights                                     None\n\n  Number of observations                           269\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               396.932     367.121\n  Degrees of freedom                               185         185\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.081\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              4271.294    3518.139\n  Degrees of freedom                               253         253\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.214\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.947       0.944\n  Tucker-Lewis Index (TLI)                       0.928       0.924\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.950\n  Robust Tucker-Lewis Index (TLI)                            0.932\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7894.627   -7894.627\n  Scaling correction factor                                  1.378\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -7696.161   -7696.161\n  Scaling correction factor                                  1.179\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               15971.254   15971.254\n  Bayesian (BIC)                             16298.373   16298.373\n  Sample-size adjusted Bayesian (SABIC)      16009.844   16009.844\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.065       0.060\n  90 Percent confidence interval - lower         0.056       0.052\n  90 Percent confidence interval - upper         0.074       0.069\n  P-value H_0: RMSEA &lt;= 0.050                    0.003       0.025\n  P-value H_0: RMSEA &gt;= 0.080                    0.003       0.000\n                                                                  \n  Robust RMSEA                                               0.063\n  90 Percent confidence interval - lower                     0.053\n  90 Percent confidence interval - upper                     0.072\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.013\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.001\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.045       0.045\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                                 Estimate  Std.Err  z-value  P(&gt;|z|)\n  Teacher_autonomy =~ teacher                                       \n    T_autonomy1                     0.787    0.128    6.159    0.000\n    T_autonomy2                     1.156    0.077   14.918    0.000\n    T_autonomy3                     0.971    0.104    9.323    0.000\n    T_competence1                  -0.058    0.072   -0.806    0.420\n    T_competence2                   0.288    0.164    1.753    0.080\n    T_competence3                   0.123    0.219    0.564    0.573\n    T_relatedness1                  0.423    0.149    2.845    0.004\n    T_relatedness2                 -0.011    0.029   -0.391    0.696\n    T_relatedness3                  0.332    0.246    1.348    0.178\n  Teacher_competence =~ teacher                                     \n    T_autonomy1                     0.261    0.119    2.184    0.029\n    T_autonomy2                    -0.035    0.039   -0.901    0.368\n    T_autonomy3                     0.087    0.085    1.022    0.307\n    T_competence1                   1.220    0.073   16.735    0.000\n    T_competence2                   0.943    0.147    6.433    0.000\n    T_competence3                   0.622    0.169    3.678    0.000\n    T_relatedness1                 -0.024    0.020   -1.247    0.212\n    T_relatedness2                  0.049    0.053    0.919    0.358\n    T_relatedness3                  0.061    0.151    0.404    0.686\n  Teacher_relatedness =~ teacher                                    \n    T_autonomy1                     0.042    0.061    0.684    0.494\n    T_autonomy2                    -0.048    0.066   -0.729    0.466\n    T_autonomy3                     0.078    0.104    0.752    0.452\n    T_competence1                   0.029    0.054    0.536    0.592\n    T_competence2                  -0.041    0.040   -1.043    0.297\n    T_competence3                   0.179    0.111    1.608    0.108\n    T_relatedness1                  0.832    0.156    5.346    0.000\n    T_relatedness2                  1.129    0.086   13.160    0.000\n    T_relatedness3                  0.316    0.198    1.593    0.111\n  Self_Meaning =~ self                                              \n    S_meaning1                      0.808    0.065   12.388    0.000\n    S_meaning2                      1.065    0.060   17.818    0.000\n    S_meaning3                      1.040    0.056   18.600    0.000\n    S_confidence1                  -0.028    0.040   -0.697    0.486\n    S_confidence2                   0.098    0.038    2.599    0.009\n    S_confidence3                  -0.016    0.017   -0.900    0.368\n    S_Intrinsic1                   -0.008    0.053   -0.157    0.875\n    S_Intrinsic2                   -0.002    0.044   -0.057    0.955\n    S_Intrinsic3                    0.009    0.037    0.231    0.818\n  Self_Confidence =~ self                                           \n    S_meaning1                      0.052    0.060    0.875    0.382\n    S_meaning2                     -0.027    0.027   -0.996    0.319\n    S_meaning3                     -0.002    0.026   -0.069    0.945\n    S_confidence1                   0.609    0.074    8.255    0.000\n    S_confidence2                   0.560    0.059    9.441    0.000\n    S_confidence3                   0.553    0.065    8.445    0.000\n    S_Intrinsic1                   -0.027    0.073   -0.374    0.708\n    S_Intrinsic2                    0.107    0.062    1.737    0.082\n    S_Intrinsic3                   -0.011    0.031   -0.354    0.723\n  Intrinsic_Motivation =~ self                                      \n    S_meaning1                      0.043    0.030    1.461    0.144\n    S_meaning2                     -0.011    0.016   -0.665    0.506\n    S_meaning3                     -0.014    0.016   -0.915    0.360\n    S_confidence1                  -0.026    0.029   -0.905    0.366\n    S_confidence2                  -0.004    0.010   -0.396    0.692\n    S_confidence3                   0.028    0.024    1.194    0.232\n    S_Intrinsic1                    0.449    0.047    9.469    0.000\n    S_Intrinsic2                    0.498    0.076    6.581    0.000\n    S_Intrinsic3                    0.634    0.078    8.120    0.000\n  Intent_to_Quit =~                                                 \n    Intnt_t_wthdr1                  1.000                           \n    Intnt_t_wthdr2                  0.946    0.033   28.953    0.000\n    Intnt_t_wthdr3                  1.017    0.031   32.987    0.000\n    Intnt_t_wthdr4                  0.683    0.074    9.170    0.000\n    Intnt_t_wthdr5                  0.665    0.053   12.648    0.000\n   Std.lv  Std.all\n                  \n    0.787    0.612\n    1.156    0.922\n    0.971    0.774\n   -0.058   -0.046\n    0.288    0.227\n    0.123    0.102\n    0.423    0.324\n   -0.011   -0.009\n    0.332    0.250\n                  \n    0.261    0.203\n   -0.035   -0.028\n    0.087    0.069\n    1.220    0.972\n    0.943    0.744\n    0.622    0.516\n   -0.024   -0.019\n    0.049    0.039\n    0.061    0.046\n                  \n    0.042    0.033\n   -0.048   -0.038\n    0.078    0.062\n    0.029    0.023\n   -0.041   -0.033\n    0.179    0.149\n    0.832    0.636\n    1.129    0.900\n    0.316    0.238\n                  \n    0.890    0.738\n    1.173    0.919\n    1.146    0.927\n   -0.031   -0.031\n    0.107    0.140\n   -0.017   -0.023\n   -0.009   -0.008\n   -0.003   -0.002\n    0.009    0.007\n                  \n    0.055    0.045\n   -0.028   -0.022\n   -0.002   -0.002\n    0.637    0.639\n    0.586    0.763\n    0.578    0.766\n   -0.028   -0.024\n    0.112    0.093\n   -0.012   -0.009\n                  \n    0.070    0.058\n   -0.018   -0.014\n   -0.023   -0.019\n   -0.043   -0.043\n   -0.006   -0.008\n    0.046    0.061\n    0.725    0.619\n    0.805    0.669\n    1.025    0.815\n                  \n    1.170    0.955\n    1.107    0.911\n    1.190    0.875\n    0.799    0.637\n    0.778    0.648\n\nRegressions:\n                          Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n  Self_Meaning ~                                                      \n    Teacher_autnmy          -0.040    0.139   -0.287    0.774   -0.036\n    Teacher_cmptnc           0.271    0.125    2.172    0.030    0.246\n    Teachr_rltdnss           0.287    0.120    2.387    0.017    0.260\n  Self_Confidence ~                                                   \n    Teacher_autnmy          -0.096    0.152   -0.630    0.529   -0.092\n    Teacher_cmptnc           0.299    0.146    2.048    0.041    0.286\n    Teachr_rltdnss          -0.306    0.159   -1.925    0.054   -0.293\n  Intrinsic_Motivation ~                                              \n    Teacher_autnmy           0.862    0.333    2.588    0.010    0.533\n    Teacher_cmptnc           0.217    0.198    1.096    0.273    0.134\n    Teachr_rltdnss           0.339    0.227    1.494    0.135    0.210\n  Intent_to_Quit ~                                                    \n    Self_Meaning            -0.123    0.078   -1.580    0.114   -0.115\n    Self_Confidenc           0.056    0.072    0.774    0.439    0.050\n    Intrinsc_Mtvtn           0.169    0.096    1.764    0.078    0.234\n    Teacher_autnmy          -0.790    0.211   -3.752    0.000   -0.676\n    Teacher_cmptnc           0.078    0.115    0.676    0.499    0.066\n    Teachr_rltdnss          -0.208    0.156   -1.337    0.181   -0.178\n  Std.all\n         \n   -0.036\n    0.246\n    0.260\n         \n   -0.092\n    0.286\n   -0.293\n         \n    0.533\n    0.134\n    0.210\n         \n   -0.115\n    0.050\n    0.234\n   -0.676\n    0.066\n   -0.178\n\nCovariances:\n                         Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n  Teacher_autonomy ~~                                                \n    Teacher_cmptnc          0.652    0.067    9.730    0.000    0.652\n    Teachr_rltdnss          0.659    0.064   10.287    0.000    0.659\n  Teacher_competence ~~                                              \n    Teachr_rltdnss          0.545    0.084    6.495    0.000    0.545\n .Self_Meaning ~~                                                    \n   .Self_Confidenc          0.195    0.083    2.346    0.019    0.195\n   .Intrinsc_Mtvtn          0.071    0.105    0.678    0.498    0.071\n .Self_Confidence ~~                                                 \n   .Intrinsc_Mtvtn          0.281    0.116    2.431    0.015    0.281\n  Std.all\n         \n    0.652\n    0.659\n         \n    0.545\n         \n    0.195\n    0.071\n         \n    0.281\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .T_autonomy1       0.642    0.085    7.529    0.000    0.642    0.388\n   .T_autonomy2       0.356    0.093    3.822    0.000    0.356    0.226\n   .T_autonomy3       0.399    0.062    6.482    0.000    0.399    0.254\n   .T_competence1     0.138    0.115    1.209    0.227    0.138    0.088\n   .T_competence2     0.338    0.072    4.723    0.000    0.338    0.210\n   .T_competence3     0.766    0.106    7.238    0.000    0.766    0.528\n   .T_relatedness1    0.409    0.080    5.113    0.000    0.409    0.239\n   .T_relatedness2    0.254    0.161    1.579    0.114    0.254    0.161\n   .T_relatedness3    1.363    0.119   11.408    0.000    1.363    0.773\n   .S_meaning1        0.600    0.084    7.099    0.000    0.600    0.412\n   .S_meaning2        0.274    0.089    3.093    0.002    0.274    0.168\n   .S_meaning3        0.231    0.052    4.420    0.000    0.231    0.151\n   .S_confidence1     0.593    0.136    4.354    0.000    0.593    0.598\n   .S_confidence2     0.218    0.053    4.138    0.000    0.218    0.370\n   .S_confidence3     0.232    0.046    4.988    0.000    0.232    0.407\n   .S_Intrinsic1      0.855    0.105    8.179    0.000    0.855    0.622\n   .S_Intrinsic2      0.772    0.101    7.655    0.000    0.772    0.534\n   .S_Intrinsic3      0.528    0.098    5.370    0.000    0.528    0.334\n   .Intnt_t_wthdr1    0.133    0.031    4.277    0.000    0.133    0.088\n   .Intnt_t_wthdr2    0.253    0.038    6.622    0.000    0.253    0.171\n   .Intnt_t_wthdr3    0.432    0.057    7.549    0.000    0.432    0.234\n   .Intnt_t_wthdr4    0.936    0.137    6.820    0.000    0.936    0.595\n   .Intnt_t_wthdr5    0.836    0.104    8.004    0.000    0.836    0.580\n    Teacher_autnmy    1.000                               1.000    1.000\n    Teacher_cmptnc    1.000                               1.000    1.000\n    Teachr_rltdnss    1.000                               1.000    1.000\n   .Self_Meaning      1.000                               0.824    0.824\n   .Self_Confidenc    1.000                               0.914    0.914\n   .Intrinsc_Mtvtn    1.000                               0.383    0.383\n   .Intent_to_Quit    0.786    0.093    8.477    0.000    0.574    0.574\n\n\n\n\ncfa1 &lt;- ' ## Specify the measurement model\n\n  # \"teacher\" factors\n  Teacher_autonomy    =~    T_autonomy1 +    T_autonomy2 +    T_autonomy3\n  Teacher_competence  =~  T_competence1 +  T_competence2 +  T_competence3\n  Teacher_relatedness =~ T_relatedness1 + T_relatedness2 + T_relatedness3\n\n  # \"self\" factors\n  Self_Meaning         =~    S_meaning1 +    S_meaning2 +    S_meaning3\n  Self_Confidence      =~ S_confidence1 + S_confidence2 + S_confidence3\n  Intrinsic_Motivation =~  S_Intrinsic1 +  S_Intrinsic2 +  S_Intrinsic3\n\n  # defining the outcome variable\n  Intent_to_Quit =~ Intent_to_withdraw1 + Intent_to_withdraw2 + Intent_to_withdraw3 + Intent_to_withdraw4 + Intent_to_withdraw5\n\n  # specify the structural model\n  Self_Meaning ~ Teacher_autonomy + Teacher_competence + Teacher_relatedness\n  Self_Confidence ~ Teacher_autonomy + Teacher_competence + Teacher_relatedness\n  Intrinsic_Motivation ~ Teacher_autonomy + Teacher_competence + Teacher_relatedness\n  Intent_to_Quit ~ Self_Meaning + Self_Confidence + Intrinsic_Motivation +\n                   Teacher_autonomy + Teacher_competence + Teacher_relatedness\n\n  # residual covariances among mediating factors in Block 2 (\"self\")\n  # (not automatically estimated due to being predictors as well,\n  #  but ESEM rotation allows their covariances to be nonzero)\n  Self_Meaning    ~~ Self_Confidence + Intrinsic_Motivation\n  Self_Confidence ~~ Intrinsic_Motivation\n'\n\n\nfit1 &lt;- sem(\n    model = cfa1, data = study1_dat,\n    estimator = \"MLR\", std.lv = TRUE\n)\n\n\nsemPlot::semPaths(\n    fit1,\n    what = \"col\", whatLabels = \"no\", style = \"mx\",\n    layout = \"tree\", nCharNodes = 7,\n    shapeMan = \"rectangle\", sizeMan = 6, sizeMan2 = 4\n)\n\n\n\n\n\n\n\n\n\nsummary(fit1, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 53 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        67\n\n  Number of observations                           269\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               459.107     419.040\n  Degrees of freedom                               209         209\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.096\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              4271.294    3518.139\n  Degrees of freedom                               253         253\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.214\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.938       0.936\n  Tucker-Lewis Index (TLI)                       0.925       0.922\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.942\n  Robust Tucker-Lewis Index (TLI)                            0.930\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7925.715   -7925.715\n  Scaling correction factor                                  1.439\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -7696.161   -7696.161\n  Scaling correction factor                                  1.179\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               15985.429   15985.429\n  Bayesian (BIC)                             16226.275   16226.275\n  Sample-size adjusted Bayesian (SABIC)      16013.842   16013.842\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.067       0.061\n  90 Percent confidence interval - lower         0.058       0.053\n  90 Percent confidence interval - upper         0.075       0.069\n  P-value H_0: RMSEA &lt;= 0.050                    0.001       0.013\n  P-value H_0: RMSEA &gt;= 0.080                    0.004       0.000\n                                                                  \n  Robust RMSEA                                               0.064\n  90 Percent confidence interval - lower                     0.055\n  90 Percent confidence interval - upper                     0.073\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.006\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.001\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.053       0.053\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                          Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n  Teacher_autonomy =~                                                 \n    T_autonomy1              1.016    0.059   17.270    0.000    1.016\n    T_autonomy2              1.066    0.050   21.119    0.000    1.066\n    T_autonomy3              1.093    0.046   23.977    0.000    1.093\n  Teacher_competence =~                                               \n    T_competence1            1.129    0.053   21.180    0.000    1.129\n    T_competence2            1.164    0.050   23.337    0.000    1.164\n    T_competence3            0.819    0.074   11.027    0.000    0.819\n  Teacher_relatedness =~                                              \n    T_relatedness1           1.191    0.048   24.632    0.000    1.191\n    T_relatedness2           1.053    0.052   20.124    0.000    1.053\n    T_relatedness3           0.628    0.089    7.015    0.000    0.628\n  Self_Meaning =~                                                     \n    S_meaning1               0.839    0.061   13.822    0.000    0.920\n    S_meaning2               1.061    0.060   17.603    0.000    1.163\n    S_meaning3               1.039    0.058   17.821    0.000    1.139\n  Self_Confidence =~                                                  \n    S_confidence1            0.582    0.071    8.139    0.000    0.617\n    S_confidence2            0.578    0.057   10.172    0.000    0.612\n    S_confidence3            0.538    0.066    8.172    0.000    0.570\n  Intrinsic_Motivation =~                                             \n    S_Intrinsic1             0.443    0.050    8.881    0.000    0.719\n    S_Intrinsic2             0.501    0.074    6.773    0.000    0.812\n    S_Intrinsic3             0.634    0.069    9.250    0.000    1.028\n  Intent_to_Quit =~                                                   \n    Intnt_t_wthdr1           0.893    0.050   17.918    0.000    1.171\n    Intnt_t_wthdr2           0.844    0.048   17.608    0.000    1.107\n    Intnt_t_wthdr3           0.907    0.053   17.267    0.000    1.189\n    Intnt_t_wthdr4           0.608    0.069    8.834    0.000    0.798\n    Intnt_t_wthdr5           0.593    0.055   10.801    0.000    0.778\n  Std.all\n         \n    0.790\n    0.850\n    0.871\n         \n    0.900\n    0.918\n    0.681\n         \n    0.911\n    0.840\n    0.473\n         \n    0.763\n    0.911\n    0.922\n         \n    0.619\n    0.798\n    0.756\n         \n    0.614\n    0.675\n    0.817\n         \n    0.956\n    0.910\n    0.875\n    0.636\n    0.648\n\nRegressions:\n                         Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n  Self_Meaning ~                                                     \n    Teacher_autnmy         -0.170    0.192   -0.882    0.378   -0.155\n    Teacher_cmptnc          0.335    0.160    2.096    0.036    0.305\n    Teachr_rltdnss          0.308    0.157    1.963    0.050    0.281\n  Self_Confidence ~                                                  \n    Teacher_autnmy         -0.080    0.158   -0.511    0.610   -0.076\n    Teacher_cmptnc          0.437    0.134    3.251    0.001    0.412\n    Teachr_rltdnss         -0.403    0.160   -2.526    0.012   -0.380\n  Intrinsic_Motivation ~                                             \n    Teacher_autnmy          0.886    0.283    3.131    0.002    0.546\n    Teacher_cmptnc          0.128    0.192    0.666    0.505    0.079\n    Teachr_rltdnss          0.342    0.213    1.606    0.108    0.211\n  Intent_to_Quit ~                                                   \n    Self_Meaning           -0.151    0.087   -1.731    0.083   -0.126\n    Self_Confidenc          0.051    0.085    0.596    0.551    0.041\n    Intrinsc_Mtvtn          0.197    0.109    1.804    0.071    0.244\n    Teacher_autnmy         -0.899    0.267   -3.368    0.001   -0.686\n    Teacher_cmptnc          0.155    0.143    1.083    0.279    0.118\n    Teachr_rltdnss         -0.259    0.191   -1.356    0.175   -0.197\n  Std.all\n         \n   -0.155\n    0.305\n    0.281\n         \n   -0.076\n    0.412\n   -0.380\n         \n    0.546\n    0.079\n    0.211\n         \n   -0.126\n    0.041\n    0.244\n   -0.686\n    0.118\n   -0.197\n\nCovariances:\n                        Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n .Self_Meaning ~~                                                   \n   .Self_Confidenc         0.241    0.081    2.970    0.003    0.241\n   .Intrinsc_Mtvtn         0.093    0.106    0.876    0.381    0.093\n .Self_Confidence ~~                                                \n   .Intrinsc_Mtvtn         0.334    0.119    2.801    0.005    0.334\n  Teacher_autonomy ~~                                               \n    Teacher_cmptnc         0.765    0.044   17.327    0.000    0.765\n    Teachr_rltdnss         0.798    0.042   18.935    0.000    0.798\n  Teacher_competence ~~                                             \n    Teachr_rltdnss         0.662    0.049   13.510    0.000    0.662\n  Std.all\n         \n    0.241\n    0.093\n         \n    0.334\n         \n    0.765\n    0.798\n         \n    0.662\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .T_autonomy1       0.623    0.086    7.243    0.000    0.623    0.376\n   .T_autonomy2       0.435    0.063    6.871    0.000    0.435    0.277\n   .T_autonomy3       0.380    0.062    6.153    0.000    0.380    0.241\n   .T_competence1     0.301    0.063    4.791    0.000    0.301    0.191\n   .T_competence2     0.254    0.058    4.380    0.000    0.254    0.158\n   .T_competence3     0.778    0.116    6.726    0.000    0.778    0.537\n   .T_relatedness1    0.291    0.077    3.752    0.000    0.291    0.170\n   .T_relatedness2    0.464    0.066    7.012    0.000    0.464    0.295\n   .T_relatedness3    1.369    0.121   11.283    0.000    1.369    0.777\n   .S_meaning1        0.608    0.088    6.906    0.000    0.608    0.418\n   .S_meaning2        0.278    0.089    3.115    0.002    0.278    0.171\n   .S_meaning3        0.230    0.052    4.434    0.000    0.230    0.151\n   .S_confidence1     0.612    0.139    4.413    0.000    0.612    0.616\n   .S_confidence2     0.214    0.055    3.893    0.000    0.214    0.363\n   .S_confidence3     0.244    0.049    4.982    0.000    0.244    0.429\n   .S_Intrinsic1      0.857    0.105    8.146    0.000    0.857    0.624\n   .S_Intrinsic2      0.786    0.108    7.292    0.000    0.786    0.544\n   .S_Intrinsic3      0.528    0.090    5.836    0.000    0.528    0.333\n   .Intnt_t_wthdr1    0.130    0.031    4.223    0.000    0.130    0.087\n   .Intnt_t_wthdr2    0.254    0.038    6.685    0.000    0.254    0.172\n   .Intnt_t_wthdr3    0.433    0.057    7.569    0.000    0.433    0.235\n   .Intnt_t_wthdr4    0.937    0.137    6.830    0.000    0.937    0.595\n   .Intnt_t_wthdr5    0.836    0.104    8.005    0.000    0.836    0.580\n    Teacher_autnmy    1.000                               1.000    1.000\n    Teacher_cmptnc    1.000                               1.000    1.000\n    Teachr_rltdnss    1.000                               1.000    1.000\n   .Self_Meaning      1.000                               0.832    0.832\n   .Self_Confidenc    1.000                               0.889    0.889\n   .Intrinsc_Mtvtn    1.000                               0.380    0.380\n   .Intent_to_Quit    1.000                               0.582    0.582\n\n\n\n\n# Extract model fit statistics from out1 and fit1\nfit_stats_out1 &lt;- fitMeasures(out1, c(\"chisq\", \"df\", \"rmsea\", \"rmsea.ci.lower\", \"rmsea.ci.upper\", \"cfi\", \"tli\"))\nfit_stats_fit1 &lt;- fitMeasures(fit1, c(\"chisq\", \"df\", \"rmsea\", \"rmsea.ci.lower\", \"rmsea.ci.upper\", \"cfi\", \"tli\"))\n\n# Create a tibble with the extracted fit statistics\nfit_table &lt;- tibble(\n    Model = c(\"CFA-based model\", \"Set-ESEM-based model\"),\n    chisq = c(fit_stats_fit1[\"chisq\"], fit_stats_out1[\"chisq\"]),\n    df = c(fit_stats_fit1[\"df\"], fit_stats_out1[\"df\"]),\n    RMSEA = c(fit_stats_fit1[\"rmsea\"], fit_stats_out1[\"rmsea\"]),\n    `RMSEA 90% CI` = c(\n        sprintf(\"(%.3f, %.3f)\", fit_stats_fit1[\"rmsea.ci.lower\"], fit_stats_fit1[\"rmsea.ci.upper\"]),\n        sprintf(\"(%.3f, %.3f)\", fit_stats_out1[\"rmsea.ci.lower\"], fit_stats_out1[\"rmsea.ci.upper\"])\n    ),\n    CFI = c(fit_stats_fit1[\"cfi\"], fit_stats_out1[\"cfi\"]),\n    TLI = c(fit_stats_fit1[\"tli\"], fit_stats_out1[\"tli\"])\n)\n\n# Convert numeric columns to formatted strings with three decimal places\nfit_table &lt;- fit_table %&gt;%\n    mutate(\n        across(where(is.numeric), ~ sprintf(\"%.3f\", .)),\n        chisq = sprintf(\"%.3f\", as.numeric(chisq)), # Ensure chisq is formatted correctly\n        df = as.character(df) # Convert df to character for consistent formatting\n    )\n\n# Calculate column widths for alignment\ncol_widths &lt;- fit_table %&gt;%\n    summarise(across(everything(), ~ max(nchar(.), na.rm = TRUE)))\n\n# Create text-based table output\nheader &lt;- paste(\n    str_pad(\"Model\", col_widths$Model, side = \"right\"),\n    str_pad(\"chisq\", col_widths$chisq, side = \"right\"),\n    str_pad(\"df\", col_widths$df, side = \"right\"),\n    str_pad(\"RMSEA\", col_widths$RMSEA, side = \"right\"),\n    str_pad(\"RMSEA 90% CI\", col_widths$`RMSEA 90% CI`, side = \"right\"),\n    str_pad(\"CFI\", col_widths$CFI, side = \"right\"),\n    str_pad(\"TLI\", col_widths$TLI, side = \"right\"),\n    sep = \" | \"\n)\nseparator &lt;- strrep(\"-\", nchar(header))\n\n# Print header and separator\ncat(header, \"\\n\")\ncat(separator, \"\\n\")\n\n# Print each row formatted with aligned columns\nfit_table %&gt;%\n    mutate(\n        Model = str_pad(Model, col_widths$Model, side = \"right\"),\n        chisq = str_pad(chisq, col_widths$chisq, side = \"right\"),\n        df = str_pad(df, col_widths$df, side = \"right\"),\n        RMSEA = str_pad(RMSEA, col_widths$RMSEA, side = \"right\"),\n        `RMSEA 90% CI` = str_pad(`RMSEA 90% CI`, col_widths$`RMSEA 90% CI`, side = \"right\"),\n        CFI = str_pad(CFI, col_widths$CFI, side = \"right\"),\n        TLI = str_pad(TLI, col_widths$TLI, side = \"right\")\n    ) %&gt;%\n    rowwise() %&gt;%\n    mutate(row_text = paste(Model, chisq, df, RMSEA, `RMSEA 90% CI`, CFI, TLI, sep = \" | \")) %&gt;%\n    pull(row_text) %&gt;%\n    cat(sep = \"\\n\")\n\nModel                | chisq   | df      | RMSEA | RMSEA 90% CI   | CFI   | TLI   \n--------------------------------------------------------------------------------- \nCFA-based model      | 459.107 | 209.000 | 0.067 | (0.058, 0.075) | 0.938 | 0.925\nSet-ESEM-based model | 396.932 | 185.000 | 0.065 | (0.056, 0.074) | 0.947 | 0.928\n\n\nPer stimare i modelli, Marsh & Alamer (2024) utilizzano la versione robusta della massima verosimiglianza (MLR). Per valutare la qualità dei modelli, Marsh & Alamer (2024) considerano il chi-quadro robusto (χ²) con i relativi gradi di libertà e valore p (Yuan & Bentler, 2000), oltre che il Comparative Fit Index (CFI), il TLI e il RMSEA con il suo intervallo di confidenza al 90%. I valori di CFI, TLI e RMSEA riportati nei due esempi sono quelli nella versione robusta.\nI risultati in Tabella precedente indicano che sia il modello strutturale basato su CFA che quello basato su set-ESEM hanno un buon adattamento ai dati. Nel loro tutorial, Marsh & Alamer (2024) si focalizziano sulle differenze nelle relazioni strutturali tra i due modelli, e non approfondiscono il modello di misura.\nLa Tabella 2 dell’articolo di Marsh & Alamer (2024) mostra i coefficienti di percorso per entrambi i modelli. Sebbene entrambi i modelli mostrino un adattamento accettabile, il modello strutturale set-ESEM si adatta meglio ai dati (+0.01 in TLI/CFI). Inoltre, l’Akaike Information Criterion (AIC), il BIC e il BIC corretto per la dimensione del campione nel modello CFA sono rispettivamente 15985.43, 16226.27 e 16013.84, mentre nel modello ESEM sono 15971.25, 16298.37 e 16009.84. I valori più bassi di AIC e BIC corretti per campione nel modello ESEM indicano un miglior adattamento rispetto al modello CFA.\nInoltre, le correlazioni tra le variabili latenti esogene tendono a essere sovrastimate nel modello CFA (Shao et al., 2022). Ad esempio, la correlazione tra Autonomia_Insegnante e Relazionalità_Insegnante è di .80 nel modello CFA ma di .51 nel modello set-ESEM (Δr = .29). Ciò può influenzare la stima dei parametri a causa della collinearità derivante da una specificazione non ottimale del modello di misura.\nAlcuni percorsi diventano significativi nel modello set-ESEM, come il percorso Competenza_Insegnante → Motivazione_Intrinseca, che non è significativo nel CFA (β = .08, p = .51) ma diventa significativo nel set-ESEM (β = .19, p = .03). Un’altra osservazione riguarda il percorso Relazionalità_Insegnante → Intenzione_di_Ritiro, non significativo nel modello CFA (β = −.20, p = .18) ma significativo nel set-ESEM (β = −.28, p = .01).\nIn sintesi, ciascun modello può portare a conclusioni diverse sulle relazioni tra variabili latenti. Dato che il set-ESEM si adatta meglio ai dati, i coefficienti di percorso ottenuti possono essere interpretati come più affidabili rispetto al modello CFA.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Exploratory structural equation modelling</span>"
    ]
  },
  {
    "objectID": "chapters/sem/13_esem.html#studio-2-utilizzo-del-set-esem-per-valutare-modelli-longitudinali",
    "href": "chapters/sem/13_esem.html#studio-2-utilizzo-del-set-esem-per-valutare-modelli-longitudinali",
    "title": "57  Exploratory structural equation modelling",
    "section": "57.5 Studio 2: Utilizzo del set-ESEM per valutare modelli longitudinali",
    "text": "57.5 Studio 2: Utilizzo del set-ESEM per valutare modelli longitudinali\nLo Studio 2 di Marsh & Alamer (2024) illustra un secondo scenario, in cui il ricercatore dispone di costrutti raccolti in momenti temporali differenti. In questo esempio, sono stati misurati tre costrutti in due tempi diversi. I primi due costrutti, autonomia e motivazione intrinseca, sono stati descritti nello Studio 1 e derivano dalla teoria dell’autodeterminazione (Ryan & Deci, 2017). Il terzo costrutto è l’autoefficacia, che riflette la convinzione degli studenti circa la propria capacità di ottenere risultati desiderati e prevenire quelli dannosi (Woodrow, 2006). La ricerca suggerisce che questi tre fattori (autonomia, motivazione intrinseca e autoefficacia) agiscono collettivamente come motivazioni parallele che favoriscono risultati positivi (Alamer et al., 2023; Noels, 2023; Ryan & Deci, 2017).\nUn modello che stima i costrutti al tempo 1 per prevedere i corrispondenti al tempo 2 (controllando la stabilità della misura nel tempo) e che valuta anche il loro effetto sulla variabile di esito può essere meglio analizzato con il set-ESEM piuttosto che con il full-ESEM (Marsh et al., 2020). Nei modelli longitudinali SEM, è spesso necessario correlare le unicità dello stesso item nel tempo (Marsh & Hau, 1996). Inoltre, il ricercatore può voler applicare l’invarianza di misura per garantire la stabilità della misura nel tempo, imponendo vincoli di uguaglianza longitudinale sui carichi fattoriali, possibile con il codice in formato long per il set-ESEM in lavaan.\nEcco un esempio di vincoli di uguaglianza sui quattro item di self-confidence nei due tempi. Le etichette a, b, c* e d* indicano vincoli di uguaglianza sui carichi primari: lo stesso item ha la stessa etichetta nei due tempi. Parametri con la stessa etichetta sono stimati come uguali. Seguendo la prassi CFA per l’invarianza di misura, bisogna impostare la scala latente per un gruppo di riferimento o occasione (qui, il tempo 1), lasciando liberi i restanti. Il valore mancante, NA*, libera la varianza del fattore al tempo 2. In lavaan, per preservare i vincoli di uguaglianza tra blocchi in una soluzione ruotata, è necessario equare tutti i carichi, affinché il blocco del tempo 2 rispecchi quello del tempo 1. Sebbene vincoli di parziale invarianza siano più realistici, essi non sarebbero mantenuti in una soluzione ruotata.\nEcco un esempio di sintassi per gli item di self-confidence:\nefa(\"time1\")*SelfConfidenceT1 =~ a*SelfConf1T1 + b*SelfConf2T1 +\nc*SelfConf3T1 + d*SelfConf4T1 + e*Intr1T1 + f*Intr2T1 + g*Intr3T1 +\nh*Auton1T1 + i*Auton2T1 + j*Auton3T1 + k*Auton4T1\nefa(\"time2\")*SelfConfidenceT2 =~ a*SelfConf1T2 + b*SelfConf2T2 +\nc*SelfConf3T2 + d*SelfConf4T2 + e*Intr1T2 + f*Intr2T2 + g*Intr3T2 +\nh*Auton1T2 + i*Auton2T2 + j*Auton3T2 + k*Auton4T2\n## liberazione della varianza del fattore al Tempo 2\nSelfConfidenceT2 ~~ NA*SelfConfidenceT2\nL’utilizzo dell’invarianza di misura nel set-ESEM permette di ridurre la soluzione a un singolo blocco ESEM, più parsimonioso e spesso vantaggioso per l’analisi longitudinale. Tuttavia, per mantenere il focus sulle applicazioni standard del CFA e del set-ESEM, riportiamo solo gli indici di adattamento dei modelli con invarianza di misura senza discutere i coefficienti di percorso. Forniamo comunque la sintassi R nel repository OSF per consentire ai lettori di riprodurre i risultati completi.\nPer testare un modello longitudinale set-ESEM, Marsh & Alamer (2024) stimano solo i cross-loading tra i fattori nello stesso tempo. Ad esempio, gli item del tempo 1 per autonomia, motivazione intrinseca e autoefficacia hanno cross-loading sui fattori del tempo 1, ma non su quelli del tempo 2. Allo stesso modo, gli item del tempo 2 hanno cross-loading sui fattori del tempo 2, ma non su quelli del tempo 1. Si noti che le correlazioni tra le unicità dello stesso item nei due tempi sono stimate ma non visualizzate in figura per semplicità (solo un esempio di correlazione tra unicità è mostrato tra Aut1 e Aut1T2).\nMisure\nPer valutare l’autonomia, sono stati utilizzati quattro item della scala BPN-L2 già descritta in precedenza. Un esempio di item è stato presentato nello Studio 1. La motivazione intrinseca è stata misurata con tre item della scala SDT-L2 (Alamer, 2022). La self-confidence (fiducia in sé stessi) è stata valutata tramite quattro item adottati da Dörnyei e Ushioda (2021; vedi anche Dörnyei & Ryan, 2015), come descritto nello Studio 1. Le misurazioni si basano su una scala Likert a cinque punti, con risposte da 1 (fortemente in disaccordo) a 5 (fortemente d’accordo).\nIl livello di competenza linguistica è stato misurato tramite un test di collocamento in inglese come seconda lingua. Il test include item sulle quattro abilità principali: vocabolario, grammatica, lettura e scrittura. È stato sviluppato consultando il corso di lingua Unlock, ideato dall’Università di Cambridge (Ostrowska et al., 2021). Sebbene il formato del test possa variare, consiste principalmente in domande a scelta multipla e in esercizi di completamento. Un campione simulato è incluso nel repository OSF. Il punteggio totale del test va da 0 a 20, con una media di 14.2 e una deviazione standard di 4.7.\n\n\n\n\n\n\nFigura 57.3\n\n\n\n\nstudy2_dat &lt;- rio::import(\n    here::here(\n        \"data\", \"marsh_alamer\", \"Study_2_data.csv\"\n    )\n)\n\n\nesem2_config &lt;- '\n\n  # The measurement model\n  # WITHOUT equality constraints on factor loadings\n\n  # Time 1 Set-ESEM\n  efa(\"time1\")*SelfConfidenceT1 =~ SelfConf1T1 + SelfConf2T1 + SelfConf3T1 + SelfConf4T1 + Intr1T1 + Intr2T1 + Intr3T1 + Auton1T1 + Auton2T1 + Auton3T1 + Auton4T1\n  efa(\"time1\")*IntrinsicT1 =~ SelfConf1T1 + SelfConf2T1 + SelfConf3T1 + SelfConf4T1 + Intr1T1 + Intr2T1 + Intr3T1 + Auton1T1 + Auton2T1 + Auton3T1 + Auton4T1\n  efa(\"time1\")*AutonomyT1 =~ SelfConf1T1 + SelfConf2T1 + SelfConf3T1 + SelfConf4T1 + Intr1T1 + Intr2T1 + Intr3T1 + Auton1T1 + Auton2T1 + Auton3T1 + Auton4T1\n\n  # Time 2 Set-ESEM\n  efa(\"time2\")*SelfConfidenceT2 =~ SelfConf1T2 + SelfConf2T2 + SelfConf3T2 + SelfConf4T2 + Intr1T2 + Intr2T2 + Intr3T2 + Auton1T2 + Auton2T2 + Auton3T2 + Auton4T2\n  efa(\"time2\")*IntrinsicT2 =~ SelfConf1T2 + SelfConf2T2 + SelfConf3T2 + SelfConf4T2 + Intr1T2 + Intr2T2 + Intr3T2 + Auton1T2 + Auton2T2 + Auton3T2 + Auton4T2\n  efa(\"time2\")*AutonomyT2 =~ SelfConf1T2 + SelfConf2T2 + SelfConf3T2 + SelfConf4T2 + Intr1T2 + Intr2T2 + Intr3T2 + Auton1T2 + Auton2T2 + Auton3T2 + Auton4T2\n\n  # The structural model\n  SelfConfidenceT2 ~ SelfConfidenceT1\n  IntrinsicT2 ~ IntrinsicT1\n  AutonomyT2 ~ AutonomyT1\n  L2_achievement ~ SelfConfidenceT1 + IntrinsicT1 + AutonomyT1 + SelfConfidenceT2 + IntrinsicT2 + AutonomyT2\n\n  # Residual correlations\n  SelfConf1T2 ~~ SelfConf1T1\n  SelfConf2T2 ~~ SelfConf2T1\n  SelfConf3T2 ~~ SelfConf3T1\n  SelfConf4T2 ~~SelfConf4T1\n\n  Auton1T2 ~~ Auton1T1\n  Auton2T2 ~~ Auton2T1\n  Auton3T2 ~~ Auton3T1\n  Auton4T2 ~~ Auton4T1\n\n  Intr1T2 ~~ Intr1T1\n  Intr2T2 ~~ Intr2T1\n  Intr3T2 ~~ Intr3T1\n'\n\n\nout2_config &lt;- sem(\n    model = esem2_config,\n    data = study2_dat,\n    estimator = \"MLR\", # verbose = TRUE,\n    rotation = \"geomin\",\n    rotation.args = list(geomin.epsilon = 0.005)\n)\n\n\nesem2_metric &lt;- '\n\n  # The measurement model\n  # WITH equality constraints on factor loadings\n\n  # Time 1 Set-ESEM\n  efa(\"time1\")*SelfConfidenceT1 =~ a*SelfConf1T1 + b*SelfConf2T1 + c*SelfConf3T1 + d*SelfConf4T1 + e*Intr1T1 + f*Intr2T1 + g*Intr3T1 + h*Auton1T1 + i*Auton2T1 + j*Auton3T1 + k*Auton4T1\n  efa(\"time1\")*IntrinsicT1 =~ aa*SelfConf1T1 + bb*SelfConf2T1 + cc*SelfConf3T1 + dd*SelfConf4T1 + ee*Intr1T1 + ff*Intr2T1 + gg*Intr3T1 + hh*Auton1T1 + ii*Auton2T1 + jj*Auton3T1 + kk*Auton4T1\n  efa(\"time1\")*AutonomyT1 =~ aaa*SelfConf1T1 + bbb*SelfConf2T1 + ccc*SelfConf3T1 + ddd*SelfConf4T1 + eee*Intr1T1 + fff*Intr2T1 + ggg*Intr3T1 + hhh*Auton1T1 + iii*Auton2T1 + jjj*Auton3T1 + kkk*Auton4T1\n\n  # Time 2 Set-ESEM\n  efa(\"time2\")*SelfConfidenceT2 =~ a*SelfConf1T2 + b*SelfConf2T2 + c*SelfConf3T2 + d*SelfConf4T2 + e*Intr1T2 + f*Intr2T2 + g*Intr3T2 + h*Auton1T2 + i*Auton2T2 + j*Auton3T2 + k*Auton4T2\n  efa(\"time2\")*IntrinsicT2 =~ aa*SelfConf1T2 + bb*SelfConf2T2 + cc*SelfConf3T2 + dd*SelfConf4T2 + ee*Intr1T2 + ff*Intr2T2 + gg*Intr3T2 + hh*Auton1T2 + ii*Auton2T2 + jj*Auton3T2 + kk*Auton4T2\n  efa(\"time2\")*AutonomyT2 =~ aaa*SelfConf1T2 + bbb*SelfConf2T2 + ccc*SelfConf3T2 + ddd*SelfConf4T2 + eee*Intr1T2 + fff*Intr2T2 + ggg*Intr3T2 + hhh*Auton1T2 + iii*Auton2T2 + jjj*Auton3T2 + kkk*Auton4T2\n\n  # Free factors variances at Time 2\n  SelfConfidenceT2 ~~ NA*SelfConfidenceT2\n  IntrinsicT2 ~~ NA*IntrinsicT2\n  AutonomyT2 ~~ NA*AutonomyT2\n\n  # The structural model\n  SelfConfidenceT2 ~ SelfConfidenceT1\n  IntrinsicT2 ~ IntrinsicT1\n  AutonomyT2 ~ AutonomyT1\n  L2_achievement ~ SelfConfidenceT1 + IntrinsicT1 + AutonomyT1 + SelfConfidenceT2 + IntrinsicT2 + AutonomyT2\n\n  # Residual correlations\n  SelfConf1T2 ~~ SelfConf1T1\n  SelfConf2T2 ~~ SelfConf2T1\n  SelfConf3T2 ~~ SelfConf3T1\n  SelfConf4T2 ~~SelfConf4T1\n\n  Auton1T2 ~~ Auton1T1\n  Auton2T2 ~~ Auton2T1\n  Auton3T2 ~~ Auton3T1\n  Auton4T2 ~~ Auton4T1\n\n  Intr1T2 ~~ Intr1T1\n  Intr2T2 ~~ Intr2T1\n  Intr3T2 ~~ Intr3T1\n\n'\n\n\nout2_metric &lt;- sem(\n    model = esem2_metric, data = study2_dat,\n    estimator = \"MLR\", # verbose = TRUE,\n    rotation = \"geomin\", rotation.args = list(geomin.epsilon = 0.005)\n)\n\n\nlavTestLRT(out2_config, out2_metric)\n\n\nA anova: 2 x 7\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nDf diff\nPr(&gt;Chisq)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nout2_config\n173\n23779.97\n24191.6\n310.5786\nNA\nNA\nNA\n\n\nout2_metric\n197\n23767.68\n24083.4\n346.2858\n31.43027\n24\n0.1417601\n\n\n\n\n\nMarsh & Alamer (2024) considerano anche i corrispondenti modelli CFA, senza invarianza di misurazione e con invarianza di misurazione.\n\ncfa2_config &lt;- \"\n\n  # The measurement model\n  # WITHOUT equality constraints on factor loadings\n\n  # Time 1 Set-ESEM\n  SelfConfidenceT1 =~ SelfConf1T1 + SelfConf2T1 + SelfConf3T1 + SelfConf4T1\n  IntrinsicT1      =~     Intr1T1 +     Intr2T1 +     Intr3T1\n  AutonomyT1       =~    Auton1T1 +    Auton2T1 +    Auton3T1 +    Auton4T1\n\n  # Time 2 Set-ESEM\n  SelfConfidenceT2 =~ SelfConf1T2 + SelfConf2T2 + SelfConf3T2 + SelfConf4T2\n  IntrinsicT2      =~     Intr1T2 +     Intr2T2 +     Intr3T2\n  AutonomyT2       =~    Auton1T2 +    Auton2T2 +    Auton3T2 +    Auton4T2\n\n  ## The structural model\n  SelfConfidenceT2 ~ SelfConfidenceT1\n  IntrinsicT2 ~ IntrinsicT1\n  AutonomyT2 ~ AutonomyT1\n  L2_achievement ~ SelfConfidenceT1 + IntrinsicT1 + AutonomyT1 + SelfConfidenceT2 + IntrinsicT2 + AutonomyT2\n\n  # Residual correlations\n  SelfConf1T2 ~~ SelfConf1T1\n  SelfConf2T2 ~~ SelfConf2T1\n  SelfConf3T2 ~~ SelfConf3T1\n  SelfConf4T2 ~~SelfConf4T1\n\n  Auton1T2 ~~ Auton1T1\n  Auton2T2 ~~ Auton2T1\n  Auton3T2 ~~ Auton3T1\n  Auton4T2 ~~ Auton4T1\n\n  Intr1T2 ~~ Intr1T1\n  Intr2T2 ~~ Intr2T1\n  Intr3T2 ~~ Intr3T1\n\n  # Residual correlations among Time-2 factors\n  # (not automatically estimated due to being predictors as well,\n  #  but ESEM rotation allows their covariances to be nonzero)\n  SelfConfidenceT2 ~~ IntrinsicT2 + AutonomyT2\n  IntrinsicT2      ~~ AutonomyT2\n\n\"\n\n\nfit2_config &lt;- sem(\n    model = cfa2_config,\n    data = study2_dat,\n    estimator = \"MLR\",\n    std.lv = TRUE\n)\n\n\ncfa2_metric &lt;- \"\n\n  # The measurement model\n  # WITHOUT equality constraints on factor loadings\n\n  # Time 1 Set-ESEM\n  SelfConfidenceT1 =~ a*SelfConf1T1 + b*SelfConf2T1 + c*SelfConf3T1 + d*SelfConf4T1\n  IntrinsicT1      =~     e*Intr1T1 +     f*Intr2T1 +     g*Intr3T1\n  AutonomyT1       =~    h*Auton1T1 +    i*Auton2T1 +    j*Auton3T1 +    k*Auton4T1\n\n  # Time 2 Set-ESEM\n  SelfConfidenceT2 =~ a*SelfConf1T2 + b*SelfConf2T2 + c*SelfConf3T2 + d*SelfConf4T2\n  IntrinsicT2      =~     e*Intr1T2 +     f*Intr2T2 +     g*Intr3T2\n  AutonomyT2       =~    h*Auton1T2 +    i*Auton2T2 +    j*Auton3T2 +    k*Auton4T2\n\n  # Free factors variances at Time 2\n  SelfConfidenceT2 ~~ NA*SelfConfidenceT2\n  IntrinsicT2 ~~ NA*IntrinsicT2\n  AutonomyT2 ~~ NA*AutonomyT2\n\n  # The structural model\n  SelfConfidenceT2 ~ SelfConfidenceT1\n  IntrinsicT2 ~ IntrinsicT1\n  AutonomyT2 ~ AutonomyT1\n  L2_achievement ~ SelfConfidenceT1 + IntrinsicT1 + AutonomyT1 + SelfConfidenceT2 + IntrinsicT2 + AutonomyT2\n\n  # Residual correlations\n  SelfConf1T2 ~~ SelfConf1T1\n  SelfConf2T2 ~~ SelfConf2T1\n  SelfConf3T2 ~~ SelfConf3T1\n  SelfConf4T2 ~~SelfConf4T1\n\n  Auton1T2 ~~ Auton1T1\n  Auton2T2 ~~ Auton2T1\n  Auton3T2 ~~ Auton3T1\n  Auton4T2 ~~ Auton4T1\n\n  Intr1T2 ~~ Intr1T1\n  Intr2T2 ~~ Intr2T1\n  Intr3T2 ~~ Intr3T1\n\n  # Residual correlations among Time-2 factors\n  # (not automatically estimated due to being predictors as well,\n  #  but ESEM rotation allows their covariances to be nonzero)\n  SelfConfidenceT2 ~~ IntrinsicT2 + AutonomyT2\n  IntrinsicT2      ~~ AutonomyT2\n\n\"\n\n\nfit2_metric &lt;- sem(\n    model = cfa2_metric,\n    data = study2_dat,\n    estimator = \"MLR\",\n    std.lv = TRUE\n)\n\n\nlavTestLRT(fit2_config, fit2_metric)\n\n\nA anova: 2 x 7\n\n\n\nDf\nAIC\nBIC\nChisq\nChisq diff\nDf diff\nPr(&gt;Chisq)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\nfit2_config\n205\n23790.88\n24074.63\n385.4927\nNA\nNA\nNA\n\n\nfit2_metric\n213\n23782.68\n24034.45\n393.2852\n7.126655\n8\n0.5230313\n\n\n\n\n\n\nsummary(fit2_metric, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 46 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        74\n  Number of equality constraints                    11\n\n  Number of observations                           402\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               393.285     353.480\n  Degrees of freedom                               213         213\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.113\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              3534.015    2982.265\n  Degrees of freedom                               253         253\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.185\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.945       0.949\n  Tucker-Lewis Index (TLI)                       0.935       0.939\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.952\n  Robust Tucker-Lewis Index (TLI)                            0.943\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -11828.338  -11828.338\n  Scaling correction factor                                  1.148\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -11631.695  -11631.695\n  Scaling correction factor                                  1.167\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               23782.675   23782.675\n  Bayesian (BIC)                             24034.452   24034.452\n  Sample-size adjusted Bayesian (SABIC)      23834.547   23834.547\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.046       0.041\n  90 Percent confidence interval - lower         0.039       0.033\n  90 Percent confidence interval - upper         0.053       0.047\n  P-value H_0: RMSEA &lt;= 0.050                    0.827       0.988\n  P-value H_0: RMSEA &gt;= 0.080                    0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.043\n  90 Percent confidence interval - lower                     0.035\n  90 Percent confidence interval - upper                     0.050\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.938\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.054       0.054\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                      Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SelfConfidenceT1 =~                                                      \n    SelfCnf1T1 (a)       0.561    0.042   13.228    0.000    0.561    0.597\n    SelfCnf2T1 (b)       0.690    0.046   15.048    0.000    0.690    0.720\n    SelfCnf3T1 (c)       0.593    0.043   13.730    0.000    0.593    0.705\n    SelfCnf4T1 (d)       0.499    0.036   13.888    0.000    0.499    0.570\n  IntrinsicT1 =~                                                           \n    Intr1T1    (e)       0.677    0.052   12.893    0.000    0.677    0.654\n    Intr2T1    (f)       0.773    0.056   13.765    0.000    0.773    0.720\n    Intr3T1    (g)       0.834    0.048   17.412    0.000    0.834    0.784\n  AutonomyT1 =~                                                            \n    Auton1T1   (h)       0.518    0.050   10.284    0.000    0.518    0.480\n    Auton2T1   (i)       0.748    0.048   15.658    0.000    0.748    0.640\n    Auton3T1   (j)       0.845    0.048   17.508    0.000    0.845    0.776\n    Auton4T1   (k)       0.580    0.040   14.552    0.000    0.580    0.624\n  SelfConfidenceT2 =~                                                      \n    SelfCnf1T2 (a)       0.561    0.042   13.228    0.000    0.564    0.641\n    SelfCnf2T2 (b)       0.690    0.046   15.048    0.000    0.694    0.738\n    SelfCnf3T2 (c)       0.593    0.043   13.730    0.000    0.597    0.689\n    SelfCnf4T2 (d)       0.499    0.036   13.888    0.000    0.502    0.571\n  IntrinsicT2 =~                                                           \n    Intr1T2    (e)       0.677    0.052   12.893    0.000    0.662    0.659\n    Intr2T2    (f)       0.773    0.056   13.765    0.000    0.756    0.763\n    Intr3T2    (g)       0.834    0.048   17.412    0.000    0.817    0.820\n  AutonomyT2 =~                                                            \n    Auton1T2   (h)       0.518    0.050   10.284    0.000    0.554    0.501\n    Auton2T2   (i)       0.748    0.048   15.658    0.000    0.800    0.671\n    Auton3T2   (j)       0.845    0.048   17.508    0.000    0.905    0.798\n    Auton4T2   (k)       0.580    0.040   14.552    0.000    0.621    0.628\n\nRegressions:\n                     Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  SelfConfidenceT2 ~                                                      \n    SelfConfidncT1      0.661    0.081    8.185    0.000    0.656    0.656\n  IntrinsicT2 ~                                                           \n    IntrinsicT1         0.595    0.062    9.650    0.000    0.608    0.608\n  AutonomyT2 ~                                                            \n    AutonomyT1          0.479    0.072    6.661    0.000    0.448    0.448\n  L2_achievement ~                                                        \n    SelfConfidncT1      0.616    0.144    4.283    0.000    0.616    0.302\n    IntrinsicT1         0.200    0.109    1.834    0.067    0.200    0.098\n    AutonomyT1          0.228    0.114    2.002    0.045    0.228    0.112\n    SelfConfidncT2      0.794    0.150    5.290    0.000    0.799    0.391\n    IntrinsicT2         0.172    0.102    1.679    0.093    0.168    0.082\n    AutonomyT2          0.442    0.089    4.956    0.000    0.473    0.232\n\nCovariances:\n                      Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .SelfConf1T1 ~~                                                           \n   .SelfConf1T2          0.111    0.032    3.438    0.001    0.111    0.217\n .SelfConf2T1 ~~                                                           \n   .SelfConf2T2          0.088    0.032    2.748    0.006    0.088    0.208\n .SelfConf3T1 ~~                                                           \n   .SelfConf3T2          0.056    0.026    2.181    0.029    0.056    0.150\n .SelfConf4T1 ~~                                                           \n   .SelfConf4T2          0.009    0.030    0.315    0.753    0.009    0.018\n .Auton1T1 ~~                                                              \n   .Auton1T2             0.187    0.053    3.527    0.000    0.187    0.206\n .Auton2T1 ~~                                                              \n   .Auton2T2             0.149    0.053    2.786    0.005    0.149    0.187\n .Auton3T1 ~~                                                              \n   .Auton3T2             0.044    0.038    1.163    0.245    0.044    0.094\n .Auton4T1 ~~                                                              \n   .Auton4T2             0.088    0.034    2.567    0.010    0.088    0.157\n .Intr1T1 ~~                                                               \n   .Intr1T2              0.089    0.056    1.604    0.109    0.089    0.151\n .Intr2T1 ~~                                                               \n   .Intr2T2              0.124    0.047    2.647    0.008    0.124    0.259\n .Intr3T1 ~~                                                               \n   .Intr3T2              0.038    0.040    0.950    0.342    0.038    0.101\n .SelfConfidenceT2 ~~                                                      \n   .IntrinsicT2          0.211    0.055    3.809    0.000    0.358    0.358\n   .AutonomyT2           0.216    0.063    3.441    0.001    0.297    0.297\n .IntrinsicT2 ~~                                                           \n   .AutonomyT2           0.168    0.059    2.831    0.005    0.226    0.226\n  SelfConfidenceT1 ~~                                                      \n    IntrinsicT1          0.411    0.072    5.687    0.000    0.411    0.411\n    AutonomyT1           0.551    0.058    9.466    0.000    0.551    0.551\n  IntrinsicT1 ~~                                                           \n    AutonomyT1           0.170    0.069    2.460    0.014    0.170    0.170\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .SelfConfidncT2    0.577    0.090    6.433    0.000    0.570    0.570\n   .IntrinsicT2       0.604    0.106    5.714    0.000    0.631    0.631\n   .AutonomyT2        0.916    0.142    6.444    0.000    0.799    0.799\n   .SelfConf1T1       0.568    0.062    9.150    0.000    0.568    0.644\n   .SelfConf2T1       0.442    0.068    6.524    0.000    0.442    0.481\n   .SelfConf3T1       0.354    0.036    9.910    0.000    0.354    0.502\n   .SelfConf4T1       0.518    0.045   11.580    0.000    0.518    0.676\n   .Intr1T1           0.613    0.086    7.156    0.000    0.613    0.572\n   .Intr2T1           0.555    0.080    6.945    0.000    0.555    0.482\n   .Intr3T1           0.438    0.057    7.654    0.000    0.438    0.386\n   .Auton1T1          0.895    0.069   12.885    0.000    0.895    0.770\n   .Auton2T1          0.807    0.075   10.759    0.000    0.807    0.591\n   .Auton3T1          0.472    0.058    8.127    0.000    0.472    0.398\n   .Auton4T1          0.529    0.043   12.199    0.000    0.529    0.611\n   .SelfConf1T2       0.457    0.047    9.726    0.000    0.457    0.589\n   .SelfConf2T2       0.403    0.043    9.369    0.000    0.403    0.455\n   .SelfConf3T2       0.394    0.045    8.841    0.000    0.394    0.526\n   .SelfConf4T2       0.522    0.043   12.121    0.000    0.522    0.674\n   .Intr1T2           0.571    0.089    6.450    0.000    0.571    0.565\n   .Intr2T2           0.410    0.058    7.055    0.000    0.410    0.418\n   .Intr3T2           0.324    0.058    5.591    0.000    0.324    0.327\n   .Auton1T2          0.915    0.069   13.291    0.000    0.915    0.749\n   .Auton2T2          0.782    0.079    9.948    0.000    0.782    0.550\n   .Auton3T2          0.466    0.059    7.863    0.000    0.466    0.363\n   .Auton4T2          0.594    0.055   10.841    0.000    0.594    0.606\n   .L2_achievement    0.904    0.117    7.745    0.000    0.904    0.217\n    SelfConfidncT1    1.000                               1.000    1.000\n    IntrinsicT1       1.000                               1.000    1.000\n    AutonomyT1        1.000                               1.000    1.000\n\n\n\nRisultati\nStima dei modelli strutturali set-ESEM e CFA\nMarsh & Alamer (2024) stimano i modelli strutturali basati su set-ESEM e CFA usando l’MLR. I risultati dei due modelli sono riportati nella Tabella 3 e indicano che entrambi offrono un buon adattamento ai dati. Si noti che questi risultati si riferiscono ai modelli con invarianza metrica, poiché essi hanno mostrato un adattamento simile ai modelli configurali. Tuttavia, il modello set-ESEM ha mostrato un adattamento migliore ai dati (ΔTLI/CFI = +.01). Gli indici AIC, BIC e BIC corretto per la dimensione del campione per il modello CFA sono rispettivamente 23,782.68, 24,034.45 e 23,834.55, mentre per il modello ESEM sono 23,764.25, 24,067.98 e 23,826.82. Valori più bassi di AIC e BIC corretto nel modello ESEM indicano un adattamento migliore di questo modello.\nCome mostrato nella Tabella 4, le correlazioni tra i fattori esogeni sono notevolmente più basse nel modello strutturale set-ESEM rispetto al modello CFA. Ad esempio, la correlazione tra Self_Confidence_T1 e Autonomy_T1 è 0.55 nel modello CFA, ma solo 0.29 nel modello set-ESEM (Δr = .26). Questi valori di correlazione più bassi influenzano la predizione degli effetti nel modello strutturale, come spiegato di seguito. In particolare, alcuni coefficienti di percorso differiscono in dimensione e valore di p tra i due modelli.\nTABELLA 3: Indici di adattamento per i modelli longitudinali CFA e set-ESEM *Significativo a p &lt; .01.\n\n\n\n\n\n\n\n\n\n\n\n\nModello\nχ²\ndf\nRMSEA\nIntervallo di confidenza RMSEA (90%)\nCFI\nTLI\n\n\n\n\nModello basato su CFA\n346.24*\n205\n.044\n(.036, .052)\n.951\n.940\n\n\nModello CFA con invarianza metrica\n353.48*\n213\n.043\n(.035, .050)\n.952\n.943\n\n\nModello basato su set-ESEM\n283.98*\n173\n.042\n(.033, .050)\n.962\n.945\n\n\nModello set-ESEM con invarianza\n314.93*\n200\n.040\n(.031, .048)\n.961\n.950\n\n\n\nTABELLA 4: Coefficienti di percorso nei due modelli con invarianza metrica\n\n\n\n\n\n\n\n\n\nVariabile dipendente\nPredittore\nRisultati CFA\nRisultati set-ESEM\n\n\n\n\nSelf_Confidence_T2\nSelf_Confidence_T1\n.66 (p &lt; .01)\n.66 (p &lt; .01)\n\n\nIntrinsic_T2\nIntrinsic_T1\n.61 (p &lt; .01)\n.55 (p &lt; .01)\n\n\nAutonomy_T2\nAutonomy_T1\n.45 (p &lt; .01)\n.42 (p &lt; .01)\n\n\nL2_achievement\nSelf_Confidence_T1\n.30 (p &lt; .01)\n.27 (p &lt; .01)\n\n\nL2_achievement\nIntrinsic_T1\n.10 (p = .07)\n.14 (p &lt; .01)\n\n\nL2_achievement\nAutonomy_T1\n.11 (p = .05)\n.16 (p &lt; .01)\n\n\nL2_achievement\nSelf_Confidence_T2\n.39 (p &lt; .01)\n.40 (p &lt; .01)\n\n\nL2_achievement\nIntrinsic_T2\n.08 (p = .09)\n.12 (p = .01)\n\n\nL2_achievement\nAutonomy_T2\n.23 (p &lt; .01)\n.26 (p &lt; .01)\n\n\n\nLe correlazioni tra fattori mostrano differenze sostanziali nei due modelli, con valori inferiori per il set-ESEM. Ad esempio, nel modello CFA, la correlazione tra Self_Confidence_T1 e Autonomy_T1 è 0.55 (p &lt; .01), mentre nel modello set-ESEM è 0.29 (p &lt; .01).\nAlcuni coefficienti di percorso mostrano anche risultati differenti tra i modelli CFA e set-ESEM. Ad esempio, il modello CFA indica che la motivazione intrinseca al tempo 1 e al tempo 2 non è significativa come predittore del successo linguistico (e.g., Intrinsic_T1 → L2_achievement, β = .10, p = .07 e Intrinsic_T2 → L2_achievement, β = .08, p = .09), mentre nel modello set-ESEM questi effetti risultano significativi (Intrinsic_T1 → L2_achievement, β = .14, p &lt; .01, e Intrinsic_T2 → L2_achievement, β = .12, p = .01). La significatività di questi coefficienti di percorso nel modello set-ESEM, ma non nel CFA, porta a conclusioni diverse sugli effetti predittivi delle variabili.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Exploratory structural equation modelling</span>"
    ]
  },
  {
    "objectID": "chapters/sem/13_esem.html#riflessioni-conclusive",
    "href": "chapters/sem/13_esem.html#riflessioni-conclusive",
    "title": "57  Exploratory structural equation modelling",
    "section": "57.6 Riflessioni Conclusive",
    "text": "57.6 Riflessioni Conclusive\nIn questo tutorial, Marsh & Alamer (2024) introducono l’ESEM, focalizzandosi in particolare sul set-ESEM. Sebbene il set-ESEM sia generalmente utilizzato per la valutazione dei modelli di misurazione, gli autori approfondiscono la sua applicazione nei modelli strutturali. In particolare, esplorano le ragioni concettuali che giustificano il ricorso al set-ESEM rispetto al modello CFA e all’ESEM completamente rilassato. Il set-ESEM rappresenta un compromesso tra l’ESEM completo e il CFA, consentendo di specificare “mini-set” indipendenti di ESEM in un’unica soluzione (Marsh et al., 2020). In alcuni casi, l’ESEM completamente rilassato può risultare non necessario, inappropriato o tecnicamente impraticabile. Gli autori illustrano due esempi in cui il set-ESEM risulta preferibile rispetto all’ESEM completo, confrontandone i risultati con quelli del CFA.\nGli esempi presentati suggeriscono che il set-ESEM offre una rappresentazione più utile dei dati rispetto al CFA. Sebbene i modelli CFA sembrino adattarsi ai dati, le correlazioni tra i fattori esogeni nei modelli CFA risultano sistematicamente più elevate, sia nello Studio 1 che nello Studio 2. Tali alte correlazioni possono influenzare negativamente i coefficienti di percorso nel modello strutturale a causa di una possibile multicollinearità (Mai et al., 2018; Morin, 2023).\nNello Studio 1, i coefficienti di percorso differiscono significativamente tra i modelli strutturali basati su CFA e quelli basati su set-ESEM, portando a conclusioni differenti sugli effetti tra le variabili. Ad esempio, l’effetto della percezione dell’insegnante come promotore di competenza sulla motivazione intrinseca degli studenti risulta non significativo nel modello CFA, ma significativo nel modello set-ESEM. Inoltre, il CFA suggerisce che percepire l’insegnante come promotore di relazionalità non ha un impatto rilevante sull’intenzione degli studenti di abbandonare lo studio della lingua, mentre il modello set-ESEM rileva un effetto significativo e negativo. Solo il modello set-ESEM evidenzia quindi l’importante ruolo degli insegnanti nel sostenere la perseveranza degli studenti (Alamer & Al Khateeb, 2023; Alamer, Al Khateeb, & Jeno, 2023; Noels, 2023).\nNello Studio 2, i coefficienti di percorso mostrano variazioni analoghe tra i modelli CFA e set-ESEM, con differenze nelle conclusioni sulle relazioni longitudinali. Ad esempio, il modello strutturale CFA suggerisce che la motivazione intrinseca al tempo 1 e al tempo 2 ha poca rilevanza predittiva per il successo linguistico futuro, una conclusione che contrasta con l’importanza attribuita alla motivazione intrinseca nella letteratura (Alamer, 2022; Alamer & Alrabai, 2023; Horwood et al., 2021).\nIn sintesi, i due esempi empirici mostrano come i coefficienti di percorso siano significativi nel modello set-ESEM, ma non nel CFA. Questo è un aspetto importante da considerare nelle ricerche applicate. Tuttavia, rimane il quesito su quale modello sia più affidabile. Gli autori suggeriscono che questo va stabilito valutando il goodness of fit e l’assenza di multicollinearità. In entrambi gli esempi, i modelli set-ESEM si adattano meglio ai dati e presentano correlazioni tra i fattori più basse, suggerendo tassi di errore di tipo II più bassi per i coefficienti di percorso.\nSecondo Marsh & Alamer (2024), i principali vantaggi del set-ESEM sono i seguenti.\n\nOffre un equilibrio ottimale tra parsimonia (è più parsimonioso dell’ESEM completo) e un migliore goodness of fit (si adatta spesso meglio del CFA).\nPermette di includere costrutti teorici distinti, ma separati, in un unico modello (rispetto all’ESEM completo).\nUtilizzando una rotazione target, il set-ESEM consente un approccio confermativo (diversamente dalle rotazioni meccaniche come la geomin).\nRende possibile testare modelli strutturali che l’ESEM completo non può gestire.\nOffre correlazioni tra fattori più realistiche (rispetto al CFA), migliorando la valutazione della validità discriminante.\nTrasformando il modello di misura in un modello strutturale, gli effetti (coefficienti di percorso) risultano meno attenuati e più accurati, con tassi di errore di tipo II inferiori (rispetto al CFA).\n\nIl set-ESEM ha aperto nuove possibilità per l’applicazione di modelli SEM, specialmente per quei modelli strutturali che studiano gli effetti tra variabili. Tuttavia, presenta alcune limitazioni: sebbene più parsimonioso rispetto all’ESEM completo, rimane meno parsimonioso rispetto al CFA. Per questo motivo, quando le correlazioni tra fattori e gli indici di adattamento sono simili tra CFA e set-ESEM, il CFA dovrebbe essere preferito per ragioni di parsimonia. Tuttavia, se la teoria suggerisce cross-loading non trascurabili, il set-ESEM può essere preferibile.\nIn conclusione, in questo tutorial, Marsh & Alamer (2024) illustrano l’importanza del set-ESEM nello studio dei modelli strutturali e raccomandano l’utilizzo del set-ESEM per i modelli di misurazione e strutturali, quando l’ESEM completo è tecnicamente inadeguato o meno flessibile. Attraverso due esempi empirici, gli autori mostrano che il set-ESEM migliora l’adattamento ai dati e supporta i coefficienti di percorso. A causa delle alte correlazioni tra fattori esogeni nel CFA, gli effetti nel modello strutturale risultavano attenuati. Di conseguenza, alcuni coefficienti di percorso erano significativi solo nel set-ESEM. Sebbene i modelli CFA mostravano un adattamento accettabile, non erano esenti da problemi di collinearità, che potevano portare a bias nei parametri (Shao et al., 2022; Tabachnick & Fidell, 2023). Senza il set-ESEM, affidandosi solo al CFA, si sarebbero ottenute interpretazioni diverse sui rapporti tra variabili, con implicazioni distinte sugli stessi dati.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Exploratory structural equation modelling</span>"
    ]
  },
  {
    "objectID": "chapters/sem/13_esem.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/13_esem.html#informazioni-sullambiente-di-sviluppo",
    "title": "57  Exploratory structural equation modelling",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\n\n\n\n\nMarsh, H., & Alamer, A. (2024). When and how to use set-exploratory structural equation modelling to test structural models: A tutorial using the R package lavaan. British Journal of Mathematical and Statistical Psychology.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Exploratory structural equation modelling</span>"
    ]
  },
  {
    "objectID": "chapters/sem/14_sem_power.html",
    "href": "chapters/sem/14_sem_power.html",
    "title": "58  Dimensione Campionaria e Analisi della Potenza",
    "section": "",
    "text": "58.1 Introduzione\nBuchberger et al. (2024) osservano che, nell’ultimo decennio, la questione della bassa potenza statistica e delle sue conseguenze sull’interpretazione dei risultati scientifici ha attirato un’attenzione crescente nella ricerca psicologica. La potenza statistica rappresenta la capacità di rilevare effetti reali, ovvero la probabilità di individuare correttamente un effetto quando è effettivamente presente. Questo valore dipende strettamente dalla dimensione campionaria, dall’entità dell’effetto che si intende rilevare e dall’affidabilità delle misurazioni.\nLa bassa potenza rappresenta un problema in quanto può limitare l’utilità scientifica dei risultati; inoltre, anche se si rilevano effetti significativi, questi potrebbero non riflettere associazioni reali. Per garantire che gli studi empirici siano adeguatamente progettati per rilevare gli effetti nei campioni esaminati, è cruciale calcolare in anticipo la potenza e stabilire la dimensione campionaria necessaria, evitando studi sotto-dimensionati.\nIn genere, in psicologia, si considera adeguata una potenza intorno a 0.80, anche se alcuni autori suggeriscono di puntare a valori più elevati, come 0.95, per bilanciare meglio gli errori di tipo I e tipo II.\nDiversi autori hanno proposto regole pratiche per stabilire la dimensione campionaria nei modelli SEM, come un numero minimo di osservazioni o un rapporto tra osservazioni e parametri da stimare. Tuttavia, queste regole empiriche possono essere fuorvianti, poiché la potenza nei SEM dipende anche da fattori quali l’entità dei carichi fattoriali e la complessità del modello. Inoltre, la dimensione campionaria necessaria dipende dalla specifica domanda di ricerca.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Dimensione Campionaria e Analisi della Potenza</span>"
    ]
  },
  {
    "objectID": "chapters/sem/14_sem_power.html#obiettivi-principali-dei-modelli-sem",
    "href": "chapters/sem/14_sem_power.html#obiettivi-principali-dei-modelli-sem",
    "title": "58  Dimensione Campionaria e Analisi della Potenza",
    "section": "58.2 Obiettivi Principali dei Modelli SEM",
    "text": "58.2 Obiettivi Principali dei Modelli SEM\nBuchberger et al. (2024) fanno notare che i due obiettivi principali dei SEM sono:\n\nRilevare un effetto specifico, come determinare se la correlazione tra due fattori latenti è diversa da zero.\nConfrontare modelli, verificando quale tra due o più modelli spiega meglio i dati.\n\nQuesti due obiettivi richiedono approcci diversi: il primo riguarda la potenza per rilevare un effetto mirato, mentre il secondo si concentra sulla potenza per individuare errori di specificazione del modello. Nei confronti tra modelli non nidificati (non derivabili l’uno dall’altro tramite restrizioni parametriche), i test basati sul \\(\\chi^2\\) non sono utilizzabili; è quindi necessario ricorrere a metodi alternativi, come le simulazioni Monte Carlo.\nMolti studi SEM mirano a determinare se un parametro specifico differisca da un valore atteso. In tal caso, si confronta un modello in cui il parametro è stimato liberamente con uno in cui è fissato a un valore specifico. Se il confronto tra i modelli suggerisce una differenza significativa, si può concludere che il parametro differisce effettivamente dal valore atteso.\nPer molti ricercatori, è rilevante individuare quale tra modelli teorici concorrenti descriva meglio i dati. Quando i modelli sono nidificati, è possibile usare metodi analitici basati sul \\(\\chi^2\\), mentre per modelli non nidificati si ricorre a tecniche di randomizzazione, come le simulazioni Monte Carlo. Questo approccio consente di stimare la potenza per confrontare modelli non nidificati senza compromettere le assunzioni teoriche del modello.\nIn conclusione, la determinazione della dimensione campionaria e l’analisi di potenza per i SEM richiedono una valutazione accurata della specificità della domanda di ricerca e delle ipotesi del modello, utilizzando approcci sia analitici che di simulazione per ottenere stime affidabili.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Dimensione Campionaria e Analisi della Potenza</span>"
    ]
  },
  {
    "objectID": "chapters/sem/14_sem_power.html#dimensione-campionaria-per-confronti-tra-gruppi",
    "href": "chapters/sem/14_sem_power.html#dimensione-campionaria-per-confronti-tra-gruppi",
    "title": "58  Dimensione Campionaria e Analisi della Potenza",
    "section": "58.3 Dimensione Campionaria per Confronti tra Gruppi",
    "text": "58.3 Dimensione Campionaria per Confronti tra Gruppi\nPer introdurre il metodo della simulazione Monte Carlo applicato all’analisi di potenza nei modelli SEM, seguiremo il tutorial di Chen & Yung (2023). Prima di esplorare l’uso della simulazione Monte Carlo per stimare la potenza in contesti SEM, è utile applicare questo metodo a un caso più semplice: il calcolo della dimensione campionaria necessaria per rilevare una differenza clinicamente rilevante tra due gruppi. Questo esempio introduttivo aiuterà a chiarire il funzionamento della simulazione Monte Carlo. Successivamente, applicheremo lo stesso approccio alla determinazione della potenza per i modelli SEM, un caso più complesso che richiede tecniche avanzate di simulazione per stimare accuratamente la potenza.\n\n58.3.1 Formula per la Dimensione Campionaria per Gruppo\nLa formula per determinare la dimensione campionaria per ciascun gruppo, per rilevare una differenza clinicamente significativa tra due gruppi, è:\n\\[\nn \\geq 2 \\left(\\frac{s^2}{\\delta^2}\\right) \\left[z_{1 - \\alpha/2} + z_{1 - \\beta}\\right]^2,\n\\tag{58.1}\\]\ndove:\n\n\\(s\\) è la stima della deviazione standard (assumendo varianze omogenee tra i gruppi);\n\\(\\delta\\) è la differenza clinicamente rilevante tra i gruppi, cioè l’effetto minimo che si desidera rilevare;\n\\(z_{1 - \\alpha/2}\\) e \\(z_{1 - \\beta}\\) sono i valori critici della distribuzione normale standard per i limiti degli errori di tipo I e tipo II.\n\n\n\n58.3.2 Derivazione\nPer calcolare la dimensione campionaria necessaria a confrontare le medie di due gruppi indipendenti, seguiamo i passaggi teorici necessari per derivare la formula finale. Il nostro interesse è quantificare la differenza tra le medie dei due gruppi, indicata come \\(\\mu_1 - \\mu_2\\). Supponiamo che le due medie siano stimate da campioni con deviazione standard comune \\(\\sigma\\).\nPoiché ciascuna media di campione (\\(\\bar{X}_1\\) per il gruppo 1 e \\(\\bar{X}_2\\) per il gruppo 2) ha varianza \\(\\frac{\\sigma^2}{n}\\) e i campioni sono indipendenti, la varianza della differenza \\(\\bar{X}_1 - \\bar{X}_2\\) è data dalla somma delle varianze:\n\\[\n\\text{Var}(\\bar{X}_1 - \\bar{X}_2) = \\frac{\\sigma^2}{n} + \\frac{\\sigma^2}{n} = \\frac{2\\sigma^2}{n}.\n\\]\nDa qui, la deviazione standard della differenza tra le medie è quindi:\n\\[\n\\text{Deviazione standard} = \\sqrt{\\frac{2\\sigma^2}{n}}.\n\\]\nPer il teorema del limite centrale, questa differenza tra le medie segue una distribuzione normale, dato che \\(n\\) è sufficientemente grande.\nPer determinare la potenza del test e la dimensione campionaria necessaria, definiamo:\n\nIpotesi nulla \\(H_0\\): \\(\\mu_1 - \\mu_2 = 0\\) (assenza di differenza significativa).\nIpotesi alternativa \\(H_1\\): \\(\\mu_1 - \\mu_2 = \\delta\\), dove \\(\\delta\\) rappresenta una differenza significativa che vogliamo rilevare.\nLivello di significatività \\(\\alpha\\): probabilità di rifiutare \\(H_0\\) quando è vera (errore di Tipo I).\nPotenza desiderata \\(1-\\beta\\): probabilità di rilevare una vera differenza \\(\\delta\\) (cioè non commettere un errore di Tipo II).\n\nSotto l’ipotesi nulla, la differenza tra le medie campionarie, \\(\\bar{X}_1 - \\bar{X}_2\\), ha media zero e deviazione standard \\(\\sqrt{\\frac{2\\sigma^2}{n}}\\). Possiamo quindi utilizzare la statistica \\(Z\\) per standardizzare questa differenza:\n\\[\nZ = \\frac{(\\bar{X}_1 - \\bar{X}_2) - 0}{\\sqrt{\\frac{2\\sigma^2}{n}}}.\n\\]\nIn un test a due code con livello di significatività \\(\\alpha\\), rifiutiamo \\(H_0\\) se il valore assoluto di \\(Z\\) supera \\(z_{1-\\alpha/2}\\), ovvero se:\n\\[\n|\\bar{X}_1 - \\bar{X}_2| &gt; z_{1-\\alpha/2} \\cdot \\sqrt{\\frac{2\\sigma^2}{n}}.\n\\]\nQuesto ci dice che il valore critico per rifiutare \\(H_0\\) è pari a \\(z_{1-\\alpha/2} \\cdot \\sqrt{\\frac{2\\sigma^2}{n}}\\). Tuttavia, poiché vogliamo assicurare una potenza del test pari a \\(1-\\beta\\), la differenza \\(\\delta\\) deve superare questo valore critico con probabilità \\(1 - \\beta\\).\nSotto l’ipotesi alternativa \\(H_1\\), la differenza attesa tra le medie campionarie è \\(\\delta\\), e quindi standardizziamo rispetto alla deviazione standard della differenza:\n\\[\nZ = \\frac{\\delta}{\\sqrt{\\frac{2\\sigma^2}{n}}}.\n\\]\nImponiamo ora che la somma dei valori critici \\(z_{1-\\alpha/2}\\) e \\(z_{1-\\beta}\\) sia uguale a questa statistica test, ottenendo:\n\\[\n\\delta = \\sqrt{\\frac{2\\sigma^2}{n}} (z_{1-\\alpha/2} + z_{1-\\beta}).\n\\]\nRisolviamo questa equazione per \\(n\\) al fine di ottenere la dimensione campionaria necessaria:\n\\[\nn \\geq \\frac{2\\sigma^2}{\\delta^2} (z_{1-\\alpha/2} + z_{1-\\beta})^2.\n\\]\nNella pratica, non sempre conosciamo \\(\\sigma^2\\); pertanto, lo sostituiamo con la stima \\(s^2\\), ottenendo:\n\\[\nn \\geq 2 \\left(\\frac{s^2}{\\delta^2}\\right) [z_{1-\\alpha/2} + z_{1-\\beta}]^2.\n\\]\nQuesta formula fornisce un modo pratico per calcolare la dimensione campionaria minima necessaria per garantire che il test abbia la potenza desiderata, tenendo conto della variabilità dei dati (\\(s^2\\)), dell’errore di Tipo I (\\(\\alpha\\)), dell’errore di Tipo II (\\(\\beta\\)), e della differenza minima rilevante \\(\\delta\\).\nIn conclusione, questa derivazione della dimensione campionaria è utile poiché:\n\nConsente di specificare il livello di controllo sugli errori di Tipo I e Tipo II.\nTiene conto della variabilità campionaria attraverso la stima \\(s^2\\).\nPermette di impostare una differenza minima rilevante da rilevare tra i gruppi.\nFornisce una stima della dimensione campionaria necessaria per garantire la potenza statistica richiesta.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Dimensione Campionaria e Analisi della Potenza</span>"
    ]
  },
  {
    "objectID": "chapters/sem/14_sem_power.html#esempio-pratico",
    "href": "chapters/sem/14_sem_power.html#esempio-pratico",
    "title": "58  Dimensione Campionaria e Analisi della Potenza",
    "section": "58.4 Esempio Pratico",
    "text": "58.4 Esempio Pratico\nConsideriamo i dati seguenti, ponendo \\(\\alpha\\) = 0.05 e la potenza \\(1-\\beta\\) = 0.80:\n\nmu2 &lt;- 1.2\nmu1 &lt;- 1\nsd &lt;- 0.5 # SD of each group\n\nalpha &lt;- 0.05\nbeta &lt;- 0.2\n\nCalcoliamo la dimensione minima campionaria per ciascun gruppo:\n\n# Mean difference\ndelta &lt;- mu2 - mu1\n\n# Required sample size\nn &lt;- 2 * sd^2 / delta^2 * (qnorm(1 - alpha / 2) + qnorm(1 - beta))^2\nn\n\n98.1109966793636",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Dimensione Campionaria e Analisi della Potenza</span>"
    ]
  },
  {
    "objectID": "chapters/sem/14_sem_power.html#collegamento-con-la-modellazione-a-equazioni-strutturali",
    "href": "chapters/sem/14_sem_power.html#collegamento-con-la-modellazione-a-equazioni-strutturali",
    "title": "58  Dimensione Campionaria e Analisi della Potenza",
    "section": "58.5 Collegamento con la Modellazione a Equazioni Strutturali",
    "text": "58.5 Collegamento con la Modellazione a Equazioni Strutturali\nIl confronto tra due gruppi è un caso relativamente semplice rispetto alla complessità dei modelli a equazioni strutturali (SEM), i quali spesso richiedono approcci avanzati per il calcolo della potenza, come il Monte Carlo Simulation-Based Approach (MCSB). Per comprendere meglio questo metodo, è utile applicare inizialmente il MCSB a un caso base come il confronto tra le medie di due gruppi. Successivamente, estenderemo questo approccio per includere i SEM, dove la complessità delle relazioni e dei parametri richiede strumenti di simulazione più sofisticati.\n\n58.5.1 Applicazione del Metodo MCSB\nPer implementare il metodo MCSB, dobbiamo definire due componenti principali:\n\nIl modello generativo dei dati, che descrive i parametri noti e serve per simulare i dati, come le differenze attese tra i gruppi e la variabilità delle misure.\nIl modello di stima dei parametri, che specifica come i parametri saranno stimati dai dati simulati, utilizzando un software di modellazione SEM come lavaan o simsem.\n\n\n\n58.5.2 Riformulazione del Confronto tra Trattamenti come Modello di Regressione\nPossiamo rappresentare il confronto tra gruppi anche come modello di regressione lineare:\n\\[\ny = \\beta_0 + \\beta_1 \\cdot TRT + \\epsilon,\n\\]\ndove:\n\n\\(y\\) è la variabile di esito,\n\\(TRT\\) è una variabile indicatrice che differenzia i gruppi (1 per il gruppo trattato, 0 per il gruppo di controllo),\n\\(\\beta_0\\) rappresenta la media del gruppo di controllo,\n\\(\\beta_1\\) rappresenta la differenza media tra i gruppi,\n\\(\\epsilon\\) è il termine di errore, distribuito normalmente con media zero.\n\n\n\n58.5.3 Implementazione in R dell’Approccio MCSB con simsem\nPer calcolare la potenza e stimare correttamente i parametri, è necessario determinare la dimensione dell’effetto e la varianza residua dell’esito \\(y\\) in questa equazione. Di seguito, i passaggi per implementare il metodo MCSB:\n\nCalcolo del \\(d\\) di Cohen:\nSupponiamo di avere una differenza clinicamente rilevante \\(\\delta = 0.2\\) e una deviazione standard \\(sd = 0.5\\). La dimensione dell’effetto standardizzata, o \\(d\\) di Cohen, è calcolata come:\n\\[\nd = \\frac{\\delta}{sd} = \\frac{0.2}{0.5} = 0.4.\n\\]\nConversione di \\(d\\) in Coefficiente di Correlazione \\(r\\):\nSpesso è utile esprimere la dimensione dell’effetto in termini di coefficiente di correlazione \\(r\\), che riflette la forza dell’associazione tra trattamento e esito. Il pacchetto compute.es in R include la funzione des() per convertire \\(d\\) in un valore di correlazione equivalente \\(r\\).\nUsando des(d, n.1 = n, n.2 = n), otteniamo un valore approssimativo di \\(r \\approx 0.2\\).\nCalcolo della Varianza di \\(y\\):\nIn questo contesto, il coefficiente \\(r = 0.2\\) rappresenta l’associazione tra il trattamento (TRT) e l’esito \\(y\\). La varianza totale di \\(y\\) può essere separata in una parte spiegata (dall’effetto di \\(TRT\\)) e una parte non spiegata. La varianza residua di \\(y\\), dopo aver tenuto conto del predittore, è data da \\(1 - r^2\\), poiché \\(r^2\\) rappresenta la proporzione della varianza di \\(y\\) spiegata dal trattamento:\n\\[\n\\text{var}(y) = 1 - r^2.\n\\]\nSostituendo \\(r = 0.2\\):\n\\[\n\\text{var}(y) = 1 - (0.2)^2 = 1 - 0.04 = 0.96.\n\\]\n\nQuesti passaggi possono essere implementati in R utilizzando il pacchetto compute.es, facilitando il calcolo e la configurazione della simulazione per l’analisi di potenza con simsem.\n\n# Calcolo dell'ES in termini di d\nd &lt;- delta / sd\n# print(d) # E.g., [1] 0.4\n\n# Conversione in altre misure di ES\nd2ES &lt;- des(d, n.1 = n, n.2 = n, verbose = FALSE)\n\n# Estrazione di r\nr &lt;- d2ES$r\nprint(r) # E.g., [1] 0.2\n\n# Calcolo della varianza di y\nvar.y &lt;- 1 - r**2\nprint(var.y) # E.g., [1] 0.96\n\n[1] 0.2\n[1] 0.96\n\n\nPer utilizzare simsem, è necessario specificare il processo di generazione dei dati. Per il caso presente abbiamo:\n\n# Modello di generazione dei dati\ndatMod &lt;- \"\n    # Regressione con correlazione nota di 0.2\n    y ~ 0.2*TRT\n    # Varianza dell'errore\n    y ~~ 0.96*y\n\"\n\nIl valore \\(0.2\\) in y ~ 0.2*TRT rappresenta tecnicamente un coefficiente di regressione. Tuttavia, se le variabili sono standardizzate, questo valore coincide con la correlazione tra la variabile predittore \\(TRT\\) e la variabile di risposta \\(y\\).\nIn questo contesto, considerare il coefficiente di correlazione è utile poiché ci permette di calcolare facilmente la varianza residua di \\(y\\). Conoscendo \\(r\\), infatti, possiamo stabilire la varianza residua come \\(1 - r^2\\), semplificando la specifica del modello.\nIl modello di stima è il seguente:\n\n# Modello di stima\nestMod &lt;- \"\n    # Regressione\n    y ~ TRT\n\"\n\nPossiamo ora eseguire la simulazione MCSB:\n\nsimOut &lt;- sim(\n    nRep = 1000, \n    generate = datMod, \n    model = estMod, \n    n = 198,\n    lavaanfun = \"sem\", \n    seed = 123, \n    silent = TRUE\n)\n\nNella simulazione eseguita con simOut, il parametro nRep indica il numero di repliche, che è stato impostato a 1000 per ridurre i tempi di esecuzione. Idealmente, sarebbe preferibile un numero più elevato (ad esempio &gt;10.000) per ottenere risultati più precisi. Il parametro generate specifica il modello di generazione dei dati (qui datMod), mentre model collega al modello di stima (qui estMod) utilizzando la funzione lavaanfun = \"sem\". La dimensione del campione, n, è impostata a 198 per il calcolo della potenza statistica, e seed consente di riprodurre la simulazione con gli stessi risultati. L’opzione silent = TRUE sopprime l’output intermedio per semplificare la visualizzazione.\nEsaminiamo l’output della simulazione.\n\nsummaryParam(simOut)\n\n\nA data.frame: 2 x 10\n\n\n\nEstimate Average\nEstimate SD\nAverage SE\nPower (Not equal 0)\nStd Est\nStd Est SD\nStd Ave SE\nAverage Param\nAverage Bias\nCoverage\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\ny~TRT\n0.202\n0.0693\n0.0698\n0.816\n0.201\n0.0675\n0.0671\n0.20\n0.00219\n0.953\n\n\ny~~y\n0.956\n0.0969\n0.0960\n1.000\n0.955\n0.0271\n0.0267\n0.96\n-0.00433\n0.942\n\n\n\n\n\nL’output mostra diverse colonne relative ai parametri stimati con due righe: la prima riga è per lo stimatore della correlazione tra \\(y\\) e TRT (cioè il coefficiente di regressione), mentre la seconda riga si riferisce alla varianza residua di \\(y\\) in y ~~ y.\n\nEstimate Average: È la media delle stime dei parametri nelle 1000 repliche. I valori ottenuti (0.202 per y ~ TRT e 0.956 per y ~~ y) sono molto vicini ai valori simulati, cioè \\(r = 0.2\\) e \\(\\text{var}(y) = 0.96\\), indicando che le stime sono coerenti con i parametri di partenza.\nEstimate SD: È la deviazione standard delle stime dei parametri tra le repliche. Rappresenta la variabilità delle stime nei vari set di dati simulati, e aiuta a capire la precisione delle stime.\nAverage SE: È la media degli errori standard delle stime in tutte le repliche, un’indicazione della variabilità stimata del parametro per ogni replica.\nPower (Not equal 0): Questa colonna indica la proporzione di repliche in cui i parametri sono risultati significativamente diversi da zero. Qui la potenza statistica per y ~ TRT è 0.816, vicina al valore target di 0.80. La lieve differenza potrebbe essere ridotta aumentando nRep.\nAverage Param: Rappresenta i valori medi dei parametri simulati. In questo caso, sono i valori di partenza utilizzati nel modello di generazione dei dati, cioè 0.20 per y ~ TRT e 0.96 per y ~~ y.\nAverage Bias: È la differenza tra i valori medi stimati e i parametri di partenza. Nel nostro caso, il bias è molto piccolo, indicando che le stime sono ben centrate attorno ai parametri di partenza.\nCoverage: Indica la percentuale di intervalli di confidenza che contengono i valori veri dei parametri. In generale, ci si aspetta una copertura intorno al 95% per un intervallo di confidenza al 95%, qui abbiamo coperture di 95.3% e 94.2%, il che è in linea con le aspettative.\n\nIn sintesi, i risultati della simulazione mostrano stime accurate e, con una numerosità campionaria totale di \\(n = 198\\), raggiungono una potenza statistica vicina all’obiettivo di 0.80, accompagnata da un bias minimo e una buona copertura.\nIn altre parole, questa dimostrazione evidenzia come la tecnica di simulazione MCSB riesca, nel caso del confronto tra le medie di due gruppi, a collegare la dimensione del campione alla potenza del test, tenendo conto dell’ampiezza dell’effetto e della variabilità campionaria, in linea con le aspettative teoriche.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Dimensione Campionaria e Analisi della Potenza</span>"
    ]
  },
  {
    "objectID": "chapters/sem/14_sem_power.html#analisi-di-potenza-nei-modelli-sem",
    "href": "chapters/sem/14_sem_power.html#analisi-di-potenza-nei-modelli-sem",
    "title": "58  Dimensione Campionaria e Analisi della Potenza",
    "section": "58.6 Analisi di Potenza nei Modelli SEM",
    "text": "58.6 Analisi di Potenza nei Modelli SEM\nDopo aver compreso l’approccio MCSB e la sua implementazione in simsem, possiamo ora illustrare come calcolare la potenza per modelli di equazioni strutturali (SEM) generali. Un esempio è offerto da Chen & Yung (2023), che utilizzano un modello di crescita latente (LGC) per analizzare i dati di Byrne (2013).\n\nIl dataset raccoglie i dati longitudinali raccolti da 405 donne di Hong Kong, sottoposte a valutazione post-chirurgica per il tumore al seno (Byrne, 2013).\n\nI dati, disponibili nel file hkcancer_red2.dat, includono 10 variabili:\n\nID: identificatore di ciascuna partecipante.\nMOOD1, MOOD4, MOOD8: valutazioni soggettive dello stato d’animo a 1, 4 e 8 mesi dall’intervento; punteggi più alti indicano un umore peggiore.\nSOCADJ1, SOCADJ4, SOCADJ8: misure di adattamento sociale a 1, 4 e 8 mesi; punteggi più alti indicano un miglior adattamento sociale.\nAge: età della partecipante al momento dell’intervento.\nAgeGrp: categoria di età (dichotomizzata) con ‘Younger’ (&lt; 50 anni) e ‘Older’ (&gt; 50 anni).\nSurgTX: tipo di intervento chirurgico, distinguendo tra lumpectomia e mastectomia.\n\n\n58.6.1 Modello di Crescita Latente (LGC)\nIl modello di crescita latente consente di modellare le traiettorie di crescita nel tempo e di confrontare differenze tra gruppi (ad esempio, tra gruppi di età o di intervento). Questo modello permette di analizzare sia le variazioni intra-individuali che quelle inter-individuali nel contesto delle traiettorie longitudinali. Nel caso del dataset sul tumore al seno, possiamo modellare sia il cambiamento longitudinale dello stato d’animo (MOOD) che dell’adattamento sociale (SOCADJ) lungo un periodo di 8 mesi, includendo le variazioni tra le partecipanti nelle loro traiettorie.\nChen & Yung (2023) affrontano la questione della determinazione della dimensione campionaria necessaria per rilevare un effetto significativo su “MOOD” con una potenza statistica pari a 0.80.\n\n\n58.6.2 Calcolo della Dimensione Campionaria\nPer determinare la dimensione campionaria necessaria a rilevare l’effetto dell’intervento chirurgico su “MOOD”, è necessario specificare il modello generativo dei dati, che comprende tutti i parametri del modello. Questi parametri descrivono il cambiamento atteso dello stato d’animo nel tempo in relazione al tipo di intervento.\nPrima di procedere con l’analisi di potenza, carichiamo i dati in R per adattare il modello in base alle osservazioni empiriche. Ecco come eseguire il caricamento dei dati:\n\ndCancer &lt;- rio::import(here::here(\"data\", \"hkcancer_red2.dat\"))\n\n\n# Replace all \"*\" with NA in dCancer\ndCancer[dCancer == \"*\"] &lt;- NA\nhead(dCancer)\n\n\nA data.frame: 6 x 11\n\n\n\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\nV10\nV11\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n3\n2\n15.000000\nNA\nNA\n95.906250\nNA\nNA\n70\n1\n1\n\n\n2\n2\n3\n16.000000\n25.000000\n22.000000\n114.888889\n105.111111\n90.444444\n47\n0\n1\n\n\n3\n2\n2\n37.000000\n26.000000\n25.000000\n80.666667\n95.333333\n95.333333\n47\n0\n1\n\n\n4\n2\n2\n19.000000\n16.000000\n15.000000\n112.838710\n108.580645\n99.000000\n52\n1\n1\n\n\n5\n1\n2\n13.000000\n16.000000\n14.000000\n115.000000\n105.000000\n101.000000\n43\n0\n1\n\n\n6\n1\n1\n21.000000\n28.000000\n19.000000\n106.451613\n114.967742\n107.516129\n34\n0\n0\n\n\n\n\n\n\nvar_names &lt;- c(\n\"X1\", \"X2\", \"MOOD1\", \"MOOD4\", \"MOOD8\", \"SOCADJ1\", \"SOCADJ4\", \"SOCADJ8\", \"Age\",  \n\"AgeGrp\", \"SurgTx\")\n\nnames(dCancer) &lt;- var_names\nhead(dCancer)\n\n\nA data.frame: 6 x 11\n\n\n\nX1\nX2\nMOOD1\nMOOD4\nMOOD8\nSOCADJ1\nSOCADJ4\nSOCADJ8\nAge\nAgeGrp\nSurgTx\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n3\n2\n15.000000\nNA\nNA\n95.906250\nNA\nNA\n70\n1\n1\n\n\n2\n2\n3\n16.000000\n25.000000\n22.000000\n114.888889\n105.111111\n90.444444\n47\n0\n1\n\n\n3\n2\n2\n37.000000\n26.000000\n25.000000\n80.666667\n95.333333\n95.333333\n47\n0\n1\n\n\n4\n2\n2\n19.000000\n16.000000\n15.000000\n112.838710\n108.580645\n99.000000\n52\n1\n1\n\n\n5\n1\n2\n13.000000\n16.000000\n14.000000\n115.000000\n105.000000\n101.000000\n43\n0\n1\n\n\n6\n1\n1\n21.000000\n28.000000\n19.000000\n106.451613\n114.967742\n107.516129\n34\n0\n0\n\n\n\n\n\nQuesta preparazione ci consentirà di configurare il modello LGC con simsem, impostando i parametri di interesse in modo da simulare la potenza con il metodo MCSB e verificare la capacità del modello di rilevare gli effetti desiderati su “MOOD”.\nDefiniamo il modello a crescita latente.\n\nmod4MOOD &lt;- \"\n    # Intercept and Slope with fixed-coefficients\n    iMOOD =~ 1*MOOD1 + 1*MOOD4 + 1*MOOD8\n    sMOOD =~ 0*MOOD1 + 1*MOOD4 + 2.33*MOOD8\n    \n    # Regression with a labeled for simulation\n    iMOOD ~ SurgTx\n    sMOOD ~ a*SurgTx\n\"\n\nQuesto modello di crescita latente è utilizzato per analizzare l’evoluzione dello stato d’animo (MOOD) di un gruppo di persone a tre momenti distinti nel tempo (1, 4 e 8 mesi dopo l’intervento chirurgico).\nIl modello ha due componenti principali: intercetta e pendenza. Queste rappresentano rispettivamente il valore iniziale e la velocità di cambiamento dello stato d’animo nel tempo.\n\nIntercetta (iMOOD): l’intercetta rappresenta il punto di partenza o il livello iniziale dello stato d’animo (MOOD). Nel modello, è definita come:\niMOOD =~ 1*MOOD1 + 1*MOOD4 + 1*MOOD8\nQui, iMOOD è una variabile latente che cattura il livello medio dello stato d’animo in ciascun momento, usando coefficienti fissati a 1 per indicare che ogni misura contribuisce allo stesso modo alla stima dell’intercetta. Questo implica che l’intercetta è il valore di base dello stato d’animo comune a tutti i partecipanti nei tre punti temporali.\nPendenza (sMOOD): la pendenza rappresenta la velocità e la direzione del cambiamento dello stato d’animo nel tempo.\nsMOOD =~ 0*MOOD1 + 1*MOOD4 + 2.33*MOOD8\nQui, sMOOD è una variabile latente che descrive come cambia lo stato d’animo nei diversi momenti temporali. I coefficienti (0, 1, 2.33) riflettono l’intervallo di tempo tra le misurazioni (ad esempio, da 1 a 8 mesi). Il coefficiente 2.33 per MOOD8 indica che l’effetto temporale si accumula, essendo il punto finale del periodo di osservazione.\n\nIl modello include anche delle regressioni che collegano l’intercetta e la pendenza a una variabile predittiva, il tipo di intervento chirurgico (SurgTx), che distingue tra due gruppi (lumpectomia e mastectomia).\n\niMOOD ~ SurgTx: questa regressione rappresenta l’effetto del tipo di intervento sul livello iniziale dello stato d’animo. In altre parole, cerca di vedere se esiste una differenza nello stato d’animo iniziale in base al tipo di intervento ricevuto.\nsMOOD ~ a*SurgTx: questa regressione, con un coefficiente etichettato a, rappresenta l’effetto del tipo di intervento sul tasso di cambiamento dello stato d’animo nel tempo. Il coefficiente a mostra se il tipo di intervento influisce sulla velocità di miglioramento o peggioramento dello stato d’animo lungo i mesi.\n\nIn sintesi, questo modello di crescita latente analizza sia il livello iniziale dello stato d’animo (intercetta) sia il cambiamento nel tempo (pendenza) e verifica se il tipo di intervento chirurgico influenza questi due aspetti.\nAdattiamo il modello ai dati.\n\n# Call growth function to fit the LGC\nfitMOOD &lt;- growth(mod4MOOD,\n    data = dCancer, \n    estimator = \"MLR\",\n    missing = \"fiml\"\n)\n\n\nsummary(fitMOOD) |&gt; print()\n\nlavaan 0.6-19 ended normally after 75 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        10\n\n  Number of observations                           405\n  Number of missing patterns                         8\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                 2.022       2.077\n  Degrees of freedom                                 2           2\n  P-value (Chi-square)                           0.364       0.354\n  Scaling correction factor                                  0.974\n    Yuan-Bentler correction (Mplus variant)                       \n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  iMOOD =~                                            \n    MOOD1             1.000                           \n    MOOD4             1.000                           \n    MOOD8             1.000                           \n  sMOOD =~                                            \n    MOOD1             0.000                           \n    MOOD4             1.000                           \n    MOOD8             2.330                           \n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  iMOOD ~                                             \n    SurgTx           -0.118    0.758   -0.155    0.876\n  sMOOD ~                                             \n    SurgTx     (a)   -0.282    0.332   -0.850    0.395\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .iMOOD ~~                                            \n   .sMOOD            -2.650    1.581   -1.676    0.094\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .iMOOD            10.550    0.671   15.715    0.000\n   .sMOOD            -0.389    0.298   -1.304    0.192\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .MOOD1            13.492    3.384    3.987    0.000\n   .MOOD4            18.792    2.415    7.782    0.000\n   .MOOD8             5.398    3.880    1.391    0.164\n   .iMOOD            25.827    3.574    7.226    0.000\n   .sMOOD             2.563    1.259    2.036    0.042\n\n\n\n\nsemPlot::semPaths(fitMOOD,\n    what = \"col\", \n    whatLabels = \"par\", \n    nCharNodes = 8,\n    shapeMan = \"rectangle\", \n    sizeMan = 8, \n    sizeMan2 = 7\n)\n\n\n\n\n\n\n\n\nUna volta ottenute le stime dei parametri del modello, possiamo utilizzarle per costruire la componente di generazione dei dati come segue (nel codice seguente, utilizzo i valori usati da Chen & Yung (2023) per riprodurre il loro risultato).\n\ndat.mod4MOOD &lt;- \"\n    # Intercept and Slope with MOOD\n    iMOOD =~ 1*MOOD1 + 1*MOOD4 + 1*MOOD8\n    sMOOD =~ 0*MOOD1 + 1*MOOD4 + 2.33*MOOD8\n    # residual variances for observed\n    MOOD1 ~~ 14.307*MOOD1\n    MOOD4 ~~ 18.637*MOOD4\n    MOOD8 ~~ 6.745*MOOD8\n    # Regression paths to covariates\n    iMOOD ~ (-0.116)*SurgTx\n    sMOOD ~ a*SurgTx + (-0.332)*SurgTx\n    # latent Intercepts\n    iMOOD ~ 21.683*1\n    sMOOD ~ 0.004*1\n    # latent variances/coVariances\n    iMOOD ~~ 25.826*iMOOD\n    sMOOD ~~ 2.272*sMOOD\n    iMOOD ~~ (-2.135)*sMOOD\n    # mean and variance for SurgTX\n    SurgTx ~ 0.5*1\n    SurgTx ~~ 0.25*SurgTx\n\"\n\nPossiamo ora usare la funzione sim() di simsem. Seguendo il tutorial di Chen & Yung (2023), fissiamo la dimensione campionaria complesiva a n = 405:\n\nsimOut1 &lt;- sim(\n    nRep = 1000, \n    generate = dat.mod4MOOD,\n    model = mod4MOOD, \n    n = 405, \n    lavaanfun = \"growth\",\n    seed = 123, \n    silent = TRUE\n)\n\n\n# simulation output\nsummary(simOut1)\n\nRESULT OBJECT\nModel Type\n[1] \"lavaan\"\n========= Fit Indices Cutoffs ============\n           Alpha\nFit Indices      0.1     0.05     0.01    0.001     Mean     SD\n      chisq    4.455    5.665    8.871   11.378    1.980  1.888\n      aic   7571.694 7588.105 7624.726 7667.762 7508.592 49.361\n      bic   7611.733 7628.144 7664.765 7707.801 7548.631 49.361\n      rmsea    0.055    0.067    0.092    0.108    0.016  0.025\n      cfi      0.994    0.991    0.983    0.975    0.998  0.004\n      tli      0.982    0.972    0.948    0.926    1.000  0.014\n      srmr     0.018    0.020    0.026    0.028    0.011  0.005\n========= Parameter Estimates and Standard Errors ============\n             Estimate Average Estimate SD Average SE Power (Not equal 0)\niMOOD~SurgTx           -0.088       0.614      0.607               0.056\na                      -0.337       0.244      0.242               0.287\nMOOD1~~MOOD1           14.312       2.882      2.816               1.000\nMOOD4~~MOOD4           18.563       1.831      1.846               1.000\nMOOD8~~MOOD8            6.843       3.477      3.401               0.536\niMOOD~~iMOOD           25.691       3.168      3.189               1.000\nsMOOD~~sMOOD            2.211       1.124      1.100               0.512\niMOOD~~sMOOD           -2.072       1.278      1.284               0.359\niMOOD~1                21.664       0.440      0.429               1.000\nsMOOD~1                 0.007       0.167      0.171               0.043\n             Std Est Std Est SD Std Ave SE Average Param Average Bias\niMOOD~SurgTx  -0.009      0.061      0.060        -0.116        0.028\na             -0.126      0.129      0.195        -0.332       -0.005\nMOOD1~~MOOD1   0.357      0.067      0.066        14.307        0.005\nMOOD4~~MOOD4   0.437      0.031      0.031        18.637       -0.074\nMOOD8~~MOOD8   0.195      0.099      0.096         6.745        0.098\niMOOD~~iMOOD   0.996      0.005      0.006        25.826       -0.135\nsMOOD~~sMOOD   0.968      0.147      0.311         2.272       -0.061\niMOOD~~sMOOD  -0.272      0.459     13.990        -2.135        0.063\niMOOD~1        4.291      0.282      0.282        21.683       -0.019\nsMOOD~1        0.005      0.146      0.194         0.004        0.003\n             Coverage\niMOOD~SurgTx    0.949\na               0.955\nMOOD1~~MOOD1    0.946\nMOOD4~~MOOD4    0.947\nMOOD8~~MOOD8    0.949\niMOOD~~iMOOD    0.956\nsMOOD~~sMOOD    0.940\niMOOD~~sMOOD    0.949\niMOOD~1         0.938\nsMOOD~1         0.956\n========= Correlation between Fit Indices ============\n       chisq    aic    bic  rmsea    cfi    tli   srmr\nchisq  1.000 -0.012 -0.012  0.955 -0.939 -0.995  0.957\naic   -0.012  1.000  1.000 -0.002 -0.014  0.016 -0.040\nbic   -0.012  1.000  1.000 -0.002 -0.014  0.016 -0.040\nrmsea  0.955 -0.002 -0.002  1.000 -0.932 -0.949  0.890\ncfi   -0.939 -0.014 -0.014 -0.932  1.000  0.941 -0.818\ntli   -0.995  0.016  0.016 -0.949  0.941  1.000 -0.957\nsrmr   0.957 -0.040 -0.040  0.890 -0.818 -0.957  1.000\n================== Replications =====================\nNumber of replications = 1000 \nNumber of converged replications = 947 \nNumber of nonconverged replications: \n   1. Nonconvergent Results = 0 \n   2. Nonconvergent results from multiple imputation = 0 \n   3. At least one SE were negative or NA = 0 \n   4. Nonpositive-definite latent or observed (residual) covariance matrix \n      (e.g., Heywood case or linear dependency) = 53 \n\n\nPossiamo osservare che la potenza associata al parametro \\(a\\) (cioè, l’effetto dell’intervento chirurgico) è 0.287, il che non sorprende, dato che sapevamo già che questo parametro non è statisticamente significativo con la dimensione campionaria attuale di 405.\nLa domanda successiva è quindi quale dimensione campionaria sia necessaria per ottenere una potenza pari a 0.80. A questo fine, è necessario applicare l’approccio MCSB per una serie di dimensioni campionarie, in modo da calcolare le rispettive potenze.\nChen & Yung (2023) costruiscono una curva che mostra la relazione tra dimensione campionaria e potenza, così da identificare la dimensione campionaria necessaria per raggiungere una potenza di 0.80. Per questo scopo, Chen & Yung (2023) utilizzano n = rep(seq(400, 2000, by = 200), 500), per eseguire l’MCSB su una sequenza di dimensioni campionarie da 400 a 2000, con incrementi di 200 (ovvero: 400, 600, 800, 1000, 1200, 1400, 1600, 1800, e 2000), ciascuna delle quali sarà utilizzata per 500 simulazioni.\n\n# Simulation for sequential sample size\nsimAll &lt;- sim(\n    nRep = NULL, \n    generate = dat.mod4MOOD,\n    model = mod4MOOD, \n    n = rep(seq(400, 2000, 200), 500),\n    lavaanfun = \"growth\", \n    seed = 123, \n    silent = TRUE\n    ) \n\n\n# Print the simulations\nsummary(simAll)\n\nRESULT OBJECT\nModel Type\n[1] \"lavaan\"\n========= Fit Indices Cutoffs ============\n     N chisq   aic   bic rmsea   cfi   tli  srmr\n1  400  6.19  7503  7545 0.061 0.993 0.978 0.019\n2  800  6.04 14933 14979 0.053 0.994 0.983 0.016\n3 1200  5.88 22363 22413 0.044 0.996 0.987 0.013\n4 1600  5.73 29793 29847 0.036 0.997 0.992 0.011\n5 2000  5.57 37223 37281 0.027 0.999 0.996 0.008\n========= Parameter Estimates and Standard Errors ============\n             Estimate Average Estimate SD Average SE Power (Not equal 0)\niMOOD~SurgTx           -0.122       0.400      0.387               0.066\na                      -0.327       0.163      0.154               0.607\nMOOD1~~MOOD1           14.298       1.831      1.794               1.000\nMOOD4~~MOOD4           18.662       1.230      1.178               1.000\nMOOD8~~MOOD8            6.700       2.223      2.163               0.855\niMOOD~~iMOOD           25.761       2.094      2.033               1.000\nsMOOD~~sMOOD            2.265       0.722      0.700               0.877\niMOOD~~sMOOD           -2.130       0.855      0.818               0.761\niMOOD~1                21.687       0.284      0.273               1.000\nsMOOD~1                 0.002       0.116      0.109               0.058\n             Std Est Std Est SD Std Ave SE Average Param Average Bias\niMOOD~SurgTx  -0.012      0.039      0.038        -0.116       -0.006\na             -0.113      0.079      0.329        -0.332        0.005\nMOOD1~~MOOD1   0.357      0.043      0.042        14.307       -0.009\nMOOD4~~MOOD4   0.439      0.020      0.020        18.637        0.025\nMOOD8~~MOOD8   0.191      0.063      0.061         6.745       -0.045\niMOOD~~iMOOD   0.998      0.003      0.003        25.826       -0.065\nsMOOD~~sMOOD   0.981      0.164      1.757         2.272       -0.007\niMOOD~~sMOOD  -0.272      0.097      0.134        -2.135        0.005\niMOOD~1        4.280      0.186      0.178        21.683        0.004\nsMOOD~1        0.000      0.086      0.169         0.004       -0.002\n             Coverage r_coef.n r_se.n\niMOOD~SurgTx    0.949   -0.008 -0.933\na               0.943   -0.022 -0.933\nMOOD1~~MOOD1    0.953   -0.007 -0.931\nMOOD4~~MOOD4    0.950   -0.022 -0.925\nMOOD8~~MOOD8    0.952    0.016 -0.933\niMOOD~~iMOOD    0.951   -0.001 -0.929\nsMOOD~~sMOOD    0.955   -0.004 -0.935\niMOOD~~sMOOD    0.948    0.012 -0.934\niMOOD~1         0.948   -0.003 -0.935\nsMOOD~1         0.942    0.012 -0.935\n========= Correlation between Fit Indices ============\n       chisq    aic    bic  rmsea    cfi    tli   srmr      n\nchisq  1.000 -0.007 -0.007  0.911 -0.780 -0.854  0.825 -0.007\naic   -0.007  1.000  1.000 -0.158  0.204  0.015 -0.427  1.000\nbic   -0.007  1.000  1.000 -0.158  0.204  0.015 -0.427  1.000\nrmsea  0.911 -0.158 -0.158  1.000 -0.875 -0.897  0.866 -0.158\ncfi   -0.780  0.204  0.204 -0.875  1.000  0.930 -0.776  0.204\ntli   -0.854  0.015  0.015 -0.897  0.930  1.000 -0.818  0.015\nsrmr   0.825 -0.427 -0.427  0.866 -0.776 -0.818  1.000 -0.427\nn     -0.007  1.000  1.000 -0.158  0.204  0.015 -0.427  1.000\n================== Replications =====================\nNumber of replications = 4500 \nNumber of converged replications = 4455 \nNumber of nonconverged replications: \n   1. Nonconvergent Results = 0 \n   2. Nonconvergent results from multiple imputation = 0 \n   3. At least one SE were negative or NA = 0 \n   4. Nonpositive-definite latent or observed (residual) covariance matrix \n      (e.g., Heywood case or linear dependency) = 45 \nNOTE: The sample size is varying.\n\n\nUtilizzando la funzione getPower, è quindi ottenere le stime di potenza per ciascuna dimensione campionaria come segue:\n\nLGC.N &lt;- getPower(simAll)\n# Find the samplesize for 80% power\nfindPower(LGC.N, \"N\", 0.8) |&gt; print()\n\niMOOD~SurgTx            a MOOD1~~MOOD1 MOOD4~~MOOD4 MOOD8~~MOOD8 \n          NA         1710          Inf          Inf          816 \niMOOD~~iMOOD sMOOD~~sMOOD iMOOD~~sMOOD      iMOOD~1      sMOOD~1 \n         Inf          748         1161          Inf           NA \n\n\nPertanto, in base alla simulazione di Chen & Yung (2023), sarebbe necessaria una dimensione campionaria di 1710 per ottenere una potenza di 0.80.\nQuesto risultato può essere mostrato graficamente nella figura seguente. In questa figura, la linea tratteggiata orizzontale indica la potenza a 0.80, mentre la linea con la freccia che va da questa linea orizzontale a \\(N\\) = 1710 indica la dimensione campionaria determinata dalla curva di potenza.\n\n# Call plotPower to plot the power to sample size\nplotPower(simAll, powerParam = \"a\")\n# Add a horizontal line of 0.80\nabline(h = 0.8, lwd = 2, lty = 8)\narrows(1710, 0.8, 1710, 0, lwd = 2)\ntext(\n    1710,\n    -0.018, \"N = 1710\"\n)",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Dimensione Campionaria e Analisi della Potenza</span>"
    ]
  },
  {
    "objectID": "chapters/sem/14_sem_power.html#riflessioni-conclusive",
    "href": "chapters/sem/14_sem_power.html#riflessioni-conclusive",
    "title": "58  Dimensione Campionaria e Analisi della Potenza",
    "section": "58.7 Riflessioni Conclusive",
    "text": "58.7 Riflessioni Conclusive\nIn questo capitolo, abbiamo illustrato come determinare la dimensione campionaria e calcolare la potenza statistica utilizzando l’approccio basato sulla simulazione Monte Carlo. Questo metodo rappresenta l’approccio più generale e flessibile per ottenere stime affidabili della dimensione campionaria e della potenza necessarie alla progettazione di studi in vari contesti di ricerca, inclusi i modelli a equazioni strutturali (SEM).\nLa simulazione Monte Carlo consente di affrontare situazioni complesse e realistiche che spesso non sono gestibili con metodi analitici tradizionali. Essa permette infatti di modellare vari scenari, includendo la variabilità dei parametri e le incertezze che caratterizzano i dati empirici. Inoltre, il metodo può essere applicato a una vasta gamma di modelli statistici, garantendo flessibilità nell’adattamento a diversi contesti di studio e domande di ricerca.\nIn conclusione, il metodo basato sulla simulazione Monte Carlo offre un potente strumento per pianificare studi empirici robusti e ben fondati, permettendo ai ricercatori di prendere decisioni informate riguardo alla dimensione del campione e alla configurazione del modello in relazione agli obiettivi specifici del loro studio.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Dimensione Campionaria e Analisi della Potenza</span>"
    ]
  },
  {
    "objectID": "chapters/sem/14_sem_power.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/14_sem_power.html#informazioni-sullambiente-di-sviluppo",
    "title": "58  Dimensione Campionaria e Analisi della Potenza",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nRandom number generation:\n RNG:     L'Ecuyer-CMRG \n Normal:  Inversion \n Sample:  Rejection \n \nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] quantreg_5.99     compute.es_0.2-5  simsem_0.5-16     kableExtra_1.4.0 \n [5] MASS_7.3-61       viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [9] ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1 \n[13] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19     psych_2.4.6.26   \n[17] scales_1.3.0      markdown_1.13     knitr_1.49        lubridate_1.9.3  \n[21] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[25] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n[29] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.2       later_1.3.2         pbdZMQ_0.3-13      \n  [4] R.oo_1.27.0         XML_3.99-0.17       rpart_4.1.23       \n  [7] lifecycle_1.0.4     rstatix_0.7.2       rprojroot_2.0.4    \n [10] lattice_0.22-6      rockchalk_1.8.157   backports_1.5.0    \n [13] magrittr_2.0.3      openxlsx_4.2.7.1    Hmisc_5.2-0        \n [16] rmarkdown_2.29      httpuv_1.6.15       qgraph_1.9.8       \n [19] zip_2.3.1           pbapply_1.7-2       minqa_1.2.8        \n [22] multcomp_1.4-26     abind_1.4-8         quadprog_1.5-8     \n [25] R.utils_2.12.3      nnet_7.3-19         TH.data_1.1-2      \n [28] sandwich_3.1-1      openintro_2.5.0     arm_1.14-4         \n [31] MatrixModels_0.5-3  airports_0.1.0      svglite_2.1.3      \n [34] codetools_0.2-20    xml2_1.3.6          tidyselect_1.2.1   \n [37] farver_2.1.2        lme4_1.1-35.5       stats4_4.4.2       \n [40] base64enc_0.1-3     jsonlite_1.8.9      Formula_1.2-5      \n [43] survival_3.7-0      emmeans_1.10.5      systemfonts_1.1.0  \n [46] tools_4.4.2         rio_1.2.3           Rcpp_1.0.13-1      \n [49] glue_1.8.0          mnormt_2.1.1        xfun_0.49          \n [52] IRdisplay_1.1       withr_3.0.2         fastmap_1.2.0      \n [55] boot_1.3-31         fansi_1.0.6         SparseM_1.84-2     \n [58] digest_0.6.37       mi_1.1              timechange_0.3.0   \n [61] R6_2.5.1            mime_0.12           estimability_1.5.1 \n [64] colorspace_2.1-1    Cairo_1.6-2         gtools_3.9.5       \n [67] jpeg_0.1-10         R.methodsS3_1.8.2   utf8_1.2.4         \n [70] generics_0.1.3      data.table_1.16.2   corpcor_1.6.10     \n [73] usdata_0.3.1        htmlwidgets_1.6.4   pkgconfig_2.0.3    \n [76] sem_3.1-16          gtable_0.3.6        htmltools_0.5.8.1  \n [79] carData_3.0-5       png_0.1-8           rstudioapi_0.17.1  \n [82] tzdb_0.4.0          reshape2_1.4.4      uuid_1.2-1         \n [85] coda_0.19-4.1       checkmate_2.3.2     nlme_3.1-166       \n [88] nloptr_2.1.1        repr_1.1.7          zoo_1.8-12         \n [91] parallel_4.4.2      miniUI_0.1.1.1      foreign_0.8-87     \n [94] pillar_1.9.0        grid_4.4.2          vctrs_0.6.5        \n [97] promises_1.3.0      car_3.1-3           OpenMx_2.21.13     \n[100] xtable_1.8-4        cluster_2.1.6       htmlTable_2.4.3    \n[103] evaluate_1.0.1      pbivnorm_0.6.0      mvtnorm_1.3-2      \n[106] cli_3.6.3           kutils_1.73         compiler_4.4.2     \n[109] rlang_1.1.4         crayon_1.5.3        ggsignif_0.6.4     \n[112] fdrtool_1.2.18      plyr_1.8.9          stringi_1.8.4      \n[115] munsell_0.5.1       lisrelToR_0.3       pacman_0.5.1       \n[118] Matrix_1.7-1        IRkernel_1.3.2      hms_1.1.3          \n[121] glasso_1.11         shiny_1.9.1         igraph_2.1.1       \n[124] broom_1.0.7         RcppParallel_5.1.9  cherryblossom_0.1.0\n\n\n\n\n\n\nBuchberger, E. S., Ngo, C. T., Peikert, A., Brandmaier, A. M., & Werkle-Bergner, M. (2024). Estimating statistical power for structural equation models in developmental cognitive science: A tutorial in R: Power simulation for SEMs. Behavior Research Methods, 1–18.\n\n\nByrne, B. M. (2013). Structural equation modeling with Mplus: Basic concepts, applications, and programming. routledge.\n\n\nChen, D.-G., & Yung, Y.-F. (2023). Structural Equation Modeling Using R/SAS: A Step-by-step Approach with Real Data Analysis. CRC Press.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Dimensione Campionaria e Analisi della Potenza</span>"
    ]
  },
  {
    "objectID": "chapters/sem/E_01.html",
    "href": "chapters/sem/E_01.html",
    "title": "59  ✏️ Esercizi",
    "section": "",
    "text": "Nello studio di {cite:t}weiss2018difficulties viene esaminata la relazione tra la difficiltà di regolare le emozioni positive e l’abuso di alcol e di sostanze. Gli autori propongono due modelli SEM. Si riproduca l’analisi svolta da {cite:t}weiss2018difficulties usando lavaan.\n\nsource(\"../_common.R\")\n\nsuppressPackageStartupMessages({\n    library(\"lavaan\")\n    library(\"lavaanExtra\")\n    library(\"lavaanPlot\")\n    library(\"psych\")\n    library(\"dplyr\") \n    library(\"tidyr\")\n    library(\"knitr\")\n    library(\"mvnormalTest\")\n    library(\"semPlot\")\n    library(\"DiagrammeRsvg\")\n    library(\"rsvg\")\n    library(\"effectsize\")\n})\nset.seed(42)\n\nNello studio di {cite:t}weiss2018difficulties\n\nLa difficoltà di regolare le emozioni positive viene misurata con la Difficulties in Emotion Regulation Scale – Positive (DERS-P; Weiss, Gratz, & Lavender, 2015), che comprende le sottoscale di Acceptance, Impulse, e Goals.\nL’abuso di sostanze viene misurato con la Drug Abuse Screening Test (DAST; Skinner, 1982).\nL’abuso di alcol viene misurato con la Alcohol Use Disorder Identification Test (AUDIT; Saunders, Aasland, Babor, De la Fuente, & Grant, 1993), con le sottoscale di Hazardous Consumption, Dependence, e Consequences.\n\nI dati di un campione di 284 partecipanti sono riportati nella forma di una matrice di correlazione.\n\nlower &lt;- \"\n   1\n   .38 1\n   .41 .64 1\n   .34 .44 .30 1\n   .29 .12 .27 .06 1\n   .29 .22 .20 .17 .54 1\n   .30 .15 .23 .09 .73 .69 1\n\"\n\n\ndat_cov &lt;- lavaan::getCov(\n    lower,\n    names = c(\"dmis\", \"con\", \"dep\", \"consu\", \"acc\", \"goal\", \"imp\")\n)\nprint(dat_cov)\n\n      dmis  con  dep consu  acc goal  imp\ndmis  1.00 0.38 0.41  0.34 0.29 0.29 0.30\ncon   0.38 1.00 0.64  0.44 0.12 0.22 0.15\ndep   0.41 0.64 1.00  0.30 0.27 0.20 0.23\nconsu 0.34 0.44 0.30  1.00 0.06 0.17 0.09\nacc   0.29 0.12 0.27  0.06 1.00 0.54 0.73\ngoal  0.29 0.22 0.20  0.17 0.54 1.00 0.69\nimp   0.30 0.15 0.23  0.09 0.73 0.69 1.00\n\n\nIn questo studio, gli autori adottano due modelli SEM distinti per analizzare i dati. Nel primo modello, si postula che la difficoltà nella regolazione delle emozioni positive funzioni come variabile esogena, influenzando sia l’abuso di sostanze sia l’abuso di alcol. Inoltre, si ipotizza una correlazione tra abuso di sostanze e abuso di alcol, suggerendo una possibile interdipendenza tra questi due comportamenti problematici.\nPer quanto riguarda le variabili latenti specifiche, la difficoltà di regolare le emozioni positive, indicata come drpe, è rappresentata da una variabile latente che si basa su tre indicatori.Parallelamente, l’abuso di alcol, etichettato come amis, è concepito come una seconda variabile latente, anch’essa identificata tramite tre indicatori distinti.\n\nmod &lt;- \"\n  drpe =~ NA*acc + goal + imp\n  amis =~ NA*con + dep + consu\n  amis ~ drpe\n  dmis ~ drpe\n  dmis ~~ amis\n  drpe ~~ 1*drpe\n  amis ~~ 1*amis\n\"\n\nAdattiamo il modello ai dati con sem().\n\nfit &lt;- lavaan::sem(mod, sample.cov = dat_cov, sample.nobs = 284)\n\nEsaminiamo i risultati.\n\nstandardizedSolution(fit) |&gt; print()\n\n     lhs op   rhs est.std    se      z pvalue ci.lower ci.upper\n1   drpe =~   acc   0.770 0.031 24.982  0.000    0.710    0.830\n2   drpe =~  goal   0.728 0.033 21.849  0.000    0.663    0.794\n3   drpe =~   imp   0.945 0.024 39.322  0.000    0.898    0.992\n4   amis =~   con   0.837 0.039 21.217  0.000    0.759    0.914\n5   amis =~   dep   0.756 0.041 18.420  0.000    0.676    0.837\n6   amis =~ consu   0.494 0.052  9.439  0.000    0.392    0.597\n7   amis  ~  drpe   0.254 0.066  3.863  0.000    0.125    0.383\n8   dmis  ~  drpe   0.334 0.056  6.001  0.000    0.225    0.443\n9   amis ~~  dmis   0.458 0.055  8.303  0.000    0.350    0.567\n10  drpe ~~  drpe   1.000 0.000     NA     NA    1.000    1.000\n11  amis ~~  amis   0.936 0.033 28.023  0.000    0.870    1.001\n12   acc ~~   acc   0.407 0.047  8.575  0.000    0.314    0.500\n13  goal ~~  goal   0.470 0.049  9.677  0.000    0.375    0.565\n14   imp ~~   imp   0.107 0.045  2.349  0.019    0.018    0.196\n15   con ~~   con   0.300 0.066  4.551  0.000    0.171    0.430\n16   dep ~~   dep   0.428 0.062  6.900  0.000    0.307    0.550\n17 consu ~~ consu   0.756 0.052 14.595  0.000    0.654    0.857\n18  dmis ~~  dmis   0.889 0.037 23.960  0.000    0.816    0.961\n\n\nCreiamo un path diagram.\n\nsemPaths(fit,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 0.9,\n    style = \"ram\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\nGli autori esplorano un modello alternativo nel quale le relazioni causali vengono rovesciate: in questo caso è la difficoltà di regolazione delle emozioni positive ad essere la variabile esogena, e l’abuso di sostanze e l’abuso di alcol sono le variabili esogene.\n\nmod_alt &lt;- \"\n  drpe =~ NA*acc + goal + imp\n  amis =~ NA*con + dep + consu\n  drpe ~ amis + dmis\n  dmis ~~ amis\n  drpe ~~ 1*drpe\n  amis ~~ 1*amis\n\"\n\n\nfit_alt &lt;- sem(mod_alt, sample.cov = dat_cov, sample.nobs = 311)\n\n\nstandardizedSolution(fit_alt) |&gt; print()\n\n     lhs op   rhs est.std    se      z pvalue ci.lower ci.upper\n1   drpe =~   acc   0.770 0.029 26.143  0.000    0.712    0.828\n2   drpe =~  goal   0.728 0.032 22.864  0.000    0.666    0.791\n3   drpe =~   imp   0.945 0.023 41.149  0.000    0.900    0.990\n4   amis =~   con   0.837 0.038 22.203  0.000    0.763    0.910\n5   amis =~   dep   0.756 0.039 19.276  0.000    0.679    0.833\n6   amis =~ consu   0.494 0.050  9.877  0.000    0.396    0.592\n7   drpe  ~  amis   0.115 0.075  1.549  0.121   -0.031    0.261\n8   drpe  ~  dmis   0.276 0.066  4.189  0.000    0.147    0.405\n9   amis ~~  dmis   0.503 0.050 10.122  0.000    0.405    0.600\n10  drpe ~~  drpe   0.879 0.037 23.633  0.000    0.806    0.952\n11  amis ~~  amis   1.000 0.000     NA     NA    1.000    1.000\n12   acc ~~   acc   0.407 0.045  8.973  0.000    0.318    0.496\n13  goal ~~  goal   0.470 0.046 10.126  0.000    0.379    0.561\n14   imp ~~   imp   0.107 0.043  2.458  0.014    0.022    0.192\n15   con ~~   con   0.300 0.063  4.763  0.000    0.177    0.424\n16   dep ~~   dep   0.428 0.059  7.221  0.000    0.312    0.545\n17 consu ~~ consu   0.756 0.049 15.273  0.000    0.659    0.853\n18  dmis ~~  dmis   1.000 0.000     NA     NA    1.000    1.000\n\n\n\nsemPaths(fit_alt,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 0.9,\n    style = \"ram\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\nVedremo in seguito come sia possibile eseguire un test statistico per stabilire quale di due modelli sia più appropriato. Anticipando qui tale discussione, applichiamo il test del rapporto di verosimiglianze.\n\nlavTestLRT(fit, fit_alt) |&gt; print()\n\n\nChi-Squared Difference Test\n\n        Df    AIC    BIC  Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nfit     12 4963.6 5022.0 38.211                                    \nfit_alt 12 5433.1 5492.9 41.844     3.6327     0       0           \n\n\nI risultati di questo test suggeriscono che il primo modello è maggiormente appropriato per descrivere i dati raccolti da {cite:t}weiss2018difficulties.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/sem/15_prior_pred_mod_check.html",
    "href": "chapters/sem/15_prior_pred_mod_check.html",
    "title": "60  Prior Predictive Model Checking",
    "section": "",
    "text": "60.1 Introduzione\nIn questo capitolo verrà discusso l’approccio del Bayesian prior predictive similarity checking proposto da Bonifay et al. (2024).",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Prior Predictive Model Checking</span>"
    ]
  },
  {
    "objectID": "chapters/sem/15_prior_pred_mod_check.html#replicabilità-e-gof",
    "href": "chapters/sem/15_prior_pred_mod_check.html#replicabilità-e-gof",
    "title": "60  Prior Predictive Model Checking",
    "section": "60.2 Replicabilità e GOF",
    "text": "60.2 Replicabilità e GOF\nIl progresso scientifico empirico si basa sulla replicabilità, ossia la capacità di riprodurre i risultati di uno studio precedente seguendo le stesse procedure con nuovi dati (Bollen et al., 2015). Le ricerche sulla replicabilità in psicologia si sono spesso concentrate sugli effetti sperimentali (e.g., Klein, 2014; Open Science Collaboration, 2015; Youyou et al., 2023), ma molti ambiti della disciplina si fondano sull’uso di modelli statistici piuttosto che su un disegno sperimentale. Anche in questi contesti, è essenziale verificare il grado di replicabilità dei modelli statistici utilizzati.\nUn modo per quantificare la replicabilità dei modelli (sia nell’analisi di regressione, nei modelli a equazioni strutturali (SEM), nella teoria della risposta al item, nei modelli di rete o in altri contesti di modellizzazione) è valutare la bontà di adattamento (GOF, goodness of fit) del modello ai dati osservati. Storicamente, molti ricercatori in psicologia hanno considerato la replicabilità di un modello principalmente come la capacità di riprodurre la bontà di adattamento di uno studio precedente: «[il] miglior adattamento del modello… ha replicato i risultati» di ricerche precedenti (Whiteman et al., 2022, p. 132), «il miglior adattamento… ha replicato i risultati precedenti» (Giuntoli et al., 2021, p. 1668), «un adattamento sostanzialmente migliore… ha replicato l’approccio classico» (Fernández de la Cruz et al., 2018, p. 608). Tuttavia, Bonifay et al. (2024) fanno notare come questa pratica meriti un’attenta considerazione, in quanto la mera replicazione di una buona bontà di adattamento non è sufficiente per confermare la validità del modello statistico originale e della teoria sottostante.\nBonifay et al. (2024) propongono il seguente esempio. Si considerino le matrici di covarianza simulate mostrate nella riga superiore della Figura 1. La matrice a sinistra (Pannello B) rappresenta le covarianze tra le variabili di uno studio originale, mentre le altre due (Pannelli C e D) rappresentano le covarianze delle stesse variabili in due dataset di replicazione. Questo scenario illustra il tipico caso di replicazione del modello, in cui la stessa struttura viene adattata a dataset differenti, lasciando liberi i parametri. Sebbene le differenze nei dati siano evidenti, un modello con due fattori correlati si adatta bene a ciascuna matrice di covarianza (indice di adattamento comparativo [CFI] elevato, i.e., ≥ 0.95). Una bontà di adattamento elevata indica che il modello rappresenta adeguatamente le covarianze all’interno del dataset originale e delle repliche, ma non informa sul fatto che il modello rifletta le stesse relazioni tra le variabili. Come mostrato in Figura 1, affidarsi esclusivamente alla bontà di adattamento può portare a ignorare differenze significative nei pattern di dati.\n\n# Load Correlation Matrices ----\ncor_mats &lt;- readRDS(\n    here::here(\"data\", \"Bonifay\", \"figure1_cormat.RDS\")\n)\n\n# Create and Save Correlation Plots ----\ncorrplot(cor_mats[[1]],\n    method = \"circle\", type = \"full\",\n    tl.col = \"black\", tl.cex = 2.5, cl.pos = \"n\"\n)\n\ncorrplot(cor_mats[[2]],\n    method = \"circle\", type = \"full\",\n    tl.col = \"black\", tl.cex = 2.5, cl.pos = \"n\"\n)\n\ncorrplot(cor_mats[[3]],\n    method = \"circle\", type = \"full\",\n    tl.col = \"black\", tl.cex = 2.5, cl.pos = \"n\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncor_mats\n\n\n    $original\n        \n\nA matrix: 6 x 6 of type dbl\n\n\n\ny1\ny2\ny3\ny4\ny5\ny6\n\n\n\n\ny1\n1.000\n0.682\n0.638\n0.185\n0.507\n0.571\n\n\ny2\n0.682\n1.000\n0.644\n0.275\n0.661\n0.705\n\n\ny3\n0.638\n0.644\n1.000\n0.189\n0.473\n0.532\n\n\ny4\n0.185\n0.275\n0.189\n1.000\n0.468\n0.477\n\n\ny5\n0.507\n0.661\n0.473\n0.468\n1.000\n0.730\n\n\ny6\n0.571\n0.705\n0.532\n0.477\n0.730\n1.000\n\n\n\n\n\n    $replication1\n        \n\nA matrix: 6 x 6 of type dbl\n\n\n\ny1\ny2\ny3\ny4\ny5\ny6\n\n\n\n\ny1\n1.000\n0.389\n0.503\n0.527\n0.501\n0.391\n\n\ny2\n0.389\n1.000\n0.536\n0.432\n0.420\n0.423\n\n\ny3\n0.503\n0.536\n1.000\n0.782\n0.707\n0.493\n\n\ny4\n0.527\n0.432\n0.782\n1.000\n0.766\n0.452\n\n\ny5\n0.501\n0.420\n0.707\n0.766\n1.000\n0.380\n\n\ny6\n0.391\n0.423\n0.493\n0.452\n0.380\n1.000\n\n\n\n\n\n    $replication2\n        \n\nA matrix: 6 x 6 of type dbl\n\n\n\ny1\ny2\ny3\ny4\ny5\ny6\n\n\n\n\ny1\n1.0000\n-0.2736\n-0.3580\n0.0224\n0.1361\n0.0264\n\n\ny2\n-0.2736\n1.0000\n0.5371\n0.0917\n0.0198\n-0.0611\n\n\ny3\n-0.3580\n0.5371\n1.0000\n-0.0743\n-0.0419\n-0.0680\n\n\ny4\n0.0224\n0.0917\n-0.0743\n1.0000\n0.3464\n0.4743\n\n\ny5\n0.1361\n0.0198\n-0.0419\n0.3464\n1.0000\n0.3296\n\n\ny6\n0.0264\n-0.0611\n-0.0680\n0.4743\n0.3296\n1.0000\n\n\n\n\n\n\n\n\n\nM &lt;- '\n    F1 =~ NA*y1 + y2 + y3\n    F2 =~ NA*y4 + y5 + y6\n    F1 ~~ 1*F1\n    F2 ~~ 1 * F2\n'\n\nfit_original &lt;- cfa(model = M, sample.cov = cor_mats$original, sample.nobs = 1000)\nparameterEstimates(fit_original)\n\n\nA lavaan.data.frame: 15 x 9\n\n\nlhs\nop\nrhs\nest\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nF1\n=~\ny1\n0.772\n0.0278\n27.73\n0\n0.718\n0.827\n\n\nF1\n=~\ny2\n0.899\n0.0260\n34.56\n0\n0.848\n0.949\n\n\nF1\n=~\ny3\n0.732\n0.0284\n25.77\n0\n0.677\n0.788\n\n\nF2\n=~\ny4\n0.502\n0.0311\n16.11\n0\n0.441\n0.563\n\n\nF2\n=~\ny5\n0.823\n0.0272\n30.22\n0\n0.770\n0.876\n\n\nF2\n=~\ny6\n0.903\n0.0262\n34.53\n0\n0.852\n0.954\n\n\nF1\n~~\nF1\n1.000\n0.0000\nNA\nNA\n1.000\n1.000\n\n\nF2\n~~\nF2\n1.000\n0.0000\nNA\nNA\n1.000\n1.000\n\n\ny1\n~~\ny1\n0.403\n0.0224\n18.00\n0\n0.359\n0.447\n\n\ny2\n~~\ny2\n0.192\n0.0183\n10.49\n0\n0.156\n0.227\n\n\ny3\n~~\ny3\n0.463\n0.0243\n19.03\n0\n0.415\n0.511\n\n\ny4\n~~\ny4\n0.747\n0.0348\n21.45\n0\n0.679\n0.816\n\n\ny5\n~~\ny5\n0.322\n0.0206\n15.59\n0\n0.281\n0.362\n\n\ny6\n~~\ny6\n0.183\n0.0192\n9.53\n0\n0.146\n0.221\n\n\nF1\n~~\nF2\n0.835\n0.0156\n53.59\n0\n0.804\n0.865\n\n\n\n\n\n\nfitMeasures(fit_original, \"cfi\") |&gt; round(2)\n\ncfi: 0.96\n\n\n\nsemPaths(fit_original,\n    whatLabels = \"std\",\n    sizeMan = 8,\n    edge.label.cex = 0.7,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\n\nfit_rep1 &lt;- cfa(model = M, sample.cov = cor_mats$replication1, sample.nobs = 1000)\nparameterEstimates(fit_rep1)\n\n\nA lavaan.data.frame: 15 x 9\n\n\nlhs\nop\nrhs\nest\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nF1\n=~\ny1\n0.596\n0.0300\n19.88\n0\n0.537\n0.655\n\n\nF1\n=~\ny2\n0.566\n0.0303\n18.69\n0\n0.507\n0.626\n\n\nF1\n=~\ny3\n0.899\n0.0265\n33.95\n0\n0.847\n0.951\n\n\nF2\n=~\ny4\n0.904\n0.0254\n35.67\n0\n0.855\n0.954\n\n\nF2\n=~\ny5\n0.830\n0.0265\n31.34\n0\n0.778\n0.882\n\n\nF2\n=~\ny6\n0.524\n0.0306\n17.14\n0\n0.464\n0.584\n\n\nF1\n~~\nF1\n1.000\n0.0000\nNA\nNA\n1.000\n1.000\n\n\nF2\n~~\nF2\n1.000\n0.0000\nNA\nNA\n1.000\n1.000\n\n\ny1\n~~\ny1\n0.644\n0.0306\n21.02\n0\n0.584\n0.704\n\n\ny2\n~~\ny2\n0.678\n0.0320\n21.21\n0\n0.616\n0.741\n\n\ny3\n~~\ny3\n0.191\n0.0203\n9.41\n0\n0.151\n0.231\n\n\ny4\n~~\ny4\n0.181\n0.0154\n11.73\n0\n0.151\n0.211\n\n\ny5\n~~\ny5\n0.310\n0.0180\n17.23\n0\n0.274\n0.345\n\n\ny6\n~~\ny6\n0.724\n0.0336\n21.54\n0\n0.658\n0.790\n\n\nF1\n~~\nF2\n0.959\n0.0127\n75.76\n0\n0.934\n0.984\n\n\n\n\n\n\nfitMeasures(fit_rep1, \"cfi\") |&gt; round(2)\n\ncfi: 0.96\n\n\n\nfit_rep2 &lt;- cfa(model = M, sample.cov = cor_mats$replication2, sample.nobs = 1000)\nparameterEstimates(fit_rep2)\n\n\nA lavaan.data.frame: 15 x 9\n\n\nlhs\nop\nrhs\nest\nse\nz\npvalue\nci.lower\nci.upper\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nF1\n=~\ny1\n0.422\n0.0363\n11.62\n0.000000\n0.35098\n0.493\n\n\nF1\n=~\ny2\n-0.627\n0.0412\n-15.23\n0.000000\n-0.70783\n-0.546\n\n\nF1\n=~\ny3\n-0.855\n0.0479\n-17.86\n0.000000\n-0.94899\n-0.761\n\n\nF2\n=~\ny4\n0.701\n0.0418\n16.78\n0.000000\n0.61899\n0.783\n\n\nF2\n=~\ny5\n0.491\n0.0370\n13.26\n0.000000\n0.41836\n0.563\n\n\nF2\n=~\ny6\n0.676\n0.0411\n16.43\n0.000000\n0.59526\n0.757\n\n\nF1\n~~\nF1\n1.000\n0.0000\nNA\nNA\n1.00000\n1.000\n\n\nF2\n~~\nF2\n1.000\n0.0000\nNA\nNA\n1.00000\n1.000\n\n\ny1\n~~\ny1\n0.821\n0.0405\n20.27\n0.000000\n0.74139\n0.900\n\n\ny2\n~~\ny2\n0.606\n0.0463\n13.09\n0.000000\n0.51506\n0.696\n\n\ny3\n~~\ny3\n0.268\n0.0707\n3.79\n0.000152\n0.12918\n0.406\n\n\ny4\n~~\ny4\n0.508\n0.0496\n10.23\n0.000000\n0.41057\n0.605\n\n\ny5\n~~\ny5\n0.758\n0.0403\n18.81\n0.000000\n0.67902\n0.837\n\n\ny6\n~~\ny6\n0.542\n0.0477\n11.37\n0.000000\n0.44876\n0.636\n\n\nF1\n~~\nF2\n0.091\n0.0432\n2.11\n0.035024\n0.00639\n0.176\n\n\n\n\n\n\nfitMeasures(fit_rep2, \"cfi\") |&gt; round(2)\n\ncfi: 0.94\n\n\nA complicare ulteriormente le cose, l’adattamento dello stesso modello a ciascuna matrice di dati produce stime dei parametri molto variabili. Ad esempio, il fattore di carico standardizzato l21 (secondo indicatore sul primo fattore) è stimato a 0.90, 0.57 e -2.63 nelle tre matrici, mentre la correlazione tra i fattori (c21) varia da quasi indipendenza (0.09) a una sovrapposizione quasi totale (0.96).\nIn sintesi, la bontà di adattamento non ci fornisce alcuna indicazione sul grado di somiglianza tra il dataset di replicazione, i parametri del modello e quelli dello studio originale, mentre una forte somiglianza tra questi elementi è cruciale per valutare il successo della replicazione.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Prior Predictive Model Checking</span>"
    ]
  },
  {
    "objectID": "chapters/sem/15_prior_pred_mod_check.html#bontà-di-adattamento-e-replicazione",
    "href": "chapters/sem/15_prior_pred_mod_check.html#bontà-di-adattamento-e-replicazione",
    "title": "60  Prior Predictive Model Checking",
    "section": "60.3 Bontà di Adattamento e Replicazione",
    "text": "60.3 Bontà di Adattamento e Replicazione\nNel loro approfondimento critico sull’uso della bontà di adattamento come strumento per testare le teorie, Roberts & Pashler (2000) hanno sostenuto che «dimostrare che una teoria si adatta ai dati… è quasi privo di significato» (p. 361; vedi anche Vanpaemel, 2020). In particolare, hanno individuato tre limiti del GOF che ne impediscono l’uso come supporto teorico solido:\n\nNon chiarisce cosa predice una teoria.\n\nNon spiega la variabilità dei dati.\n\nNon considera la probabilità a priori che la teoria possa adattarsi a qualsiasi insieme di dati plausibili.\n\nDi conseguenza, Roberts & Pashler (2000) hanno concluso che il GOF fornisce un supporto convincente a una teoria solo quando sia i dati che la teoria sono ben vincolati, ovvero quando i dati non sono troppo variabili e la teoria non è troppo flessibile. Tuttavia, in un singolo studio, tali vincoli possono essere difficili da definire e applicare, anche per la mancanza di criteri di riferimento (ad esempio, cosa significa dire che i dati «non sono troppo variabili»? Variabili rispetto a cosa? E in che misura?).\nNel contesto delle repliche, però, il confronto con lo studio originale offre un chiaro riferimento per caratterizzare la variabilità dei dati e delle stime dei parametri del modello. Questo consente di estendere naturalmente le tre critiche di Roberts & Pashler (2000) al tema della replicazione:\n\nPrevisione limitata del risultato della replica\nLa bontà di adattamento dello studio originale non fornisce alcuna informazione sostanziale sull’esito della replica. Il fatto che un modello si sia adattato bene nello studio originale non implica che si replichino aspetti inferenziali più importanti, come i pattern dei dati o le stime dei parametri.\nAssenza di indicazioni sulla somiglianza tra i dati originali e quelli della replica\nDue set di dati possono presentare pattern nettamente distinti, potenzialmente derivanti da meccanismi di generazione diversi. Tuttavia, il modello potrebbe mascherare queste differenze, compromettendo l’accuratezza delle inferenze.\nTendenza intrinseca del modello ad adattarsi bene\nSe un modello possiede una forte predisposizione ad adattarsi bene ai dati (Bonifay & Cai, 2017; Falk & Muthukrishna, 2023), una buona bontà di adattamento per i dati originali e di replica non rappresenta una sorpresa né un valore scientifico. In tali casi, il GOF può replicarsi indipendentemente dai pattern specifici dei dati che il modello intende rappresentare.\n\nIdealmente, i ricercatori possono essere fiduciosi che i loro risultati offrano un supporto alla teoria alla base del modello statistico solo se dimostrano che i dati della replica non sono più variabili rispetto ai dati originali, e che le stime dei parametri nella replica non riflettono una maggiore flessibilità rispetto a quelle originali.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Prior Predictive Model Checking</span>"
    ]
  },
  {
    "objectID": "chapters/sem/15_prior_pred_mod_check.html#definire-lobiettivo-della-replica",
    "href": "chapters/sem/15_prior_pred_mod_check.html#definire-lobiettivo-della-replica",
    "title": "60  Prior Predictive Model Checking",
    "section": "60.4 Definire l’Obiettivo della Replica",
    "text": "60.4 Definire l’Obiettivo della Replica\nIn psicologia, ottenere una somiglianza perfetta tra studi è poco pratico e probabilmente non necessario, anche a causa dell’eterogeneità intrinseca della popolazione (McShane et al., 2019). Piuttosto, i ricercatori dovrebbero focalizzarsi sugli aspetti specifici dello studio originale che intendono replicare. A tal proposito, la Figura 2 illustra un approccio più ragionevole per definire obiettivi di replicazione.\n\n\n\n\n\n\nFigura 60.1: Definire Obiettivi Chiari è Essenziale per Indagare la Replicazione dei Modelli Statistici. Nota: Per andare oltre la semplice replicazione della bontà di adattamento (Goodness of Fit), i ricercatori devono mirare alle aree più significative e centrali del bersaglio. Questo richiede test più rigorosi dei dati e/o dei parametri del modello, che possono essere condotti utilizzando tecniche bayesiane di verifica della similarità predittiva a priori, con l’impiego di distribuzioni a priori sempre più informative, come illustrato nella parte inferiore della figura. La replicazione informata dalla teoria richiede, come minimo, di verificare ipotesi specifiche sui dati e/o sui parametri del modello. La replicazione empirica, sia approssimativa che ravvicinata, implica invece il controllo che i dati replicati e/o i parametri del modello siano rispettivamente approssimativamente o strettamente simili a quelli dello studio originale. (Figura tratta da Bonifay et al., 2024)\n\n\n\n\nCerchio esterno del bersaglio: rappresenta la pratica attuale nelle scienze sociali, spesso limitata a verificare che il modello originale abbia una buona bontà di adattamento ai dati della replica, senza considerare le caratteristiche empiriche dello studio originale. Questa pratica offre il supporto più debole alla teoria originale.\nCerchi interni: rappresentano obiettivi progressivamente più ambiziosi.\n\n\n60.4.1 Obiettivi di replica:\n\nReplicazione informata dalla teoria: un ricercatore interessato alle implicazioni teoriche più ampie dello studio originale può puntare alla teoria sottostante, formulando ipotesi specifiche sui dati o sulle stime dei parametri (es. “Per supportare l’associazione positiva tra x e y, il coefficiente di replica b deve avere un valore positivo”). Questo approccio supera la semplice verifica del GOF e merita studi dedicati.\nReplicazione empirica approssimativa: un ricercatore che desidera replicare direttamente i risultati empirici può puntare allo studio originale, testando la somiglianza approssimativa tra i dati e il modello della replica rispetto a quelli originali. Ad esempio, si può verificare se le covarianze tra i dati di replica riflettono quelle dello studio originale o se le stime dei parametri sono simili (es. “Per una replica approssimativa, b1 deve essere tra 0.4 e 0.7”).\nReplicazione empirica ravvicinata: è il test più rigoroso, in cui le stime devono essere estremamente simili a quelle originali (es. “Per una replica ravvicinata, b1 deve essere tra 0.52 e 0.58”). Riuscire a soddisfare tali criteri fornisce prove solide che il modello cattura lo stesso segnale in entrambi gli studi.\n\n\n\n60.4.2 Test progressivi e rischiosi\nLa struttura a cerchi concentrici del bersaglio rappresenta una sequenza di test sempre più stringenti. Come osservato da Roberts & Pashler (2000), i test di bontà di adattamento sono spesso troppo facili da superare, rendendoli deboli come prova di replica. La replicazione basata sul GOF è l’obiettivo più facile (e per alcuni modelli può comportare un rischio di fallimento praticamente nullo), offrendo il supporto più debole ai risultati originali.\n\nCerchi interni del bersaglio: man mano che ci si avvicina al centro, il rischio di fallimento aumenta, ma aumenta anche la forza delle prove a favore della replica. La replicazione informata dalla teoria è più rigorosa rispetto al GOF, offrendo supporto alla teoria sottostante. La replicazione empirica approssimativa è ancora più rischiosa, ma fornisce prove solide di somiglianza tra dati e parametri. Infine, la replicazione empirica ravvicinata è il test più rischioso, ma il suo successo rappresenta una prova molto forte della replica dei risultati originali.\n\nCome sottolineato da Waller & Meehl (2002), «i test rischiosi sono i mezzi più efficienti per valutare la solidità di una teoria».\nNella seconda parte dell’articolo, Bonifay et al. (2024) presentano un metodo statistico per quantificare la somiglianza tra i dati originali e replicati, oltre che tra le stime dei parametri. Discutono un esempio concreto nel contesto della modellizzazione della struttura latente della psicopatologia e forniscono raccomandazioni per futuri studi di replicazione.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Prior Predictive Model Checking</span>"
    ]
  },
  {
    "objectID": "chapters/sem/15_prior_pred_mod_check.html#verifica-predittiva-a-priori-dei-modelli",
    "href": "chapters/sem/15_prior_pred_mod_check.html#verifica-predittiva-a-priori-dei-modelli",
    "title": "60  Prior Predictive Model Checking",
    "section": "60.5 Verifica Predittiva a Priori dei Modelli",
    "text": "60.5 Verifica Predittiva a Priori dei Modelli\nPer indagare formalmente la somiglianza tra dati originali e di replica, nonché tra le stime dei parametri, Bonifay et al. (2024) propongono di utilizzare la verifica predittiva a priori bayesiana (Prior Predictive Model Checking, PrPMC; Box, 1980; Evans & Moshonov, 2006; Gelman et al., 2017). Questa tecnica sfrutta le distribuzioni a priori per valutare le implicazioni del modello prima di includere i dati osservati nell’analisi.\nIn un’analisi bayesiana, l’obiettivo principale è calcolare la distribuzione a posteriori \\(p(\\theta \\mid y)\\), combinando le informazioni sui dati osservati \\(y\\) e sul parametro sconosciuto \\(\\theta\\) tramite il teorema di Bayes:\n\\[\np(\\theta \\mid y) \\propto p(y \\mid \\theta)p(\\theta),\n\\]\ndove:\n\n\\(p(\\theta)\\) è la distribuzione a priori dei parametri.\n\\(p(y \\mid \\theta)\\) è la funzione di verosimiglianza dei dati dato il modello.\n\nLa PrPMC consiste nel generare campioni predittivi ipotetici per ciascuna variabile osservata, utilizzando esclusivamente le distribuzioni a priori definite sui parametri del modello. Questi campioni rappresentano scenari plausibili in base alle aspettative incorporate nei priori, permettendo una valutazione preliminare del modello.\nSupponiamo di voler stimare l’altezza media degli scalatori negli Stati Uniti, ipotizzando che sia vicina alla media nazionale di 168 cm (Fryar et al., 2021). Possiamo rappresentare questa aspettativa con una distribuzione normale \\(N(\\mu = 168, \\sigma = 10)\\). Generando campioni predittivi da questa distribuzione, otteniamo valori che variano plausibilmente attorno a questa media. Se la conoscenza sugli scalatori suggerisse un valore maggiore o minore, potremmo affinare i priori (ad esempio, aumentando \\(\\mu\\) o riducendo \\(\\sigma\\)) prima di raccogliere i dati.\nLa PrPMC può essere estesa per confrontare i dati osservati con i campioni predittivi. Questo confronto utilizza una statistica di test o una quantità di test per valutare la somiglianza tra i dati osservati e le aspettative predittive.\n\nStatistica di test: una proprietà statistica dei dati (es. mediana, range).\nQuantità di test: una proprietà dipendente dai dati e dal modello (es. stime dei parametri o indici di bontà di adattamento).\n\nAd esempio, per gli scalatori, potremmo confrontare la media delle altezze osservate con le medie dei campioni predittivi. Se la media osservata si trovasse agli estremi della distribuzione predittiva (ad esempio, \\(prpp \\leq 0.05\\) o \\(prpp \\geq 0.95\\)), ciò indicherebbe una discrepanza sistematica tra i dati osservati e le aspettative a priori.\nBonifay et al. (2024) propongono l’uso della PrPMC per verificare la somiglianza tra dati originali e di replica, così come tra le stime dei parametri del modello. Questo approccio, chiamato verifica di similarità predittiva a priori, consente di valutare:\n\nSomiglianza dei dati: confrontando la distribuzione dei dati replicati con le aspettative dei dati originali (es. intercorrelazioni tra item).\nSomiglianza dei parametri: confrontando le stime dei parametri derivanti dai dati di replica con quelle predette dal modello originale. Ad esempio, i caricamenti fattoriali stimati dal modello originale possono essere confrontati con quelli derivati da campioni predittivi a priori.\n\nCome illustrato nella Figura 60.1, i cerchi concentrici rappresentano diversi livelli di rischio e severità nei test di replica:\n\nCerchio esterno: distribuzioni a priori diffuse, che riflettono una bassa restrizione sui dati e sui parametri, portando a test meno rigorosi e meno significativi.\nCentro del bersaglio: distribuzioni a priori altamente informative, con restrizioni strette sui dati e sui parametri, producendo test più rigorosi e significativi.\n\nSe il valore \\(prpp\\) risultante si trova tra \\(0.05\\) e \\(0.95\\), possiamo concludere che i dati e/o i parametri replicati sono coerenti con le aspettative a priori. Questo approccio consente di condurre test progressivamente più stringenti e di acquisire prove più solide del successo della replica.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Prior Predictive Model Checking</span>"
    ]
  },
  {
    "objectID": "chapters/sem/15_prior_pred_mod_check.html#riflessioni-conclusive",
    "href": "chapters/sem/15_prior_pred_mod_check.html#riflessioni-conclusive",
    "title": "60  Prior Predictive Model Checking",
    "section": "60.6 Riflessioni Conclusive",
    "text": "60.6 Riflessioni Conclusive\nIn conclusione, la verifica predittiva a priori offre un metodo formale per quantificare la somiglianza tra studi originali e repliche, sia a livello di dati che di parametri. Implementando questa metodologia, i ricercatori possono definire obiettivi chiari e condurre analisi rigorose per valutare il successo della replicazione, migliorando così la robustezza delle conclusioni dello studio.",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Prior Predictive Model Checking</span>"
    ]
  },
  {
    "objectID": "chapters/sem/15_prior_pred_mod_check.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/sem/15_prior_pred_mod_check.html#informazioni-sullambiente-di-sviluppo",
    "title": "60  Prior Predictive Model Checking",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nRandom number generation:\n RNG:     L'Ecuyer-CMRG \n Normal:  Inversion \n Sample:  Rejection \n \nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] quantreg_5.99     compute.es_0.2-5  simsem_0.5-16     kableExtra_1.4.0 \n [5] MASS_7.3-61       viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n [9] ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1 \n[13] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19     psych_2.4.6.26   \n[17] scales_1.3.0      markdown_1.13     knitr_1.49        lubridate_1.9.3  \n[21] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[25] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n[29] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.2       later_1.3.2         pbdZMQ_0.3-13      \n  [4] R.oo_1.27.0         XML_3.99-0.17       rpart_4.1.23       \n  [7] lifecycle_1.0.4     rstatix_0.7.2       rprojroot_2.0.4    \n [10] lattice_0.22-6      rockchalk_1.8.157   backports_1.5.0    \n [13] magrittr_2.0.3      openxlsx_4.2.7.1    Hmisc_5.2-0        \n [16] rmarkdown_2.29      httpuv_1.6.15       qgraph_1.9.8       \n [19] zip_2.3.1           pbapply_1.7-2       minqa_1.2.8        \n [22] multcomp_1.4-26     abind_1.4-8         quadprog_1.5-8     \n [25] R.utils_2.12.3      nnet_7.3-19         TH.data_1.1-2      \n [28] sandwich_3.1-1      openintro_2.5.0     arm_1.14-4         \n [31] MatrixModels_0.5-3  airports_0.1.0      svglite_2.1.3      \n [34] codetools_0.2-20    xml2_1.3.6          tidyselect_1.2.1   \n [37] farver_2.1.2        lme4_1.1-35.5       stats4_4.4.2       \n [40] base64enc_0.1-3     jsonlite_1.8.9      Formula_1.2-5      \n [43] survival_3.7-0      emmeans_1.10.5      systemfonts_1.1.0  \n [46] tools_4.4.2         rio_1.2.3           Rcpp_1.0.13-1      \n [49] glue_1.8.0          mnormt_2.1.1        xfun_0.49          \n [52] IRdisplay_1.1       withr_3.0.2         fastmap_1.2.0      \n [55] boot_1.3-31         fansi_1.0.6         SparseM_1.84-2     \n [58] digest_0.6.37       mi_1.1              timechange_0.3.0   \n [61] R6_2.5.1            mime_0.12           estimability_1.5.1 \n [64] colorspace_2.1-1    Cairo_1.6-2         gtools_3.9.5       \n [67] jpeg_0.1-10         R.methodsS3_1.8.2   utf8_1.2.4         \n [70] generics_0.1.3      data.table_1.16.2   corpcor_1.6.10     \n [73] usdata_0.3.1        htmlwidgets_1.6.4   pkgconfig_2.0.3    \n [76] sem_3.1-16          gtable_0.3.6        htmltools_0.5.8.1  \n [79] carData_3.0-5       png_0.1-8           rstudioapi_0.17.1  \n [82] tzdb_0.4.0          reshape2_1.4.4      uuid_1.2-1         \n [85] coda_0.19-4.1       checkmate_2.3.2     nlme_3.1-166       \n [88] nloptr_2.1.1        repr_1.1.7          zoo_1.8-12         \n [91] parallel_4.4.2      miniUI_0.1.1.1      foreign_0.8-87     \n [94] pillar_1.9.0        grid_4.4.2          vctrs_0.6.5        \n [97] promises_1.3.0      car_3.1-3           OpenMx_2.21.13     \n[100] xtable_1.8-4        cluster_2.1.6       htmlTable_2.4.3    \n[103] evaluate_1.0.1      pbivnorm_0.6.0      mvtnorm_1.3-2      \n[106] cli_3.6.3           kutils_1.73         compiler_4.4.2     \n[109] rlang_1.1.4         crayon_1.5.3        ggsignif_0.6.4     \n[112] fdrtool_1.2.18      plyr_1.8.9          stringi_1.8.4      \n[115] munsell_0.5.1       lisrelToR_0.3       pacman_0.5.1       \n[118] Matrix_1.7-1        IRkernel_1.3.2      hms_1.1.3          \n[121] glasso_1.11         shiny_1.9.1         igraph_2.1.1       \n[124] broom_1.0.7         RcppParallel_5.1.9  cherryblossom_0.1.0\n\n\n\n\n\n\nBonifay, W., Winter, S. D., Skoblow, H. F., & Watts, A. L. (2024). Good fit is weak evidence of replication: increasing rigor through prior predictive similarity checking. Assessment, 10731911241234118.\n\n\nRoberts, S., & Pashler, H. (2000). How persuasive is a good fit? A comment on theory testing. Psychological Review, 107(2), 358–367.\n\n\nWaller, N. G., & Meehl, P. E. (2002). Risky tests, verisimilitude, and path analysis. Psychological Methods, 7(3), 323–337. https://doi.org/10.1037/1082-989X.7.3.323",
    "crumbs": [
      "SEM",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Prior Predictive Model Checking</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html",
    "href": "chapters/mokken/01_core_issues.html",
    "title": "61  Analisi della Scala di Mokken",
    "section": "",
    "text": "61.1 Introduzione\nL’Analisi delle Scale Mokken (MSA), così chiamata in onore del matematico e scienziato politico olandese Robert J. Mokken, è un insieme di metodi basati sulla Teoria Non Parametrica della Risposta agli Item (NIRT) che consente di valutare l’adeguatezza dei dati rispetto ai modelli non parametrici. Nella Teoria della Risposta agli Item (IRT), i costrutti psicologici sono considerati latenti, cioè non direttamente osservabili, ma si manifestano attraverso le risposte ai test. Le risposte degli individui agli item di un test riflettono la loro posizione su un continuum latente e indicano il grado in cui possiedono il costrutto oggetto di misurazione.\nTuttavia, la relazione tra gli item di un test e le risposte dei partecipanti non sempre rappresenta fedelmente il costrutto in questione. I modelli della IRT offrono strumenti per esaminare la congruenza e la rilevanza degli item rispetto alla variabile latente sottostante. I modelli della MSA, in particolare, sono modelli probabilistici non parametrici basati su tratti latenti e giocano un ruolo fondamentale nella validazione degli strumenti psicometrici, ordinando sia i rispondenti che gli item lungo una scala ordinale. Applicabili sia a item dicotomici che politomici, i modelli MSA sono meno restrittivi rispetto ai modelli IRT parametrici, in quanto non assumono una forma specifica per la funzione di risposta agli item. Questa flessibilità rende i modelli MSA strumenti preziosi, pur comportando alcune limitazioni interpretative.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#analisi-delle-scale-di-mokken-msa",
    "href": "chapters/mokken/01_core_issues.html#analisi-delle-scale-di-mokken-msa",
    "title": "61  Analisi della Scala di Mokken",
    "section": "61.2 Analisi delle Scale di Mokken (MSA)",
    "text": "61.2 Analisi delle Scale di Mokken (MSA)\nL’Analisi delle Scale di Mokken (MSA) trova le sue radici nel modello di Guttman, ampliandone la portata grazie a un approccio probabilistico che supera le rigidità del modello deterministico originale. Il modello di Guttman si basa sul principio di perfetta cumulatività, secondo cui un rispondente che risponde correttamente a un item più difficile dovrebbe necessariamente rispondere correttamente anche a tutti gli item più semplici. Sebbene questa proprietà consenta di costruire scale gerarchiche, la sua applicazione pratica risulta spesso eccessivamente rigida, poiché non tiene conto delle naturali deviazioni che emergono nei dati empirici.\nLa MSA, grazie al suo approccio probabilistico, supera le rigidità del modello deterministico di Guttman, permettendo di gestire eventuali violazioni della cumulatività senza compromettere la rappresentazione delle relazioni tra item e tratti latenti. Questo rende la MSA più aderente alla complessità delle risposte umane, che spesso risentono di variabili esterne o di fattori imprevedibili.\nSimile per filosofia al modello di Rasch, la MSA è meno vincolante nelle sue assunzioni, risultando più flessibile nell’adattarsi ai dati reali. Questa caratteristica la rende particolarmente utile per misurare costrutti psicologici complessi e multidimensionali. Inoltre, la sua capacità di individuare strutture sottostanti robuste ne fa uno strumento fondamentale per lo sviluppo e la validazione di scale psicometriche, garantendo al contempo una base solida per l’interpretazione e l’utilizzo dei risultati in contesti di ricerca e applicazione.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#assunzioni-dellanalisi-delle-scale-di-mokken",
    "href": "chapters/mokken/01_core_issues.html#assunzioni-dellanalisi-delle-scale-di-mokken",
    "title": "61  Analisi della Scala di Mokken",
    "section": "61.3 Assunzioni dell’Analisi delle Scale di Mokken",
    "text": "61.3 Assunzioni dell’Analisi delle Scale di Mokken\nLa MSA, pur essendo un approccio non parametrico, condivide alcune assunzioni fondamentali con i modelli parametrici della Teoria della Risposta agli Item (IRT). Tuttavia, queste assunzioni vengono applicate in modo più flessibile, rendendole più adatte a situazioni in cui i dati non soddisfano i rigorosi requisiti dei modelli parametrici. Le principali assunzioni della MSA sono descritte di seguito.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#assunzione-di-unidimensionalità",
    "href": "chapters/mokken/01_core_issues.html#assunzione-di-unidimensionalità",
    "title": "61  Analisi della Scala di Mokken",
    "section": "61.4 Assunzione di Unidimensionalità",
    "text": "61.4 Assunzione di Unidimensionalità\nL’assunzione di unidimensionalità richiede che le risposte agli item siano governate da un unico tratto latente, ovvero che tutti gli item della scala misurino lo stesso costrutto psicologico. Questo tratto rappresenta una variabile sottostante non osservabile che determina le risposte ai singoli item. Nella pratica, progettare scale che misurano un unico tratto latente facilita l’interpretazione dei punteggi, riduce la complessità analitica e aumenta la validità delle conclusioni.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#assunzione-di-indipendenza-locale",
    "href": "chapters/mokken/01_core_issues.html#assunzione-di-indipendenza-locale",
    "title": "61  Analisi della Scala di Mokken",
    "section": "61.5 Assunzione di Indipendenza Locale",
    "text": "61.5 Assunzione di Indipendenza Locale\nL’indipendenza locale è una delle assunzioni fondamentali nell’Analisi delle Scale di Mokken e più in generale nella Teoria della Risposta agli Item (IRT). Questa proprietà stabilisce che, una volta fissato il tratto latente \\(\\theta\\) (il livello della variabile latente che governa le risposte), le risposte agli item siano condizionalmente indipendenti. In altre parole, dato \\(\\theta\\), il comportamento di un rispondente su un item non influenza le sue risposte sugli altri item.\nL’indipendenza locale può essere espressa formalmente come:\n\\[\nP(X = x \\mid \\theta) = \\prod_{i=1}^k P(X_i = x_i \\mid \\theta),\n\\]\ndove:\n\n\\(X\\) rappresenta il vettore delle risposte agli \\(k\\) item (\\(X = (X_1, X_2, \\dots, X_k)\\)),\n\\(x\\) è un’istanza di risposte (\\(x = (x_1, x_2, \\dots, x_k)\\)),\n\\(P(X_i = x_i \\mid \\theta)\\) è la probabilità di rispondere \\(x_i\\) all’item \\(i\\), dato il livello del tratto latente \\(\\theta\\).\n\nL’assunzione implica che ogni probabilità condizionata dipende esclusivamente da \\(\\theta\\) e non dalle risposte agli altri item.\nL’indipendenza locale ha due conseguenze fondamentali:\n\nCovarianza nulla tra item (dato \\(\\theta\\)): La correlazione osservata tra le risposte agli item è attribuibile esclusivamente alla variabilità di \\(\\theta\\). Se il tratto latente è fissato, la covarianza tra due item qualsiasi sarà pari a zero.\nRappresentazione pura del tratto latente: Le risposte riflettono il tratto latente senza essere influenzate da altre relazioni o interazioni tra gli item (ad esempio, sovrapposizione di contenuto o sequenze logiche negli item).\n\nQuando l’indipendenza locale non è rispettata, le risposte agli item possono essere influenzate da fattori esterni (come stanchezza, contesto, o similarità degli item) o da interazioni tra gli item stessi (ad esempio, se rispondere a un item fornisce un indizio per rispondere a un altro).\nUn esempio di violazione è un bias di contenuto: se due item condividono un contenuto simile (ad esempio, entrambi riguardano competenze matematiche legate alle frazioni), le risposte possono essere correlate anche oltre ciò che il tratto latente \\(\\theta\\) può spiegare.\nIn conclusione, l’indipendenza locale garantisce che le risposte ai test riflettano esclusivamente il tratto latente, senza essere influenzate da fattori esterni o interazioni tra item. Sebbene fondamentale, questa assunzione può essere violata in situazioni pratiche, e la sua verifica è essenziale per valutare la validità di un modello di misura come l’MSA o l’IRT.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#assunzione-di-monotonicità-latente",
    "href": "chapters/mokken/01_core_issues.html#assunzione-di-monotonicità-latente",
    "title": "61  Analisi della Scala di Mokken",
    "section": "61.6 Assunzione di Monotonicità Latente",
    "text": "61.6 Assunzione di Monotonicità Latente\nL’assunzione di monotonicità latente stabilisce che, all’aumentare del tratto latente (\\(\\theta\\)), la probabilità di rispondere correttamente a un item (nel caso dicotomico) o di scegliere una categoria di risposta più alta (nel caso politomico) debba aumentare o rimanere costante. Matematicamente:\n\\[\nP_i(\\theta_a) \\leq P_i(\\theta_b) \\quad \\text{per} \\quad \\theta_a \\leq \\theta_b,\n\\]\ndove \\(P_i(\\theta)\\) rappresenta la probabilità di rispondere correttamente o di selezionare una risposta superiore all’item \\(i\\).\nIn pratica, questo significa che i rispondenti con livelli più elevati del tratto latente hanno una maggiore probabilità di ottenere punteggi più alti, garantendo una relazione coerente e interpretabile tra tratto latente e prestazione. Questa proprietà è particolarmente importante per costruire scale che riflettano una struttura ordinale robusta.\n\n61.6.1 Verifica della Monotonicità\nLa monotonicità richiede che la probabilità di rispondere correttamente a un item aumenti (o almeno non diminuisca) con l’aumento del livello del tratto latente sottostante (\\(\\theta\\)) o del restscore (punteggio residuo).\nIl restscore è il punteggio totale di un soggetto sugli item di un test, escluso l’item che si sta analizzando. Esso rappresenta una stima indiretta del livello del tratto latente di un soggetto, basata sulle sue risposte agli altri item.\n\n\nEsempio 61.1 Supponiamo di avere un test con 10 item e un soggetto che ha risposto nel modo seguente:\n\n\n\nItem\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nRisposta\n1\n1\n0\n1\n0\n1\n1\n1\n0\n1\n\n\n\nSe stiamo analizzando il comportamento dell’item 10, il restscore sarà calcolato come la somma delle risposte agli altri 9 item:\n\\[\n\\text{Restscore} = 1 + 1 + 0 + 1 + 0 + 1 + 1 + 1 + 0 = 6.\n\\]\nIl restscore fornisce un’indicazione del livello complessivo di abilità del soggetto, escludendo il contributo dell’item 10.\n\n\n\n\n61.6.2 Monotonicità e Funzioni di Risposta agli Item (IRF)\nPer verificare la monotonicità, si analizza la relazione tra i restscore e la probabilità di rispondere correttamente a un dato item. In un modello monotono, questa relazione deve essere non decrescente: all’aumentare del restscore, la probabilità di una risposta corretta deve crescere o rimanere costante.\nLa monotonicità può essere verificata tracciando le Funzioni di Risposta agli Item (IRF), che rappresentano graficamente come varia la probabilità di rispondere correttamente a un item in funzione del restscore.\n\n\nEsempio 61.2 Consideriamo un test con 5 item e i seguenti dati aggregati (restscore e frequenze di risposte corrette all’item 5):\n\n\n\n\n\n\n\n\n\nRestscore\nFrequenza Totale\nRisposte Corrette\nProbabilità Correttezza\n\n\n\n\n0\n5\n1\n\\(1/5 = 0.20\\)\n\n\n1\n10\n3\n\\(3/10 = 0.30\\)\n\n\n2\n15\n6\n\\(6/15 = 0.40\\)\n\n\n3\n10\n5\n\\(5/10 = 0.50\\)\n\n\n4\n5\n4\n\\(4/5 = 0.80\\)\n\n\n\nVerifica della Monotonicità.\n\nLa probabilità di rispondere correttamente aumenta con il restscore (0.20, 0.30, 0.40, 0.50, 0.80). Questo comportamento conferma la monotonicità.\nPer rappresentare visivamente questa relazione, si può tracciare un grafico:\n\n\nrestscore &lt;- c(0, 1, 2, 3, 4)\nprob_correct &lt;- c(0.20, 0.30, 0.40, 0.50, 0.80)\n\n# Grafico\nplot(restscore, prob_correct,\n    type = \"b\", pch = 16, col = \"blue\",\n    xlab = \"Restscore\", ylab = \"Probabilita' di Risposta Corretta\",\n    main = \"Funzione di Risposta all'Item (IRF)\"\n)\n\n\n\n\n\n\n\n\nIl grafico dovrebbe mostrare una curva non decrescente, confermando la monotonicità.\n\n\n\n\n61.6.3 Aggregazione dei Restscores\nQuando i restscore sono calcolati su un piccolo numero di rispondenti, le stime delle probabilità possono essere instabili. Per migliorare l’affidabilità, i restscore possono essere aggregati in gruppi.\n\n\nEsercizio 61.1 Se il numero di rispondenti con restscore pari a \\(0, 1\\) o \\(2\\) è troppo basso per stimare con precisione la probabilità di risposta corretta, si possono combinare i gruppi \\(0-2\\). In tal caso:\n\n\n\n\n\n\n\n\n\nGruppo Restscore\nFrequenza Totale\nRisposte Corrette\nProbabilità Correttezza\n\n\n\n\n0-2\n30\n10\n\\(10/30 = 0.33\\)\n\n\n3-4\n15\n9\n\\(9/15 = 0.60\\)\n\n\n\nQuesta aggregazione riduce la variabilità statistica mantenendo il focus sulla relazione tra restscore e probabilità di risposta corretta.\n\n\nIn conclusione, la verifica della monotonicità è cruciale per garantire che gli item di una scala rispettino l’assunzione fondamentale di relazione crescente tra il tratto latente (o il restscore) e la probabilità di risposta corretta. L’analisi grafica delle IRF e l’aggregazione dei restscore rappresentano strumenti pratici ed efficaci per individuare eventuali violazioni di questa proprietà.\n\n\n61.6.4 Monotonicità e Coefficienti di Scalabilità\nNell’MSA, la monotonicità è valutata anche attraverso i coefficienti di scalabilità (\\(H_i\\) per singoli item e \\(H_{ij}\\) per coppie di item). Per garantire la validità del Modello di Omogeneità Monotona (MHM), le covarianze tra tutte le coppie di item (\\(H_{ij}\\)) devono essere non negative. Tuttavia, la non negatività dei coefficienti di scalabilità non garantisce necessariamente che le IRF siano monotone. In pratica, item con valori di \\(H_i\\) superiori a 0.30 sono considerati accettabili.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#assunzione-di-non-intersezione-delle-funzioni-di-risposta",
    "href": "chapters/mokken/01_core_issues.html#assunzione-di-non-intersezione-delle-funzioni-di-risposta",
    "title": "61  Analisi della Scala di Mokken",
    "section": "61.7 Assunzione di Non-Intersezione delle Funzioni di Risposta",
    "text": "61.7 Assunzione di Non-Intersezione delle Funzioni di Risposta\nL’assunzione di non-intersezione delle funzioni di risposta (IRF) prevede che le probabilità di successo su item più difficili non superino mai quelle relative a item più facili, per ogni livello del tratto latente. In altre parole, le IRF devono essere ordinate in modo che la probabilità di rispondere correttamente a un item più difficile sia sempre inferiore o uguale rispetto a un item meno difficile. Formalmente, questa proprietà può essere espressa come:\n\\[\nP_1(\\theta) \\leq P_2(\\theta) \\leq ... \\leq P_k(\\theta) \\quad \\text{per ogni} \\ \\theta\n\\]\nL’intersezione delle IRF comporterebbe una violazione dell’ordinamento degli item, il che renderebbe difficile interpretare i risultati della scala.\nIn sintesi, l’Analisi delle Scale di Mokken si basa su assunzioni chiave simili a quelle dell’IRT, ma le implementa in un contesto non parametrico. La verifica di queste assunzioni garantisce la validità delle scale costruite e la corretta interpretazione dei risultati, rendendo l’MSA uno strumento potente per la costruzione di scale psicometriche robuste.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#modelli-della-mokken-scale-analysis",
    "href": "chapters/mokken/01_core_issues.html#modelli-della-mokken-scale-analysis",
    "title": "61  Analisi della Scala di Mokken",
    "section": "61.8 Modelli della Mokken Scale Analysis",
    "text": "61.8 Modelli della Mokken Scale Analysis\nDalle suddette assunzioni derivano due modelli della Mokken Scale Analysis:\n\nModello di Monotonicità Omogenea (Mokken, 1971): rispetta le prime tre assunzioni (unidimensionalità, indipendenza locale e monotonicità latente). Questo modello permette di ordinare i rispondenti in base al tratto latente.\nModello di Doppia Monotonicità: rispetta tutte e quattro le assunzioni (unidimensionalità, indipendenza locale, monotonicità latente e non-intersezione). Consente di ordinare non solo i rispondenti, ma anche gli item in termini di difficoltà.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#coefficienti-di-scalabilità",
    "href": "chapters/mokken/01_core_issues.html#coefficienti-di-scalabilità",
    "title": "61  Analisi della Scala di Mokken",
    "section": "61.9 Coefficienti di Scalabilità",
    "text": "61.9 Coefficienti di Scalabilità\nI coefficienti di scalabilità sono strumenti essenziali nell’Analisi delle Scale di Mokken (MSA) per valutare la qualità degli item e della scala complessiva. Questi coefficienti permettono di verificare quanto le risposte ai test seguano un ordinamento coerente lungo un continuum latente. Si basano sulla covarianza tra i punteggi degli item e i restscore (o tra coppie di item) e forniscono indicazioni chiave sulla qualità degli item e sulla capacità della scala di misurare un costrutto unidimensionale.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#coefficiente-per-singolo-item-h_j",
    "href": "chapters/mokken/01_core_issues.html#coefficiente-per-singolo-item-h_j",
    "title": "61  Analisi della Scala di Mokken",
    "section": "61.10 Coefficiente per Singolo Item (\\(H_j\\))",
    "text": "61.10 Coefficiente per Singolo Item (\\(H_j\\))\nIl coefficiente \\(H_j\\) misura la capacità di un singolo item di discriminare tra i rispondenti lungo il continuum latente. La formula è:\n\\[\nH_j = \\frac{\\text{COV}(X_j, R_{-j})}{\\text{COV}(X_j, R_{-j})^{\\text{max}}},\n\\]\ndove:\n\n\\(X_j\\): è il punteggio del singolo item \\(j\\),\n\\(R_{-j}\\): è il restscore, cioè la somma dei punteggi di tutti gli altri item escluso \\(j\\),\n\\(\\text{COV}(X_j, R_{-j})^{\\text{max}}\\): è la covarianza massima teorica tra \\(X_j\\) e \\(R_{-j}\\), calcolata in assenza di errori di Guttman.\n\nInterpretazione:\n\n\\(H_j &gt; 0.30\\): L’item contribuisce efficacemente alla scala.\n\\(H_j &lt; 0.30\\): L’item discrimina poco e potrebbe non essere adatto.\n\n\n\nEsempio 61.3 Consideriamo un test con 4 item e i seguenti punteggi per 5 rispondenti:\n\n\n\n\n\n\n\n\n\n\n\nRispondente\n\\(X_1\\)\n\\(X_2\\)\n\\(X_3\\)\n\\(X_4\\)\nRestscore (\\(R_{-1}\\))\n\n\n\n\nA\n1\n1\n1\n0\n\\(1 + 1 + 0 = 2\\)\n\n\nB\n1\n1\n0\n1\n\\(1 + 0 + 1 = 2\\)\n\n\nC\n0\n1\n0\n0\n\\(1 + 0 + 0 = 1\\)\n\n\nD\n1\n1\n1\n1\n\\(1 + 1 + 1 = 3\\)\n\n\nE\n0\n0\n0\n0\n\\(0 + 0 + 0 = 0\\)\n\n\n\nCalcoliamo:\n\n\\(\\text{COV}(X_1, R_{-1})\\): Covarianza tra \\(X_1\\) e \\(R_{-1}\\).\n\\(\\text{COV}(X_1, R_{-1})^{\\text{max}}\\): Covarianza teorica massima in assenza di errori di Guttman.\n\nSupponiamo che:\n\\[\n\\text{COV}(X_1, R_{-1}) = 0.8 \\quad \\text{e} \\quad \\text{COV}(X_1, R_{-1})^{\\text{max}} = 1.\n\\]\nIl coefficiente \\(H_1\\) è:\n\\[\nH_1 = \\frac{\\text{COV}(X_1, R_{-1})}{\\text{COV}(X_1, R_{-1})^{\\text{max}}} = \\frac{0.8}{1} = 0.8.\n\\]\nQuesto valore indica che \\(X_1\\) contribuisce in modo rilevante alla scala.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#coefficiente-per-coppie-di-item-h_ij",
    "href": "chapters/mokken/01_core_issues.html#coefficiente-per-coppie-di-item-h_ij",
    "title": "61  Analisi della Scala di Mokken",
    "section": "61.11 Coefficiente per Coppie di Item (\\(H_{ij}\\))",
    "text": "61.11 Coefficiente per Coppie di Item (\\(H_{ij}\\))\nIl coefficiente \\(H_{ij}\\) misura la coerenza tra due item (\\(X_i\\) e \\(X_j\\)) rispetto al continuum latente. Valuta quanto la relazione osservata tra i due item sia conforme al modello di omogeneità monotona, che presuppone che entrambi gli item misurino lo stesso tratto latente in modo coerente.\nSi calcola come:\n\\[\nH_{ij} = \\frac{\\text{COV}(X_i, X_j)}{\\text{COV}(X_i, X_j)^{\\text{max}}},\n\\]\ndove:\n\n\\(\\text{COV}(X_i, X_j)\\): è la covarianza osservata tra i punteggi degli item \\(X_i\\) e \\(X_j\\),\n\\(\\text{COV}(X_i, X_j)^{\\text{max}}\\): è la covarianza massima teorica, calcolata assumendo assenza di errori di Guttman e pieno rispetto del modello di Guttman.\n\nInterpretazione:\n\n\\(H_{ij} &gt; 0\\):\n\nGli item sono coerenti con il modello di omogeneità monotona.\nValori positivi indicano che le risposte ai due item sono ordinate in modo simile rispetto al continuum latente.\n\n\\(H_{ij} &lt; 0\\):\n\nPotrebbero esserci problemi di:\n\nMultidimensionalità: Gli item potrebbero misurare tratti latenti differenti.\nViolazioni della monotonicità: La relazione tra i punteggi degli item potrebbe non seguire un andamento coerente con l’aumento del tratto latente.\n\n\n\\(H_{ij} \\approx 0\\):\n\nIndica una relazione molto debole tra gli item, suggerendo che la loro associazione con il continuum latente è minima o casuale.\n\n\nNota: valori di \\(H_{ij}\\) vicini a 1 indicano un’elevata coerenza tra gli item rispetto al modello ideale. Tuttavia, anche in presenza di \\(H_{ij} &gt; 0\\), valori relativamente bassi possono suggerire che gli item necessitano di una revisione o che contribuiscono in modo limitato alla scala complessiva.\n\n\nEsempio 61.4 Supponiamo che:\n\n\\(\\text{COV}(X_1, X_2) = 0.6\\),\n\\(\\text{COV}(X_1, X_2)^{\\text{max}} = 1\\).\n\nIl coefficiente è:\n\\[\nH_{12} = \\frac{0.6}{1} = 0.6.\n\\]\nUn valore positivo e relativamente alto indica coerenza tra gli item \\(X_1\\) e \\(X_2\\).",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#coefficiente-complessivo-della-scala-h",
    "href": "chapters/mokken/01_core_issues.html#coefficiente-complessivo-della-scala-h",
    "title": "61  Analisi della Scala di Mokken",
    "section": "61.12 Coefficiente Complessivo della Scala (\\(H\\))",
    "text": "61.12 Coefficiente Complessivo della Scala (\\(H\\))\nIl coefficiente complessivo \\(H\\) rappresenta una misura sintetica della qualità della scala nel suo insieme, combinando il contributo di tutti gli item. Valuta quanto bene le risposte degli item seguano un ordinamento coerente lungo il continuum latente, tenendo conto della covarianza osservata tra gli item e i loro restscore.\nSi calcola come:\n\\[\nH = \\frac{\\sum_{j=1}^J \\text{COV}(X_j, R_{-j})}{\\sum_{j=1}^J \\text{COV}(X_j, R_{-j})^{\\text{max}}},\n\\]\ndove:\n\n\\(J\\): è il numero totale di item,\n\\(\\text{COV}(X_j, R_{-j})\\): è la covarianza osservata tra il punteggio di un item \\(X_j\\) e il suo restscore \\(R_{-j}\\),\n\\(\\text{COV}(X_j, R_{-j})^{\\text{max}}\\): è la covarianza massima teorica tra \\(X_j\\) e \\(R_{-j}\\), calcolata assumendo assenza di errori di Guttman.\n\nInterpretazione dei Valori di \\(H\\):\n\n\\(H &lt; 0.30\\): Scala debole.\n\nGli item non contribuiscono significativamente alla creazione di un ordinamento coerente.\nLa scala potrebbe essere inadeguata per misurare il costrutto latente.\n\n\\(0.30 \\leq H &lt; 0.40\\): Scala accettabile.\n\nLa scala è utilizzabile, ma con limitazioni. Potrebbe richiedere una revisione o l’eliminazione di alcuni item deboli.\n\n\\(0.40 \\leq H &lt; 0.50\\): Scala di qualità media.\n\nLa scala mostra una struttura adeguata e un buon livello di coerenza tra gli item.\n\n\\(H \\geq 0.50\\): Scala forte.\n\nGli item formano una scala ben costruita, con un’elevata coerenza e capacità discriminativa.\n\n\nConsiderazioni pratiche:\n\nValori elevati di \\(H\\):\n\nIndicano che la scala è efficace nel distinguere i rispondenti lungo il continuum latente.\nRiflettono una bassa presenza di errori di Guttman.\n\nValori bassi di \\(H\\):\n\nPossono indicare che gli item non sono sufficientemente coerenti tra loro.\nSuggeriscono la necessità di rivedere gli item o il costrutto sottostante.\n\n\nIn conclusione, il coefficiente complessivo \\(H\\) valuta la qualità globale di una scala. Esso integra i contributi degli item individuali in una misura unica, permettendo di identificare eventuali debolezze nella struttura della scala e fornendo una base per migliorare la coerenza e la validità degli strumenti di misura.\n\n\nEsempio 61.5 Supponiamo di avere una scala con 5 item (\\(J = 5\\)) e i seguenti valori:\n\n\\(\\sum_{j=1}^J \\text{COV}(X_j, R_{-j}) = 3.2\\),\n\\(\\sum_{j=1}^J \\text{COV}(X_j, R_{-j})^{\\text{max}} = 5.0\\).\n\nIl coefficiente complessivo della scala è:\n\\[\nH = \\frac{\\sum_{j=1}^J \\text{COV}(X_j, R_{-j})}{\\sum_{j=1}^J \\text{COV}(X_j, R_{-j})^{\\text{max}}} = \\frac{3.2}{5.0} = 0.64.\n\\]\nInterpretazione:\n\nCon \\(H = 0.64\\), la scala può essere considerata forte, con un’ottima capacità di misurare il costrutto latente e distinguere tra i rispondenti.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#errori-standard-nei-coefficienti-di-scalabilità",
    "href": "chapters/mokken/01_core_issues.html#errori-standard-nei-coefficienti-di-scalabilità",
    "title": "61  Analisi della Scala di Mokken",
    "section": "61.13 Errori Standard nei Coefficienti di Scalabilità",
    "text": "61.13 Errori Standard nei Coefficienti di Scalabilità\nGli errori standard (SE) svolgono un ruolo fondamentale nell’interpretazione dei coefficienti di scalabilità (\\(H\\), \\(H_j\\), \\(H_{ij}\\)) nella MSA. Forniscono una misura dell’incertezza associata alle stime e aiutano a valutare quanto il coefficiente stimato rappresenti accuratamente il valore reale nella popolazione.\nUn errore standard elevato rispetto al coefficiente stimato indica una maggiore incertezza. Ad esempio, se \\(H_j = 0.30\\) e \\(SE = 0.08\\), il coefficiente potrebbe essere inferiore alla soglia accettabile di 0.30 nella popolazione, sollevando dubbi sulla scalabilità dell’item.\nL’errore standard permette di costruire intervalli di confidenza per quantificare la precisione della stima. La formula per il 95% CI è:\n\\[\n\\text{95\\% CI} = H_j \\pm (1.96 \\times SE).\n\\]\nL’intervallo di confidenza non descrive dove si trova il vero valore del coefficiente nella popolazione nel 95% dei campioni. Piuttosto:\n\nIn un gran numero di campioni prelevati dalla stessa popolazione, il 95% degli intervalli di confidenza calcolati includerà il vero valore del coefficiente nella popolazione.\nL’intervallo di confidenza riflette l’incertezza della stima in un campione specifico, ma non garantisce che il vero valore del coefficiente sia contenuto nell’intervallo per quel campione particolare.\n\n\n\nEsempio 61.7 Se un coefficiente di scalabilità ha \\(H_j = 0.30\\) e un errore standard di \\(SE = 0.10\\), il 95% CI sarà:\n\\[\n\\text{95\\% CI} = 0.30 \\pm (1.96 \\times 0.10) = [0.10, 0.50].\n\\]\nQuesto intervallo indica che, in una procedura ripetuta su molti campioni, il vero valore di \\(H_j\\) sarà compreso tra 0.10 e 0.50 nel 95% dei casi.\n\nIn conclusione, la costruzione degli intervalli di confidenza tramite l’errore standard è uno strumento utile per quantificare l’incertezza associata a una stima, ma è importante ricordare che l’interpretazione corretta si basa sulla frequenza degli intervalli che includono il vero valore in campioni ripetuti, piuttosto che su una probabilità legata al valore specifico in un singolo campione.\n\n61.13.1 Fattori che Influenzano l’Errore Standard\nL’errore standard è influenzato da diversi fattori, tra cui:\n\nDimensione del campione:\n\nCampioni più grandi riducono l’errore standard, migliorando la precisione della stima.\nTuttavia, anche in campioni ampi, altri fattori possono influenzare la precisione.\n\nDistribuzione dei punteggi degli item:\n\nDistribuzioni asimmetriche o altamente polarizzate (molti punteggi estremi, ad esempio tutto 0 o tutto 1) aumentano l’errore standard.\n\nEterogeneità degli item:\n\nSe gli item misurano costrutti diversi o non sono ben rappresentati dal modello di omogeneità monotona, i coefficienti di scalabilità diventano meno affidabili, con un conseguente aumento dell’errore standard.\n\n\n\n\n61.13.2 Utilizzo Pratico degli Errori Standard\nGli errori standard (\\(SE\\)) offrono una misura dell’incertezza associata alla stima dei coefficienti di scalabilità (\\(H_j\\)) e forniscono informazioni cruciali per valutare l’idoneità degli item in una scala psicometrica. Quando il coefficiente \\(H_j\\) è basso o il suo intervallo di confidenza include valori inferiori alla soglia accettabile di 0.30, l’item potrebbe non essere adeguato a contribuire alla scala.\nCome discusso da Mokken (1971), item con bassi coefficienti di scalabilità (\\(H_j\\)) sono spesso associati a errori di Guttman, che segnalano deviazioni dalla struttura ideale del modello di Guttman. In presenza di questi errori, le risposte degli item non seguono un ordinamento coerente rispetto al continuum latente, riducendo la capacità dell’item di classificare correttamente i rispondenti.\nCrișan et al. (2020) avvertono che eliminare automaticamente gli item con bassi coefficienti di scalabilità può avere conseguenze negative:\n\nPro: La rimozione di item con basso \\(H_j\\) può migliorare l’affidabilità psicometrica della scala e aumentare la coerenza dei risultati.\nContro: L’eliminazione indiscriminata può ridurre la copertura del costrutto e compromettere la validità teorica della scala, specialmente se l’item misura un aspetto unico o rilevante del costrutto latente.\n\nPertanto, la decisione di mantenere o rimuovere un item dovrebbe considerare sia aspetti psicometrici sia teorici.\n\n\n61.13.3 Criteri per Valutare gli Item\n\nValidità Teorica:\n\nL’item misura un aspetto centrale del costrutto latente?\nSe l’item è fondamentale per rappresentare un sottodominio importante del costrutto, potrebbe essere mantenuto anche con un basso \\(H_j\\).\n\nValidità Psicometrica:\n\nL’item contribuisce alla coerenza e alla precisione della scala?\nUn \\(H_j &lt; 0.30\\) o un intervallo di confidenza che comprende valori negativi o prossimi allo zero indicano che l’item ha un contributo limitato o negativo.\n\n\nIn conclusione, l’analisi degli errori standard è essenziale per interpretare la precisione dei coefficienti di scalabilità e valutare l’idoneità degli item. Sebbene item con bassi \\(H_j\\) siano spesso candidati per la rimozione, è importante considerare il loro ruolo teorico e la loro validità psicometrica nel contesto della scala complessiva. La combinazione di analisi quantitative e valutazioni teoriche garantisce una scala robusta e ben rappresentativa del costrutto latente.\n\n\nEsempio 61.6 Item con basso \\(H_j\\): se \\(H_j = 0.20\\) e il 95% CI è \\([0.05, 0.35]\\), il coefficiente è inferiore alla soglia accettabile di 0.30. Tuttavia, prima di eliminare l’item, si dovrebbe verificare se:\n\nl’item copre un aspetto importante del costrutto;\nla sua rimozione influisce sull’affidabilità della scala.\n\n\n\n\n\n61.14 Estensione della MSA agli Item Politomici\nLa MSA, inizialmente concepita per item dicotomici, è stata ampliata da Molenaar (1982a, 1997) per includere item politomici, come quelli tipici delle scale Likert. Questa estensione conserva i principi fondamentali della MSA per gli item dicotomici, adattandoli per affrontare le specificità degli item con più categorie di risposta.\nNel caso degli item politomici, la MSA non si limita a valutare il comportamento dell’intero item rispetto al tratto latente, ma esamina anche i singoli “passaggi” tra categorie di risposta consecutive. Ad esempio, in un item Likert a cinque punti (da “fortemente in disaccordo” a “fortemente d’accordo”), ci sono quattro passaggi distinti tra le categorie. Ogni passaggio rappresenta una transizione tra due categorie adiacenti, e la probabilità di effettuare questa transizione dipende dal livello del tratto latente (\\(\\theta\\)).\nPer ogni passaggio, viene definita una Funzione di Risposta del Passaggio dell’Item (Item Step Response Function, ISRF), che descrive la probabilità che un individuo con un certo livello di \\(\\theta\\) scelga una categoria specifica o una categoria superiore.\nLe ISRF giocano un ruolo chiave nell’analisi degli item politomici. Esse:\n\nDescrivono la probabilità di transizione: Collegano la probabilità di scegliere una determinata categoria o una categoria superiore al livello del tratto latente.\nRappresentano il contributo di ogni passaggio: Consentono di analizzare come ciascun passaggio tra categorie rifletta un avanzamento lungo il continuum latente.\n\nPer un item a cinque punti, per esempio, le ISRF rappresentano le probabilità delle seguenti transizioni:\n\nPassaggio 1: Da “fortemente in disaccordo” a “in disaccordo o superiore.”\nPassaggio 2: Da “in disaccordo” a “neutrale o superiore.”\nPassaggio 3: Da “neutrale” a “d’accordo o superiore.”\nPassaggio 4: Da “d’accordo” a “fortemente d’accordo.”\n\nQueste probabilità devono essere monotone crescenti, ossia aumentare con il livello di \\(\\theta\\).\n\n61.14.1 Assunzioni Fondamentali\nL’estensione della MSA agli item politomici conserva le assunzioni centrali del modello per gli item dicotomici, ma le applica ai singoli passaggi tra categorie:\n\nMonotonicità delle ISRF:\n\nLa probabilità di scegliere una categoria \\(k\\) o superiore deve aumentare monotonicamente con il tratto latente \\(\\theta\\).\nCiò garantisce che le categorie siano ordinate in modo significativo e rappresentino livelli crescenti del tratto latente.\n\nOrdinamento delle categorie:\n\nOgni categoria di risposta deve riflettere un livello progressivamente più alto del tratto latente.\nAd esempio, un passaggio da “neutrale” a “d’accordo” dovrebbe rappresentare un incremento significativo lungo il continuum di \\(\\theta\\).\n\nValidità del modello di omogeneità monotona:\n\nAnche per gli item politomici, il modello presuppone che gli item misurino lo stesso tratto latente in modo coerente e monotono.\n\n\nL’analisi degli item politomici con la MSA offre numerosi vantaggi:\n\nMisurazione più sfumata: Permette di cogliere differenze più sottili tra i rispondenti grazie alla maggiore varietà di opzioni di risposta.\nApprofondimento delle relazioni tra categorie: Analizzando i singoli passaggi, si può comprendere meglio come le diverse categorie di risposta riflettano i livelli del tratto latente.\nAdattabilità a scale Likert e simili: La MSA estesa è particolarmente utile in contesti come l’assessment psicologico, dove le risposte politomiche sono comuni.\n\nIn conclusione, l’estensione della MSA agli item politomici amplia l’applicabilità di questo modello, rendendolo particolarmente adatto per scale con più livelli di risposta. Attraverso l’analisi dei singoli passaggi, è possibile verificare se le categorie sono coerenti con il tratto latente e se ogni passaggio rappresenta un avanzamento significativo lungo il continuum.\n\n\n\n61.15 Mokken Reliability Coefficient\nL’affidabilità di un test psicometrico si riferisce alla sua capacità di fornire risultati coerenti nel tempo o attraverso somministrazioni ripetute. Sebbene l’alfa di Cronbach sia l’indicatore di affidabilità più diffuso, presenta alcune limitazioni, soprattutto quando gli item non sono omogenei o quando si lavora con scale ordinali. In questi casi, il coefficiente \\(\\rho\\) di Mokken (Mokken reliability coefficient o \\(\\rho_M\\)) rappresenta un’alternativa valida, basandosi su assunzioni meno restrittive e adattandosi meglio ai dati ordinali.\nIl coefficiente \\(\\rho\\) di Mokken è una misura di consistenza interna che deriva dalla teoria della scala non parametrica di Mokken, utilizzata principalmente per dati ordinali. A differenza dell’alfa di Cronbach, \\(\\rho\\) non richiede che gli item soddisfino rigorosi criteri di omogeneità o equivalenza essenziale, rendendolo particolarmente utile in contesti in cui i dati non rispettano pienamente i presupposti della teoria classica dei test (CTT).\nIl coefficiente \\(\\rho\\) di Mokken si fonda su due principi principali:\n\nMonotonicità: la probabilità di rispondere in modo favorevole a un item aumenta con il livello della variabile latente.\nIndipendenza locale: le risposte agli item sono condizionalmente indipendenti dato il livello della variabile latente.\n\nQueste assunzioni permettono una maggiore flessibilità rispetto ai modelli parametrici, come il modello di Rasch, e lo rendono idoneo per scale ordinali, come quelle Likert.\nIl coefficiente \\(\\rho\\) di Mokken misura il rapporto tra la varianza spiegata dal punteggio totale (\\(S^2_T\\)) e la varianza totale osservata. La formula è la seguente:\n\\[\n\\rho = 1 - \\frac{\\sum_{i=1}^k S^2_i}{S^2_T},\n\\]\ndove:\n\n\\(k\\): numero di item del test,\n\\(S^2_i\\): varianza di ciascun item,\n\\(S^2_T\\): varianza totale del punteggio del test (somma dei punteggi degli item).\n\nInterpretazione di \\(\\rho\\):\n\n\\(\\rho\\) varia tra 0 e 1.\n\nvalori vicini a 1 indicano maggiore affidabilità.\n\n\\(\\rho &gt; 0.70\\): accettabile per la ricerca esplorativa.\n\n\\(\\rho &gt; 0.80\\): preferibile per applicazioni pratiche.\n\n\nVantaggi di \\(\\rho\\) di Mokken:\n\na differenza dell’alfa di Cronbach, che assume una scala intervallare, \\(\\rho\\) può essere utilizzato con dati ordinali come le scale Likert.\nnon richiede che tutti gli item abbiano correlazioni simili (omogeneità), né che riflettano lo stesso grado di relazione con la variabile latente.\npuò essere applicato a test che non rispettano i presupposti della teoria classica dei test o di modelli parametrici più rigidi.\n\nLimitazioni di \\(\\rho\\) di Mokken:\n\nse gli item non rispettano l’assunzione di monotonicità, l’interpretazione del coefficiente può risultare ambigua.\nil coefficiente è appropriato per scale unidimensionali; in presenza di più dimensioni, è necessario analizzare ogni sottoscala separatamente.\n\nIn conclusione, il coefficiente \\(\\rho\\) di Mokken è uno strumento efficace per valutare l’affidabilità di scale ordinali, offrendo un’alternativa all’alfa di Cronbach quando le assunzioni della teoria classica dei test non sono soddisfatte. Grazie alla sua flessibilità e semplicità, \\(\\rho\\) è particolarmente indicato per scale psicometriche utilizzate in contesti esplorativi o applicativi. Tuttavia, è essenziale verificare la monotonicità degli item e considerare la dimensionalità della scala per un’interpretazione accurata.\n\n61.15.1 Procedura di Selezione Automatica degli Item\nLa Procedura di Selezione Automatica degli Item (AISP) è una metodologia impiegata nella MSA per selezionare insiemi di item che rispettino le assunzioni del Modello di Mokken (MHM). A differenza di tecniche più comuni come l’analisi fattoriale o l’analisi parallela, l’AISP non determina esplicitamente la dimensionalità dei dati. Piuttosto, si basa sui coefficienti di scalabilità per identificare gruppi di item che misurano lo stesso costrutto latente, formando così una o più scale.\n\n61.15.1.1 Come Funziona l’AISP\nL’AISP segue un approccio iterativo che:\n\nSeleziona l’item iniziale, valutato come più rappresentativo di una dimensione, utilizzando il coefficiente di scalabilità individuale (\\(H_i\\)).\nSuccessivamente, analizza le coppie di item (\\(H_{ij}\\)) per identificare un insieme scalabile di item che misurano lo stesso costrutto.\n\nUn item viene incluso in una scala se:\n\nIl suo coefficiente di scalabilità individuale (\\(H_i\\)) supera una soglia predefinita (\\(c\\), solitamente pari a 0.30);\nLa covarianza tra ogni coppia di item (\\(H_{ij}\\)) è positiva e superiore alla stessa soglia.\n\nSe un item non soddisfa questi criteri, la procedura tenta di assegnarlo a una nuova scala. Questo processo iterativo continua finché tutti gli item non sono stati assegnati a una scala o esclusi come non scalabili.\n\n\n61.15.1.2 Criteri di Selezione\n\nCoefficiente di scalabilità individuale (\\(H_i\\)): Ogni item deve mostrare una buona capacità di discriminazione lungo la dimensione latente, con un valore di \\(H_i &gt; c\\).\n\nCoefficiente di scalabilità di coppia (\\(H_{ij}\\)): Gli item di una scala devono essere correlati positivamente e mostrare coerenza con il costrutto misurato.\n\nLa procedura inizia selezionando la coppia di item con il valore più alto di \\(H_{ij}\\). Successivamente, vengono aggiunti nuovi item che rispettano i criteri sopra descritti. Gli item esclusi per una scala possono essere testati per formarne altre, ma se non soddisfano i criteri per alcuna scala, vengono classificati come non scalabili.\n\n\n61.15.1.3 Caratteristiche dell’AISP\n\nScalabilità: Gli item selezionati formano scale che misurano un tratto latente comune e sono in grado di ordinare i rispondenti lungo il continuum del costrutto.\nNeutralità rispetto alla difficoltà degli item: L’AISP non è influenzata dalla distribuzione delle risposte o dalla difficoltà degli item, rendendola adatta sia per item dicotomici sia politomici.\nRigorosità statistica: La procedura non forza soluzioni. Se nessuna coppia di item soddisfa i criteri di scalabilità (\\(H_{ij} &gt; c\\)), non viene generata alcuna scala, a differenza dell’analisi fattoriale, che potrebbe imporre soluzioni anche in presenza di dati incoerenti.\n\n\n\n61.15.1.4 Soglia (\\(c\\)) e Implicazioni\nLa scelta della soglia \\(c\\) è cruciale:\n\nValore alto di \\(c\\): Maggiore precisione nella selezione degli item, ma rischio di escluderne troppi, riducendo la lunghezza della scala e la copertura del costrutto.\nValore basso di \\(c\\): Aumenta il numero di item inclusi, ma rischia di compromettere la coerenza e la validità della scala.\n\nUn valore comunemente utilizzato è \\(c = 0.30\\), che rappresenta un buon equilibrio tra inclusività e rigore. Tuttavia, il valore ottimale dipende dagli obiettivi dello studio e dalla natura dei dati.\n\n\n61.15.1.5 Limitazioni dell’AISP\n\nRelazioni tra dimensioni: L’AISP può risultare meno efficace in presenza di dimensioni fortemente correlate o di item che saturano su più dimensioni, poiché non distingue esplicitamente tali situazioni.\nDipendenza dai dati statistici: L’AISP si basa esclusivamente sui coefficienti di scalabilità e non considera le relazioni teoriche tra gli item. Questo può portare all’inclusione di item con scarso valore teorico o alla loro esclusione inappropriata.\nUnidimensionalità richiesta: Gli item selezionati da una scala devono riflettere un unico costrutto. In caso di multidimensionalità, occorre considerare scale separate.\n\nIn conclusione, l’AISP offre un’alternativa flessibile all’analisi fattoriale per la costruzione di scale psicometriche, particolarmente utile quando si lavora con dati ordinali. Sebbene non forzi soluzioni e tenga conto della scalabilità degli item, è fondamentale integrarla con considerazioni teoriche per garantire che le scale siano valide e utili nel contesto di applicazione.\n\n\n\n\n61.16 Ordinamento Invariante degli Item (IIO)\nL’Ordinamento Invariante degli Item (Invariant Item Ordering, IIO) è un concetto chiave nell’ambito delle scale di Mokken, che garantisce che l’ordine di difficoltà degli item rimanga costante attraverso diversi sottogruppi di persone nella popolazione target. Questo principio assicura che le differenze nei punteggi totali riflettano differenze effettive nel costrutto latente misurato, e non siano influenzate da variazioni nell’interpretazione o nella difficoltà percepita degli item.\n\n61.16.1 Cos’è l’IIO?\nL’IIO richiede che gli item di una scala possano essere ordinati in base alla loro difficoltà in modo stabile e uniforme per tutti i partecipanti, indipendentemente dalle loro caratteristiche personali (ad esempio, abilità, età, genere o livello del tratto misurato). In pratica, questo significa che:\n\nUn partecipante con un punteggio totale più alto deve avere una probabilità maggiore di rispondere correttamente (o positivamente) a ogni item rispetto a un partecipante con un punteggio totale più basso.\nGli item più difficili devono rimanere tali per tutti i partecipanti, e lo stesso vale per quelli più facili.\n\n\n\n61.16.2 Perché l’IIO è importante?\n\nValidità dei confronti\nL’IIO garantisce che i punteggi totali siano interpretabili in modo uniforme tra gruppi diversi. Se l’ordine degli item varia tra sottogruppi, i punteggi totali potrebbero non riflettere effettivamente il costrutto latente, compromettendo la validità dei confronti tra gruppi.\nInterpretazione coerente\nPermette di interpretare i punteggi del test in modo uniforme per tutti i partecipanti, indipendentemente dalle loro caratteristiche. Ad esempio, in una scala sulla depressione, un punteggio più alto indica costantemente una maggiore gravità, e la progressione degli item da più semplici (es. stanchezza) a più complessi (es. ideazione suicidaria) è coerente.\nIdentificazione di bias e DIF\nL’IIO consente di rilevare la Funzione Differenziale degli Item (Differential Item Functioning, DIF) e bias che potrebbero indicare che un item è percepito in modo diverso da sottogruppi specifici. Ad esempio, un item potrebbe essere interpretato diversamente in base al genere o al contesto culturale.\n\n\n\n61.16.3 IIO nei Test Psicologici\nNell’ambito dei test psicologici (come scale per ansia o depressione) l’IIO implica che:\n\nUn individuo con un punteggio totale più alto debba manifestare tutti i sintomi o le competenze presenti in un individuo con un punteggio inferiore, più eventuali sintomi o competenze aggiuntive.\nAd esempio, una persona con una gravità depressiva maggiore (punteggio più alto) dovrebbe rispondere positivamente a item più gravi oltre che a quelli più lievi.\nLa difficoltà degli item deve essere ordinata in modo invariato per tutti i partecipanti.\nAd esempio, un problema matematico considerato più difficile deve esserlo per tutti gli studenti, indipendentemente dal livello generale di abilità.\n\n\n\n61.16.4 Come Verificare l’IIO?\nLa verifica dell’IIO può essere effettuata attraverso diverse tecniche statistiche:\n\nMetodo dei gruppi di restscore\nGli item vengono confrontati tra gruppi di partecipanti suddivisi in base ai punteggi totali, escluso l’item in esame. Questo approccio consente di analizzare se l’ordine degli item rimane costante nei diversi livelli del costrutto.\nProporzioni \\(P(++)\\) e \\(P(--)\\)\nSi analizzano le proporzioni di risposte positive (\\(P(++)\\)) o negative (\\(P(--)\\)) per coppie di item. Per rispettare l’IIO, le proporzioni devono essere coerenti con l’ordine di difficoltà teorico.\nTest di monotonicità\nUtilizzato per verificare che la probabilità di una risposta positiva a un item aumenti con il livello del punteggio totale. Questo è un presupposto fondamentale per l’IIO.\nAnalisi di divisione del restscore\nI restscore vengono divisi in intervalli e si valuta se l’ordine degli item rimane invariato in base a questi sottogruppi.\n\n\n\n61.16.5 Cosa Succede se l’IIO Non È Rispettato?\nSe l’IIO viene violato, si possono verificare le seguenti problematiche:\n\nPresenza di DIF o bias\nGli item possono riflettere caratteristiche diverse da quelle del costrutto latente, rendendo i confronti tra sottogruppi non validi.\nPunteggi totali non interpretabili\nLe differenze nei punteggi totali potrebbero non riflettere differenze reali nel costrutto latente, invalidando il test.\nCompromissione della validità costruttuale\nLa violazione dell’IIO può mettere in dubbio la capacità della scala di misurare un costrutto latente comune.\n\n\n\n61.16.6 Vantaggi e Limiti dell’IIO\nL’IIO offre vantaggi importanti:\n\nPermette una valutazione uniforme e interpretabile dei punteggi totali.\nAiuta a identificare potenziali bias negli item.\nSi applica sia a dati dicotomici che politomici, rendendolo versatile.\n\nTuttavia, presenta anche alcune limitazioni:\n\nDifficoltà di verifica: La verifica empirica dell’IIO richiede analisi specifiche che non sempre vengono incluse negli studi psicometrici.\nSensibilità alla dimensionalità: L’IIO è rilevante solo per scale unidimensionali. In caso di scale multidimensionali, è necessario verificare l’IIO separatamente per ciascuna dimensione.\nRigidità: Un eccessivo focus sull’IIO potrebbe portare all’esclusione di item potenzialmente utili ma che non rispettano perfettamente i criteri.\n\nIn conclusione, l’Ordinamento Invariante degli Item è un requisito fondamentale per garantire la validità e l’affidabilità delle scale di Mokken e di altri strumenti psicometrici. La sua verifica permette di assicurare che i punteggi totali siano interpretabili in modo coerente e che gli item riflettano realmente il costrutto latente. Tuttavia, per una valutazione completa, l’IIO dovrebbe essere integrato con altre analisi psicometriche e una riflessione teorica sui contenuti della scala, specialmente in contesti clinici o educativi dove accuratezza e validità sono essenziali.\n\n\n\n61.17 Dimensione del Campione\nLa determinazione della dimensione del campione è un aspetto cruciale nella ricerca psicometrica. Sebbene esistano linee guida consolidate per molti test statistici, l’MSA rappresenta un’area in cui la ricerca sulla dimensione del campione è ancora limitata. Questa lacuna pone sfide per i ricercatori, che devono bilanciare l’accuratezza dei risultati con le risorse disponibili.\nCampioni troppo piccoli possono portare a due rischi principali:\n\nFalsi positivi: una scala viene erroneamente identificata come valida in un campione ridotto, quando in realtà non esiste nella popolazione.\n\nFalsi negativi: una scala realmente esistente non viene rilevata a causa della scarsa potenza statistica del campione.\n\nDiversi studi hanno esplorato le dimensioni minime del campione necessarie per applicare correttamente l’MSA, con particolare attenzione a procedure come l’Automated Item Selection Procedure (AISP) e l’Algoritmo Genetico (GA).\nStudio di Straat et al. (2014). Straat e colleghi hanno investigato come diversi fattori influenzino la dimensione del campione necessaria, tra cui la lunghezza del test, i valori dei coefficienti di scalabilità (\\(H_i\\)) e la correlazione tra dimensioni sottostanti nella scala.\nRisultati principali:\n\nIl valore di \\(H_i\\) è stato identificato come il fattore più influente:\n\nPer valori bassi di \\(H_i\\) (es. \\(H_i \\approx 0.22\\)), erano necessari campioni molto grandi (750-2500 partecipanti) per ottenere una classificazione accurata.\n\nPer valori moderati o alti di \\(H_i\\) (es. \\(H_i \\approx 0.42\\)), campioni più piccoli (50-250 partecipanti) erano sufficienti.\n\n\nLa lunghezza del test ha avuto un impatto minore sulla precisione. Test più lunghi non hanno necessariamente richiesto campioni più grandi.\n\nLa correlazione tra dimensioni ha interagito con \\(H_i\\), influenzando la necessità di campioni più grandi per scale multidimensionali.\n\nLinee guida suggerite da Straat et al.:\n\nPer \\(H_i \\approx 0.22\\):\n\nPrecisione moderata: 750-1000 partecipanti.\n\nPrecisione alta: almeno 1250-2500 partecipanti.\n\n\nPer \\(H_i \\approx 0.42\\):\n\nPrecisione moderata: 50 partecipanti.\n\nPrecisione alta: almeno 250 partecipanti.\n\n\nStudio di Watson et al. (2018). Watson e colleghi hanno utilizzato dati reali per analizzare l’effetto della dimensione del campione sui coefficienti di scalabilità (\\(H\\) e \\(H_i\\)). Lo studio includeva 7510 partecipanti e un questionario di 14 item con scala Likert a 5 punti. Sono stati generati campioni di diverse dimensioni (da 50 a 1000 partecipanti) utilizzando tecniche di bootstrapping.\nRisultati principali:\n\nValori medi di \\(H\\) e \\(H_i\\): Non variavano significativamente tra campioni di dimensioni diverse, suggerendo una certa stabilità delle stime puntuali anche per campioni più piccoli.\n\nIntervalli di confidenza (CI):\n\nCampioni piccoli (\\(N = 50\\)) producevano intervalli di confidenza molto ampi. In oltre il 50% dei casi, il limite inferiore del CI per \\(H_i\\) era inferiore alla soglia critica di 0.30.\n\nCampioni grandi (\\(N = 1000\\)) producevano intervalli più stretti, con valori di \\(H_i\\) sempre superiori a 0.30.\n\n\nErrori standard: Campioni piccoli aumentavano la probabilità di errori nella classificazione degli item, portando a decisioni sbagliate sull’inclusione/esclusione.\n\n\n61.17.1 Implicazioni per la Ricerca\nPrecisione delle stime. Mentre i valori medi di \\(H\\) e \\(H_i\\) possono rimanere relativamente stabili, la dimensione del campione influenza notevolmente la larghezza degli intervalli di confidenza. Stime puntuali apparentemente affidabili potrebbero nascondere elevata incertezza per campioni piccoli.\nDecisioni sugli item. Campioni piccoli aumentano il rischio di errori nella selezione degli item, compromettendo la validità della scala. Un’adeguata dimensione del campione è essenziale per garantire la qualità delle decisioni basate sui coefficienti di scalabilità.\nBilanciamento tra risorse e precisione. Non sempre è pratico utilizzare campioni molto grandi. Tuttavia, quando le risorse sono limitate, è cruciale pianificare attentamente la dimensione del campione per ottimizzare l’efficienza senza sacrificare la precisione.\nIn conclusione, la determinazione della dimensione del campione nell’MSA dipende da variabili chiave come il coefficiente di scalabilità \\(H_i\\), la lunghezza della scala e la struttura multidimensionale dei dati. Sebbene campioni più grandi siano generalmente preferibili per ottenere risultati più precisi e intervalli di confidenza più stretti, non sempre sono necessari.\nFuturi studi potrebbero fornire indicazioni più dettagliate, sviluppando simulazioni e linee guida empiriche che aiutino i ricercatori a determinare la dimensione ottimale del campione per specifici scenari applicativi. L’obiettivo finale rimane quello di bilanciare precisione e praticità, garantendo risultati validi senza spreco di risorse.\n\n\n\n61.18 Confronto tra il Modello di Rasch e la MSA\nIl Modello di Rasch (RM) e la MSA rappresentano due approcci distinti alla teoria della risposta all’item (IRT), ciascuno con caratteristiche e assunzioni specifiche che influenzano la loro applicabilità. Mentre il RM è un modello parametrico altamente restrittivo, l’MSA appartiene alla categoria dei modelli non parametrici, offrendo maggiore flessibilità nell’analisi dei dati.\nIl Modello di Rasch si basa su una relazione parametrica tra l’abilità del rispondente (\\(\\theta\\)) e la difficoltà dell’item (\\(\\delta\\)), descritta da una funzione logistica. Questo implica che la probabilità di risposta corretta a un item dipende esclusivamente da \\(\\theta\\) e \\(\\delta\\) e che le curve caratteristiche degli item (IRFs) sono parallele e hanno la stessa pendenza. Una delle caratteristiche distintive del RM è che i punteggi grezzi totali sono sufficienti per stimare i parametri delle persone e degli item, rendendo possibile una scala metrica comune che consente confronti diretti e indipendenti da campione e item. Tuttavia, questa precisione deriva da assunzioni stringenti, come la necessità di unidimensionalità, monotonicità e indipendenza locale delle risposte.\nL’MSA, invece, appartiene ai modelli non parametrici della IRT (NIRT), che non vincolano le IRFs a una forma specifica. Questo approccio si concentra sull’ordinamento delle persone lungo una scala latente, senza stimare direttamente \\(\\theta\\), ma utilizzando i punteggi grezzi per determinare un ordine relativo. La maggiore flessibilità dell’MSA consente di analizzare dati che non soddisfano le rigide assunzioni parametriche del RM, rendendolo particolarmente utile per scale ordinali o situazioni in cui le IRFs non seguono una forma logistica.\nIl Modello di Monotonicità Omogenea (MHM), il più semplice tra i modelli di Mokken, si basa su tre assunzioni: unidimensionalità, monotonicità e indipendenza locale. Questo modello consente di ordinare le persone su una scala unidimensionale ordinale, garantendo che l’ordine sia coerente per tutti gli item. Una variante più restrittiva, il Modello di Monotonicità Doppia (DMM), aggiunge il requisito che le IRFs non si intersechino, permettendo così di produrre scale ordinali distinte sia per persone che per item. In entrambi i casi, il punteggio grezzo totale fornisce informazioni utili senza dover stimare direttamente parametri come \\(\\theta\\).\nUna differenza cruciale tra il Modello di Rasch e l’MSA riguarda il tipo di scala che producono. Il RM genera una scala metrica comune per persone e item, consentendo misurazioni precise e confronti generalizzabili. L’MSA, invece, si limita a scale ordinali, dove l’ordine relativo è garantito ma non vi è una misura metrica assoluta. Questa differenza riflette il maggiore rigore del RM rispetto alla flessibilità dell’MSA. Inoltre, mentre il RM richiede che gli item seguano una funzione logistica, l’MSA permette l’inclusione di item che non rispettano questa forma, evitando così di scartare dati potenzialmente utili.\nDal punto di vista applicativo, il RM è particolarmente indicato in contesti in cui la precisione e la generalizzabilità sono fondamentali, come nei test standardizzati. Tuttavia, la sua rigidità può limitarne l’uso con dati complessi o non conformi alle sue assunzioni. L’MSA, al contrario, offre un’opzione più flessibile per l’analisi esplorativa e per dati ordinali o non lineari, risultando utile in ambiti come la psicologia e le scienze sociali. La scelta tra i due dipende dagli obiettivi dello studio e dalle caratteristiche specifiche dei dati.\n\n\n61.19 Confronto tra la CTT e la MSA\nLa CTT e la MSA rappresentano due approcci distinti alla misurazione psicometrica, ciascuno con i propri punti di forza e limiti. Sebbene condividano alcune somiglianze concettuali, si differenziano in modo significativo per quanto riguarda le assunzioni, la flessibilità e la capacità di testare empiricamente i modelli.\nLa CTT si basa su un insieme di assunzioni fondamentali che guidano l’interpretazione dei punteggi dei test. In questo framework, il punteggio osservato è la somma di un punteggio vero e di un errore di misura. Si assume che l’errore abbia una media di zero su prove ripetute, che sia non correlato con i punteggi veri, e che i punteggi di errore di due test somministrati agli stessi individui siano non correlati. I punteggi grezzi totali vengono considerati un’indicazione diretta delle posizioni dei soggetti sul continuum del tratto latente, mentre la proporzione di risposte corrette misura la facilità degli item, e la correlazione corretta tra un item e il punteggio totale del test ne determina la capacità discriminativa. L’affidabilità, intesa come correlazione tra punteggi osservati su due forme parallele dello stesso test, costituisce un aspetto cruciale della CTT.\nDal punto di vista metodologico, la CTT e la MSA condividono alcune somiglianze. Ad esempio, il coefficiente di scalabilità dell’item (\\(H_i\\)) nella MSA è concettualmente simile alla correlazione corretta tra un item e il punteggio totale del test nella CTT. Inoltre, il coefficiente di scalabilità tra coppie di item (\\(H_{ij}\\)) nella MSA può essere paragonato alle correlazioni tra coppie di item nella CTT. Infine, il coefficiente di scalabilità complessivo (\\(H\\)) nella MSA è analogo agli indici di discriminazione media degli item nella CTT. Questi paralleli rendono i due approcci confrontabili nella loro capacità di descrivere i dati dei test.\nTuttavia, esistono differenze sostanziali tra i due approcci. La CTT si basa su modelli teorici che non possono essere testati direttamente con i dati, poiché si assume implicitamente che le sue condizioni siano soddisfatte. Al contrario, la MSA permette di verificare empiricamente le assunzioni che ne stanno alla base, come l’indipendenza locale, l’unidimensionalità e la monotonicità. Ad esempio, un coefficiente di scalabilità negativo in un modello di Mokken smentirebbe gli assiomi del Modello di Monotonicità Omogenea (MHM), evidenziando che i dati non sono coerenti con il modello. Questa capacità di testare empiricamente le assunzioni rende la MSA più robusta e trasparente rispetto alla CTT.\nLa CTT offre un quadro teorico consolidato e intuitivo per interpretare i punteggi dei test, ma presenta limitazioni nei contesti in cui le assunzioni non possono essere verificate o quando i dati violano condizioni fondamentali, come l’unidimensionalità. La MSA, d’altra parte, si distingue per la sua maggiore flessibilità, consentendo l’analisi di dati ordinali e la valutazione della struttura sottostante dei dati in modo più rigoroso. Questa flessibilità la rende particolarmente utile per l’analisi esplorativa di scale e per la validazione di strumenti psicometrici.\nIn sintesi, la CTT e la MSA non sono necessariamente in opposizione, ma piuttosto complementari. La CTT fornisce una base teorica utile per la misurazione, mentre la MSA aggiunge strumenti pratici per testare empiricamente la validità delle scale e la struttura dei dati. L’integrazione di entrambi gli approcci può fornire una comprensione più approfondita e completa dei dati psicologici.\n\n\n61.20 Riflessioni Conclusive\nIl tema centrale di questo capitolo ha riguardato la necessità di mettere in discussione l’assunzione, spesso implicita nella CTT, che i punteggi grezzi rappresentino dati ordinali. Tale presupposto, lungi dall’essere scontato, richiede una verifica empirica per evitare interpretazioni fuorvianti. In questo contesto, la MSA offre strumenti potenti per validare empiricamente questa ipotesi. I coefficienti di scalabilità (\\(H\\), \\(H_i\\), \\(H_{ij}\\)) permettono di dimostrare che un test misura effettivamente un costrutto unidimensionale, mentre il principio dell’Ordinamento Invariante degli Item (IIO) garantisce che tale misurazione sia precisa e coerente attraverso tutti i livelli di abilità dei rispondenti.\nL’MSA rappresenta un’importante evoluzione metodologica rispetto allo scaling di Guttman, offrendo un approccio meno restrittivo e più flessibile nell’analisi dei dati empirici. La sua applicazione consente di esaminare in dettaglio la struttura delle risposte agli item e di valutare la relazione tra i punteggi dei rispondenti e il costrutto latente. In definitiva, la MSA fornisce un utile quadro analitico per validare la qualità degli strumenti di misurazione psicologica.\n\n\n\n\n\n\nWind, S. A. (2017). An instructional module on Mokken scale analysis. Educational Measurement: Issues and Practice, 36(2), 50–66.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#estensione-della-msa-agli-item-politomici",
    "href": "chapters/mokken/01_core_issues.html#estensione-della-msa-agli-item-politomici",
    "title": "61  Analisi della Scala di Mokken",
    "section": "61.14 Estensione della MSA agli Item Politomici",
    "text": "61.14 Estensione della MSA agli Item Politomici\nLa MSA, inizialmente concepita per item dicotomici, è stata ampliata da Molenaar (1982a, 1997) per includere item politomici, come quelli tipici delle scale Likert. Questa estensione conserva i principi fondamentali della MSA per gli item dicotomici, adattandoli per affrontare le specificità degli item con più categorie di risposta.\nNel caso degli item politomici, la MSA non si limita a valutare il comportamento dell’intero item rispetto al tratto latente, ma esamina anche i singoli “passaggi” tra categorie di risposta consecutive. Ad esempio, in un item Likert a cinque punti (da “fortemente in disaccordo” a “fortemente d’accordo”), ci sono quattro passaggi distinti tra le categorie. Ogni passaggio rappresenta una transizione tra due categorie adiacenti, e la probabilità di effettuare questa transizione dipende dal livello del tratto latente (\\(\\theta\\)).\nPer ogni passaggio, viene definita una Funzione di Risposta del Passaggio dell’Item (Item Step Response Function, ISRF), che descrive la probabilità che un individuo con un certo livello di \\(\\theta\\) scelga una categoria specifica o una categoria superiore.\nLe ISRF giocano un ruolo chiave nell’analisi degli item politomici. Esse:\n\nDescrivono la probabilità di transizione: Collegano la probabilità di scegliere una determinata categoria o una categoria superiore al livello del tratto latente.\nRappresentano il contributo di ogni passaggio: Consentono di analizzare come ciascun passaggio tra categorie rifletta un avanzamento lungo il continuum latente.\n\nPer un item a cinque punti, per esempio, le ISRF rappresentano le probabilità delle seguenti transizioni:\n\nPassaggio 1: Da “fortemente in disaccordo” a “in disaccordo o superiore.”\nPassaggio 2: Da “in disaccordo” a “neutrale o superiore.”\nPassaggio 3: Da “neutrale” a “d’accordo o superiore.”\nPassaggio 4: Da “d’accordo” a “fortemente d’accordo.”\n\nQueste probabilità devono essere monotone crescenti, ossia aumentare con il livello di \\(\\theta\\).\n\n61.14.1 Assunzioni Fondamentali\nL’estensione della MSA agli item politomici conserva le assunzioni centrali del modello per gli item dicotomici, ma le applica ai singoli passaggi tra categorie:\n\nMonotonicità delle ISRF:\n\nLa probabilità di scegliere una categoria \\(k\\) o superiore deve aumentare monotonicamente con il tratto latente \\(\\theta\\).\nCiò garantisce che le categorie siano ordinate in modo significativo e rappresentino livelli crescenti del tratto latente.\n\nOrdinamento delle categorie:\n\nOgni categoria di risposta deve riflettere un livello progressivamente più alto del tratto latente.\nAd esempio, un passaggio da “neutrale” a “d’accordo” dovrebbe rappresentare un incremento significativo lungo il continuum di \\(\\theta\\).\n\nValidità del modello di omogeneità monotona:\n\nAnche per gli item politomici, il modello presuppone che gli item misurino lo stesso tratto latente in modo coerente e monotono.\n\n\nL’analisi degli item politomici con la MSA offre numerosi vantaggi:\n\nMisurazione più sfumata: Permette di cogliere differenze più sottili tra i rispondenti grazie alla maggiore varietà di opzioni di risposta.\nApprofondimento delle relazioni tra categorie: Analizzando i singoli passaggi, si può comprendere meglio come le diverse categorie di risposta riflettano i livelli del tratto latente.\nAdattabilità a scale Likert e simili: La MSA estesa è particolarmente utile in contesti come l’assessment psicologico, dove le risposte politomiche sono comuni.\n\nIn conclusione, l’estensione della MSA agli item politomici amplia l’applicabilità di questo modello, rendendolo particolarmente adatto per scale con più livelli di risposta. Attraverso l’analisi dei singoli passaggi, è possibile verificare se le categorie sono coerenti con il tratto latente e se ogni passaggio rappresenta un avanzamento significativo lungo il continuum.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#mokken-reliability-coefficient",
    "href": "chapters/mokken/01_core_issues.html#mokken-reliability-coefficient",
    "title": "61  Analisi della Scala di Mokken",
    "section": "61.15 Mokken Reliability Coefficient",
    "text": "61.15 Mokken Reliability Coefficient\nL’affidabilità di un test psicometrico si riferisce alla sua capacità di fornire risultati coerenti nel tempo o attraverso somministrazioni ripetute. Sebbene l’alfa di Cronbach sia l’indicatore di affidabilità più diffuso, presenta alcune limitazioni, soprattutto quando gli item non sono omogenei o quando si lavora con scale ordinali. In questi casi, il coefficiente \\(\\rho\\) di Mokken (Mokken reliability coefficient o \\(\\rho_M\\)) rappresenta un’alternativa valida, basandosi su assunzioni meno restrittive e adattandosi meglio ai dati ordinali.\nIl coefficiente \\(\\rho\\) di Mokken è una misura di consistenza interna che deriva dalla teoria della scala non parametrica di Mokken, utilizzata principalmente per dati ordinali. A differenza dell’alfa di Cronbach, \\(\\rho\\) non richiede che gli item soddisfino rigorosi criteri di omogeneità o equivalenza essenziale, rendendolo particolarmente utile in contesti in cui i dati non rispettano pienamente i presupposti della teoria classica dei test (CTT).\nIl coefficiente \\(\\rho\\) di Mokken si fonda su due principi principali:\n\nMonotonicità: la probabilità di rispondere in modo favorevole a un item aumenta con il livello della variabile latente.\nIndipendenza locale: le risposte agli item sono condizionalmente indipendenti dato il livello della variabile latente.\n\nQueste assunzioni permettono una maggiore flessibilità rispetto ai modelli parametrici, come il modello di Rasch, e lo rendono idoneo per scale ordinali, come quelle Likert.\nIl coefficiente \\(\\rho\\) di Mokken misura il rapporto tra la varianza spiegata dal punteggio totale (\\(S^2_T\\)) e la varianza totale osservata. La formula è la seguente:\n\\[\n\\rho = 1 - \\frac{\\sum_{i=1}^k S^2_i}{S^2_T},\n\\]\ndove:\n\n\\(k\\): numero di item del test,\n\\(S^2_i\\): varianza di ciascun item,\n\\(S^2_T\\): varianza totale del punteggio del test (somma dei punteggi degli item).\n\nInterpretazione di \\(\\rho\\):\n\n\\(\\rho\\) varia tra 0 e 1.\n\nvalori vicini a 1 indicano maggiore affidabilità.\n\n\\(\\rho &gt; 0.70\\): accettabile per la ricerca esplorativa.\n\n\\(\\rho &gt; 0.80\\): preferibile per applicazioni pratiche.\n\n\nVantaggi di \\(\\rho\\) di Mokken:\n\na differenza dell’alfa di Cronbach, che assume una scala intervallare, \\(\\rho\\) può essere utilizzato con dati ordinali come le scale Likert.\nnon richiede che tutti gli item abbiano correlazioni simili (omogeneità), né che riflettano lo stesso grado di relazione con la variabile latente.\npuò essere applicato a test che non rispettano i presupposti della teoria classica dei test o di modelli parametrici più rigidi.\n\nLimitazioni di \\(\\rho\\) di Mokken:\n\nse gli item non rispettano l’assunzione di monotonicità, l’interpretazione del coefficiente può risultare ambigua.\nil coefficiente è appropriato per scale unidimensionali; in presenza di più dimensioni, è necessario analizzare ogni sottoscala separatamente.\n\nIn conclusione, il coefficiente \\(\\rho\\) di Mokken è uno strumento efficace per valutare l’affidabilità di scale ordinali, offrendo un’alternativa all’alfa di Cronbach quando le assunzioni della teoria classica dei test non sono soddisfatte. Grazie alla sua flessibilità e semplicità, \\(\\rho\\) è particolarmente indicato per scale psicometriche utilizzate in contesti esplorativi o applicativi. Tuttavia, è essenziale verificare la monotonicità degli item e considerare la dimensionalità della scala per un’interpretazione accurata.\n\n61.15.1 Procedura di Selezione Automatica degli Item\nLa Procedura di Selezione Automatica degli Item (AISP) è una metodologia impiegata nella MSA per selezionare insiemi di item che rispettino le assunzioni del Modello di Mokken (MHM). A differenza di tecniche più comuni come l’analisi fattoriale o l’analisi parallela, l’AISP non determina esplicitamente la dimensionalità dei dati. Piuttosto, si basa sui coefficienti di scalabilità per identificare gruppi di item che misurano lo stesso costrutto latente, formando così una o più scale.\n\n61.15.1.1 Come Funziona l’AISP\nL’AISP segue un approccio iterativo che:\n\nSeleziona l’item iniziale, valutato come più rappresentativo di una dimensione, utilizzando il coefficiente di scalabilità individuale (\\(H_i\\)).\nSuccessivamente, analizza le coppie di item (\\(H_{ij}\\)) per identificare un insieme scalabile di item che misurano lo stesso costrutto.\n\nUn item viene incluso in una scala se:\n\nIl suo coefficiente di scalabilità individuale (\\(H_i\\)) supera una soglia predefinita (\\(c\\), solitamente pari a 0.30);\nLa covarianza tra ogni coppia di item (\\(H_{ij}\\)) è positiva e superiore alla stessa soglia.\n\nSe un item non soddisfa questi criteri, la procedura tenta di assegnarlo a una nuova scala. Questo processo iterativo continua finché tutti gli item non sono stati assegnati a una scala o esclusi come non scalabili.\n\n\n61.15.1.2 Criteri di Selezione\n\nCoefficiente di scalabilità individuale (\\(H_i\\)): Ogni item deve mostrare una buona capacità di discriminazione lungo la dimensione latente, con un valore di \\(H_i &gt; c\\).\n\nCoefficiente di scalabilità di coppia (\\(H_{ij}\\)): Gli item di una scala devono essere correlati positivamente e mostrare coerenza con il costrutto misurato.\n\nLa procedura inizia selezionando la coppia di item con il valore più alto di \\(H_{ij}\\). Successivamente, vengono aggiunti nuovi item che rispettano i criteri sopra descritti. Gli item esclusi per una scala possono essere testati per formarne altre, ma se non soddisfano i criteri per alcuna scala, vengono classificati come non scalabili.\n\n\n61.15.1.3 Caratteristiche dell’AISP\n\nScalabilità: Gli item selezionati formano scale che misurano un tratto latente comune e sono in grado di ordinare i rispondenti lungo il continuum del costrutto.\nNeutralità rispetto alla difficoltà degli item: L’AISP non è influenzata dalla distribuzione delle risposte o dalla difficoltà degli item, rendendola adatta sia per item dicotomici sia politomici.\nRigorosità statistica: La procedura non forza soluzioni. Se nessuna coppia di item soddisfa i criteri di scalabilità (\\(H_{ij} &gt; c\\)), non viene generata alcuna scala, a differenza dell’analisi fattoriale, che potrebbe imporre soluzioni anche in presenza di dati incoerenti.\n\n\n\n61.15.1.4 Soglia (\\(c\\)) e Implicazioni\nLa scelta della soglia \\(c\\) è cruciale:\n\nValore alto di \\(c\\): Maggiore precisione nella selezione degli item, ma rischio di escluderne troppi, riducendo la lunghezza della scala e la copertura del costrutto.\nValore basso di \\(c\\): Aumenta il numero di item inclusi, ma rischia di compromettere la coerenza e la validità della scala.\n\nUn valore comunemente utilizzato è \\(c = 0.30\\), che rappresenta un buon equilibrio tra inclusività e rigore. Tuttavia, il valore ottimale dipende dagli obiettivi dello studio e dalla natura dei dati.\n\n\n61.15.1.5 Limitazioni dell’AISP\n\nRelazioni tra dimensioni: L’AISP può risultare meno efficace in presenza di dimensioni fortemente correlate o di item che saturano su più dimensioni, poiché non distingue esplicitamente tali situazioni.\nDipendenza dai dati statistici: L’AISP si basa esclusivamente sui coefficienti di scalabilità e non considera le relazioni teoriche tra gli item. Questo può portare all’inclusione di item con scarso valore teorico o alla loro esclusione inappropriata.\nUnidimensionalità richiesta: Gli item selezionati da una scala devono riflettere un unico costrutto. In caso di multidimensionalità, occorre considerare scale separate.\n\nIn conclusione, l’AISP offre un’alternativa flessibile all’analisi fattoriale per la costruzione di scale psicometriche, particolarmente utile quando si lavora con dati ordinali. Sebbene non forzi soluzioni e tenga conto della scalabilità degli item, è fondamentale integrarla con considerazioni teoriche per garantire che le scale siano valide e utili nel contesto di applicazione.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#ordinamento-invariante-degli-item-iio",
    "href": "chapters/mokken/01_core_issues.html#ordinamento-invariante-degli-item-iio",
    "title": "61  Analisi della Scala di Mokken",
    "section": "61.16 Ordinamento Invariante degli Item (IIO)",
    "text": "61.16 Ordinamento Invariante degli Item (IIO)\nL’Ordinamento Invariante degli Item (Invariant Item Ordering, IIO) è un concetto chiave nell’ambito delle scale di Mokken, che garantisce che l’ordine di difficoltà degli item rimanga costante attraverso diversi sottogruppi di persone nella popolazione target. Questo principio assicura che le differenze nei punteggi totali riflettano differenze effettive nel costrutto latente misurato, e non siano influenzate da variazioni nell’interpretazione o nella difficoltà percepita degli item.\n\n61.16.1 Cos’è l’IIO?\nL’IIO richiede che gli item di una scala possano essere ordinati in base alla loro difficoltà in modo stabile e uniforme per tutti i partecipanti, indipendentemente dalle loro caratteristiche personali (ad esempio, abilità, età, genere o livello del tratto misurato). In pratica, questo significa che:\n\nUn partecipante con un punteggio totale più alto deve avere una probabilità maggiore di rispondere correttamente (o positivamente) a ogni item rispetto a un partecipante con un punteggio totale più basso.\nGli item più difficili devono rimanere tali per tutti i partecipanti, e lo stesso vale per quelli più facili.\n\n\n\n61.16.2 Perché l’IIO è importante?\n\nValidità dei confronti\nL’IIO garantisce che i punteggi totali siano interpretabili in modo uniforme tra gruppi diversi. Se l’ordine degli item varia tra sottogruppi, i punteggi totali potrebbero non riflettere effettivamente il costrutto latente, compromettendo la validità dei confronti tra gruppi.\nInterpretazione coerente\nPermette di interpretare i punteggi del test in modo uniforme per tutti i partecipanti, indipendentemente dalle loro caratteristiche. Ad esempio, in una scala sulla depressione, un punteggio più alto indica costantemente una maggiore gravità, e la progressione degli item da più semplici (es. stanchezza) a più complessi (es. ideazione suicidaria) è coerente.\nIdentificazione di bias e DIF\nL’IIO consente di rilevare la Funzione Differenziale degli Item (Differential Item Functioning, DIF) e bias che potrebbero indicare che un item è percepito in modo diverso da sottogruppi specifici. Ad esempio, un item potrebbe essere interpretato diversamente in base al genere o al contesto culturale.\n\n\n\n61.16.3 IIO nei Test Psicologici\nNell’ambito dei test psicologici (come scale per ansia o depressione) l’IIO implica che:\n\nUn individuo con un punteggio totale più alto debba manifestare tutti i sintomi o le competenze presenti in un individuo con un punteggio inferiore, più eventuali sintomi o competenze aggiuntive.\nAd esempio, una persona con una gravità depressiva maggiore (punteggio più alto) dovrebbe rispondere positivamente a item più gravi oltre che a quelli più lievi.\nLa difficoltà degli item deve essere ordinata in modo invariato per tutti i partecipanti.\nAd esempio, un problema matematico considerato più difficile deve esserlo per tutti gli studenti, indipendentemente dal livello generale di abilità.\n\n\n\n61.16.4 Come Verificare l’IIO?\nLa verifica dell’IIO può essere effettuata attraverso diverse tecniche statistiche:\n\nMetodo dei gruppi di restscore\nGli item vengono confrontati tra gruppi di partecipanti suddivisi in base ai punteggi totali, escluso l’item in esame. Questo approccio consente di analizzare se l’ordine degli item rimane costante nei diversi livelli del costrutto.\nProporzioni \\(P(++)\\) e \\(P(--)\\)\nSi analizzano le proporzioni di risposte positive (\\(P(++)\\)) o negative (\\(P(--)\\)) per coppie di item. Per rispettare l’IIO, le proporzioni devono essere coerenti con l’ordine di difficoltà teorico.\nTest di monotonicità\nUtilizzato per verificare che la probabilità di una risposta positiva a un item aumenti con il livello del punteggio totale. Questo è un presupposto fondamentale per l’IIO.\nAnalisi di divisione del restscore\nI restscore vengono divisi in intervalli e si valuta se l’ordine degli item rimane invariato in base a questi sottogruppi.\n\n\n\n61.16.5 Cosa Succede se l’IIO Non È Rispettato?\nSe l’IIO viene violato, si possono verificare le seguenti problematiche:\n\nPresenza di DIF o bias\nGli item possono riflettere caratteristiche diverse da quelle del costrutto latente, rendendo i confronti tra sottogruppi non validi.\nPunteggi totali non interpretabili\nLe differenze nei punteggi totali potrebbero non riflettere differenze reali nel costrutto latente, invalidando il test.\nCompromissione della validità costruttuale\nLa violazione dell’IIO può mettere in dubbio la capacità della scala di misurare un costrutto latente comune.\n\n\n\n61.16.6 Vantaggi e Limiti dell’IIO\nL’IIO offre vantaggi importanti:\n\nPermette una valutazione uniforme e interpretabile dei punteggi totali.\nAiuta a identificare potenziali bias negli item.\nSi applica sia a dati dicotomici che politomici, rendendolo versatile.\n\nTuttavia, presenta anche alcune limitazioni:\n\nDifficoltà di verifica: La verifica empirica dell’IIO richiede analisi specifiche che non sempre vengono incluse negli studi psicometrici.\nSensibilità alla dimensionalità: L’IIO è rilevante solo per scale unidimensionali. In caso di scale multidimensionali, è necessario verificare l’IIO separatamente per ciascuna dimensione.\nRigidità: Un eccessivo focus sull’IIO potrebbe portare all’esclusione di item potenzialmente utili ma che non rispettano perfettamente i criteri.\n\nIn conclusione, l’Ordinamento Invariante degli Item è un requisito fondamentale per garantire la validità e l’affidabilità delle scale di Mokken e di altri strumenti psicometrici. La sua verifica permette di assicurare che i punteggi totali siano interpretabili in modo coerente e che gli item riflettano realmente il costrutto latente. Tuttavia, per una valutazione completa, l’IIO dovrebbe essere integrato con altre analisi psicometriche e una riflessione teorica sui contenuti della scala, specialmente in contesti clinici o educativi dove accuratezza e validità sono essenziali.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#dimensione-del-campione",
    "href": "chapters/mokken/01_core_issues.html#dimensione-del-campione",
    "title": "61  Analisi della Scala di Mokken",
    "section": "61.17 Dimensione del Campione",
    "text": "61.17 Dimensione del Campione\nLa determinazione della dimensione del campione è un aspetto cruciale nella ricerca psicometrica. Sebbene esistano linee guida consolidate per molti test statistici, l’MSA rappresenta un’area in cui la ricerca sulla dimensione del campione è ancora limitata. Questa lacuna pone sfide per i ricercatori, che devono bilanciare l’accuratezza dei risultati con le risorse disponibili.\nCampioni troppo piccoli possono portare a due rischi principali:\n\nFalsi positivi: una scala viene erroneamente identificata come valida in un campione ridotto, quando in realtà non esiste nella popolazione.\n\nFalsi negativi: una scala realmente esistente non viene rilevata a causa della scarsa potenza statistica del campione.\n\nDiversi studi hanno esplorato le dimensioni minime del campione necessarie per applicare correttamente l’MSA, con particolare attenzione a procedure come l’Automated Item Selection Procedure (AISP) e l’Algoritmo Genetico (GA).\nStudio di Straat et al. (2014). Straat e colleghi hanno investigato come diversi fattori influenzino la dimensione del campione necessaria, tra cui la lunghezza del test, i valori dei coefficienti di scalabilità (\\(H_i\\)) e la correlazione tra dimensioni sottostanti nella scala.\nRisultati principali:\n\nIl valore di \\(H_i\\) è stato identificato come il fattore più influente:\n\nPer valori bassi di \\(H_i\\) (es. \\(H_i \\approx 0.22\\)), erano necessari campioni molto grandi (750-2500 partecipanti) per ottenere una classificazione accurata.\n\nPer valori moderati o alti di \\(H_i\\) (es. \\(H_i \\approx 0.42\\)), campioni più piccoli (50-250 partecipanti) erano sufficienti.\n\n\nLa lunghezza del test ha avuto un impatto minore sulla precisione. Test più lunghi non hanno necessariamente richiesto campioni più grandi.\n\nLa correlazione tra dimensioni ha interagito con \\(H_i\\), influenzando la necessità di campioni più grandi per scale multidimensionali.\n\nLinee guida suggerite da Straat et al.:\n\nPer \\(H_i \\approx 0.22\\):\n\nPrecisione moderata: 750-1000 partecipanti.\n\nPrecisione alta: almeno 1250-2500 partecipanti.\n\n\nPer \\(H_i \\approx 0.42\\):\n\nPrecisione moderata: 50 partecipanti.\n\nPrecisione alta: almeno 250 partecipanti.\n\n\nStudio di Watson et al. (2018). Watson e colleghi hanno utilizzato dati reali per analizzare l’effetto della dimensione del campione sui coefficienti di scalabilità (\\(H\\) e \\(H_i\\)). Lo studio includeva 7510 partecipanti e un questionario di 14 item con scala Likert a 5 punti. Sono stati generati campioni di diverse dimensioni (da 50 a 1000 partecipanti) utilizzando tecniche di bootstrapping.\nRisultati principali:\n\nValori medi di \\(H\\) e \\(H_i\\): Non variavano significativamente tra campioni di dimensioni diverse, suggerendo una certa stabilità delle stime puntuali anche per campioni più piccoli.\n\nIntervalli di confidenza (CI):\n\nCampioni piccoli (\\(N = 50\\)) producevano intervalli di confidenza molto ampi. In oltre il 50% dei casi, il limite inferiore del CI per \\(H_i\\) era inferiore alla soglia critica di 0.30.\n\nCampioni grandi (\\(N = 1000\\)) producevano intervalli più stretti, con valori di \\(H_i\\) sempre superiori a 0.30.\n\n\nErrori standard: Campioni piccoli aumentavano la probabilità di errori nella classificazione degli item, portando a decisioni sbagliate sull’inclusione/esclusione.\n\n\n61.17.1 Implicazioni per la Ricerca\nPrecisione delle stime. Mentre i valori medi di \\(H\\) e \\(H_i\\) possono rimanere relativamente stabili, la dimensione del campione influenza notevolmente la larghezza degli intervalli di confidenza. Stime puntuali apparentemente affidabili potrebbero nascondere elevata incertezza per campioni piccoli.\nDecisioni sugli item. Campioni piccoli aumentano il rischio di errori nella selezione degli item, compromettendo la validità della scala. Un’adeguata dimensione del campione è essenziale per garantire la qualità delle decisioni basate sui coefficienti di scalabilità.\nBilanciamento tra risorse e precisione. Non sempre è pratico utilizzare campioni molto grandi. Tuttavia, quando le risorse sono limitate, è cruciale pianificare attentamente la dimensione del campione per ottimizzare l’efficienza senza sacrificare la precisione.\nIn conclusione, la determinazione della dimensione del campione nell’MSA dipende da variabili chiave come il coefficiente di scalabilità \\(H_i\\), la lunghezza della scala e la struttura multidimensionale dei dati. Sebbene campioni più grandi siano generalmente preferibili per ottenere risultati più precisi e intervalli di confidenza più stretti, non sempre sono necessari.\nFuturi studi potrebbero fornire indicazioni più dettagliate, sviluppando simulazioni e linee guida empiriche che aiutino i ricercatori a determinare la dimensione ottimale del campione per specifici scenari applicativi. L’obiettivo finale rimane quello di bilanciare precisione e praticità, garantendo risultati validi senza spreco di risorse.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#confronto-tra-il-modello-di-rasch-e-la-msa",
    "href": "chapters/mokken/01_core_issues.html#confronto-tra-il-modello-di-rasch-e-la-msa",
    "title": "61  Analisi della Scala di Mokken",
    "section": "61.18 Confronto tra il Modello di Rasch e la MSA",
    "text": "61.18 Confronto tra il Modello di Rasch e la MSA\nIl Modello di Rasch (RM) e la MSA rappresentano due approcci distinti alla teoria della risposta all’item (IRT), ciascuno con caratteristiche e assunzioni specifiche che influenzano la loro applicabilità. Mentre il RM è un modello parametrico altamente restrittivo, l’MSA appartiene alla categoria dei modelli non parametrici, offrendo maggiore flessibilità nell’analisi dei dati.\nIl Modello di Rasch si basa su una relazione parametrica tra l’abilità del rispondente (\\(\\theta\\)) e la difficoltà dell’item (\\(\\delta\\)), descritta da una funzione logistica. Questo implica che la probabilità di risposta corretta a un item dipende esclusivamente da \\(\\theta\\) e \\(\\delta\\) e che le curve caratteristiche degli item (IRFs) sono parallele e hanno la stessa pendenza. Una delle caratteristiche distintive del RM è che i punteggi grezzi totali sono sufficienti per stimare i parametri delle persone e degli item, rendendo possibile una scala metrica comune che consente confronti diretti e indipendenti da campione e item. Tuttavia, questa precisione deriva da assunzioni stringenti, come la necessità di unidimensionalità, monotonicità e indipendenza locale delle risposte.\nL’MSA, invece, appartiene ai modelli non parametrici della IRT (NIRT), che non vincolano le IRFs a una forma specifica. Questo approccio si concentra sull’ordinamento delle persone lungo una scala latente, senza stimare direttamente \\(\\theta\\), ma utilizzando i punteggi grezzi per determinare un ordine relativo. La maggiore flessibilità dell’MSA consente di analizzare dati che non soddisfano le rigide assunzioni parametriche del RM, rendendolo particolarmente utile per scale ordinali o situazioni in cui le IRFs non seguono una forma logistica.\nIl Modello di Monotonicità Omogenea (MHM), il più semplice tra i modelli di Mokken, si basa su tre assunzioni: unidimensionalità, monotonicità e indipendenza locale. Questo modello consente di ordinare le persone su una scala unidimensionale ordinale, garantendo che l’ordine sia coerente per tutti gli item. Una variante più restrittiva, il Modello di Monotonicità Doppia (DMM), aggiunge il requisito che le IRFs non si intersechino, permettendo così di produrre scale ordinali distinte sia per persone che per item. In entrambi i casi, il punteggio grezzo totale fornisce informazioni utili senza dover stimare direttamente parametri come \\(\\theta\\).\nUna differenza cruciale tra il Modello di Rasch e l’MSA riguarda il tipo di scala che producono. Il RM genera una scala metrica comune per persone e item, consentendo misurazioni precise e confronti generalizzabili. L’MSA, invece, si limita a scale ordinali, dove l’ordine relativo è garantito ma non vi è una misura metrica assoluta. Questa differenza riflette il maggiore rigore del RM rispetto alla flessibilità dell’MSA. Inoltre, mentre il RM richiede che gli item seguano una funzione logistica, l’MSA permette l’inclusione di item che non rispettano questa forma, evitando così di scartare dati potenzialmente utili.\nDal punto di vista applicativo, il RM è particolarmente indicato in contesti in cui la precisione e la generalizzabilità sono fondamentali, come nei test standardizzati. Tuttavia, la sua rigidità può limitarne l’uso con dati complessi o non conformi alle sue assunzioni. L’MSA, al contrario, offre un’opzione più flessibile per l’analisi esplorativa e per dati ordinali o non lineari, risultando utile in ambiti come la psicologia e le scienze sociali. La scelta tra i due dipende dagli obiettivi dello studio e dalle caratteristiche specifiche dei dati.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#confronto-tra-la-ctt-e-la-msa",
    "href": "chapters/mokken/01_core_issues.html#confronto-tra-la-ctt-e-la-msa",
    "title": "61  Analisi della Scala di Mokken",
    "section": "61.19 Confronto tra la CTT e la MSA",
    "text": "61.19 Confronto tra la CTT e la MSA\nLa CTT e la MSA rappresentano due approcci distinti alla misurazione psicometrica, ciascuno con i propri punti di forza e limiti. Sebbene condividano alcune somiglianze concettuali, si differenziano in modo significativo per quanto riguarda le assunzioni, la flessibilità e la capacità di testare empiricamente i modelli.\nLa CTT si basa su un insieme di assunzioni fondamentali che guidano l’interpretazione dei punteggi dei test. In questo framework, il punteggio osservato è la somma di un punteggio vero e di un errore di misura. Si assume che l’errore abbia una media di zero su prove ripetute, che sia non correlato con i punteggi veri, e che i punteggi di errore di due test somministrati agli stessi individui siano non correlati. I punteggi grezzi totali vengono considerati un’indicazione diretta delle posizioni dei soggetti sul continuum del tratto latente, mentre la proporzione di risposte corrette misura la facilità degli item, e la correlazione corretta tra un item e il punteggio totale del test ne determina la capacità discriminativa. L’affidabilità, intesa come correlazione tra punteggi osservati su due forme parallele dello stesso test, costituisce un aspetto cruciale della CTT.\nDal punto di vista metodologico, la CTT e la MSA condividono alcune somiglianze. Ad esempio, il coefficiente di scalabilità dell’item (\\(H_i\\)) nella MSA è concettualmente simile alla correlazione corretta tra un item e il punteggio totale del test nella CTT. Inoltre, il coefficiente di scalabilità tra coppie di item (\\(H_{ij}\\)) nella MSA può essere paragonato alle correlazioni tra coppie di item nella CTT. Infine, il coefficiente di scalabilità complessivo (\\(H\\)) nella MSA è analogo agli indici di discriminazione media degli item nella CTT. Questi paralleli rendono i due approcci confrontabili nella loro capacità di descrivere i dati dei test.\nTuttavia, esistono differenze sostanziali tra i due approcci. La CTT si basa su modelli teorici che non possono essere testati direttamente con i dati, poiché si assume implicitamente che le sue condizioni siano soddisfatte. Al contrario, la MSA permette di verificare empiricamente le assunzioni che ne stanno alla base, come l’indipendenza locale, l’unidimensionalità e la monotonicità. Ad esempio, un coefficiente di scalabilità negativo in un modello di Mokken smentirebbe gli assiomi del Modello di Monotonicità Omogenea (MHM), evidenziando che i dati non sono coerenti con il modello. Questa capacità di testare empiricamente le assunzioni rende la MSA più robusta e trasparente rispetto alla CTT.\nLa CTT offre un quadro teorico consolidato e intuitivo per interpretare i punteggi dei test, ma presenta limitazioni nei contesti in cui le assunzioni non possono essere verificate o quando i dati violano condizioni fondamentali, come l’unidimensionalità. La MSA, d’altra parte, si distingue per la sua maggiore flessibilità, consentendo l’analisi di dati ordinali e la valutazione della struttura sottostante dei dati in modo più rigoroso. Questa flessibilità la rende particolarmente utile per l’analisi esplorativa di scale e per la validazione di strumenti psicometrici.\nIn sintesi, la CTT e la MSA non sono necessariamente in opposizione, ma piuttosto complementari. La CTT fornisce una base teorica utile per la misurazione, mentre la MSA aggiunge strumenti pratici per testare empiricamente la validità delle scale e la struttura dei dati. L’integrazione di entrambi gli approcci può fornire una comprensione più approfondita e completa dei dati psicologici.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/01_core_issues.html#riflessioni-conclusive",
    "href": "chapters/mokken/01_core_issues.html#riflessioni-conclusive",
    "title": "61  Analisi della Scala di Mokken",
    "section": "61.20 Riflessioni Conclusive",
    "text": "61.20 Riflessioni Conclusive\nIl tema centrale di questo capitolo ha riguardato la necessità di mettere in discussione l’assunzione, spesso implicita nella CTT, che i punteggi grezzi rappresentino dati ordinali. Tale presupposto, lungi dall’essere scontato, richiede una verifica empirica per evitare interpretazioni fuorvianti. In questo contesto, la MSA offre strumenti potenti per validare empiricamente questa ipotesi. I coefficienti di scalabilità (\\(H\\), \\(H_i\\), \\(H_{ij}\\)) permettono di dimostrare che un test misura effettivamente un costrutto unidimensionale, mentre il principio dell’Ordinamento Invariante degli Item (IIO) garantisce che tale misurazione sia precisa e coerente attraverso tutti i livelli di abilità dei rispondenti.\nL’MSA rappresenta un’importante evoluzione metodologica rispetto allo scaling di Guttman, offrendo un approccio meno restrittivo e più flessibile nell’analisi dei dati empirici. La sua applicazione consente di esaminare in dettaglio la struttura delle risposte agli item e di valutare la relazione tra i punteggi dei rispondenti e il costrutto latente. In definitiva, la MSA fornisce un utile quadro analitico per validare la qualità degli strumenti di misurazione psicologica.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Analisi della Scala di Mokken</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_applications.html",
    "href": "chapters/mokken/02_applications.html",
    "title": "62  Applicazione Pratica",
    "section": "",
    "text": "62.1 Introduzione\nIn questo capitolo, esamineremo l’applicazione pratica dei concetti e delle metodologie esplorate nel precedente capitolo, affrontando un’analisi dettagliata di un set di dati concreti. Il nostro focus è un caso di studio di grande rilevanza psicologica: l’indagine condotta dai ricercatori dell’ospedale Meyer, mirata a comprendere la capacità di coping delle famiglie di fronte alla diagnosi di una grave malattia in un figlio.\nQuesto lavoro non solo rappresenta un’opportunità per mettere in pratica le teorie e i metodi discussi, ma offre anche una finestra su questioni di vitale importanza nel campo della psicologia. Affrontare tematiche così delicate ci permette di esplorare le dinamiche familiari in situazioni di stress estremo, fornendo intuizioni preziose che possono guidare interventi psicosociali efficaci.\nPer garantire la massima accuratezza e rilevanza dei nostri risultati, iniziamo con un’attenta preparazione e pulizia dei dati. Questo passo ci assicura che l’analisi sia condotta su informazioni ben distribuite e rappresentative, eliminando gli item con eccessiva asimmetria e curtosi.\nAttraverso questa esplorazione approfondita, miriamo a dimostrare come le competenze metodologiche e analitiche possano essere efficacemente applicate a questioni di profondo impatto psicologico, evidenziando il potere dell’analisi statistica nel trasformare set di dati complessi in comprensioni approfondite e applicabili.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_applications.html#importazione-dei-dati",
    "href": "chapters/mokken/02_applications.html#importazione-dei-dati",
    "title": "62  Applicazione Pratica",
    "section": "62.2 Importazione dei dati",
    "text": "62.2 Importazione dei dati\n\ndf_tot &lt;- readRDS(\"../../data/fai_2022_11_20.rds\")\n\ntemp &lt;- df_tot |&gt; \n  dplyr::filter(FLAG == \"keep\")\ntemp$FLAG &lt;- NULL\n\nPer migliorare la qualità del nostro set di dati, rimuoveremo gli item che presentano livelli eccessivi di asimmetria (skewness) e curtosi. Questo passaggio è fondamentale per garantire che i nostri dati siano ben distribuiti e rappresentativi, migliorando così l’affidabilità e la validità delle nostre analisi. Gli item con asimmetria e curtosi estreme possono infatti distorcere i risultati degli analisi statistiche e influenzare negativamente le conclusioni tratte dallo studio.\n\nitems_stats &lt;- psych::describe(temp)\n\nitems_skew_kurt_bad &lt;- items_stats |&gt;\n    dplyr::filter(skew &gt; 2.5 | kurtosis &gt; 7.5) |&gt;\n    row.names()\nprint(items_skew_kurt_bad)\n\n [1] \"other\"                  \"child_birth_place\"      \"has_chronic_disease\"   \n [4] \"hospitalization_number\" \"emergency_care_number\"  \"divorce\"               \n [7] \"low_income\"             \"change_address\"         \"change_city\"           \n[10] \"is_mother_italian\"      \"is_father_italian\"      \"is_father_working\"     \n[13] \"is_child_italian\"       \"FAI_24\"                 \"FAI_32\"                \n[16] \"FAI_52\"                 \"FAI_53\"                 \"FAI_61\"                \n[19] \"FAI_74\"                 \"FAI_76\"                 \"FAI_77\"                \n[22] \"FAI_138\"                \"FAI_152\"                \"FAI_174\"               \n[25] \"FAI_175\"                \"FAI_182\"                \"FAI_193\"               \n\n\n\n# Select the strings starting with \"FAI_\"\nbad_fai_items &lt;- grep(\"^FAI_\", items_skew_kurt_bad, value = TRUE)\n\ndf &lt;- temp |&gt;\n    dplyr::select(!any_of(bad_fai_items))\n\nCi concentreremo qui su un sottoinsieme di item, ovvero quelli che riguardano l’area delle caratteristiche del bambino.\n\n# First subscale: items names.\nitem_subscale &lt;- c(\n    \"FAI_49\", \"FAI_106\", \"FAI_60\", \"FAI_124\", \"FAI_86\",\n    \"FAI_47\", \"FAI_121\", \"FAI_167\", \"FAI_99\",\n    \"FAI_63\", \"FAI_168\", \"FAI_5\", \"FAI_132\", \"FAI_85\", \"FAI_81\",\n    \"FAI_83\",\n    # \"FAI_152\",  \"FAI_175\",\n    \"FAI_57\", \"FAI_91\", \"FAI_135\", \"FAI_1\"\n)\n\n# Select only the items of this subscale.\nsubscale_data &lt;- df %&gt;%\n    dplyr::select(all_of(item_subscale))\ndim(subscale_data)\n\n\n45320",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_applications.html#statistiche-descrittive",
    "href": "chapters/mokken/02_applications.html#statistiche-descrittive",
    "title": "62  Applicazione Pratica",
    "section": "62.3 Statistiche descrittive",
    "text": "62.3 Statistiche descrittive\nEsaminiamo le statistiche descrittive degli item.\n\npsych::describe(subscale_data)\n\n\nA psych: 20 x 13\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nFAI_49\n1\n453\n0.5629139\n1.0320709\n0\n0.3085399\n0.0000\n0\n4\n4\n2.0665609\n3.6052375\n0.04849092\n\n\nFAI_106\n2\n453\n0.6710817\n0.9911011\n0\n0.4710744\n0.0000\n0\n4\n4\n1.6036473\n2.0835785\n0.04656599\n\n\nFAI_60\n3\n453\n0.9403974\n1.2169441\n0\n0.7272727\n0.0000\n0\n4\n4\n1.1792903\n0.2955897\n0.05717702\n\n\nFAI_124\n4\n453\n1.0463576\n1.1955175\n1\n0.8429752\n1.4826\n0\n4\n4\n1.1667416\n0.4658086\n0.05617031\n\n\nFAI_86\n5\n453\n1.1898455\n1.0171900\n1\n1.0606061\n1.4826\n0\n4\n4\n0.9491960\n0.6428214\n0.04779175\n\n\nFAI_47\n6\n453\n1.6247241\n1.1132778\n2\n1.5950413\n1.4826\n0\n4\n4\n0.1994007\n-0.7340689\n0.05230635\n\n\nFAI_121\n7\n453\n0.4900662\n0.7121035\n0\n0.3746556\n0.0000\n0\n4\n4\n1.8321794\n4.5725145\n0.03345754\n\n\nFAI_167\n8\n453\n1.9072848\n1.0306218\n2\n1.8815427\n1.4826\n0\n4\n4\n0.1974188\n-0.2695266\n0.04842284\n\n\nFAI_99\n9\n453\n2.0618102\n1.1543198\n2\n2.0220386\n1.4826\n0\n4\n4\n0.3447637\n-0.7883135\n0.05423468\n\n\nFAI_63\n10\n453\n1.1501104\n1.1615070\n1\n1.0192837\n1.4826\n0\n4\n4\n0.7295382\n-0.4985964\n0.05457236\n\n\nFAI_168\n11\n453\n0.7748344\n1.1298770\n0\n0.5399449\n0.0000\n0\n4\n4\n1.4492317\n1.1504289\n0.05308625\n\n\nFAI_5\n12\n453\n1.0132450\n1.1805209\n1\n0.8264463\n1.4826\n0\n4\n4\n1.0371531\n0.1238107\n0.05546571\n\n\nFAI_132\n13\n453\n1.4238411\n1.2360930\n1\n1.3002755\n1.4826\n0\n4\n4\n0.5953586\n-0.6157883\n0.05807672\n\n\nFAI_85\n14\n453\n1.3311258\n0.9027136\n1\n1.3002755\n1.4826\n0\n4\n4\n0.2930726\n-0.1115428\n0.04241319\n\n\nFAI_81\n15\n453\n1.7969095\n1.2685052\n2\n1.7465565\n1.4826\n0\n4\n4\n0.2930069\n-1.0004309\n0.05959957\n\n\nFAI_83\n16\n453\n1.1986755\n1.1825793\n1\n1.0468320\n1.4826\n0\n4\n4\n0.8458362\n-0.1601818\n0.05556242\n\n\nFAI_57\n17\n453\n0.3863135\n0.7123983\n0\n0.2341598\n0.0000\n0\n4\n4\n2.2165283\n5.6437160\n0.03347139\n\n\nFAI_91\n18\n453\n0.5320088\n0.9298228\n0\n0.3057851\n0.0000\n0\n4\n4\n2.1393688\n4.4338138\n0.04368689\n\n\nFAI_135\n19\n453\n0.4039735\n0.7686801\n0\n0.2231405\n0.0000\n0\n4\n4\n2.2914674\n5.8874641\n0.03611574\n\n\nFAI_1\n20\n453\n0.4547461\n0.7530503\n0\n0.2892562\n0.0000\n0\n4\n4\n1.8594704\n3.7153232\n0.03538139\n\n\n\n\n\nEsaminiamo la distribuzione del punteggio totale.\n\nscores &lt;- apply(subscale_data, 1, sum)\nhist(scores)",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_applications.html#automated-item-selection-procedure-aisp",
    "href": "chapters/mokken/02_applications.html#automated-item-selection-procedure-aisp",
    "title": "62  Applicazione Pratica",
    "section": "62.4 Automated Item Selection Procedure (AISP)",
    "text": "62.4 Automated Item Selection Procedure (AISP)\nIl primo passo nell’Analisi delle Scale Mokken (MSA) consiste nell’esecuzione dell’Automated Item Selection Procedure (AISP). Come precedentemente discusso, questa analisi ricerca insiemi di item (scale) che si conformano al modello di omogeneità monotona. Similmente all’analisi fattoriale esplorativa, l’AISP è un metodo per suddividere i dati in diverse sottoscale che soddisfano i criteri della MSA, includendo possibilmente anche l’individuazione di eventuali item non scalabili. Per eseguire la AISP, è necessario eseguire il codice seguente.\n\nsubscale_data &lt;- as.data.frame(subscale_data)\nscale &lt;- aisp(subscale_data, verbose = FALSE)\nprint(scale)\n\n        0.3\nFAI_49    1\nFAI_106   1\nFAI_60    1\nFAI_124   1\nFAI_86    0\nFAI_47    2\nFAI_121   0\nFAI_167   0\nFAI_99    0\nFAI_63    2\nFAI_168   0\nFAI_5     1\nFAI_132   2\nFAI_85    2\nFAI_81    1\nFAI_83    1\nFAI_57    2\nFAI_91    0\nFAI_135   0\nFAI_1     0\n\n\nNell’output precedente, FAI_* sono le etichette degli item considerati. Il valore .30 in alto indica il limite inferiore del coefficiente di scalabilità per la costruzione delle scale. ‘1’ indica che l’item appartiene alla scala 1 e 2 significa che l’item appartiene alla scala 2. ‘0’ indica che l’item non è scalabile. Dei 20 item di questa scala, sette item formano la scala 1 mentre altri cinque item formano la scala 2. Sette item risultano non scalabili.\nÈ possibile modificare sia il limite inferiore c (il limite inferiore predefinito è .30) sia il livello di alpha, che di default è .05. Per esempio:\n\nscale &lt;- aisp(subscale_data, lowerbound = 0.4, alpha = 0.1)\nprint(scale)\n\n        0.4\nFAI_49    1\nFAI_106   1\nFAI_60    1\nFAI_124   1\nFAI_86    0\nFAI_47    0\nFAI_121   0\nFAI_167   0\nFAI_99    0\nFAI_63    3\nFAI_168   0\nFAI_5     2\nFAI_132   0\nFAI_85    0\nFAI_81    2\nFAI_83    2\nFAI_57    3\nFAI_91    0\nFAI_135   0\nFAI_1     0\n\n\nSi noti che modificare il valore predefinito di c cambia la struttura della scala. Sijtsma e van der Ark (2017) hanno mostrato che il valore predefinito di .30 è quello che si dimostra più utile nella maggior parte delle applicazioni pratiche. Tuttavia, raccomandano di eseguire l’AISP 12 volte con c=0, .05, .10, .15, .20, .25, .30, .35, .40, .45, .50 e .55, così da potere esaminare le seguenti condizioni:\n\nNei dati unidimensionali, all’aumentare di c, si trova successivamente (a) la maggior parte o tutti gli item in una scala; (b) una scala più piccola; e (c) una o poche scale piccole e diversi item non scalabili. Viene raccomandato di prendere il risultato della fase (a) come finale.\nNei dati multidimensionali, all’aumentare di c, si trova successivamente (a) la maggior parte o tutti gli item in una scala; (b) due o più scale; e (c) due o più scale più piccole e diversi item non scalabili. Prendere il risultato della fase (b) come finale.\n\nLa decisione finale sulla struttura dei dati dovrebbe essere presa dal ricercatore sulla base di considerazioni teoriche e non sono statistiche.\nPer eseguire l’AISP con molteplici limiti inferiori, possiamo usare l’istruzione seguente:\n\naisp(\n    subscale_data, \n    lowerbound = c(.05, .10, .15, .20, .25, .30, .35, .40, .45, .50, .55), \n    verbose = FALSE\n)\n\n\nA matrix: 20 x 11 of type dbl\n\n\n\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n\n\n\n\nFAI_49\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nFAI_106\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nFAI_60\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nFAI_124\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nFAI_86\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nFAI_47\n1\n1\n1\n1\n2\n2\n2\n0\n0\n0\n0\n\n\nFAI_121\n1\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n\n\nFAI_167\n1\n1\n1\n2\n0\n0\n0\n0\n0\n0\n0\n\n\nFAI_99\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nFAI_63\n1\n1\n1\n1\n2\n2\n2\n3\n3\n3\n0\n\n\nFAI_168\n1\n1\n1\n1\n3\n0\n0\n0\n0\n0\n0\n\n\nFAI_5\n1\n1\n1\n1\n1\n1\n0\n2\n0\n0\n0\n\n\nFAI_132\n1\n1\n1\n1\n2\n2\n0\n0\n0\n0\n0\n\n\nFAI_85\n2\n2\n0\n2\n2\n2\n2\n0\n0\n0\n0\n\n\nFAI_81\n1\n1\n1\n1\n1\n1\n1\n2\n2\n2\n2\n\n\nFAI_83\n1\n1\n1\n1\n1\n1\n1\n2\n2\n2\n2\n\n\nFAI_57\n1\n1\n1\n1\n2\n2\n2\n3\n3\n3\n0\n\n\nFAI_91\n1\n1\n1\n1\n3\n0\n0\n0\n0\n0\n0\n\n\nFAI_135\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nFAI_1\n2\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\nPer eseguire un’analisi della dimensionalità utilizzando un metodo diverso, ovvero l’algoritmo genetico (Straat, van der Ark & Sijtsma, 2013), si può utilizzare il seguente codice:\n\nscale &lt;- aisp(subscale_data, search = \"ga\")\nprint(scale)\n\n        0.3\nFAI_49    1\nFAI_106   1\nFAI_60    1\nFAI_124   1\nFAI_86    0\nFAI_47    2\nFAI_121   0\nFAI_167   0\nFAI_99    0\nFAI_63    2\nFAI_168   0\nFAI_5     1\nFAI_132   2\nFAI_85    2\nFAI_81    1\nFAI_83    1\nFAI_57    2\nFAI_91    0\nFAI_135   0\nFAI_1     0\n\n\nI risultati dell’algoritmo genetico sono equivalenti a quelli ottenuti utilizzando il limite inferiore raccomandato di 0.3.\nI risultati dell’analisi della dimensionalità ottenuti tramite l’AISP e l’algoritmo genetico (GA) dovrebbero essere replicabili in altri campioni. Pertanto, nelle procedure AISP e GA, la dimensione del campione è un fattore critico. Sijtsma e Molenaar (2002) affermano che l’AISP richiede almeno 100 partecipanti. Tuttavia, in studi basati su simulazioni di Monte Carlo, Straat, van der Ark e Sijtsma (2014) hanno dimostrato che sia l’AISP sia il GA necessitano di un campione compreso tra 250 e 500 partecipanti quando la qualità degli item (ovvero la loro capacità discriminante) è elevata, e tra 1250 e 1750 partecipanti quando la qualità degli item è scarsa.\nPossiamo selezionare gli item della scala 1 individuata dalla AISP nel modo seguente.\n\naisp.lb &lt;- aisp(subscale_data, lowerbound = .3)\ngood_items &lt;- subscale_data[, aisp.lb == 1]\nnames(good_items) |&gt; print()\n\n[1] \"FAI_49\"  \"FAI_106\" \"FAI_60\"  \"FAI_124\" \"FAI_5\"   \"FAI_81\"  \"FAI_83\"",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_applications.html#scalability-coefficients",
    "href": "chapters/mokken/02_applications.html#scalability-coefficients",
    "title": "62  Applicazione Pratica",
    "section": "62.5 Scalability Coefficients",
    "text": "62.5 Scalability Coefficients\nIl codice seguente ritorna i valori \\(H_{ij}\\), \\(H_j\\), e \\(H\\). Nelle parentesi tonde sono riportati gli errori standard.\n\nscal_coef &lt;- coefH(good_items, se = TRUE)\nscal_coef |&gt; print()\n\n$Hij\n        FAI_49  se      FAI_106 se      FAI_60  se      FAI_124 se      FAI_5  \nFAI_49                   0.561  (0.057)  0.608  (0.054)  0.601  (0.048)  0.212 \nFAI_106  0.561  (0.057)                  0.617  (0.042)  0.722  (0.039)  0.317 \nFAI_60   0.608  (0.054)  0.617  (0.042)                  0.688  (0.037)  0.280 \nFAI_124  0.601  (0.048)  0.722  (0.039)  0.688  (0.037)                  0.288 \nFAI_5    0.212  (0.056)  0.317  (0.055)  0.280  (0.052)  0.288  (0.052)        \nFAI_81   0.254  (0.060)  0.354  (0.056)  0.324  (0.050)  0.381  (0.053)  0.413 \nFAI_83   0.223  (0.057)  0.381  (0.056)  0.430  (0.047)  0.387  (0.048)  0.409 \n        se      FAI_81  se      FAI_83  se     \nFAI_49  (0.056)  0.254  (0.060)  0.223  (0.057)\nFAI_106 (0.055)  0.354  (0.056)  0.381  (0.056)\nFAI_60  (0.052)  0.324  (0.050)  0.430  (0.047)\nFAI_124 (0.052)  0.381  (0.053)  0.387  (0.048)\nFAI_5            0.413  (0.049)  0.409  (0.048)\nFAI_81  (0.049)                  0.551  (0.041)\nFAI_83  (0.048)  0.551  (0.041)                \n\n$Hi\n        Item H  se     \nFAI_49    0.410 (0.039)\nFAI_106   0.493 (0.032)\nFAI_60    0.484 (0.029)\nFAI_124   0.502 (0.029)\nFAI_5     0.323 (0.037)\nFAI_81    0.386 (0.034)\nFAI_83    0.404 (0.032)\n\n$H\nScale H      se \n  0.428 (0.026) \n\n$Hij\n        FAI_49  se      FAI_106 se      FAI_60  se      FAI_124 se      FAI_5  \nFAI_49                   0.561  (0.057)  0.608  (0.054)  0.601  (0.048)  0.212 \nFAI_106  0.561  (0.057)                  0.617  (0.042)  0.722  (0.039)  0.317 \nFAI_60   0.608  (0.054)  0.617  (0.042)                  0.688  (0.037)  0.280 \nFAI_124  0.601  (0.048)  0.722  (0.039)  0.688  (0.037)                  0.288 \nFAI_5    0.212  (0.056)  0.317  (0.055)  0.280  (0.052)  0.288  (0.052)        \nFAI_81   0.254  (0.060)  0.354  (0.056)  0.324  (0.050)  0.381  (0.053)  0.413 \nFAI_83   0.223  (0.057)  0.381  (0.056)  0.430  (0.047)  0.387  (0.048)  0.409 \n        se      FAI_81  se      FAI_83  se     \nFAI_49  (0.056)  0.254  (0.060)  0.223  (0.057)\nFAI_106 (0.055)  0.354  (0.056)  0.381  (0.056)\nFAI_60  (0.052)  0.324  (0.050)  0.430  (0.047)\nFAI_124 (0.052)  0.381  (0.053)  0.387  (0.048)\nFAI_5            0.413  (0.049)  0.409  (0.048)\nFAI_81  (0.049)                  0.551  (0.041)\nFAI_83  (0.048)  0.551  (0.041)                \n\n$Hi\n        Item H  se     \nFAI_49    0.410 (0.039)\nFAI_106   0.493 (0.032)\nFAI_60    0.484 (0.029)\nFAI_124   0.502 (0.029)\nFAI_5     0.323 (0.037)\nFAI_81    0.386 (0.034)\nFAI_83    0.404 (0.032)\n\n$H\nScale H      se \n  0.428 (0.026) \n\n$covHij\n               [,1]          [,2]          [,3]          [,4]          [,5]\n [1,]  3.256522e-03  1.972567e-03  1.673569e-03  6.850269e-04  0.0009303758\n [2,]  1.972567e-03  2.872632e-03  1.379662e-03  5.619181e-04  0.0007468355\n [3,]  1.673569e-03  1.379662e-03  2.324218e-03  7.130034e-04  0.0009755748\n [4,]  6.850269e-04  5.619181e-04  7.130034e-04  3.132843e-03  0.0014577910\n [5,]  9.303758e-04  7.468355e-04  9.755748e-04  1.457791e-03  0.0035999264\n [6,]  1.176059e-03  1.372448e-03  1.121228e-03  1.001618e-03  0.0007258314\n [7,]  1.143505e-03  6.796210e-04  6.075535e-04  3.611057e-04  0.0005325189\n [8,]  9.205140e-04  4.625925e-04  3.783489e-04  2.484545e-04  0.0003248843\n [9,]  1.281966e-04  2.342448e-04  1.519768e-04  1.410060e-03  0.0006882034\n[10,]  4.801610e-05  3.000776e-04  1.972389e-04  6.442391e-04  0.0016398978\n[11,]  6.330714e-05  1.877564e-04  2.994296e-04  4.332023e-04  0.0002887708\n[12,]  7.230668e-04  6.943860e-04  7.140702e-04  2.950943e-04  0.0003590450\n[13,]  3.194478e-04  4.391644e-04  3.080059e-04  1.808926e-03  0.0006665414\n[14,]  3.310593e-04  2.500548e-04  2.591961e-04  5.841647e-04  0.0016242307\n[15,] -2.626538e-05  1.143234e-04  3.461109e-04  5.949934e-04  0.0002896028\n[16,]  1.885976e-04  2.599218e-04  3.321093e-04  1.573800e-03  0.0006905796\n[17,]  1.132431e-04  1.511628e-04  5.240330e-05  5.614512e-04  0.0016082483\n[18,]  2.425335e-04  4.564482e-04  2.708591e-04  4.202745e-04  0.0002271944\n[19,]  6.016676e-05 -5.677038e-08  6.439974e-05  4.080167e-04  0.0003000318\n[20,] -6.323510e-05  2.770648e-04  9.115472e-05  4.564269e-04 -0.0000318649\n[21,] -1.452389e-04  4.045822e-05 -1.410212e-04 -8.502939e-05  0.0001437967\n               [,6]          [,7]          [,8]         [,9]        [,10]\n [1,]  1.176059e-03  1.143505e-03  9.205140e-04 1.281966e-04 0.0000480161\n [2,]  1.372448e-03  6.796210e-04  4.625925e-04 2.342448e-04 0.0003000776\n [3,]  1.121228e-03  6.075535e-04  3.783489e-04 1.519768e-04 0.0001972389\n [4,]  1.001618e-03  3.611057e-04  2.484545e-04 1.410060e-03 0.0006442391\n [5,]  7.258314e-04  5.325189e-04  3.248843e-04 6.882034e-04 0.0016398978\n [6,]  3.235792e-03  3.934074e-04  4.943276e-04 4.792224e-04 0.0002711309\n [7,]  3.934074e-04  1.765623e-03  7.886910e-04 3.994963e-04 0.0003576934\n [8,]  4.943276e-04  7.886910e-04  1.518517e-03 3.437684e-04 0.0004289051\n [9,]  4.792224e-04  3.994963e-04  3.437684e-04 3.075515e-03 0.0011512604\n[10,]  2.711309e-04  3.576934e-04  4.289051e-04 1.151260e-03 0.0030881212\n[11,]  1.580928e-03  7.872979e-04  6.366403e-04 1.439675e-03 0.0011535909\n[12,]  5.881401e-04  7.514867e-04  4.972229e-04 5.983421e-05 0.0002224072\n[13,]  7.426056e-04  4.490709e-04  9.903078e-05 1.401373e-03 0.0005654204\n[14,]  3.670699e-04  4.022907e-04  2.546547e-04 5.617317e-04 0.0015767924\n[15,]  1.394333e-03  3.075009e-04  2.545034e-04 4.954782e-04 0.0003710759\n[16,]  5.291520e-04  1.750076e-04  4.106679e-04 1.618941e-03 0.0007001615\n[17,]  2.446268e-04  2.201295e-04  2.059815e-04 6.070280e-04 0.0017308946\n[18,]  1.440363e-03  3.706182e-04  4.282926e-04 4.721104e-04 0.0005205689\n[19,] -2.408574e-05  7.725707e-05  8.859144e-05 7.086189e-04 0.0004992864\n[20,]  6.059513e-04  6.986203e-05 -5.566603e-05 8.917753e-04 0.0004276745\n[21,]  2.480873e-04 -1.299639e-05 -3.392847e-05 3.442378e-04 0.0005256114\n             [,11]         [,12]        [,13]        [,14]         [,15]\n [1,] 6.330714e-05  7.230668e-04 3.194478e-04 0.0003310593 -2.626538e-05\n [2,] 1.877564e-04  6.943860e-04 4.391644e-04 0.0002500548  1.143234e-04\n [3,] 2.994296e-04  7.140702e-04 3.080059e-04 0.0002591961  3.461109e-04\n [4,] 4.332023e-04  2.950943e-04 1.808926e-03 0.0005841647  5.949934e-04\n [5,] 2.887708e-04  3.590450e-04 6.665414e-04 0.0016242307  2.896028e-04\n [6,] 1.580928e-03  5.881401e-04 7.426056e-04 0.0003670699  1.394333e-03\n [7,] 7.872979e-04  7.514867e-04 4.490709e-04 0.0004022907  3.075009e-04\n [8,] 6.366403e-04  4.972229e-04 9.903078e-05 0.0002546547  2.545034e-04\n [9,] 1.439675e-03  5.983421e-05 1.401373e-03 0.0005617317  4.954782e-04\n[10,] 1.153591e-03  2.224072e-04 5.654204e-04 0.0015767924  3.710759e-04\n[11,] 3.100653e-03  3.797460e-04 5.558660e-04 0.0004499206  1.388343e-03\n[12,] 3.797460e-04  1.353425e-03 3.361125e-04 0.0003599263  3.711701e-04\n[13,] 5.558660e-04  3.361125e-04 2.679508e-03 0.0010149171  9.133291e-04\n[14,] 4.499206e-04  3.599263e-04 1.014917e-03 0.0025074061  7.814797e-04\n[15,] 1.388343e-03  3.711701e-04 9.133291e-04 0.0007814797  2.239613e-03\n[16,] 5.671980e-04  3.549025e-04 1.809069e-03 0.0006726168  7.152364e-04\n[17,] 5.230923e-04  8.662901e-05 5.994210e-04 0.0016280518  4.484829e-04\n[18,] 1.453794e-03  3.694086e-04 6.818643e-04 0.0005001348  1.390561e-03\n[19,] 3.721344e-04  8.121422e-05 6.065452e-04 0.0004767595  4.240890e-05\n[20,] 6.949490e-04  2.048871e-04 8.608315e-04 0.0001955560  4.525888e-04\n[21,] 3.341346e-04 -7.180751e-05 6.989798e-05 0.0004952616  1.117222e-04\n             [,16]        [,17]        [,18]         [,19]         [,20]\n [1,] 1.885976e-04 1.132431e-04 0.0002425335  6.016676e-05 -6.323510e-05\n [2,] 2.599218e-04 1.511628e-04 0.0004564482 -5.677038e-08  2.770648e-04\n [3,] 3.321093e-04 5.240330e-05 0.0002708591  6.439974e-05  9.115472e-05\n [4,] 1.573800e-03 5.614512e-04 0.0004202745  4.080167e-04  4.564269e-04\n [5,] 6.905796e-04 1.608248e-03 0.0002271944  3.000318e-04 -3.186490e-05\n [6,] 5.291520e-04 2.446268e-04 0.0014403628 -2.408574e-05  6.059513e-04\n [7,] 1.750076e-04 2.201295e-04 0.0003706182  7.725707e-05  6.986203e-05\n [8,] 4.106679e-04 2.059815e-04 0.0004282926  8.859144e-05 -5.566603e-05\n [9,] 1.618941e-03 6.070280e-04 0.0004721104  7.086189e-04  8.917753e-04\n[10,] 7.001615e-04 1.730895e-03 0.0005205689  4.992864e-04  4.276745e-04\n[11,] 5.671980e-04 5.230923e-04 0.0014537943  3.721344e-04  6.949490e-04\n[12,] 3.549025e-04 8.662901e-05 0.0003694086  8.121422e-05  2.048871e-04\n[13,] 1.809069e-03 5.994210e-04 0.0006818643  6.065452e-04  8.608315e-04\n[14,] 6.726168e-04 1.628052e-03 0.0005001348  4.767595e-04  1.955560e-04\n[15,] 7.152364e-04 4.484829e-04 0.0013905614  4.240890e-05  4.525888e-04\n[16,] 2.714269e-03 9.497205e-04 0.0009504702  8.252786e-04  9.374942e-04\n[17,] 9.497205e-04 2.781060e-03 0.0011345975  3.571314e-04  1.293712e-04\n[18,] 9.504702e-04 1.134597e-03 0.0023072463  5.416910e-05  4.234757e-04\n[19,] 8.252786e-04 3.571314e-04 0.0000541691  2.416128e-03  1.043338e-03\n[20,] 9.374942e-04 1.293712e-04 0.0004234757  1.043338e-03  2.288321e-03\n[21,] 8.680063e-05 4.749021e-04 0.0002621942  4.096861e-04  5.441687e-04\n              [,21]\n [1,] -1.452389e-04\n [2,]  4.045822e-05\n [3,] -1.410212e-04\n [4,] -8.502939e-05\n [5,]  1.437967e-04\n [6,]  2.480873e-04\n [7,] -1.299639e-05\n [8,] -3.392847e-05\n [9,]  3.442378e-04\n[10,]  5.256114e-04\n[11,]  3.341346e-04\n[12,] -7.180751e-05\n[13,]  6.989798e-05\n[14,]  4.952616e-04\n[15,]  1.117222e-04\n[16,]  8.680063e-05\n[17,]  4.749021e-04\n[18,]  2.621942e-04\n[19,]  4.096861e-04\n[20,]  5.441687e-04\n[21,]  1.663587e-03\n\n$covHi\n             [,1]         [,2]         [,3]         [,4]         [,5]\n[1,] 0.0014980127 0.0007465455 0.0007793789 0.0007134071 0.0005453263\n[2,] 0.0007465455 0.0010331381 0.0006387837 0.0006398164 0.0005589013\n[3,] 0.0007793789 0.0006387837 0.0008679826 0.0006400363 0.0005827344\n[4,] 0.0007134071 0.0006398164 0.0006400363 0.0008379947 0.0005613310\n[5,] 0.0005453263 0.0005589013 0.0005827344 0.0005613310 0.0013482139\n[6,] 0.0004596982 0.0005289703 0.0005019538 0.0005248655 0.0006078101\n[7,] 0.0004856933 0.0005415021 0.0005393324 0.0005410202 0.0005874255\n             [,6]         [,7]\n[1,] 0.0004596982 0.0004856933\n[2,] 0.0005289703 0.0005415021\n[3,] 0.0005019538 0.0005393324\n[4,] 0.0005248655 0.0005410202\n[5,] 0.0006078101 0.0005874255\n[6,] 0.0011488908 0.0004461349\n[7,] 0.0004461349 0.0010515910\n\n$covH\n             [,1]\n[1,] 0.0006624019\n\n\n\nPossiamo interpretare i coefficienti di scalabilità nel modo seguente.\n\nCoefficiente di Scalabilità tra Coppie di Item (Hij): Per ogni coppia di item (i, j), il coefficiente \\(H_{ij}\\) valuta l’efficacia con cui questi due item riflettono la variabile latente. Un coefficiente \\(H_{ij}\\) positivo per coppie di item appartenenti alla stessa scala di Mokken indica che questi item sono coerenti e misurano efficacemente la stessa variabile latente. Matematicamente, \\(H_{ij}\\) è definito per ogni coppia di item i e j, dove i, j = 1, …, J.\nCoeffiente di Scalabilità dell’Item (Hj): Il coefficiente \\(H_{j}\\) di un singolo item è analogo ai parametri di discriminazione nei modelli IRT parametrici. Esprime l’efficacia con cui un item distingue tra individui a diversi livelli della variabile latente. Per essere considerato efficace, \\(H_{j}\\) dovrebbe superare un certo limite inferiore, generalmente stabilito a c &gt; 0.3.\nCoefficiente di Scalabilità del Test (H): H rappresenta la scalabilità complessiva dell’intero insieme di item. L’interpretazione di H segue le seguenti soglie euristiche:\n\nDebole: se 0.3 ≤ H &lt; 0.4.\nModerato: se 0.4 ≤ H &lt; 0.5.\nForte: se H &gt; 0.5.\n\nQuesti valori indicano la forza con cui l’insieme di item misura la variabile latente. I coefficienti di scalabilità degli item forniscono indicazioni sulla discriminazione degli item e sulla loro aderenza al modello di omogeneità monotona. Item con bassa discriminazione non contribuiscono a un ordinamento affidabile degli esaminandi e dovrebbero essere scartati.\n\nSecondo Sijtsma e Molenaar (2002), le assunzioni di unidimensionalità, indipendenza locale e monotonicità implicano le seguenti restrizioni sui coefficienti di scalabilità: - 0 ≤ \\(H_{ij}\\) ≤ 1, per tutte le coppie di item i ≠ j. - 0 ≤ \\(H_{j}\\) ≤ 1, per tutti gli item j. - 0 ≤ \\(H\\) ≤ 1, per l’intero insieme di item.\nI coefficienti di scalabilità sono fondamentali per valutare quanto efficacemente un insieme di item lavori insieme per misurare una variabile latente. Valori alti di \\(H\\) suggeriscono che l’insieme di item è fortemente correlato e misura in modo affidabile la variabile latente, garantendo che l’analisi con la MSA sia valida e affidabile.\nÈ possibile ottenere il numero di valori negativi \\(H_{ij}\\) per ciascun item usando il codice seguente.\n\nscal_coef &lt;- coefH(good_items, se = FALSE)$Hij\napply(scal_coef, 1, function(x) sum(x &lt; 0)) |&gt; print()\n\n$Hij\n           FAI_49   FAI_106    FAI_60   FAI_124     FAI_5    FAI_81    FAI_83\nFAI_49  1.0000000 0.5613569 0.6075009 0.6005396 0.2118286 0.2539023 0.2229846\nFAI_106 0.5613569 1.0000000 0.6172766 0.7223965 0.3168352 0.3539449 0.3812326\nFAI_60  0.6075009 0.6172766 1.0000000 0.6879216 0.2795178 0.3243635 0.4303254\nFAI_124 0.6005396 0.7223965 0.6879216 1.0000000 0.2879511 0.3813744 0.3866023\nFAI_5   0.2118286 0.3168352 0.2795178 0.2879511 1.0000000 0.4132827 0.4085199\nFAI_81  0.2539023 0.3539449 0.3243635 0.3813744 0.4132827 1.0000000 0.5506000\nFAI_83  0.2229846 0.3812326 0.4303254 0.3866023 0.4085199 0.5506000 1.0000000\n\n$Hi\n   FAI_49   FAI_106    FAI_60   FAI_124     FAI_5    FAI_81    FAI_83 \n0.4096841 0.4929127 0.4838063 0.5019022 0.3231299 0.3862103 0.4040312 \n\n$H\n[1] 0.4279251\n\n FAI_49 FAI_106  FAI_60 FAI_124   FAI_5  FAI_81  FAI_83 \n      0       0       0       0       0       0       0 \n\n\nL’istruzione seguente esegue la procedura frequentista del test di ipotesi, con l’ipotesi alternativa che i coefficienti di scalabilità sono maggiori di zero. Il test è unidirezionale, dunque il valore soglia della statistica \\(z\\) è 1.65.\n\ncoefZ(good_items) |&gt; print()\n\n$Zij\n           FAI_49   FAI_106    FAI_60   FAI_124    FAI_5    FAI_81    FAI_83\nFAI_49   0.000000 11.122254 11.766708 11.519386 4.077595  4.303706  4.213676\nFAI_106 11.122254  0.000000 12.328013 14.049032 6.189922  6.313023  7.368202\nFAI_60  11.766708 12.328013  0.000000 14.039308 5.741312  6.231554  8.542324\nFAI_124 11.519386 14.049032 14.039308  0.000000 5.889676  7.265907  7.779304\nFAI_5    4.077595  6.189922  5.741312  5.889676 0.000000  8.141997  8.214378\nFAI_81   4.303706  6.313023  6.231554  7.265907 8.141997  0.000000 10.794877\nFAI_83   4.213676  7.368202  8.542324  7.779304 8.214378 10.794877  0.000000\n\n$Zi\n  FAI_49  FAI_106   FAI_60  FAI_124    FAI_5   FAI_81   FAI_83 \n18.87934 23.25525 23.49005 24.24297 15.73678 17.72590 19.40083 \n\n$Z\n[1] 37.97973\n\n\n\nNel caso presente, il test indica che i coefficienti di scalabilità di tutti gli item, sia considerati singolarmente sia considerati a coppie, sono maggiori di zero. Lo stesso si può dire per il coefficiente di scalabilità della scala nel suo complesso.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_applications.html#monotonicità",
    "href": "chapters/mokken/02_applications.html#monotonicità",
    "title": "62  Applicazione Pratica",
    "section": "62.6 Monotonicità",
    "text": "62.6 Monotonicità\nCome spiegato in precedenza, la monotonicità è un’importante assunzione della MSA. La probabilità di una risposta corretta dovrebbe aumentare con θ. La monotonicità può essere esaminata con il seguente codice:\n\nmonoton &lt;- check.monotonicity(good_items)\nsummary(monoton) |&gt; print()\n\n        ItemH #ac #vi #vi/#ac maxvi sum sum/#ac zmax #zsig crit\nFAI_49   0.41  20   0       0     0   0       0    0     0    0\nFAI_106  0.49  12   0       0     0   0       0    0     0    0\nFAI_60   0.48  18   0       0     0   0       0    0     0    0\nFAI_124  0.50  21   0       0     0   0       0    0     0    0\nFAI_5    0.32  24   0       0     0   0       0    0     0    0\nFAI_81   0.39  24   0       0     0   0       0    0     0    0\nFAI_83   0.40  21   0       0     0   0       0    0     0    0\n\n\nOgni riga dell’output rappresenta un item (ad esempio, FAI_49, FAI_106, ecc.). La spiegazione delle colonne è la seguente:\n\nItemH: Il coefficiente H per ogni item, che misura l’omogeneità dell’item. Un valore più alto indica una maggiore omogeneità. Nelle scale di Mokken, si cercano generalmente valori superiori a 0.3.\n#ac (Active pairs): Il numero di coppie attive, ovvero coppie di risposte che contribuiscono alla stima dell’H.\n#vi (Violations): Il numero di violazioni della monotonicità. La monotonicità implica che, man mano che aumenta il punteggio totale del test, la probabilità di una risposta positiva all’item non diminuisce.\n#vi/#ac: Il rapporto tra il numero di violazioni e il numero di coppie attive.\nmaxvi (Maximum violation): La massima violazione osservata.\nsum (Sum of violation size): La somma delle dimensioni delle violazioni.\nsum/#ac: Il rapporto tra la somma delle dimensioni delle violazioni e il numero di coppie attive.\nzmax: Il valore massimo della statistica Z per le violazioni.\n#zsig (Number of significant Z): Numero di statistiche Z significative.\ncrit: Un criterio per giudicare se le violazioni sono problematiche.\n\nNel caso presente, sembra che non ci siano violazioni della monotonicità per nessuno degli item elencati. Questo significa che per questi item, all’aumentare del punteggio totale, non si osserva una diminuzione della probabilità di una risposta positiva, mantenendo quindi una buona coerenza interna e validità per la scala.\nUn grafico della monotonicità per una coppia di item si ottiene nel modo seguente.\n\nplot(check.monotonicity(good_items), item = c(1, 2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa figura di questo esempio illustra i grafici di monotonicità per gli Item 49 e 106. Il grafico è diviso in due pannelli. Il pannello sul lato sinistro mostra le Funzioni di Risposta del Passaggio dell’Item (ISRFs), mentre il pannello sul lato destro mostra la Funzione di Risposta all’Item (IRF) complessiva per ciascun item. Il grafico evidenzia che sia l’IRF sia le ISRFs sono sempre non decrescenti per l’item 49; per l’item 106, invece, si osserva una piccola violazione della monotonicità.\nSe esaminiamo tutti gli item dell’area relativa alle caratteristiche del bambino (non solo quelli selezionati dalla procedura AISP) notiamo come, per alcuni item, si osserva un numero di violazioni maggiore di zero.\n\nmonoton2 &lt;- check.monotonicity(subscale_data)\nsummary(monoton2) |&gt; print()\n\n        ItemH #ac #vi #vi/#ac maxvi  sum sum/#ac zmax #zsig crit\nFAI_49   0.25  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_106  0.30  15   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_60   0.30  18   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_124  0.32  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_86   0.16  21   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_47   0.23  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_121  0.18  23   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_167  0.18  24   2    0.08  0.04 0.07  0.0031 1.07     0   31\nFAI_99   0.12  24   1    0.04  0.05 0.05  0.0023 1.58     0   31\nFAI_63   0.23  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_168  0.23  18   1    0.06  0.06 0.06  0.0032 0.73     0   25\nFAI_5    0.23  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_132  0.21  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_85   0.17  19   1    0.05  0.07 0.07  0.0035 1.08     0   30\nFAI_81   0.24  24   1    0.04  0.04 0.04  0.0015 0.52     0   17\nFAI_83   0.24  24   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_57   0.21  15   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_91   0.18  21   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_135  0.15  21   1    0.05  0.03 0.03  0.0016 0.39     0   22\nFAI_1    0.10  21   0    0.00  0.00 0.00  0.0000 0.00     0    0",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_applications.html#ordinamento-invariante-degli-item",
    "href": "chapters/mokken/02_applications.html#ordinamento-invariante-degli-item",
    "title": "62  Applicazione Pratica",
    "section": "62.7 Ordinamento Invariante degli Item",
    "text": "62.7 Ordinamento Invariante degli Item\nIl passo successivo nella MSA è indagare l’ordinamento invariante degli item (IIO) o la non intersezione delle Funzioni di Risposta all’Item (IRFs). È fondamentale determinare se l’ordine degli item sia lo stesso per tutti i rispondenti con diversi livelli del tratto. Esistono diversi metodi per esaminare l’IIO. Il metodo predefinito nel pacchetto R mokken è l’IIO manifesto o MIIO (Manifest IIO) (Ligtvoet, Van der Ark, Te Marvelde & Sijtsma, 2010). Per esaminare l’ordinamento invariante degli item, eseguiamo il seguente codice.\n\niio &lt;- check.iio(good_items)\nsummary(iio) |&gt; print()\n\n$method\n[1] \"MIIO\"\n\n$item.summary\n        ItemH #ac #vi #vi/#ac maxvi  sum sum/#ac tmax #tsig crit\nFAI_81   0.39  18   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_83   0.40  18   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_124  0.50  18   1    0.06  0.31 0.31  0.0171 3.50     1   83\nFAI_5    0.32  18   2    0.11  0.42 0.73  0.0407 3.50     2  145\nFAI_60   0.48  18   1    0.06  0.42 0.42  0.0236 2.49     1   97\nFAI_106  0.49  18   0    0.00  0.00 0.00  0.0000 0.00     0    0\nFAI_49   0.41  18   0    0.00  0.00 0.00  0.0000 0.00     0    0\n\n$backward.selection\n        step 1 step 2\nFAI_81       0      0\nFAI_83       0      0\nFAI_124      1      0\nFAI_5        2     NA\nFAI_60       1      0\nFAI_106      0      0\nFAI_49       0      0\n\n$HT\n[1] 0.3358101\n\n\n\nL’output include due tabelle principali. La prima tabella contiene le seguenti colonne. La prima colonna, “ItemH”, mostra il coefficiente di scalabilità Hi per ciascun item, ‘#ac’ indica il numero totale di coppie attive, ‘#vi’ segnala il numero totale di violazioni, ‘#vi/#ac’ mostra il numero medio di violazioni per coppia attiva, ‘maxvi’ indica la massima violazione, ‘sum’ rappresenta la somma di tutte le violazioni, ‘sum/#ac’ mostra la media delle violazioni per coppia attiva, ‘tmax’ indica la statistica di test massima, ‘#tsig’ il numero di violazioni significative, e il valore ‘crit’ è una somma ponderata di altri elementi come ‘itemH’, ‘#ac’, ecc. Valori elevati di ‘crit’ indicano item di scarsa qualità (il valore 0 è perfetto, valori più alti sono peggiori).\nL’output precedente mostra che l’Item FAI_5 ha 2 violazioni. In altre parole, la IRF per questo item si interseca con la IRF di altri due item, ed entrambe queste violazioni sono significative (#tsig per questo item è 2). Poiché ha il numero più alto di violazioni, è un buon candidato per essere rimosso dal test. Rimuovere questo item risolve l’intersezione degli altri item con questo item.\nPer esaminare graficamente queste intersezione, usiamo il codice seguente.\n\nplot(iio)",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_applications.html#non-intersezione-delle-funzioni-di-risposta-degli-step-degli-item",
    "href": "chapters/mokken/02_applications.html#non-intersezione-delle-funzioni-di-risposta-degli-step-degli-item",
    "title": "62  Applicazione Pratica",
    "section": "62.8 Non intersezione delle funzioni di risposta degli step degli item",
    "text": "62.8 Non intersezione delle funzioni di risposta degli step degli item\nPer indagare sulla non intersezione delle funzioni di risposta degli step degli item si possono impiegare le matrici P++ e P– (Molenaar & Sijtsma, 2000).\n\npmatrix &lt;- check.pmatrix(good_items)\nsummary(pmatrix) |&gt; print()\n\n        ItemH  #ac #vi #vi/#ac maxvi  sum sum/#ac zmax #zsig crit\nFAI_49   0.41 1920  17    0.01  0.05 0.65   3e-04 2.93    15   67\nFAI_106  0.49 1920  10    0.01  0.06 0.39   2e-04 4.21    10   59\nFAI_60   0.48 1920  23    0.01  0.06 0.94   5e-04 3.88    22   81\nFAI_124  0.50 1920  20    0.01  0.06 0.84   4e-04 4.60    20   80\nFAI_5    0.32 1920  25    0.01  0.06 1.07   6e-04 3.88    24   92\nFAI_81   0.39 1920  28    0.01  0.05 1.09   6e-04 3.60    27   89\nFAI_83   0.40 1920  17    0.01  0.06 0.68   4e-04 4.60    16   78\n\n\nCome spiegato nelle sezioni precedenti, la colonna ‘#vi’ indica il numero di violazioni delle ISRF. La colonna ‘#zsig’ mostra il numero di violazioni statisticamente significative. Le violazioni possono anche essere controllate graficamente utilizzando il seguente codice:\n\nplot(check.pmatrix(good_items), item = 5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa non intersezione delle ISRF può essere esaminata anche con il metodo del punteggio residuo.\n\nrestscore &lt;- check.restscore(good_items) \nsummary(restscore) |&gt; print()\n\n        ItemH #ac #vi #vi/#ac maxvi  sum sum/#ac zmax #zsig crit\nFAI_49   0.41 288   5    0.02  0.08 0.23  0.0008 1.19     0   18\nFAI_106  0.49 288   7    0.02  0.07 0.33  0.0011 2.16     2   34\nFAI_60   0.48 288   6    0.02  0.14 0.47  0.0016 2.19     4   50\nFAI_124  0.50 288   9    0.03  0.15 0.56  0.0019 2.78     1   44\nFAI_5    0.32 288  14    0.05  0.15 1.01  0.0035 2.78     5   74\nFAI_81   0.39 288   8    0.03  0.10 0.44  0.0015 1.70     1   37\nFAI_83   0.40 288   9    0.03  0.07 0.40  0.0014 2.07     1   36\n\n\nI risultati del metodo del punteggio residuo sono diversi da quelli delle matrici P++ e P–. L’output viene interpretato come abbiamo fatto in precedenza. La colonna ‘#vi’ indica il numero di violazioni delle ISRF. La colonna ‘#zsig’ mostra il numero di violazioni statisticamente significative. Questa colonna mostra che il numero di violazioni statisticamente significative è molto inferiore rispetto a quelle riportate dalle matrici P++ e P–. La violazione della non intersezione può essere esaminata graficamente nel modo seguente. Per esempio, consideriamo gli item FAI_49 e FAI_5.\n\nplot(restscore, item.pairs = c(4, 5))",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_applications.html#affidabilità",
    "href": "chapters/mokken/02_applications.html#affidabilità",
    "title": "62  Applicazione Pratica",
    "section": "62.9 Affidabilità",
    "text": "62.9 Affidabilità\nIl pacchetto Mokken calcola quattro diversi coefficienti di affidabilità: l’affidabilità della scala Mokken (MS) ρ, (Mokken, 1971), Lambda-2 (Guttman, 1945), l’alpha di Cronbach (Cronbach, 1951), e il coefficiente di affidabilità della classe latente (LCRC, van der Ark, van der Palm, & Sijtsma, 2011).\n\nMS (Molenaar-Sijtsma Method): Questo indice è basato sul metodo Molenaar-Sijtsma di stima dell’affidabilità per scale non parametriche, come quelle analizzate con l’analisi Mokken. Questo metodo considera la varianza tra gli item e la varianza totale per stimare l’affidabilità.\nAlpha (Cronbach’s Alpha): L’alpha di Cronbach è forse il più noto indice di affidabilità, utilizzato per valutare la consistenza interna degli item di un test. Misura fino a che punto gli item di un test sono correlati tra loro.\nLambda-2: Un altro indice di affidabilità, simile all’alpha di Cronbach, ma talvolta considerato più robusto poiché tiene conto delle correlazioni medie tra gli item.\nLCRC (Latent Class Reliability Coefficient): Questo è un indice di affidabilità che tiene conto dell’approccio delle classi latenti. È particolarmente utile quando gli item possono essere raggruppati in sottoscale che riflettono diversi costrutti o dimensioni.\n\nPer ottenere queste stime dell’affidabilità è possibile eseguire il seguente codice.\n\ncheck.reliability(good_items, LCRC = TRUE)\n\n\n    $MS\n        0.819110476790128\n    $alpha\n        0.81675073603741\n    $lambda.2\n        0.824148383038108\n    $LCRC\n        0.843559865692261\n\n\n\nIn generale, tutti i valori ottenuti indicano che la scala in considerazione ha un’alta affidabilità. Questo significa che è probabile che produca risultati coerenti nel tempo e che gli item che la compongono siano correlati tra loro in modo significativo, contribuendo tutti a misurare lo stesso costrutto o costrutti correlati.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_applications.html#identificazione-degli-outlier",
    "href": "chapters/mokken/02_applications.html#identificazione-degli-outlier",
    "title": "62  Applicazione Pratica",
    "section": "62.10 Identificazione degli Outlier",
    "text": "62.10 Identificazione degli Outlier\nGli outlier sono persone con modelli di risposta aberranti o molti errori di Guttman. Sijtsma e van der Ark (2017) raccomandano di rimuovere gli outlier e di rieseguire l’analisi senza di essi. Se c’è una differenza notevole nei risultati, allora la rimozione degli outlier è giustificata. Per ottenere il numero di errori di Guttman per ogni rispondente eseguiamo il seguente codice:\n\ngutt &lt;- check.errors(good_items)\nprint(gutt)\n\n$Gplus\n  [1]  2  6  0  7  2  4 23 30  2  7 15  0  1  9  2  0  5 24  3  3  0  0 11 51  0\n [26]  4  2 19  9  2 60  2  9 35  2 17 39  2  4  3  3 12  3  0 10  2  0  0 37  4\n [51] 68  0  2 31  2  0 12 36  1 70 12  1 28 27 25  3  4 10  9 16 28 22 81 26  1\n [76]  0 65 28  0 25  3 59 60 40  0  5  0  0  1  0  8  3  8  6 27 12 22 16 12  4\n[101] 12  9  0  0 20 25 25  6  4  6  0 47 20 18  4  7  6 33 14 25 12  4  0  1 65\n[126] 17 83 26 65  5  5 30  2  9 42  7 13  8 16  3 41  0 12 21  4  8 26 22  3 46\n[151]  7 14 38 12 12  1  6  4 32 11  2  6 17  1  8  8 27  8  3  5 58  2 62  3  0\n[176] 12 10 45 21  0  2 10 31 19 46 11 21  2 25  9 17  2  4 20 21 49 18 19 14 16\n[201] 44  5  7 14 41 24 44  0 18  9 31 12  9 46  8  6 11 14 23 41 24  1  7 39 23\n[226]  0 23  3  6  2  2  4  0  0  8 24 38  0 16 62  2  2 21  7 25  9  0  0  8 14\n[251] 10  4  1  0  2 18  0 21 19  0 12  0 41  8  7 16 25 20  3 41  3  0  6 18  0\n[276]  4  2  4  4 12  5 13  6  9  7  6  0  6  7  2  0  0 13 26  8 41 30  2  0  6\n[301]  6  0  5  2  0  0 39 48  0 43  8 15 10  0  4 25 16  9  0  5  2  2  8  2  8\n[326]  7  1  0 53 32  9  3  0  4 13  3  6 35 30  2 34  8 10 10  5 23 22 24 12  3\n[351]  0  9 16 21 39 27 47 19 28 32  7  2 16  0  3 26 34  6  4 11  7 25  1 15  6\n[376] 10 38 17  0  6  3 38  4  0 52  5  2  3  0  0 14  0 27 22 32  8 32 14  2 31\n[401] 14 29 52 12 11 32  2  3 17  2  0  5 62 86 34  5 24 39  7 10 31  0  0 25 10\n[426] 19 31 13  7 31 36  8  0  0 28  6 23 15 52  3  1  7  4 33 24 11 24  0  2  3\n[451]  0  1  0\n\n$UGplus\n$UGplus$U1Gplus\n[1] 54.5\n\n$UGplus$U2Gplus\n[1] 354.5259\n\n\n\n\nIl vettore $Gplus mostra il numero di errori di Guttman G+ per ogni persona. I valori 54.5 e 354.5259 sono le Tukey fences per il rilevamento degli outlier. Essi segnalano i casi in cui il numero di errori di Guttman G+ è oltre il Tukey fence della distribuzione di G+.\nIl primo valore, 54.5, è il limite per il numero di errori di Guttman se la distribuzione è approssimativamente normale. I rispondenti con un numero di errori di Guttman superiore al limite possono essere considerati sospetti. Il valore 354.5259 è un limite di soglia per il numero di errori di Guttman se la distribuzione è asimmetrica.\n\nhist(gutt$Gplus)\n\n\n\n\n\n\n\n\nPer il caso presente, l’istogramma del numero di errori di Gutmann mostra che la loro distribuzione è asimmetrica positiva. Pertanto, possiamo scegliere il secondo limite soglia. Per trovare le persone con valori G+ sopra il Tucky fence superiore, eseguiamo il seguente codice.\n\nerr &lt;- gutt$Gplus \nwhich(err &gt; 354.5259)\n\n\n\n\nI risultati indicano che, in base a questo secondo criterio, non ci sono modelli di risposta sospetti nei dati.\nSe questa procedura producesse invece come risultato l’individuazione di alcuni outlier, potremmo rimuoverli dai dati nel modo seguente:\ngood_items_clean &lt;- good_items[-which(err &gt; 354.5259), ]",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_applications.html#considerazioni-conclusive",
    "href": "chapters/mokken/02_applications.html#considerazioni-conclusive",
    "title": "62  Applicazione Pratica",
    "section": "62.11 Considerazioni conclusive",
    "text": "62.11 Considerazioni conclusive\nIn questo capitolo, abbiamo esplorato un approccio pratico all’analisi di un set di dati reali, utilizzando il caso di studio sull’indagine della capacità di coping delle famiglie di fronte alla diagnosi di una grave malattia in un figlio, condotta dai ricercatori del Meyer. Attraverso una serie di passaggi metodici e analitici rigorosi, siamo riusciti a trasformare un complesso insieme di dati in informazioni comprensibili e significative.\nAbbiamo iniziato importando i dati e conducendo un’accurata pulizia per rimuovere gli item con eccessiva asimmetria e curtosi. Successivamente, abbiamo implementato la Procedura di Selezione Automatica degli Item (AISP) nell’ambito dell’Analisi delle Scale Mokken (MSA) per identificare e selezionare scale omogenee e coerenti.\nUna volta stabilite le scale, abbiamo approfondito le caratteristiche degli item, esaminando le loro statistiche descrittive e la loro distribuzione. Abbiamo anche valutato la loro affidabilità attraverso vari coefficienti, tra cui l’Alpha di Cronbach e il Coefficiente di Affidabilità della Classe Latente (LCRC), rilevando un’alta coerenza interna.\nUlteriori analisi hanno incluso la verifica della monotonicità degli item e dell’ordinamento invariante degli item (IIO), fondamentali per garantire che la scala rispettasse i principi teorici sottostanti la MSA. Infine, abbiamo esaminato la presenza di outlier, utilizzando i Tukey fences per identificare e gestire i modelli di risposta aberranti.\nIn conclusione, questo capitolo dimostra come l’applicazione metodica e sistematica delle tecniche di analisi statistica possa fornire intuizioni preziose e comprensibili da un set di dati complesso. Ciò non solo rafforza la validità e l’affidabilità della ricerca, ma fornisce anche una base solida per ulteriori indagini e interpretazioni.",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/mokken/02_applications.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/mokken/02_applications.html#informazioni-sullambiente-di-sviluppo",
    "title": "62  Applicazione Pratica",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] mokken_3.1.2         poLCA_1.6.0.1        MASS_7.3-61         \n [4] scatterplot3d_0.3-44 mirt_1.42            lattice_0.22-6      \n [7] TAM_4.2-21           CDM_8.2-6            mvtnorm_1.3-1       \n[10] ggokabeito_0.1.0     viridis_0.6.5        viridisLite_0.4.2   \n[13] ggpubr_0.6.0         ggExtra_0.10.1       bayesplot_1.11.1    \n[16] gridExtra_2.3        patchwork_1.3.0      semTools_0.5-6      \n[19] semPlot_1.1.6        lavaan_0.6-18        psych_2.4.6.26      \n[22] scales_1.3.0         markdown_1.13        knitr_1.48          \n[25] lubridate_1.9.3      forcats_1.0.0        stringr_1.5.1       \n[28] dplyr_1.1.4          purrr_1.0.2          readr_2.1.5         \n[31] tidyr_1.3.1          tibble_3.2.1         ggplot2_3.5.1       \n[34] tidyverse_2.0.0      here_1.0.1          \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.1        later_1.3.2          pbdZMQ_0.3-13       \n  [4] R.oo_1.26.0          XML_3.99-0.17        rpart_4.1.23        \n  [7] lifecycle_1.0.4      rstatix_0.7.2        rprojroot_2.0.4     \n [10] globals_0.16.3       rockchalk_1.8.157    backports_1.5.0     \n [13] magrittr_2.0.3       openxlsx_4.2.7.1     Hmisc_5.1-3         \n [16] rmarkdown_2.28       httpuv_1.6.15        qgraph_1.9.8        \n [19] zip_2.3.1            sessioninfo_1.2.2    pbapply_1.7-2       \n [22] minqa_1.2.8          audio_0.1-11         multcomp_1.4-26     \n [25] abind_1.4-8          quadprog_1.5-8       R.utils_2.12.3      \n [28] nnet_7.3-19          TH.data_1.1-2        sandwich_3.1-1      \n [31] listenv_0.9.1        testthat_3.2.1.1     RPushbullet_0.3.4   \n [34] vegan_2.6-8          arm_1.14-4           parallelly_1.38.0   \n [37] permute_0.9-7        codetools_0.2-20     tidyselect_1.2.1    \n [40] farver_2.1.2         lme4_1.1-35.5        base64enc_0.1-3     \n [43] jsonlite_1.8.9       polycor_0.8-1        progressr_0.14.0    \n [46] Formula_1.2-5        survival_3.7-0       emmeans_1.10.4      \n [49] tools_4.4.1          snow_0.4-4           Rcpp_1.0.13         \n [52] glue_1.7.0           mnormt_2.1.1         xfun_0.47           \n [55] mgcv_1.9-1           admisc_0.36          IRdisplay_1.1       \n [58] withr_3.0.1          beepr_2.0            fastmap_1.2.0       \n [61] boot_1.3-31          fansi_1.0.6          digest_0.6.37       \n [64] mi_1.1               timechange_0.3.0     R6_2.5.1            \n [67] mime_0.12            estimability_1.5.1   colorspace_2.1-1    \n [70] gtools_3.9.5         jpeg_0.1-10          R.methodsS3_1.8.2   \n [73] utf8_1.2.4           generics_0.1.3       data.table_1.16.0   \n [76] corpcor_1.6.10       SimDesign_2.17.1     htmlwidgets_1.6.4   \n [79] pkgconfig_2.0.3      sem_3.1-16           gtable_0.3.5        \n [82] brio_1.1.5           htmltools_0.5.8.1    carData_3.0-5       \n [85] png_0.1-8            rstudioapi_0.16.0    tzdb_0.4.0          \n [88] reshape2_1.4.4       uuid_1.2-1           curl_5.2.3          \n [91] coda_0.19-4.1        checkmate_2.3.2      nlme_3.1-166        \n [94] nloptr_2.1.1         repr_1.1.7           zoo_1.8-12          \n [97] parallel_4.4.1       miniUI_0.1.1.1       foreign_0.8-87      \n[100] pillar_1.9.0         grid_4.4.1           vctrs_0.6.5         \n[103] promises_1.3.0       car_3.1-2            OpenMx_2.21.12      \n[106] xtable_1.8-4         Deriv_4.1.6          cluster_2.1.6       \n[109] dcurver_0.9.2        GPArotation_2024.3-1 htmlTable_2.4.3     \n[112] evaluate_1.0.0       pbivnorm_0.6.0       cli_3.6.3           \n[115] kutils_1.73          compiler_4.4.1       rlang_1.1.4         \n[118] crayon_1.5.3         future.apply_1.11.2  ggsignif_0.6.4      \n[121] fdrtool_1.2.18       plyr_1.8.9           stringi_1.8.4       \n[124] munsell_0.5.1        lisrelToR_0.3        pacman_0.5.1        \n[127] Matrix_1.7-0         IRkernel_1.3.2       hms_1.1.3           \n[130] glasso_1.11          future_1.34.0        shiny_1.9.1         \n[133] igraph_2.0.3         broom_1.0.6          RcppParallel_5.1.9",
    "crumbs": [
      "Mokken",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Applicazione Pratica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html",
    "href": "chapters/irt/01_logistic_regr.html",
    "title": "63  Modello di Regressione Logistica",
    "section": "",
    "text": "63.1 Introduzione\nPrima di approfondire i modelli della Teoria della Risposta all’Item (IRT), è essenziale sviluppare una comprensione approfondita del modello di regressione logistica. Questo modello, largamente applicato all’analisi di dati categorici, rappresenta un punto di partenza robusto per comprendere i modelli IRT, che ne estendono i principi. La regressione logistica, infatti, stima la probabilità di un evento in funzione di variabili predittive, mentre i modelli IRT calcolano la probabilità che un esaminando risponda correttamente a un item, incorporando sia l’abilità individuale sia le caratteristiche specifiche dell’item.\nNonostante le analogie, i modelli IRT si distinguono nettamente dalla regressione logistica per la capacità di modellare simultaneamente le proprietà degli item e le abilità degli individui. Mentre la regressione logistica presuppone indipendenza tra le osservazioni, i modelli IRT tengono conto delle interdipendenze tra le risposte agli item e tra gli item stessi. In questo capitolo, ci concentreremo sull’analisi del modello di regressione logistica come base per comprendere i modelli IRT.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html#modello-di-regressione-logistica-per-variabili-binarie",
    "href": "chapters/irt/01_logistic_regr.html#modello-di-regressione-logistica-per-variabili-binarie",
    "title": "63  Modello di Regressione Logistica",
    "section": "63.2 Modello di Regressione Logistica per Variabili Binarie",
    "text": "63.2 Modello di Regressione Logistica per Variabili Binarie\nIl modello di regressione logistica si utilizza per analizzare la relazione tra una variabile dipendente dicotomica, che assume i valori di “successo” e “fallimento”, e una o più variabili indipendenti, che possono essere sia quantitative che qualitative. Consideriamo \\(n\\) osservazioni indipendenti, dove \\(Y_i\\) indica l’osservazione \\(i\\)-esima della variabile risposta, per \\(i=1, \\dots, n\\). Ogni osservazione è associata a un vettore di variabili esplicative \\((x_1, \\dots, x_p)\\). La relazione che vogliamo esaminare è tra la probabilità di successo \\(\\pi_i\\) e le variabili esplicative, espressa dalla formula:\n\\[\nP(Y=1 \\mid X=x_i) = \\pi_i.\n\\]\nIn questo contesto, la variabile dipendente \\(Y\\) segue una distribuzione di Bernoulli, con i seguenti possibili valori:\n\\[\ny_i =\n\\begin{cases}\n    1 & \\text{per un successo (per l'osservazione $i$-esima)},\\\\\n    0 & \\text{per un fallimento}.\n\\end{cases}\n\\]\nLe probabilità associate a questi valori sono rispettivamente \\(\\pi\\) per il successo e \\(1-\\pi\\) per il fallimento:\n\\[\n\\begin{aligned}\n    P(Y_i = 1) &= \\pi,\\\\\n    P(Y_i = 0) &= 1-\\pi.\n\\end{aligned}\n\\]\nQuesto modello permette quindi di studiare come le variabili esplicative influenzino la probabilità di un evento binario, quali il successo o il fallimento.\nPer illustrare, consideriamo un dataset di 100 volontari, dove age è la variabile esplicativa e chd è la variabile risposta che indica la presenza (chd = 1) o assenza (chd = 0) di disturbi cardiaci. La media condizionata \\(\\mathbb{E}(Y \\mid X=x)\\) in una popolazione può essere vista come la proporzione di valori 1 per un dato punteggio \\(x\\) sulla variabile esplicativa (ad esempio, l’età), ovvero la probabilità condizionata \\(\\pi_i\\) di osservare la presenza di sintomi cardiaci in un certo gruppo d’età:\n\\[\n\\pi_i \\equiv P(Y = 1 \\mid X = x).\n\\]\nIl valore atteso diventa:\n\\[\n\\mathbb{E}(Y \\mid x) = \\pi_i.\n\\]\nSe \\(X\\) è una variabile discreta, possiamo calcolare la proporzione di \\(Y=1\\) per ogni valore di \\(X=x\\) nel campione. Queste proporzioni rappresentano una stima non parametrica della funzione di regressione di \\(Y\\) su \\(X\\), e possono essere stimate tramite tecniche di smoothing, come indicato nella figura seguente.\n\nchdage &lt;- rio::import(\n    here::here(\n        \"data\", \"logistic_reg\", \"chdage_dat.txt\"\n    )\n)\n\n\nchdage |&gt;\n    head()\n\n\nA data.frame: 6 x 3\n\n\n\nid\nage\nchd\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n1\n20\n0\n\n\n2\n2\n23\n0\n\n\n3\n3\n24\n0\n\n\n4\n5\n25\n1\n\n\n5\n4\n25\n0\n\n\n6\n7\n26\n0\n\n\n\n\n\n\n# Calcolo delle proporzioni di Y = 1 per ogni valore di X\nprop_data &lt;- chdage %&gt;%\n    group_by(age) %&gt;% # nolint: indentation_linter.\n    summarise(prop_chd = mean(chd))\n\n# Grafico con smoothing\nggplot(prop_data, aes(x = age, y = prop_chd)) +\n    geom_point() + # Mostra i punti di proporzione\n    geom_smooth(method = \"loess\", span = 0.7) + # Regressione LOESS\n    labs(\n        title = \"Stima Non Parametrica\\n della Regressione di CHD su Age\",\n        x = \"Eta'\",\n        y = \"Proporzione di CHD = 1\"\n    )\n\n\n\n\n\n\n\n\nPer valori bassi della variabile age la proporzione condizionata di valori \\(Y=1\\) è prossima allo 0. Per valori alti dell’età la proporzione di valori \\(Y=1\\) è prossima a 1.0. A livelli di età intermedi, la curva di regressione non parametrica gradualmente approssima i valori 0 e 1 seguendo un andamento sigmoidale.\nAnche se nel caso presente la regressione non parametrica produce un risultato sensato, è utile rappresentare la dipendenza di \\(Y\\) da \\(X\\) con una semplice funzione, in particolare quando ci sono molteplici variabili esplicative.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html#modello-lineare-nelle-probabilità",
    "href": "chapters/irt/01_logistic_regr.html#modello-lineare-nelle-probabilità",
    "title": "63  Modello di Regressione Logistica",
    "section": "63.3 Modello Lineare nelle Probabilità",
    "text": "63.3 Modello Lineare nelle Probabilità\nIntroduciamo un modello lineare con le seguenti assunzioni standard:\n\\[\nY_i = \\alpha + \\beta X_i + \\varepsilon_i,\n\\]\ndove \\(\\varepsilon_i\\) segue una distribuzione normale con media 0 e varianza 1 (\\(\\varepsilon_i \\sim \\mathcal{N}(0, 1)\\)) e gli errori \\(\\varepsilon_i\\) e \\(\\varepsilon_j\\) sono indipendenti per ogni \\(i \\neq j\\). Il valore atteso di \\(Y_i\\) è quindi \\(\\mathbb{E}(Y_i) = \\alpha + \\beta X_i\\), portando a:\n\\[\n\\pi_i = \\alpha + \\beta X_i.\n\\]\nQuesto è noto come modello lineare nelle probabilità (linear probability model). Tuttavia, questo approccio presenta una limitazione significativa: non garantisce che i valori predetti di \\(\\pi_i\\) siano confinati nell’intervallo [0,1], come richiesto per le probabilità.\n\n63.3.1 Problemi di Normalità\nConsiderando che \\(Y_i\\) può assumere solo i valori 0 o 1, i residui \\(\\varepsilon_i\\) risultano anch’essi dicotomici e quindi non possono seguire una distribuzione normale. Ad esempio, se \\(Y_i=1\\) con probabilità \\(\\pi_i\\), il residuo sarà:\n\\[\n\\varepsilon_i = 1 - \\mathbb{E}(Y_i) = 1 - (\\alpha + \\beta X_i) = 1 - \\pi_i.\n\\]\nSe, invece, \\(Y_i=0\\) con probabilità \\(1-\\pi_i\\), il residuo sarà:\n\\[\n\\varepsilon_i = 0 - \\mathbb{E}(Y_i) = 0 - (\\alpha + \\beta X_i) = - \\pi_i.\n\\]\nTuttavia, se la dimensione del campione è grande, il teorema del limite centrale può mitigare l’importanza dell’assunzione di normalità per le stime dei minimi quadrati.\n\n\n63.3.2 Problematiche di Omoschedasticità\nUtilizzare il metodo dei minimi quadrati può essere inappropriato in questo contesto poiché la varianza dei residui non è costante ma dipende dalla media, e quindi dalla variabile \\(X\\). Assumendo che il modello sia lineare, abbiamo che \\(\\mathbb{E}(\\varepsilon_i)=0\\). Sfruttando le relazioni discusse in precedenza, la varianza dei residui si calcola come:\n\\[\n\\mathbb{V}(\\varepsilon_i) = (1-\\pi_i)\\pi_i.\n\\]\nConsideriamo che la varianza dei residui \\(\\varepsilon_i\\) può essere espressa come:\n\\[\n\\text{Var}(\\varepsilon_i) = \\mathbb{E}(\\varepsilon_i^2) - \\mathbb{E}(\\varepsilon_i)^2,\n\\]\ndove \\(\\mathbb{E}(\\varepsilon_i^2)\\) è il valore atteso del quadrato dei residui e \\(\\mathbb{E}(\\varepsilon_i)^2\\) è il quadrato del valore atteso dei residui.\nOra calcoliamo \\(\\mathbb{E}(\\varepsilon_i^2)\\):\n\\[\n\\begin{align*}\n\\mathbb{E}(\\varepsilon_i^2) &= \\mathbb{E}[(Y_i - \\mathbb{E}(Y_i))^2] \\\\\n&= \\mathbb{E}[(Y_i - \\pi_i)^2] \\\\\n&= \\mathbb{E}[(Y_i^2 - 2Y_i\\pi_i + \\pi_i^2)] \\\\\n&= \\mathbb{E}(Y_i^2) - 2\\mathbb{E}(Y_i\\pi_i) + \\mathbb{E}(\\pi_i^2) \\\\\n&= \\mathbb{E}(Y_i) - 2\\mathbb{E}(Y_i\\pi_i) + \\pi_i^2 \\\\\n&= \\pi_i - 2\\pi_i^2 + \\pi_i^2 \\\\\n&= \\pi_i - \\pi_i^2 \\\\\n&= \\pi_i(1 - \\pi_i)\n\\end{align*}\n\\]\nOra calcoliamo \\(\\mathbb{E}(\\varepsilon_i)^2\\):\n\\[\n\\begin{align*}\n\\mathbb{E}(\\varepsilon_i)^2 &= (\\mathbb{E}(Y_i - \\mathbb{E}(Y_i)))^2 \\\\\n&= (\\mathbb{E}(Y_i - \\pi_i))^2 \\\\\n&= (0)^2 \\\\\n&= 0\n\\end{align*}\n\\]\nQuindi, sostituendo questi risultati nella formula della varianza dei residui, otteniamo:\n\\[\n\\text{Var}(\\varepsilon_i) = \\mathbb{E}(\\varepsilon_i^2) - \\mathbb{E}(\\varepsilon_i)^2 = \\pi_i(1 - \\pi_i)\n\\]\nQuindi, abbiamo dimostrato che la varianza dei residui nella regressione logistica può essere espressa come \\((1-\\pi_i)\\pi_i\\).\nDato che \\(\\pi_i\\) dipende da \\(x\\), ciò significa che la varianza non è costante in funzione di \\(x\\). Questa eteroschedasticità dei residui rappresenta un problema per le stime dei minimi quadrati nel modello lineare, specialmente quando le probabilità \\(\\pi_i\\) sono vicine a 0 o 1.\n\n\n63.3.3 Linearità\nIl maggiore inconveniente connesso all’adozione del modello lineare nelle probabilità deriva dal fatto che la stima della probabilità di successo, \\(P(\\hat{Y}_i=1)=\\hat{\\pi}_i\\), non è necessariamente compresa nell’intervallo (\\(0,1\\)), ma può essere sia negativa sia maggiore di 1. Nel caso dell’esempio in discussione, ciò significa che la retta dei minimi quadrati produce valori attesi \\(\\hat{\\pi}\\) inferiori a 0 per bassi valori della variabile età e valori \\(\\hat{\\pi}\\) superiori a 1 per valori di età alti.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html#modello-lineare-nelle-probabilità-vincolato",
    "href": "chapters/irt/01_logistic_regr.html#modello-lineare-nelle-probabilità-vincolato",
    "title": "63  Modello di Regressione Logistica",
    "section": "63.4 Modello Lineare nelle Probabilità Vincolato",
    "text": "63.4 Modello Lineare nelle Probabilità Vincolato\nUna soluzione per mantenere \\(\\pi\\) all’interno dell’intervallo (0, 1) è la seguente specificazione del modello:\n\\[\n\\pi=\n\\begin{cases}\n  0                           &\\text{se $\\alpha + \\beta X &lt; 0$},\\\\\n  \\alpha + \\beta X           &\\text{se $0 \\leq \\alpha + \\beta X \\leq 1$},\\\\\n  1 &\\text{se $\\alpha + \\beta X &gt; 1$}.\n\\end{cases}\n\\]\nQuesto modello lineare nelle probabilità vincolato mostra alcune instabilità, soprattutto a causa della sua dipendenza critica dai valori estremi di \\(\\pi\\), dove assume i valori 0 o 1. La linearità di \\(\\pi = \\alpha + \\beta X\\) si basa fortemente sui punti in cui si verificano questi estremi. In particolare, la stima di \\(\\pi = 0\\) può essere influenzata dal valore minimo di \\(X\\) associato a \\(Y=1\\), mentre la stima di \\(\\pi = 1\\) può dipendere dal valore massimo di \\(X\\) per cui \\(Y=0\\). Questi valori estremi tendono a variare significativamente tra diversi campioni e possono diventare più estremi all’aumentare della dimensione del campione.\nLa presenza di più variabili esplicative (\\(k \\geq 2\\)) complica ulteriormente la stima dei parametri del modello. Inoltre, il modello mostra un cambiamento brusco nella pendenza della curva di regressione ai punti estremi (0 e 1 di \\(\\pi\\)), risultando poco realistico in molte situazioni pratiche. Questo rende il modello meno adatto a descrivere relazioni complesse e gradualmente variabili tra \\(\\pi\\) e \\(X\\).\nUna funzione che modella una relazione più fluida e continua tra \\(\\pi\\) e \\(X\\) sarebbe più realistica e rappresentativa delle dinamiche osservate. Questo motiva la preferenza per modelli alternativi, come il modello di regressione logistica, che tende a fornire una rappresentazione più accurata e realistica delle interazioni tra variabili dicotomiche e esplicative.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html#regressione-logistica",
    "href": "chapters/irt/01_logistic_regr.html#regressione-logistica",
    "title": "63  Modello di Regressione Logistica",
    "section": "63.5 Regressione Logistica",
    "text": "63.5 Regressione Logistica\nUn metodo efficace per gestire il problema del vincolo sulle probabilità è specificare modelli non direttamente per le probabilità stesse, ma per una loro trasformazione che elimina tale vincolo. Invece di definire un modello lineare per la probabilità condizionata \\(\\pi_i\\), si può specificare un modello lineare per il logaritmo degli odds (logit):\n\\[\n\\eta_i = \\log_e \\frac{\\pi_i}{1-\\pi_i} = \\alpha + \\beta x_i,\n\\]\nQuesto approccio non presenta problemi poiché il logit \\(\\eta\\) è sempre un numero reale, permettendo di modellare una trasformazione lineare di \\(\\pi\\). L’immagine che segue è stata generata calcolando i logit empirici, ovvero il logaritmo degli odds \\(\\log_e(\\hat{\\pi}_i/(1-\\hat{\\pi}_i))\\) per 8 intervalli in cui è stata suddivisa la variabile “età”. Questa analisi mostra chiaramente una relazione lineare tra i logit empirici e l’età, evidenziando come la regressione logistica possa efficacemente modellare e interpretare relazioni tra variabili trasformate e fattori esplicativi.\n\ndat1 &lt;- chdage %&gt;%\n    mutate(age_c = ntile(age, 8)) %&gt;% # Creare i bin di età con quantili\n    group_by(age_c) %&gt;%\n    summarise(\n        age_bin_center = (min(age) + max(age)) / 2, # Punto centrale di ciascun bin\n        proportion_heart_disease = mean(chd) # Proporzione di heart_disease\n    )\n\ndat1\n\n\nA tibble: 8 x 3\n\n\nage_c\nage_bin_center\nproportion_heart_disease\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n25.0\n0.1538\n\n\n2\n32.5\n0.0769\n\n\n3\n37.5\n0.3077\n\n\n4\n42.5\n0.3077\n\n\n5\n47.0\n0.5000\n\n\n6\n52.0\n0.5833\n\n\n7\n57.0\n0.7500\n\n\n8\n64.0\n0.8333\n\n\n\n\n\n\nxc &lt;- dat1$age_bin_center\nyc &lt;- dat1$proportion_heart_disease\n\nlogit_y &lt;- log(yc / (1 - yc))\nfit &lt;- lm(logit_y ~ xc)\nplot(\n    xc, logit_y,\n    xlab = \"Eta'\", ylab = \"Logit(Y)\",\n    main = \"\", type = \"n\"\n)\npoints(xc, logit_y, cex = 2)\nabline(fit)",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html#probabilità-odds-e-logit",
    "href": "chapters/irt/01_logistic_regr.html#probabilità-odds-e-logit",
    "title": "63  Modello di Regressione Logistica",
    "section": "63.6 Probabilità, Odds e Logit",
    "text": "63.6 Probabilità, Odds e Logit\nLa relazione tra probabilità, odds e logit viene illustrata nella tabella qui sotto. È importante notare che gli odds e il logit trasformano l’intervallo di probabilità (0, 1) in uno spettro più ampio. Gli odds rappresentano il rapporto tra la probabilità di un evento e la probabilità del suo complemento. Il logit, invece, è il logaritmo naturale degli odds, trasformando così l’intervallo di probabilità in tutta la linea dei numeri reali. Quando la probabilità è 0.5, gli odds sono 1 e il logit è 0. Logit negativi indicano probabilità inferiori a 0.5, mentre logit positivi indicano probabilità superiori a 0.5.\n\n\n\n\n\n\n\n\nProbabilità (P)\nOdds (O)\nlogit (L)\n\n\n\n\n0.01\n0.01 / 0.99 = 0.0101\n\\(\\ln(\\frac{0.01}{0.99}) = -4.60\\)\n\n\n0.05\n0.05 / 0.95 = 0.0526\n\\(\\ln(\\frac{0.05}{0.95}) = -2.94\\)\n\n\n0.10\n0.10 / 0.90 = 0.1111\n\\(\\ln(\\frac{0.10}{0.90}) = -2.20\\)\n\n\n0.30\n0.30 / 0.70 = 0.4286\n\\(\\ln(\\frac{0.30}{0.70}) = -0.85\\)\n\n\n0.50\n0.50 / 0.50 = 1\n\\(\\ln(\\frac{0.50}{0.50}) = 0.00\\)\n\n\n0.70\n0.70 / 0.30 = 2.3333\n\\(\\ln(\\frac{0.70}{0.30}) = 0.85\\)\n\n\n0.90\n0.90 / 0.10 = 9\n\\(\\ln(\\frac{0.90}{0.10}) = 2.20\\)\n\n\n0.95\n0.95 / 0.05 = 19\n\\(\\ln(\\frac{0.95}{0.05}) = 2.94\\)\n\n\n0.99\n0.99 / 0.01 = 99\n\\(\\ln(\\frac{0.99}{0.01}) = 4.60\\)\n\n\n\n\n63.6.1 Trasformazione Inversa del Logit\nLa trasformazione inversa del logit, detta antilogit, consente di trasformare i logit in probabilità:\n\\[\n  \\pi_i =\\frac{e^{\\eta_i}}{1+e^{\\eta_i}}.\n\\]\nLogit e probabilità possono dunque essere trasformati gli uni nelle altre. La trasformazione inversa del logit consente di specificare un modello non lineare per le probabilità \\(\\pi_i\\). Tale modello non lineare è detto logit, o modello di regressione logistica:\n\\[\n  \\pi_i = \\frac{e^{\\eta_i}}{1+e^{\\eta_i}} =  \\frac{e^{\\alpha + \\beta x_i}}{1+e^{\\alpha + \\beta x_i}}.\n\\]\nLa funzione logistica ben rappresenta l’andamento sigmoidale delle proporzioni di casi \\(Y=1\\), ovvero \\(\\hat{\\pi}_i = E(Y \\mid x_i)\\) (le proporzioni di presenza di disturbi coronarici), in funzione di livelli crescenti della variabile age:\n\nfm &lt;- glm(chd ~ age, family = binomial(link = \"logit\"), data = chdage)\nlogit_hat &lt;- fm$coef[1] + fm$coef[2] * chdage$age\npi_hat &lt;- exp(logit_hat) / (1 + exp(logit_hat))\n\nplot(chdage$age, pi_hat,\n    xlab = \"Eta'\",\n    ylab = \"P(CHD)\",\n    main = \"\", type = \"n\"\n)\nlines(chdage$age, pi_hat)\npoints(xc, yc, cex = 2)\n\n\n\n\n\n\n\n\nIn alternativa, possiamo usare la funzione plot_model del pacchetto sjPlot:\n\nplot_model(\n    fm,\n    type = \"pred\",\n    terms = \"age\"\n) +\nlabs(y = \"Prob(heart disease)\")",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html#modelli-lineari-generalizzati",
    "href": "chapters/irt/01_logistic_regr.html#modelli-lineari-generalizzati",
    "title": "63  Modello di Regressione Logistica",
    "section": "63.7 Modelli Lineari Generalizzati",
    "text": "63.7 Modelli Lineari Generalizzati\nNel caso di una variabile risposta binaria, il modello classico di regressione lineare si scontra con sfide specifiche:\n\nDistribuzione Binomiale: \\(Y_i\\) segue una distribuzione binomiale (con indice \\(n_i\\), potenzialmente uguale a uno nel caso individuale), rendendo non applicabile l’ipotesi di normalità.\nLimiti delle Probabilità: Utilizzando una specificazione lineare come \\(\\pi_i= \\beta_0 + \\beta_1 x_i\\), si possono ottenere stime di probabilità esterne all’intervallo 0-1.\nVarianze Non Costanti: La varianza di \\(\\varepsilon\\) varia in base alla specificazione del modello di probabilità, seguendo la formula \\(V(\\varepsilon_i)=\\pi_i(1-\\pi_i)\\).\n\nPer superare queste sfide, si utilizzano i Modelli Lineari Generalizzati (GLM). Questi modelli consentono l’uso di variabili risposta di diversa natura e includono:\n\nRegressione Lineare: Per variabili dipendenti continue e variabili esplicative continue o qualitative.\nRegressione Logistica: Per variabili risposta binarie.\nModello Loglineare di Poisson: Per modellare frequenze in tabelle di contingenza.\n\nI GLM allentano alcune ipotesi fondamentali del modello lineare classico, come linearità, normalità della componente erratica, e omoschedasticità delle osservazioni. Sono strutturati in tre componenti principali:\n\nComponente Aleatoria: Definisce la distribuzione di probabilità della variabile risposta \\(Y\\).\nComponente Sistematica: Specifica la relazione lineare tra le variabili esplicative e una trasformazione della variabile risposta.\nFunzione Legame: Trasforma la media attesa \\(\\mathbb{E}(Y)\\) in un formato che possa essere modellato linearmente rispetto alle variabili esplicative. Non è la variabile risposta stessa ad essere modellizzata direttamente, ma una sua trasformazione, come il logit nel caso della regressione logistica.\n\nEsempi di combinazioni di componenti aleatorie, funzioni di legame e sistematiche nei GLM includono:\n\n\n\n\n\n\n\n\n\nComponente Aleatoria\nFunzione Legame\nComponente Sistematica\nModello\n\n\n\n\nGaussiana\nIdentità\nContinua\nRegressione\n\n\nGaussiana\nIdentità\nCategoriale\nAnalisi della varianza\n\n\nGaussiana\nIdentità\nMista\nAnalisi della covarianza\n\n\nBinomiale\nLogit\nMista\nRegressione logistica\n\n\nPoisson\nLogaritmo\nMista\nModello Loglineare\n\n\n\nQuesta struttura rende i GLM particolarmente flessibili e adatti a una vasta gamma di situazioni statistiche, superando i limiti del modello lineare classico.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html#componente-sistematica",
    "href": "chapters/irt/01_logistic_regr.html#componente-sistematica",
    "title": "63  Modello di Regressione Logistica",
    "section": "63.8 Componente Sistematica",
    "text": "63.8 Componente Sistematica\nLa componente sistematica mette in relazione un vettore (\\(\\eta_1, \\eta_2, \\dots, \\eta_k\\)) con le variabili esplicative mediante un modello lineare. Sia \\(X_{ij}\\) il valore della \\(j\\)-esima variabile esplicativa (\\(j=1, 2, \\dots, p\\)) per l’\\(i\\)-esima osservazione (\\(i=1, \\dots, k\\)). Allora\n\\[\n\\eta_i = \\sum_j \\beta_j X_{ij}.\n\\]\nQuesta combinazione lineare di variabili esplicative è chiamata il predittore lineare. Un \\(X_{ij}=1, \\forall i\\) viene utilizzato per il coefficiente dell’intercetta del modello (talvolta denotata da \\(\\alpha\\)).",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html#componente-aleatoria",
    "href": "chapters/irt/01_logistic_regr.html#componente-aleatoria",
    "title": "63  Modello di Regressione Logistica",
    "section": "63.9 Componente Aleatoria",
    "text": "63.9 Componente Aleatoria\nLa componente aleatoria del modello suppone l’esistenza di \\(k\\) osservazioni indipendenti \\(y_1, y_2, \\dots, y_k\\), ciascuna delle quali viene trattata come la realizzazione di una variabile casuale \\(Y_i\\). Si assume che \\(Y_i\\) abbia una distribuzione binomiale:\n\\[\nY_i \\sim Bin(n_i, \\pi_i)\n\\]\ncon parametri \\(n_i\\) e \\(\\pi_i\\). Per dati individuali (uno per ciascun valore \\(x_i\\)), \\(n_i=1,\n    \\forall i\\).",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html#funzione-legame",
    "href": "chapters/irt/01_logistic_regr.html#funzione-legame",
    "title": "63  Modello di Regressione Logistica",
    "section": "63.10 Funzione Legame",
    "text": "63.10 Funzione Legame\nLa funzione legame \\(g(\\cdot)\\) mette in relazione il valore atteso della variabile risposta \\(Y_i\\) con la componente sistematica \\(\\eta_i\\) del modello. Abbiamo visto che \\(\\mathbb{E}(Y_i)=\\pi_i\\). Che relazione c’è tra \\(\\pi_i\\) e il predittore lineare \\(\\eta_i= \\alpha + \\sum_j  \\beta_j X_{ij}\\)? La risposta a questa domanda è data dalla funzione legame:\n\\[\n\\eta_i = g(\\pi_i) = \\ln{\\frac{\\pi_i}{1-\\pi_i}}\n\\]\nSi noti che la funzione legame non trasforma la variabile risposta \\(Y_i\\) ma bensì il suo valore atteso \\(\\pi_i\\).\nLa funzione legame è invertibile: anziché trasformare il valore atteso nel predittore lineare si può trasformare il predittore lineare nel valore atteso \\(\\pi_i\\):\n\\[\n\\pi_i = \\frac{e^{\\eta_i}}{1+e^{\\eta_i}} =  \\frac{e^{\\alpha + \\sum_j  \\beta_j X_{ij}}}{1+e^{\\alpha + \\sum_j  \\beta_j X_{ij}}}.\n\\]\nSi ottiene così un modello non lineare per le probabilità \\(\\pi_i\\).\n\nx &lt;- -5:5\nprob &lt;- invlogit(x)\nplot(x, prob, type = \"l\", main = \"Funzione logistica\", ylab = \"Probabilita'\", xlab = \"Valori X\")",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html#regressione-logistica-con-r",
    "href": "chapters/irt/01_logistic_regr.html#regressione-logistica-con-r",
    "title": "63  Modello di Regressione Logistica",
    "section": "63.11 Regressione Logistica con R",
    "text": "63.11 Regressione Logistica con R\nLa stima dei parametri del modello di regressione logistica per i dati in esame si ottiene in R utilizzando la funzione glm():\n\nfm &lt;- glm(chd ~ age,\n    family = binomial(link = \"logit\"),\n    data = chdage\n)\n\nSi noti che è necessario specificare sia la funzione teorica (family = binomial) della componente erratica del modello, sia la funzione legame (link = \"logit\"). L’output della funzione glm() può essere visualizzato utilizzando summary():\n\nsummary(fm) |&gt;\n    print()\n\n\nCall:\nglm(formula = chd ~ age, family = binomial(link = \"logit\"), data = chdage)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -5.3095     1.1337   -4.68  2.8e-06 ***\nage           0.1109     0.0241    4.61  4.0e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 136.66  on 99  degrees of freedom\nResidual deviance: 107.35  on 98  degrees of freedom\nAIC: 111.4\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nPer i dati dell’esempio, le probabilità predette sono uguali a\n\\[\n\\hat{\\pi}(x)=\\frac{e^{-5.309 + 0.111 \\times {\\tt age}}}{1+e^{-5.309 + 0.111 \\times {\\tt age}}}.\n\\]\nI logit stimati sono dati dall’equazione\n\\[\n\\hat{\\eta}(x)=-5.309 + 0.111 \\times {\\tt age}.\n\\]",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html#interpretazione-dei-coefficienti",
    "href": "chapters/irt/01_logistic_regr.html#interpretazione-dei-coefficienti",
    "title": "63  Modello di Regressione Logistica",
    "section": "63.12 Interpretazione dei Coefficienti",
    "text": "63.12 Interpretazione dei Coefficienti\nIl modello di regressione logistica può essere interpretato focalizzandosi sui coefficienti di regressione.\n\n63.12.1 Interpretazione Basata sui Log-Odds\n\nIntercetta (-5.30945):\n\nRappresenta il log-odds di sviluppare CHD quando l’età è 0. Benché questo non sia praticamente realistico, offre un punto di riferimento teorico.\nIl valore negativo suggerisce una bassa probabilità di CHD a 0 anni.\n\nCoefficiente di Età (0.11092):\n\nIndica la variazione dei log-odds di CHD per ogni anno aggiuntivo di età.\nUn valore positivo implica che, all’aumentare dell’età, aumenta anche la probabilità di CHD.\n\n\n\n\n63.12.2 Interpretazione Attraverso l’Odds Ratio\nPer calcolare l’odds ratio associato all’età, si esponenzia il coefficiente:\n\\[\n\\text{Odds Ratio per Età} = e^{0.11092} \\approx 1.12.\n\\]\nQuesto valore mostra la variazione degli odds di CHD per ogni incremento annuale di età. Un odds ratio maggiore di 1 indica un aumento della probabilità con l’età. Ad esempio, un odds ratio di 1.12 significa che per ogni anno in più, gli odds di avere CHD aumentano del 12%. L’odds ratio fornisce una visione relativa e intuitiva del cambiamento del rischio.\n\n\n63.12.3 Interpretazione Basata sulle Probabilità Predette\nL’interpretazione più diretta e intuitiva dei coefficienti nel modello di regressione logistica è tramite il calcolo delle probabilità predette. A differenza dei coefficienti grezzi o degli odds ratio, le probabilità predette offrono una comprensione immediata dell’impatto delle variabili.\nNel caso dello studio della probabilità di CHD in base all’età, si possono calcolare le probabilità predette di CHD per individui di diverse età. Questo metodo fornisce una rappresentazione chiara di come il rischio di CHD vari in funzione dell’età, rendendo l’interpretazione accessibile anche a chi non è esperto in statistica. Usiamo qui la funzione effect_plot() del pacchetto jtools:\n\neffect_plot(fm,\n    pred = age, interval = TRUE, plot.points = TRUE,\n    jitter = 0.05\n)",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html#riflessioni-conclusive",
    "href": "chapters/irt/01_logistic_regr.html#riflessioni-conclusive",
    "title": "63  Modello di Regressione Logistica",
    "section": "63.13 Riflessioni Conclusive",
    "text": "63.13 Riflessioni Conclusive\nNel caso di una variabile dipendente binaria \\(Y_i\\), il tradizionale modello di regressione lineare non può essere applicato. Tale problema può essere risolto applicando il modello lineare non direttamente al valore atteso della variabile risposta, ma ad una sua trasformazione, il logit. La componente sistematica del modello di regressione lineare esprime il valore atteso della variabile dipendente come una funzione lineare dei predittori: \\(\\mu_i = \\boldsymbol{\\beta x}_i\\).\nPer il modello di regressione lineare, il valore atteso di \\(Y\\) è la media delle distribuzioni condizionate \\(Y \\mid x_i\\).\nPer il modello di regressione logistica, il valore atteso della variabile risposta binaria, condizionato ad un determinato valore della variabile esplicativa (o ad un insieme di valori del vettore di variabili esplicative) è uguale alla probabilità che \\(Y\\) assuma il valore 1:\n\\[\n\\mathbb{E}(Y \\mid x_i) \\equiv Pr(Y=1 \\mid X=x_i) \\equiv \\pi_i.\n\\]\nTale valore atteso può essere interpretato come la proporzione di individui nella popolazione per i quali \\(Y=1\\) in corrispondenza di \\(X=x_i\\).\nLa componente sistematica del modello di regressione logistica rappresenta una trasformazione di \\(\\pi_i\\) come funzione lineare dei predittori:\n\\[\n\\ln \\frac{\\pi_i}{1-\\pi_i} = \\alpha + \\beta X_i.\n\\]\nIl modello è dunque lineare nei logit. Equivalentemente, esponenziando\n\\[\n\\frac{\\pi_i}{1-\\pi_i} = \\exp(\\alpha + \\beta X_i)\n\\]\nl’odds stimato di \\(Y_i=1\\) è uguale a \\(\\exp(\\alpha + \\beta X_i)\\).\nLa funzione antilogit trasforma il predittore lineare \\(\\eta_i = \\alpha + \\beta X_i\\) nelle probabilità:\n\\[\n\\pi_i = \\frac{\\exp(\\alpha+\\beta X_i)}{1 + \\exp(\\alpha+\\beta X_i)}.\n\\]\nIl modello di regressione logistica è un modello non lineare nelle probabilità (ovvero, rispetto al valore atteso della variabile risposta).\nLa funzione logistica\n\\[\n\\Lambda(\\eta) = \\frac{\\exp(\\eta)}{1 + \\exp(\\eta)}\n\\]\nviene scelta quale funzione legame per trasformare la componente lineare del modello, \\(\\eta_i =\\alpha+\\beta X_i\\), nel valore atteso della variabile dipendente, \\(\\pi_i\\). Qualunque funzione cumulativa di probabilità potrebbe fungere da funzione legame, tuttavia, è conveniente scegliere la funzione logistica per facilità di interpretazione.\nLa componente aleatoria del modello di regressione logistica, infine, ci porta a considerare la variabile dipendente come una variabile aleatoria binomiale, sia nel caso di dati raggruppati (con denomiatore binomiale uguale alla frequenza delle osservazioni in ciascun gruppo \\(n_i\\) corrispondente a modalità omogenee della/e variabile/i esplicativa/e) che nel caso di dati individuali (dove \\(n_i\\) = 1, \\(\\forall i\\)).",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/01_logistic_regr.html#session-info",
    "href": "chapters/irt/01_logistic_regr.html#session-info",
    "title": "63  Modello di Regressione Logistica",
    "section": "63.14 Session Info",
    "text": "63.14 Session Info\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] sjPlot_2.8.16        see_0.9.0            jtools_2.3.0        \n [4] effects_4.2-2        gmodels_2.19.1       LaplacesDemon_16.1.6\n [7] car_3.1-3            carData_3.0-5        MASS_7.3-61         \n[10] viridis_0.6.5        viridisLite_0.4.2    ggpubr_0.6.0        \n[13] ggExtra_0.10.1       gridExtra_2.3        patchwork_1.3.0     \n[16] bayesplot_1.11.1     semTools_0.5-6       semPlot_1.1.6       \n[19] lavaan_0.6-19        psych_2.4.6.26       scales_1.3.0        \n[22] markdown_1.13        knitr_1.49           lubridate_1.9.3     \n[25] forcats_1.0.0        stringr_1.5.1        dplyr_1.1.4         \n[28] purrr_1.0.2          readr_2.1.5          tidyr_1.3.1         \n[31] tibble_3.2.1         ggplot2_3.5.1        tidyverse_2.0.0     \n[34] here_1.0.1          \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.2       later_1.3.2         pbdZMQ_0.3-13      \n  [4] datawizard_0.13.0   XML_3.99-0.17       rpart_4.1.23       \n  [7] lifecycle_1.0.4     rstatix_0.7.2       rprojroot_2.0.4    \n [10] globals_0.16.3      lattice_0.22-6      insight_0.20.5     \n [13] rockchalk_1.8.157   backports_1.5.0     survey_4.4-2       \n [16] magrittr_2.0.3      openxlsx_4.2.7.1    Hmisc_5.2-0        \n [19] rmarkdown_2.29      httpuv_1.6.15       qgraph_1.9.8       \n [22] zip_2.3.1           RColorBrewer_1.1-3  pbapply_1.7-2      \n [25] DBI_1.2.3           minqa_1.2.8         multcomp_1.4-26    \n [28] abind_1.4-8         quadprog_1.5-8      nnet_7.3-19        \n [31] TH.data_1.1-2       sandwich_3.1-1      listenv_0.9.1      \n [34] gdata_3.0.1         openintro_2.5.0     arm_1.14-4         \n [37] performance_0.12.4  airports_0.1.0      parallelly_1.39.0  \n [40] codetools_0.2-20    tidyselect_1.2.1    ggeffects_1.7.2    \n [43] farver_2.1.2        lme4_1.1-35.5       broom.mixed_0.2.9.6\n [46] stats4_4.4.2        base64enc_0.1-3     jsonlite_1.8.9     \n [49] Formula_1.2-5       survival_3.7-0      emmeans_1.10.5     \n [52] tools_4.4.2         Rcpp_1.0.13-1       glue_1.8.0         \n [55] mnormt_2.1.1        mgcv_1.9-1          xfun_0.49          \n [58] IRdisplay_1.1       withr_3.0.2         fastmap_1.2.0      \n [61] mitools_2.4         boot_1.3-31         fansi_1.0.6        \n [64] digest_0.6.37       mi_1.1              timechange_0.3.0   \n [67] R6_2.5.1            mime_0.12           estimability_1.5.1 \n [70] colorspace_2.1-1    Cairo_1.6-2         gtools_3.9.5       \n [73] jpeg_0.1-10         utf8_1.2.4          generics_0.1.3     \n [76] data.table_1.16.2   corpcor_1.6.10      usdata_0.3.1       \n [79] htmlwidgets_1.6.4   pkgconfig_2.0.3     sem_3.1-16         \n [82] gtable_0.3.6        furrr_0.3.1         htmltools_0.5.8.1  \n [85] png_0.1-8           snakecase_0.11.1    rstudioapi_0.17.1  \n [88] tzdb_0.4.0          reshape2_1.4.4      uuid_1.2-1         \n [91] coda_0.19-4.1       checkmate_2.3.2     nlme_3.1-166       \n [94] nloptr_2.1.1        repr_1.1.7          zoo_1.8-12         \n [97] sjlabelled_1.2.0    parallel_4.4.2      miniUI_0.1.1.1     \n[100] foreign_0.8-87      pillar_1.9.0        grid_4.4.2         \n[103] vctrs_0.6.5         promises_1.3.0      OpenMx_2.21.13     \n[106] xtable_1.8-4        cluster_2.1.6       htmlTable_2.4.3    \n[109] evaluate_1.0.1      pbivnorm_0.6.0      mvtnorm_1.3-2      \n[112] cli_3.6.3           kutils_1.73         compiler_4.4.2     \n[115] rlang_1.1.4         crayon_1.5.3        ggsignif_0.6.4     \n[118] labeling_0.4.3      fdrtool_1.2.18      sjmisc_2.8.10      \n[121] plyr_1.8.9          pander_0.6.5        stringi_1.8.4      \n[124] munsell_0.5.1       lisrelToR_0.3       pacman_0.5.1       \n[127] Matrix_1.7-1        sjstats_0.19.0      IRkernel_1.3.2     \n[130] hms_1.1.3           glasso_1.11         future_1.34.0      \n[133] shiny_1.9.1         haven_2.5.4         igraph_2.1.1       \n[136] broom_1.0.7         RcppParallel_5.1.9  cherryblossom_0.1.0",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Modello di Regressione Logistica</span>"
    ]
  },
  {
    "objectID": "chapters/irt/E_01.html",
    "href": "chapters/irt/E_01.html",
    "title": "64  ✏️ Esercizi",
    "section": "",
    "text": "(esercizi-logistic-reg)=\n\nsuppressPackageStartupMessages({\n    library(\"tidyverse\")\n    library(\"car\")\n    library(\"LaplacesDemon\")\n    library(\"gmodels\")\n    library(\"effects\")\n    library(\"psych\")\n    library(\"epitools\")\n    })\n\nE1. Per chiarire il significato del coefficiente \\(\\beta\\) nel caso della regressione logistica, si utilizzino i dati chdage e si ripeta l’analisi precedente dicotomizzando l’età in funzione della media.\n\nchdage &lt;- read.table(\"../../data/logistic_reg/chdage_dat.txt\", header = TRUE)\nchdage |&gt;\n    head()\n\n\nA data.frame: 6 x 3\n\n\n\nid\nage\nchd\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n1\n20\n0\n\n\n2\n2\n23\n0\n\n\n3\n3\n24\n0\n\n\n4\n5\n25\n1\n\n\n5\n4\n25\n0\n\n\n6\n7\n26\n0\n\n\n\n\n\n\n# Calcolo della media dell'età\nmedia_eta &lt;- mean(chdage$age)\n\n# Creazione di una variabile dicotomica per l'età\nchdage$eta_categoria &lt;- ifelse(chdage$age &gt; media_eta, \"SopraMedia\", \"SottoMedia\")\n\n# Visualizzazione della tavola 2x2\ntable(chdage$eta_categoria, chdage$chd)\n\n            \n              0  1\n  SopraMedia 16 32\n  SottoMedia 41 11\n\n\nAnalizziamo la relazione tra le due variabili categoriche (età categorizzata e CHD) utilizzando una tavola di contingenza 2x2 e il test del chi-quadrato.\n\ngmodels::CrossTable(chdage$eta_categoria, chdage$chd, chisq = TRUE, missing.include = TRUE)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  100 \n\n \n                     | chdage$chd \nchdage$eta_categoria |         0 |         1 | Row Total | \n---------------------|-----------|-----------|-----------|\n          SopraMedia |        16 |        32 |        48 | \n                     |     4.717 |     6.252 |           | \n                     |     0.333 |     0.667 |     0.480 | \n                     |     0.281 |     0.744 |           | \n                     |     0.160 |     0.320 |           | \n---------------------|-----------|-----------|-----------|\n          SottoMedia |        41 |        11 |        52 | \n                     |     4.354 |     5.771 |           | \n                     |     0.788 |     0.212 |     0.520 | \n                     |     0.719 |     0.256 |           | \n                     |     0.410 |     0.110 |           | \n---------------------|-----------|-----------|-----------|\n        Column Total |        57 |        43 |       100 | \n                     |     0.570 |     0.430 |           | \n---------------------|-----------|-----------|-----------|\n\n \nStatistics for All Table Factors\n\n\nPearson's Chi-squared test \n------------------------------------------------------------\nChi^2 =  21.09448     d.f. =  1     p =  4.371863e-06 \n\nPearson's Chi-squared test with Yates' continuity correction \n------------------------------------------------------------\nChi^2 =  19.27843     d.f. =  1     p =  1.129752e-05 \n\n \n\n\nCalcoliamo l’odds ratio per valutare le probabilità relative di CHD tra le due categorie di età.\n\nepitools::oddsratio(chdage$eta_categoria, chdage$chd, conf.level = 0.95, method = \"wald\") |&gt; print()\n\n$data\n            Outcome\nPredictor     0  1 Total\n  SopraMedia 16 32    48\n  SottoMedia 41 11    52\n  Total      57 43   100\n\n$measure\n            odds ratio with 95% C.I.\nPredictor     estimate      lower     upper\n  SopraMedia 1.0000000         NA        NA\n  SottoMedia 0.1341463 0.05475114 0.3286733\n\n$p.value\n            two-sided\nPredictor      midp.exact fisher.exact   chi.square\n  SopraMedia           NA           NA           NA\n  SottoMedia 4.582028e-06 6.194004e-06 4.371863e-06\n\n$correction\n[1] FALSE\n\nattr(,\"method\")\n[1] \"Unconditional MLE & normal approximation (Wald) CI\"\n\n\nCreiamo un modello di regressione logistica per stimare l’odds ratio, ponendo CHD come variabile dipendente e l’età categorizzata come variabile indipendente. Il coefficiente del modello sarà interpretato come log odds.\n\nlogit_model &lt;- glm(formula = chd ~ eta_categoria, data = chdage, family = binomial(link = \"logit\"))\nsummary(logit_model)\n\n\nCall:\nglm(formula = chd ~ eta_categoria, family = binomial(link = \"logit\"), \n    data = chdage)\n\nCoefficients:\n                        Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)               0.6931     0.3062   2.264   0.0236 *  \neta_categoriaSottoMedia  -2.0088     0.4572  -4.394 1.12e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 136.66  on 99  degrees of freedom\nResidual deviance: 114.77  on 98  degrees of freedom\nAIC: 118.77\n\nNumber of Fisher Scoring iterations: 4\n\n\nGeneriamo l’intervallo di confidenza al 95%\n\n\nconfint(logit_model)\n\n\nA matrix: 2 x 2 of type dbl\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n0.108547\n1.318141\n\n\neta_categoriaSottoMedia\n-2.942187\n-1.140921\n\n\n\n\n\nEsponenziazione dei coefficienti per ottenere l’Odds Ratio\n\nexp(coef(logit_model))     # Odds Ratio\nexp(confint(logit_model))  # 95% CI (Odds Ratio)\n\n(Intercept)1.99999999999959eta_categoriaSottoMedia0.134146341463534\n\n\n\nA matrix: 2 x 2 of type dbl\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n1.11465733\n3.7364701\n\n\neta_categoriaSottoMedia\n0.05275022\n0.3195247\n\n\n\n\n\nNella regressione logistica, il coefficiente \\(\\beta\\) per la variabile esplicativa dicotomica (in questo caso, l’età categorizzata) indica la variazione dei log odds di CHD per la categoria “SopraMedia” rispetto alla categoria “SottoMedia”. Esponenziando questo coefficiente, si ottiene l’odds ratio, che rappresenta la variazione relativa del rischio (o delle probabilità) di CHD tra le due categorie di età.\nRicordiamo che l’interpretazione deve tenere conto del significato statistico e della grandezza dell’odds ratio, oltre che della posizione dell’intervallo di confidenza rispetto al valore neutro 1. Se l’intervallo di confidenza attraversa 1, la differenza non è statisticamente significativa.\nUn odds ratio di 1.13 può essere interpretato nel modo seguente: passando dalla categoria ‘SottoMedia’ a quella ‘SopraMedia’, gli odds di malattie coronariche aumentano del 13%. L’intervallo di confidenza al 95% per l’odds ratio si estende da 1.05 a 1.32. Questo significa che siamo il 95% confidenti che il vero incremento degli odds di malattie coronariche, passando dalla categoria ‘SottoMedia’ a ‘SopraMedia’, sia compreso tra il 5% e il 32%.\nE2. Prendiamo in considerazione lo studio condotto da Cowles e Davis (1987). In questo studio, gli autori hanno intervistato un campione di 1421 studenti universitari per determinare se fossero disposti, in linea di principio, a partecipare a ulteriori ricerche. La variabile di risposta, chiamata volunteer, è una variabile dicotomica che indica se gli studenti erano disposti o meno a partecipare. Inoltre, hanno misurato i livelli di neuroticismo ed estroversione utilizzando l’Inventario della personalità di Eysenck. I dati relativi a questo studio sono disponibili nel data frame chiamato Cowles all’interno del pacchetto R chiamato effects.\nIl problema chiede di costruire un modello GLM (Generalized Linear Model) che possa prevedere la probabilità di essere disposti a partecipare (volunteer) in base al genere degli studenti, tenendo conto dei livelli di neuroticismo ed estroversione, nonché della loro interazione. Questo modello ci permetterà di comprendere come queste variabili influenzino la probabilità di partecipazione degli studenti a ulteriori ricerche.\nDopo aver costruito il modello, il problema chiede interpretare i risultati ottenuti per comprendere come il genere, il neuroticismo e l’estroversione influenzino la volontà degli studenti di partecipare a ulteriori ricerche.\n\ndata(Cowles)\ndim(Cowles)\n\n\n14214\n\n\n\nhead(Cowles)\n\n\nA data.frame: 6 x 4\n\n\n\nneuroticism\nextraversion\nsex\nvolunteer\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;fct&gt;\n&lt;fct&gt;\n\n\n\n\n1\n16\n13\nfemale\nno\n\n\n2\n8\n14\nmale\nno\n\n\n3\n5\n16\nmale\nno\n\n\n4\n8\n20\nfemale\nno\n\n\n5\n9\n19\nmale\nno\n\n\n6\n6\n15\nmale\nno\n\n\n\n\n\n\nCowles$sex &lt;- relevel(Cowles$sex, ref = \"male\")\ncontrasts(Cowles$sex)\n\n\nA matrix: 2 x 1 of type dbl\n\n\n\nfemale\n\n\n\n\nmale\n0\n\n\nfemale\n1\n\n\n\n\n\n\nfit_cowles &lt;- glm(\n    volunteer ~ sex + neuroticism*extraversion,\n    family = binomial,\n    data = Cowles\n)\nsummary(fit_cowles)\n\n\nCall:\nglm(formula = volunteer ~ sex + neuroticism * extraversion, family = binomial, \n    data = Cowles)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              -2.605359   0.500719  -5.203 1.96e-07 ***\nsexfemale                 0.247152   0.111631   2.214  0.02683 *  \nneuroticism               0.110777   0.037648   2.942  0.00326 ** \nextraversion              0.166816   0.037719   4.423 9.75e-06 ***\nneuroticism:extraversion -0.008552   0.002934  -2.915  0.00355 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1933.5  on 1420  degrees of freedom\nResidual deviance: 1897.4  on 1416  degrees of freedom\nAIC: 1907.4\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nplot(Effect(focal.predictors = c(\"neuroticism\",\"extraversion\",\"sex\"), \n            mod = fit_cowles,\n            xlevels=list(extraversion=seq(10, 20, 2), neuroticism=10:20)),\n     multiline = TRUE)\n\n\n\n\n\n\n\n\nCalcoliamo l’Odds Ratio:\n\nexp(coef(fit_cowles)) |&gt; print()\n\n             (Intercept)                sexfemale              neuroticism \n              0.07387658               1.28037375               1.11714535 \n            extraversion neuroticism:extraversion \n              1.18153740               0.99148400 \n\n\nCalcoliamo il 95% CI dell’Odds Ratio:\n\nexp(confint(fit_cowles)) |&gt; print()\n\n                              2.5 %    97.5 %\n(Intercept)              0.02723627 0.1943229\nsexfemale                1.02911058 1.5942774\nneuroticism              1.03815341 1.2034925\nextraversion             1.09828160 1.2735034\nneuroticism:extraversion 0.98575501 0.9971703\n\n\nGli odds ratio ottenuti dal modello di regressione logistica possono essere interpretati come segue: in questo campione di studio, gli odds di partecipare a ulteriori ricerche sono risultati essere 1.28 volte più alti per le femmine rispetto ai maschi, quando vengono controllati gli effetti del neuroticismo e dell’estroversione, nonché della loro interazione.\nL’intervallo di confidenza al 95% per l’odds ratio è compreso tra 1.03 e 1.59. Questo significa che siamo sicuri al 95% che il reale aumento degli odds di partecipare a ulteriori ricerche per le femmine rispetto ai maschi, una volta che si tengono in considerazione gli effetti del neuroticismo, dell’estroversione e della loro interazione, varia tra il 3% e il 59%.\nIn altre parole, ci sono evidenze statistiche significative che suggeriscono che il genere ha un impatto sulla volontà di partecipare a ulteriori ricerche, e tale impatto è positivo per le femmine rispetto ai maschi, anche quando si considerano le influenze dei livelli di neuroticismo ed estroversione e della loro interazione.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>64</span>  <span class='chapter-title'>✏️ Esercizi</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_rasch_model.html",
    "href": "chapters/irt/02_rasch_model.html",
    "title": "65  Modello di Rasch",
    "section": "",
    "text": "65.1 Introduzione\nLa psicometria ha conosciuto un’evoluzione importante con l’introduzione della Teoria della Risposta all’Item (IRT). Questo approccio rappresenta un progresso rispetto alla CTT, soprattutto nella gestione e nella concettualizzazione degli errori di misurazione. Nel presente capitolo verranno esplorate le basi dell’IRT, evidenziandone le peculiarità e i vantaggi rispetto ai modelli tradizionali.\nA differenza della CTT, che si concentra sull’analisi del punteggio totale ottenuto in un test, l’IRT focalizza l’attenzione sulle risposte ai singoli item. Ciò permette un’analisi più approfondita e precisa delle capacità di un individuo, considerando sia le caratteristiche specifiche degli item sia il livello di abilità della persona. Il nucleo dell’IRT è rappresentato dalla modellizzazione della probabilità che un individuo risponda correttamente a un determinato item, in funzione delle caratteristiche dell’item stesso e del livello di abilità del rispondente.\nUn aspetto distintivo dell’IRT è la sua flessibilità nel trattare risposte categoriali, una tipologia di dati particolarmente comune nei test psicometrici, dove i partecipanti selezionano tra opzioni predefinite.\nNonostante i suoi vantaggi, l’implementazione dell’IRT comporta alcune sfide, tra cui la necessità di disporre di campioni di dati di dimensioni considerevoli e di affrontare una maggiore complessità nei modelli statistici. In questo capitolo introdurremo i modelli IRT e le curve caratteristiche degli item (ICC), seguendo la trattazione proposta da Debelak et al. (2022).",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_rasch_model.html#curve-caratteristiche-degli-item",
    "href": "chapters/irt/02_rasch_model.html#curve-caratteristiche-degli-item",
    "title": "65  Modello di Rasch",
    "section": "65.2 Curve Caratteristiche degli Item",
    "text": "65.2 Curve Caratteristiche degli Item\nLa CTT offre un metodo per stimare la relazione tra gli item di un test e il costrutto psicologico sottostante. Ad esempio, con un approccio basato sulla CTT, si può calcolare la correlazione tra il punteggio di un singolo item e il punteggio totale del test. Sebbene utile, questa metodologia fornisce solo una stima approssimativa della relazione tra un item e il costrutto che si intende misurare. L’IRT consente un’analisi più raffinata.\nA differenza della CTT, che si concentra sul punteggio totale del test e la sua relazione con il costrutto, l’IRT si focalizza sulla relazione tra i singoli item e il costrutto. Questo approccio risponde a domande specifiche, come ad esempio: “Dato il livello di abilità di un individuo su un costrutto, qual è la probabilità che risponda VERO a un particolare item?”\nL’IRT è un modello che permette di rappresentare le variabili latenti. In questo framework, il livello latente di un individuo su un costrutto (denotato con il simbolo theta (θ)) viene stimato sulla base delle sue risposte ai singoli item. Theta rappresenta il livello latente di abilità, o la posizione di una persona lungo un continuum psicologico sottostante.\nUna delle rappresentazioni chiave dell’IRT è la Curva Caratteristica dell’Item (ICC). L’ICC è un grafico che descrive la probabilità stimata di fornire una risposta corretta (o di manifestare un determinato comportamento o sintomo) in funzione del livello di abilità latente di una persona. Solitamente, questa relazione è modellata utilizzando una funzione logistica, che produce una curva sigmoide: a bassi livelli di abilità la probabilità di rispondere correttamente è molto bassa, aumenta progressivamente a livelli intermedi e si appiattisce a livelli alti.\nLe ICC sono strumenti fondamentali per valutare la qualità degli item. Ad esempio, un item affetto da effetto soffitto (tutti rispondono correttamente, indipendentemente dal livello di abilità) o da effetto pavimento (nessuno risponde correttamente se non a livelli molto alti di abilità) tende a fornire poche informazioni utili per distinguere tra individui con diversi livelli di abilità. Analizzare le ICC consente quindi di identificare quali item contribuiscono in modo sostanziale alla misurazione del costrutto e quali potrebbero necessitare di revisione o eliminazione.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_rasch_model.html#la-scala-di-guttman",
    "href": "chapters/irt/02_rasch_model.html#la-scala-di-guttman",
    "title": "65  Modello di Rasch",
    "section": "65.3 La Scala di Guttman",
    "text": "65.3 La Scala di Guttman\nPer comprendere meglio il concetto di Curva Caratteristica dell’Item (ICC), è utile introdurre la Scala di Guttman, che stabilisce una relazione gerarchica tra la difficoltà degli item e le abilità degli individui. In una Scala di Guttman ideale, si assume che una persona con un determinato livello di abilità risponda correttamente a tutti gli item meno difficili e sbagli quelli più difficili.\n\n65.3.1 Esempio di Scala di Guttman Perfetta\nLa seguente tabella mostra un esempio di una Scala di Guttman perfetta per cinque item, dove:\n\n1 indica una risposta corretta.\n0 indica una risposta errata.\n\n\n\n\n\n\n\n\n\n\n\n\nPattern di risposta\nItem 1\nItem 2\nItem 3\nItem 4\nItem 5\n\n\n\n\n1\n0\n0\n0\n0\n0\n\n\n2\n1\n0\n0\n0\n0\n\n\n3\n1\n1\n0\n0\n0\n\n\n4\n1\n1\n1\n0\n0\n\n\n5\n1\n1\n1\n1\n0\n\n\n6\n1\n1\n1\n1\n1\n\n\n\nIn questo modello ideale, le risposte corrette si accumulano man mano che il livello di abilità dell’individuo aumenta.\n\n\n65.3.2 Rappresentazione Grafica della Scala di Guttman\nGraficamente, la Scala di Guttman può essere rappresentata tramite le curve caratteristiche degli item (ICC), che mostrano:\n\nAsse verticale: la probabilità di rispondere correttamente a un item. Nella Scala di Guttman ideale, questa probabilità è binaria: 1 (risposta corretta) o 0 (risposta errata).\nAsse orizzontale: il livello di abilità dell’individuo.\n\nIn un test ideale basato su questa scala, un individuo risponde correttamente a tutti gli item con difficoltà inferiore o uguale al proprio livello di abilità, mentre sbaglia quelli con difficoltà superiore. Questo crea un modello di risposta scalare e prevedibile.\nLe frecce nel grafico sottostante rappresentano cinque individui con diversi livelli di abilità. Ogni freccia indica il punto in cui l’abilità di una persona interseca le curve caratteristiche degli item. Secondo il modello ideale, ogni persona dovrebbe rispondere correttamente a tutti gli item posizionati a sinistra della propria abilità sul grafico (item meno difficili) e sbagliare quelli a destra (item più difficili).\n\n\n\n\n\n\n\n\n\nSebbene la Scala di Guttman sia un modello teorico utile, raramente si osserva nei dati reali a causa di fattori come:\n\nVariabilità individuale: Le risposte possono variare anche tra persone con lo stesso livello di abilità.\nErrori di misurazione: Gli item possono essere mal formulati o interpretabili in modo ambiguo.\nInfluenza di altri fattori: Motivazione, attenzione e contesto possono influire sulle risposte.\n\nNonostante questi limiti, la Scala di Guttman rappresenta una base concettuale importante per comprendere la relazione gerarchica tra difficoltà degli item e abilità degli individui, servendo da ponte per lo sviluppo di modelli più complessi come quelli utilizzati nella IRT.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_rasch_model.html#il-modello-di-rasch",
    "href": "chapters/irt/02_rasch_model.html#il-modello-di-rasch",
    "title": "65  Modello di Rasch",
    "section": "65.4 Il Modello di Rasch",
    "text": "65.4 Il Modello di Rasch\nIl modello di Rasch rappresenta un’evoluzione rispetto allo scaling di Guttman, superandone i limiti. Ad esempio, consideriamo la competenza matematica, vista come una variabile latente: un tratto non direttamente osservabile che può essere inferito attraverso comportamenti misurabili. Supponiamo di utilizzare un test con cinque domande per valutare la competenza matematica. Le risposte a queste domande costituiscono le nostre osservazioni misurabili.\n\n65.4.1 Concetti Chiave del Modello di Rasch\nSecondo il modello di Rasch, sia la difficoltà delle domande del test sia le abilità dei partecipanti sono rappresentate lungo un unico continuum latente. Le abilità più elevate si trovano verso l’estremità superiore, mentre quelle più basse si collocano verso l’estremità inferiore. Le domande del test sono disposte lungo questo continuum in base alla loro difficoltà, denotata da \\(\\beta_i\\) per il singolo item \\(i\\). Analogamente, la competenza matematica di un individuo è rappresentata da \\(\\theta\\), che indica la sua posizione sul continuum.\nUn individuo con abilità \\(\\theta = 0\\) avrà alta probabilità di rispondere correttamente a domande facili (\\(\\beta = -2\\)) e una probabilità decrescente di rispondere correttamente a domande più difficili (\\(\\beta &gt; 0\\)). Quando la difficoltà di un item si avvicina al livello di abilità dell’individuo (\\(\\beta \\approx \\theta\\)), la probabilità di una risposta corretta o errata diventa quasi equiprobabile, riflettendo una stima probabilistica.\nLa distanza tra la competenza dell’individuo (\\(\\theta\\)) e la difficoltà dell’item (\\(\\beta\\)) è cruciale: maggiore è la distanza, più certa sarà la risposta prevista (corretta o errata). Una distanza ridotta implica maggiore incertezza, migliorando la capacità del modello di cogliere sfumature nel comportamento di risposta.\n\nEsercizio 65.1 Consideriamo i dati di un test con cinque domande, i cui risultati (risposte corrette o errate) sono riportati di seguito:\n\nmath_dat &lt;- rio::import(here::here(\"data\", \"deAyala\", \"Math.txt\"))\nhead(math_dat)\n\n\nA data.frame: 6 x 5\n\n\n\nV1\nV2\nV3\nV4\nV5\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n1\n1\n0\n0\n0\n\n\n2\n1\n1\n1\n0\n0\n\n\n3\n1\n0\n0\n0\n0\n\n\n4\n1\n1\n1\n0\n0\n\n\n5\n1\n0\n1\n1\n0\n\n\n6\n1\n1\n1\n0\n0\n\n\n\n\n\nCalcoliamo la media delle risposte corrette per ogni domanda per stimare la difficoltà degli item:\n\ncolMeans(math_dat) |&gt; print()\n\n   V1    V2    V3    V4    V5 \n0.887 0.644 0.566 0.427 0.387 \n\n\nLe medie ottenute mostrano che gli item sono ordinati per difficoltà crescente: \\(V1\\) è il più facile e \\(V5\\) il più difficile. Ora analizziamo la proporzione di risposte corrette in funzione del punteggio totale di ogni partecipante:\n\n# Calcolo dei punteggi totali\nmath_dat2 &lt;- math_dat\nmath_dat2$total_score &lt;- rowSums(math_dat2[, -1])\n\n# Preparazione dati per il grafico\nplot_data &lt;- lapply(names(math_dat2)[1:5], function(item) {\n    math_dat2 %&gt;%\n        group_by(total_score) %&gt;%\n        summarise(proportion = mean(get(item) == 1)) %&gt;%\n        mutate(item = item)\n})\n\nplot_data &lt;- do.call(rbind, plot_data)\n\n# Creazione del grafico\nggplot(plot_data, aes(x = total_score, y = proportion, group = item, color = item)) +\n    geom_line(linewidth = 1.5) +\n    labs(\n        x = \"Punteggio Totale\",\n        y = \"Proporzione di Risposte Corrette\",\n        title = \"Proporzione di Risposte Corrette in Base al Punteggio Totale\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n65.4.2 Curve Caratteristiche degli Item (ICC)\nL’analisi mostra che la probabilità di rispondere correttamente a un item aumenta con la competenza generale del partecipante. Questo risultato è coerente con il modello di Rasch, che postula che la difficoltà degli item e l’abilità dei partecipanti influenzino esclusivamente la probabilità di risposta corretta.\nLe curve caratteristiche degli item (ICC) forniscono una rappresentazione visiva di questa relazione. Il modello di Rasch utilizza una funzione logistica per modellare la probabilità \\(P(X_{pi} = 1)\\) che un partecipante \\(p\\) con abilità \\(\\theta_p\\) risponda correttamente all’item \\(i\\) con difficoltà \\(\\beta_i\\):\n\\[\nP(X_{pi} = 1 | \\theta_p, \\beta_i) = \\frac{e^{\\theta_p - \\beta_i}}{1 + e^{\\theta_p - \\beta_i}} = \\frac{1}{1 + e^{-(\\theta_p - \\beta_i)}}.\n\\tag{65.1}\\]\nQuesta funzione mostra che la probabilità di una risposta corretta dipende dalla differenza \\(\\theta_p - \\beta_i\\), che rappresenta la distanza tra abilità e difficoltà sul continuum.\n\n\n65.4.3 Interpretazione Grafica\nNel grafico seguente, ogni curva sigmoide rappresenta un item. La parallelità delle curve conferma che nel modello di Rasch solo la difficoltà degli item (\\(\\beta_i\\)) varia, mantenendo costante la pendenza.\n\nrasch_model &lt;- rasch(math_dat)\nplot(rasch_model, type = \"ICC\")\n\n\n\n\n\n\n\n\nLe curve mostrano che:\n\nItem facili (\\(\\beta_i &lt; 0\\)) hanno alte probabilità di risposta corretta per partecipanti meno abili.\nItem difficili (\\(\\beta_i &gt; 0\\)) richiedono abilità superiori per una risposta corretta.\n\nIl modello di Rasch dell’Equazione 65.1 calcola la probabilità di una risposta corretta all’item \\(i\\) basandosi sulla posizione relativa sul continuum di abilità tra la persona \\(p\\) e l’item \\(i\\) (ovvero, in base alla differenza \\(\\theta_p - \\beta_i\\)). La probabilità che una persona con un certo livello di abilità \\(\\theta_p\\) risponda correttamente all’item \\(i\\) dipende dalla differenza tra l’abilità del rispondente \\(\\theta_p\\) e la difficoltà dell’item \\(\\beta_i\\). Maggiore è questa differenza a favore dell’abilità, più alta sarà la probabilità di successo. In termini semplici, l’equazione afferma che la probabilità di una risposta corretta è funzione della distanza tra la posizione dell’individuo \\(p\\) (\\(\\theta_p\\)) e quella dell’item \\(i\\) (\\(\\beta_i\\)). L’equazione trasforma questa distanza, che potrebbe teoricamente variare da \\(-\\infty\\) a \\(+\\infty\\), in una probabilità confinata nell’intervallo [0, 1].\nÈ importante sottolineare che, sebbene teoricamente le posizioni degli item (\\(\\beta_i\\)) e delle persone (\\(\\theta_p\\)) possano estendersi da \\(-\\infty\\) a \\(+\\infty\\), nella pratica esse si collocano generalmente tra -3 e +3. Nel contesto di test di competenza, queste posizioni rappresentano i vari livelli di difficoltà: item con valori inferiori a 0.0 sono considerati “facili” (ad esempio, -2.0), mentre quelli con valori superiori a 0.0 sono “difficili” (esempio, +2.0). Gli item intorno allo 0.0 sono di difficoltà intermedia e, mentre gli item “facili” sono spesso risolti correttamente anche da persone meno abili, quelli “difficili” tendono a essere superati solo da individui più competenti.\n\n\n65.4.4 Rappresentazioni Alternative della Funzione Logistica\nLa funzione logistica utilizzata nel modello di Rasch può essere scritta in due modi: con la funzione esponenziale sia al numeratore sia al denominatore (a sinistra), oppure equivalentemente con la funzione esponenziale solo al denominatore, seguita dal suo argomento negativo (a destra):\n\\[\n\\frac{\\exp(\\theta_p - \\beta_i)}{1 + \\exp(\\theta_p - \\beta_i)} = \\frac{1}{1 + \\exp(-(\\theta_p - \\beta_i))}\n\\]\nPer dimostrare l’equivalenza delle due espressioni della funzione logistica nel modello di Rasch, seguiamo i seguenti passaggi algebrici. Per semplificare il lato destro, utilizziamo la proprietà dell’esponenziale che afferma \\(e^{-x} = \\frac{1}{e^x}\\). Quindi, riscriviamo \\(\\exp(-(\\theta_p - \\beta_i))\\) come \\(\\frac{1}{\\exp(\\theta_p - \\beta_i)}\\):\n\\[ \\frac{1}{1 + \\frac{1}{\\exp(\\theta_p - \\beta_i)}} \\]\nIl denominatore del lato destro diventa \\(1 + \\frac{1}{\\exp(\\theta_p - \\beta_i)}\\). Per combinare i termini nel denominatore, otteniamo un denominatore comune:\n\\[ \\frac{1}{\\frac{\\exp(\\theta_p - \\beta_i) + 1}{\\exp(\\theta_p - \\beta_i)}} \\]\nSimplificando ulteriormente, il denominatore diventa \\(\\exp(\\theta_p - \\beta_i) + 1\\), quindi l’intera espressione diventa:\n\\[ \\frac{1}{\\exp(\\theta_p - \\beta_i) + 1} \\]\nPossiamo ora invertire la frazione per ottenere il lato sinistro dell’equazione originale:\n\\[ \\frac{\\exp(\\theta_p - \\beta_i)}{\\exp(\\theta_p - \\beta_i) + 1} \\]\nQuindi, abbiamo dimostrato che il lato sinistro e il lato destro dell’equazione originale sono effettivamente equivalenti.\n\nEsercizio 65.2 Per illustrare come il modello di Rasch venga utilizzato per calcolare i punti su una curva caratteristica dell’item, consideriamo il seguente problema. I valori dei parametri dell’item sono:\n\na = 1 è il parametro di discriminazione dell’item,\nb = -0.5 è il parametro di difficoltà dell’item.\n\nTroviamo la probabilità di rispondere correttamente a questo item al livello di abilità theta = 1.5.\n\nicc &lt;- function(a, b, theta) {\n    1 / (1 + exp(-a * (theta - b)))\n}\n\na = 1\nb = -0.5\ntheta = 1.5\nicc(a, b, theta)\n\n0.880797077977882\n\n\n\ntheta_range &lt;- seq(-3, 3, .1)\nplot(theta_range, icc(a, b, theta_range),\n    type = \"l\", xlim = c(-3, 3), ylim = c(0, 1),\n    xlab = \"Ability\", ylab = \"Probability of Correct Response\"\n)\npoints(theta, icc(a, b, theta), cex=2)\nsegments(-3, icc(a, b, theta), theta, icc(a, b, theta), lty = \"dashed\")\nsegments(theta, icc(a, b, theta), theta, 0, lty = \"dashed\")",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_rasch_model.html#la-proprietà-di-oggettività-specifica",
    "href": "chapters/irt/02_rasch_model.html#la-proprietà-di-oggettività-specifica",
    "title": "65  Modello di Rasch",
    "section": "65.5 La Proprietà di Oggettività Specifica",
    "text": "65.5 La Proprietà di Oggettività Specifica\nUna delle caratteristiche fondamentali del modello di Rasch è la proprietà di oggettività specifica, che afferma che la differenza tra il logit della probabilità di rispondere correttamente a due diversi item, \\(i\\) e \\(j\\), è costante per ogni livello di abilità \\(\\theta\\). Questo implica che, indipendentemente dall’abilità di un rispondente, il confronto tra due item rimane stabile, riflettendo un principio fondamentale di misurazione oggettiva nel modello di Rasch.\nNel modello di Rasch, la funzione logistica trasforma i logit — i logaritmi delle quote di probabilità di una risposta corretta rispetto a una errata — in probabilità effettive. I logit sono definiti come \\(\\theta_p - \\beta_i\\), dove \\(\\theta_p\\) rappresenta il livello di abilità di una persona e \\(\\beta_i\\) la difficoltà dell’item. Questa trasformazione è cruciale per interpretare i dati raccolti nei test.\nMatematicamente, il logit della probabilità di una risposta corretta a un item è espresso dalla seguente formula:\n\\[\n\\log \\left( \\frac{\\text{Pr}(U_{pi} = 1 | \\theta_p, \\beta_i)}{\\text{Pr}(U_{pi} = 0 | \\theta_p, \\beta_i)} \\right) = \\theta_p - \\beta_i.\n\\]\nLa probabilità \\(\\pi\\) di risposta corretta è calcolata così:\n\\[\n\\pi = \\frac{\\exp(\\theta_p - \\beta_i)}{1 + \\exp(\\theta_p - \\beta_i)},\n\\]\ne il suo complemento, \\(1 - \\pi\\), è dato da:\n\\[\n1 - \\pi = \\frac{1}{1 + \\exp(\\theta_p - \\beta_i)}.\n\\]\nDefinendo le quote \\(O\\) come \\(\\exp(\\theta_p - \\beta_i)\\), otteniamo una forma semplice per il logaritmo delle quote:\n\\[\n\\log(O) = \\theta_p - \\beta_i.\n\\]\nQuesta formula dimostra che i logit sono direttamente proporzionali alla differenza tra l’abilità del rispondente e la difficoltà dell’item. Un incremento in questa differenza aumenta la probabilità di una risposta corretta, riflettendo che valori più alti sui logit indicano un vantaggio dell’abilità rispetto alla difficoltà dell’item.\n\n65.5.1 Implicazioni della Proprietà di Oggettività Specifica\nL’oggettività specifica nel modello di Rasch significa che il confronto tra due item è indipendente dall’abilità dei rispondenti. Nella pratica, ciò si traduce nel fatto che le curve caratteristiche degli item (ICC) per diversi item sono parallele lungo la scala dei logit, poiché la differenza \\(\\theta_p - \\beta_i\\) è costante tra gli item. Le curve per item con diverse difficoltà si intersecano a livelli diversi sull’asse verticale ma mantengono una pendenza costante, illustrando che ogni differenza nelle probabilità di risposta tra due item è esclusivamente attribuibile alle loro difficoltà intrinseche e non varia con il cambiamento dell’abilità dei partecipanti.\nIn sintesi, la rappresentazione logit fornisce non solo un modo per calcolare la probabilità di una risposta corretta, ma anche un metodo robusto per mantenere la comparabilità e la coerenza delle misure tra diversi item, garantendo così una valutazione equa e precisa delle abilità del rispondente.\n\n# Creazione di un dataframe con i valori di abilità (theta_p) e le difficoltà degli item (beta)\ntheta_p &lt;- seq(-3, 3, length.out = 100)\nbeta_i &lt;- -1\nbeta_j &lt;- 1\n\n# Calcolo dei logit per gli item i e j\nlogit_i &lt;- theta_p - beta_i\nlogit_j &lt;- theta_p - beta_j\n\ndata &lt;- data.frame(\n    Ability = c(theta_p, theta_p),\n    Logit = c(logit_i, logit_j),\n    Item = factor(c(rep(\"Item i (beta_i = -1)\", length(theta_p)), rep(\"Item j (beta_j = 1)\", length(theta_p))))\n)\n\nggplot(data, aes(x = Ability, y = Logit, color = Item)) +\n    geom_line() +\n    scale_color_manual(values = c(\"blue\", \"red\")) +\n    ggtitle(TeX(\"Parallel Lines for Item i and j in the Rasch Model\")) +\n    xlab(TeX(\"Ability ($\\\\theta_p$)\")) +\n    ylab(TeX(\"Logit Probability\"))",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_rasch_model.html#il-modello-di-rasch-e-lanalisi-fattoriale",
    "href": "chapters/irt/02_rasch_model.html#il-modello-di-rasch-e-lanalisi-fattoriale",
    "title": "65  Modello di Rasch",
    "section": "65.6 Il Modello di Rasch e l’Analisi Fattoriale",
    "text": "65.6 Il Modello di Rasch e l’Analisi Fattoriale\nPer una migliore comprensione del Modello di Rasch, è utile confrontarlo con l’Analisi Fattoriale. Nonostante le loro differenze come metodologie statistiche, entrambi cercano un obiettivo comune: comprendere le dimensioni latenti sottostanti alle risposte osservate nei dati. Questo parallelo aiuta a mettere in luce le somiglianze e le differenze tra i due approcci.\nNell’Analisi Fattoriale, il modello tipico è espresso come \\(Y_i = \\lambda_i \\xi + \\delta_i\\), dove \\(Y_i\\) è il punteggio osservato per l’item i-esimo, \\(\\lambda_i\\) rappresenta la saturazione fattoriale che indica quanto l’item è influenzato dal fattore latente \\(\\xi\\), e \\(\\delta_i\\) è il termine di errore specifico per quell’item. L’idea centrale è che, controllando per \\(\\xi\\), le correlazioni tra gli item \\(Y_i\\) diventano nulle, poiché qualsiasi associazione comune è spiegata dal fattore latente.\nD’altra parte, il Modello di Rasch adotta un approccio leggermente diverso, ma con lo stesso obiettivo fondamentale: identificare e gestire l’influenza di una dimensione latente (spesso chiamata abilità) sulle risposte agli item. In questo contesto, si considerano risposte dicotomiche (0 o 1), e si presume che la probabilità di una risposta corretta a un item sia una funzione logistica dell’abilità del rispondente \\(\\theta\\) e della difficoltà dell’item \\(\\delta_i\\).\nLa principale differenza tra il Modello di Rasch e l’Analisi Fattoriale risiede nella formulazione dei parametri. Mentre l’Analisi Fattoriale stima le saturazioni fattoriali per ciascun item, il Modello di Rasch assume che tutti gli item abbiano lo stesso potere discriminante, cioè sono ugualmente efficaci nel distinguere tra rispondenti con diversi livelli di abilità. Invece di concentrarsi sulle saturazioni fattoriali, il Modello di Rasch si concentra sulla stima dell’abilità dei rispondenti \\(\\theta\\) e sulla difficoltà degli item \\(\\delta_i\\), presupponendo che gli item siano equivalenti in termini di discriminazione.\nIn conclusione, sia il Modello di Rasch che l’Analisi Fattoriale mirano a isolare e controllare l’effetto di una dimensione latente sull’associazione tra gli item, cercando di spiegare le risposte osservate attraverso questa dimensione. Mentre l’Analisi Fattoriale si concentra sulla stima delle saturazioni fattoriali e sull’identificazione di fattori latenti comuni tra gli item, il Modello di Rasch si focalizza sulla stima dell’abilità dei rispondenti e sulla difficoltà degli item, fornendo un quadro dettagliato delle dinamiche che influenzano le risposte agli item.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_rasch_model.html#considerazioni-conclusive",
    "href": "chapters/irt/02_rasch_model.html#considerazioni-conclusive",
    "title": "65  Modello di Rasch",
    "section": "65.7 Considerazioni Conclusive",
    "text": "65.7 Considerazioni Conclusive\nIl modello di Rasch si distingue notevolmente dalla Teoria Classica dei Test (CTT) per diversi aspetti fondamentali. In primo luogo, il modello di Rasch consente un’analisi dettagliata sia a livello di singolo item sia per l’intero strumento di misurazione. Questo contrasta con la CTT, che si concentra principalmente sull’analisi aggregata dell’intero test, esprimendo il punteggio totale con la formula \\(X = T + E\\), dove \\(T\\) indica l’abilità vera e \\(E\\) l’errore. Il modello di Rasch, invece, modella la probabilità di una risposta corretta per ogni item specifico, seguendo l’approccio focalizzato sugli item proposto da Guttman.\nL’approccio del modello di Rasch rappresenta una deviazione significativa dalla CTT poiché analizza le risposte osservate piuttosto che semplicemente sommarle. Questo cambio di paradigma offre numerosi vantaggi rispetto alla CTT:\nPrecisione e Dettaglio: L’analisi item per item nel modello di Rasch fornisce una comprensione molto più fine di come ciascun item funziona individualmente, permettendo di identificare specifiche aree di forza o debolezza sia nei test che nei rispondenti. Questo livello di dettaglio contribuisce a una misurazione più accurata e a una riduzione degli errori di misurazione.\nSeparazione tra Attributi della Persona e Caratteristiche dell’Item: Un vantaggio chiave del modello di Rasch rispetto alla CTT è la netta distinzione tra l’abilità della persona (\\(\\theta_p\\)) e la difficoltà dell’item (\\(b_i\\)). Nel modello di Rasch, la difficoltà degli item è considerata una proprietà costante, indipendente dal gruppo di rispondenti, il che aumenta notevolmente la precisione e la flessibilità nella misurazione.\nFlessibilità nei Modelli di Risposta: Il modello di Rasch può adattarsi a diversi tipi di formati di domanda, inclusi quelli a scelta multipla, le scale Likert, e le domande aperte, permettendo un’analisi complessiva che cattura meglio la cognizione e il comportamento umano.\nValutazione Adattiva: L’IRT, e in particolare il modello di Rasch, permette valutazioni personalizzate basate sul livello di abilità dei rispondenti. Questo approccio riduce gli errori di misurazione, fornendo informazioni più precise e utili.\nAnalisi Approfondita degli Item: Il modello di Rasch consente un’analisi dettagliata degli item, valutando aspetti come la discriminazione, la difficoltà e i parametri di indovinamento. Queste informazioni sono essenziali per il miglioramento continuo degli item e dei test.\nSebbene il modello di Rasch sia ampiamente apprezzato per la sua eleganza matematica e la sua applicazione pratica, è anche oggetto di critiche per le sue assunzioni relativamente restrittive, che alcuni studiosi ritengono limitino la sua capacità di catturare la complessità delle risposte agli item in scenari reali. Tuttavia, le sue assunzioni di base fornicono un quadro robusto per l’analisi dei dati, sostenendo l’invarianza delle proprietà degli item e delle abilità delle persone attraverso diversi contesti e gruppi di rispondenti.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/02_rasch_model.html#session-info",
    "href": "chapters/irt/02_rasch_model.html#session-info",
    "title": "65  Modello di Rasch",
    "section": "65.8 Session Info",
    "text": "65.8 Session Info\n\nsessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS 15.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] grid      stats4    stats     graphics  grDevices utils     datasets \n[8] methods   base     \n\nother attached packages:\n [1] ggmirt_0.1.0      latex2exp_0.9.6   TAM_4.2-21        CDM_8.2-6        \n [5] mvtnorm_1.3-1     ltm_1.2-0         polycor_0.8-1     msm_1.8          \n [9] MASS_7.3-61       mirt_1.42         lattice_0.22-6    ggokabeito_0.1.0 \n[13] viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1   \n[17] bayesplot_1.11.1  gridExtra_2.3     patchwork_1.3.0   semTools_0.5-6   \n[21] semPlot_1.1.6     lavaan_0.6-18     psych_2.4.6.26    scales_1.3.0     \n[25] markdown_1.13     knitr_1.48        lubridate_1.9.3   forcats_1.0.0    \n[29] stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2       readr_2.1.5      \n[33] tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n[37] here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.1        later_1.3.2          pbdZMQ_0.3-13       \n  [4] R.oo_1.26.0          XML_3.99-0.17        rpart_4.1.23        \n  [7] lifecycle_1.0.4      rstatix_0.7.2        rprojroot_2.0.4     \n [10] globals_0.16.3       rockchalk_1.8.157    backports_1.5.0     \n [13] magrittr_2.0.3       openxlsx_4.2.7.1     Hmisc_5.1-3         \n [16] rmarkdown_2.28       httpuv_1.6.15        qgraph_1.9.8        \n [19] zip_2.3.1            sessioninfo_1.2.2    pbapply_1.7-2       \n [22] minqa_1.2.8          multcomp_1.4-26      abind_1.4-8         \n [25] audio_0.1-11         expm_1.0-0           quadprog_1.5-8      \n [28] R.utils_2.12.3       nnet_7.3-19          TH.data_1.1-2       \n [31] sandwich_3.1-1       listenv_0.9.1        testthat_3.2.1.1    \n [34] RPushbullet_0.3.4    vegan_2.6-8          arm_1.14-4          \n [37] parallelly_1.38.0    permute_0.9-7        codetools_0.2-20    \n [40] tidyselect_1.2.1     farver_2.1.2         lme4_1.1-35.5       \n [43] base64enc_0.1-3      jsonlite_1.8.9       progressr_0.14.0    \n [46] Formula_1.2-5        survival_3.7-0       emmeans_1.10.4      \n [49] tools_4.4.1          rio_1.2.2            snow_0.4-4          \n [52] Rcpp_1.0.13          glue_1.7.0           mnormt_2.1.1        \n [55] admisc_0.36          xfun_0.47            mgcv_1.9-1          \n [58] IRdisplay_1.1        withr_3.0.1          beepr_2.0           \n [61] fastmap_1.2.0        boot_1.3-31          fansi_1.0.6         \n [64] digest_0.6.37        mi_1.1               timechange_0.3.0    \n [67] R6_2.5.1             mime_0.12            estimability_1.5.1  \n [70] colorspace_2.1-1     gtools_3.9.5         jpeg_0.1-10         \n [73] R.methodsS3_1.8.2    utf8_1.2.4           generics_0.1.3      \n [76] data.table_1.16.0    corpcor_1.6.10       SimDesign_2.17.1    \n [79] htmlwidgets_1.6.4    pkgconfig_2.0.3      sem_3.1-16          \n [82] gtable_0.3.5         brio_1.1.5           htmltools_0.5.8.1   \n [85] carData_3.0-5        png_0.1-8            rstudioapi_0.16.0   \n [88] tzdb_0.4.0           reshape2_1.4.4       uuid_1.2-1          \n [91] coda_0.19-4.1        checkmate_2.3.2      nlme_3.1-166        \n [94] curl_5.2.3           nloptr_2.1.1         repr_1.1.7          \n [97] zoo_1.8-12           parallel_4.4.1       miniUI_0.1.1.1      \n[100] foreign_0.8-87       pillar_1.9.0         vctrs_0.6.5         \n[103] promises_1.3.0       car_3.1-2            OpenMx_2.21.12      \n[106] xtable_1.8-4         Deriv_4.1.6          cluster_2.1.6       \n[109] dcurver_0.9.2        GPArotation_2024.3-1 htmlTable_2.4.3     \n[112] evaluate_1.0.0       pbivnorm_0.6.0       cli_3.6.3           \n[115] kutils_1.73          compiler_4.4.1       rlang_1.1.4         \n[118] crayon_1.5.3         future.apply_1.11.2  ggsignif_0.6.4      \n[121] labeling_0.4.3       fdrtool_1.2.18       plyr_1.8.9          \n[124] stringi_1.8.4        munsell_0.5.1        lisrelToR_0.3       \n[127] pacman_0.5.1         Matrix_1.7-0         IRkernel_1.3.2      \n[130] hms_1.1.3            glasso_1.11          future_1.34.0       \n[133] shiny_1.9.1          igraph_2.0.3         broom_1.0.6         \n[136] RcppParallel_5.1.9  \n\n\n\n\n\n\nDebelak, R., Strobl, C., & Zeigenfuse, M. D. (2022). An introduction to the rasch model with examples in r. CRC Press.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_assumptions.html",
    "href": "chapters/irt/03_assumptions.html",
    "title": "66  Assunzioni e Proprietà del Modello di Rasch",
    "section": "",
    "text": "66.1 Introduzione\nIn questo capitolo, esaminiamo le proprietà importanti del modello di Rasch. Queste proprietà sono il motivo per cui il modello di Rasch è così teoricamente attraente e hanno portato al suo ampio utilizzo. Forse la proprietà più importante è il fatto che permette una misurazione oggettiva dei tratti latenti.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_assumptions.html#statistiche-sufficienti-definizione-e-applicazioni",
    "href": "chapters/irt/03_assumptions.html#statistiche-sufficienti-definizione-e-applicazioni",
    "title": "66  Assunzioni e Proprietà del Modello di Rasch",
    "section": "66.2 Statistiche Sufficienti: Definizione e Applicazioni",
    "text": "66.2 Statistiche Sufficienti: Definizione e Applicazioni\nUna statistica è definita come una funzione dei dati osservati e serve comunemente a riassumere caratteristiche salienti di un insieme di dati. Un esempio chiaro è la media campionaria, calcolata come:\n\\[ \\bar{x} = \\frac{1}{P} \\sum_{p=1}^{P} x_p, \\]\ndove \\(\\bar{x}\\) rappresenta la media dei valori \\(x_p\\) per individui da 1 a \\(P\\). Questa statistica è frequentemente impiegata per stimare il valore atteso di una popolazione, dato che sintetizza le informazioni riguardanti la media del campione.\nEsistono altre statistiche oltre a \\(\\bar{x}\\), ad esempio, si potrebbe utilizzare la media di soli alcuni valori selezionati, come \\(x^* = \\frac{1}{3} (x_1 + x_3 + x_5)\\). Tuttavia, questa alternativa, nonostante sia un valido stimatore, risulta generalmente meno accurata di \\(\\bar{x}\\) perché non considera tutti i valori del campione, come \\(x_2, x_4\\), ecc.\nIl concetto di statistica sufficiente emerge quando una statistica, come \\(\\bar{x}\\), cattura completamente tutte le informazioni necessarie sul parametro di interesse (in questo caso, la media) contenute nel campione. Le statistiche sufficienti sono particolarmente potenti perché, una volta conosciute, rendono superflue le informazioni aggiuntive fornite dai dati individuali.\n\n66.2.1 Applicazioni nel Modello di Rasch\nNel contesto del modello di Rasch, utilizzato in teoria della risposta agli item (IRT), si identificano statistiche sufficienti specifiche per diversi parametri:\n\nStatistica Sufficiente per \\(\\theta_p\\): Per il parametro di abilità di una persona \\(\\theta_p\\), la statistica sufficiente è il punteggio totale \\(r_p\\), ottenuto sommando tutte le risposte corrette della persona \\(p\\) ai vari item. Questo punteggio riflette direttamente l’abilità della persona senza necessità di analizzare le risposte a ogni singolo item.\nStatistica Sufficiente per \\(\\beta_i\\): Analogamente, per il parametro di difficoltà dell’item \\(\\beta_i\\), la statistica sufficiente è data dal totale delle risposte corrette \\(c_i\\) a quell’item da parte di tutte le persone. Questa somma rappresenta la difficoltà intrinseca dell’item.\n\nSecondo il modello di Rasch, la probabilità che una persona \\(p\\) risponda correttamente all’item \\(i\\) è modellata tramite una funzione logistica della differenza tra abilità e difficoltà:\n\\[\nP(Y_{pi} = 1 | \\theta_p, \\beta_i) = \\frac{e^{(\\theta_p - \\beta_i)}}{1 + e^{(\\theta_p - \\beta_i)}}.\n\\]\nIn questo modello, il punteggio totale \\(r_p\\) e il numero totale di risposte corrette \\(c_i\\) diventano essenziali, poiché la distribuzione congiunta delle risposte corrette può essere completamente descritta da questi totali. Inoltre, l’indipendenza condizionale delle risposte, data l’abilità di una persona o la difficoltà di un item, supporta l’utilizzo dei punteggi totali come statistiche sufficienti.\nQueste proprietà del modello di Rasch evidenziano come le statistiche sufficienti, quali \\(r_p\\) e \\(c_i\\), incorporino efficacemente tutte le informazioni necessarie per stimare i parametri di interesse, semplificando così il processo di analisi statistica.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_assumptions.html#considerazioni-conclusive-sul-modello-di-rasch",
    "href": "chapters/irt/03_assumptions.html#considerazioni-conclusive-sul-modello-di-rasch",
    "title": "66  Assunzioni e Proprietà del Modello di Rasch",
    "section": "66.3 Considerazioni Conclusive sul Modello di Rasch",
    "text": "66.3 Considerazioni Conclusive sul Modello di Rasch\nIl modello di Rasch si basa su tre fondamentali assunzioni che ne garantiscono la validità e l’applicabilità: unidimensionalità, monotonicità e indipendenza locale.\n\nUnidimensionalità: Quest’assunzione presuppone che le risposte agli item di un test siano influenzate principalmente da un unico tratto latente o dimensione attributiva. Questo significa che il modello assume la predominanza di una sola caratteristica o abilità nel determinare le risposte, rendendo problematica la presenza di dimensioni multiple che richiederebbero un’analisi in uno spazio multidimensionale.\nMonotonicità: L’assunzione di monotonicità stabilisce che con l’incremento del tratto latente (\\(\\theta\\)), aumenta anche la probabilità di una risposta corretta. Ciò si allinea con l’intuizione generale nella misurazione: individui con un livello più elevato del tratto latente tendono a ottenere punteggi migliori nei test.\nIndipendenza Locale: Questa assunzione afferma che, controllato il tratto latente, non dovrebbero esistere correlazioni tra le risposte a due item distinti. Eventuali associazioni osservate tra risposte a item diversi sono attribuibili unicamente al tratto latente. In altre parole, la risposta a un item non deve essere influenzata da o influenzare la risposta a un altro item, una volta controllato per il tratto latente.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_assumptions.html#indipendenza-stocastica-locale-e-applicazioni-nel-modello-di-rasch",
    "href": "chapters/irt/03_assumptions.html#indipendenza-stocastica-locale-e-applicazioni-nel-modello-di-rasch",
    "title": "66  Assunzioni e Proprietà del Modello di Rasch",
    "section": "66.4 Indipendenza Stocastica Locale e Applicazioni nel Modello di Rasch",
    "text": "66.4 Indipendenza Stocastica Locale e Applicazioni nel Modello di Rasch\n\n66.4.1 Concetto di Indipendenza Stocastica Locale\nIn molti modelli statistici si presuppone che gli eventi siano stocasticamente indipendenti, ovvero che la realizzazione di un evento non fornisca informazioni sulla realizzazione degli altri. Questa assunzione di indipendenza stocastica è fondamentale perché semplifica notevolmente il calcolo delle probabilità congiunte. Ad esempio, consideriamo il lancio ripetuto di una moneta equilibrata: la probabilità di ottenere testa in ciascun lancio è del 50%, e la probabilità congiunta di ottenere testa due volte consecutive è data dal prodotto delle probabilità individuali:\n\\[ \\text{Pr}(testa, testa) = 0.5 \\times 0.5 = 0.25. \\]\nQuesta formula utilizza l’indipendenza stocastica per calcolare la probabilità congiunta, ovvero sapere l’esito del primo lancio non cambia la probabilità del secondo.\n\n\n66.4.2 Indipendenza nel Modello di Rasch\nIl modello di Rasch, utilizzato nella teoria della risposta agli item (IRT), si basa sull’assunzione di indipendenza stocastica locale per calcolare la probabilità congiunta delle risposte a un test. Assumendo che le risposte di una persona agli item di un test siano indipendenti data la sua abilità, possiamo calcolare la probabilità congiunta moltiplicando le probabilità individuali di risposta corretta ai singoli item:\n\\[\n\\text{Pr}(U_{p1} = u_{p1}, U_{p2} = u_{p2} | \\theta_p, \\beta_1, \\beta_2) = \\text{Pr}(U_{p1} = u_{p1} | \\theta_p, \\beta_1) \\times \\text{Pr}(U_{p2} = u_{p2} | \\theta_p, \\beta_2).\n\\]\n\n\n66.4.3 Uso dei Vettori per Semplificare il Modello\nNel modello di Rasch, l’uso dei vettori di risposta e dei parametri di difficoltà degli item facilita il calcolo delle probabilità congiunte per un test di più item. Ad esempio, definiamo \\(\\beta = (\\beta_1, ..., \\beta_I)\\) come il vettore dei parametri di difficoltà e \\(U_{p\\cdot} = (U_{p1}, ..., U_{pI})\\) come il vettore casuale delle risposte della persona \\(p\\). La probabilità congiunta delle risposte si esprime come:\n\\[\n\\text{Pr}(U_{p\\cdot} = u_{p\\cdot} | \\theta_p, \\beta) = \\prod_{i=1}^{I} \\text{Pr}(U_{pi} = u_{pi} | \\theta_p, \\beta_i).\n\\]\nQuesta formulazione permette di calcolare efficientemente la probabilità congiunta di un pattern di risposte utilizzando le proprietà dei prodotti e delle esponenziali:\n\\[\n\\text{Pr}(U_{p\\cdot} = u_{p\\cdot} \\mid \\theta_p, \\beta) = \\frac{\\exp\\{\\theta_p r_p - \\sum_{i=1}^{I} u_{pi} \\beta_i\\}}{\\prod_{i=1}^{I} [1 + \\exp(\\theta_p - \\beta_i)]},\n\\]\ndove \\(r_p = \\sum_{i=1}^{I} u_{pi}\\) rappresenta il numero totale di item risolti correttamente dalla persona \\(p\\).\n\n\n66.4.4 Limitazioni e Considerazioni\nNonostante l’utilità dell’indipendenza stocastica locale, ci sono situazioni in cui questa assunzione può non essere valida, come nei test di matematica dove la soluzione di un problema dipende da quella di problemi precedenti, o nei testlet, dove gli item condividono un tema comune e le risposte possono essere correlate. In questi casi, la probabilità di risposta corretta agli item successivi può dipendere dalle risposte date agli item precedenti, violando l’assunzione di indipendenza stocastica locale.\nIn sintesi, l’indipendenza stocastica locale è un principio potente nel modello di Rasch che facilita il calcolo delle probabilità congiunte nelle valutazioni standardizzate, ma è essenziale riconoscere i suoi limiti e applicarla in modo appropriato a seconda del contesto specifico del test.\n\n\n66.4.5 Estensione della Probabilità Congiunta: Dalle Singole Risposte alla Collettività\n\n66.4.5.1 Probabilità Congiunta delle Risposte di Tutti i Partecipanti\nPossiamo estendere il concetto di probabilità congiunta da una singola persona a tutti i partecipanti di un test. Questo si realizza assumendo che le risposte di tutti i partecipanti siano stocasticamente indipendenti l’una dall’altra. A livello notazionale, raggruppiamo i parametri di abilità dei partecipanti, \\(\\theta_p\\), in un vettore \\(\\theta\\), dove ogni elemento corrisponde al parametro di abilità di un individuo. Analogamente, formiamo una matrice casuale \\(U\\), dove ogni riga rappresenta le risposte di un singolo partecipante e ogni colonna si riferisce alle risposte a un singolo item.\n\n\n66.4.5.2 Matrice delle Risposte e Probabilità Congiunta\nLe risposte effettive di ciascun partecipante sono raccolte in una matrice \\(u\\). La cella nella riga \\(p\\) e colonna \\(i\\) contiene la risposta \\(u_{pi}\\), ovvero la risposta del partecipante \\(p\\) all’item \\(i\\). Utilizzando queste matrici, la probabilità congiunta delle risposte di tutti i partecipanti ai vari item è data da:\n\\[\n\\text{Pr}(U = u | \\theta, \\beta) = \\prod_{p=1}^{P} \\frac{\\exp\\{r_p \\cdot \\theta_p - \\sum_{i=1}^{I} u_{pi} \\cdot \\beta_i\\}}{\\prod_{i=1}^{I} [1 + \\exp(\\theta_p - \\beta_i)]},\n\\]\ndove \\(r_p\\) indica il punteggio totale del partecipante \\(p\\).\n\n\n66.4.5.3 Semplificazione del Calcolo\nIl calcolo può essere ulteriormente semplificato considerando la somma di tutti i termini nel numeratore e distribuendo la somma sui termini \\(\\theta_p\\) e \\(\\beta_i\\):\n\\[\n\\text{Pr}(U = u | \\theta, \\beta) = \\frac{\\exp\\{\\sum_{p=1}^{P}r_p \\cdot \\theta_p - \\sum_{p=1}^{P}\\sum_{i=1}^{I} u_{pi} \\cdot \\beta_i\\}}{\\prod_{p=1}^{P} \\prod_{i=1}^{I} [1 + \\exp(\\theta_p - \\beta_i)]}.\n\\]\nInvertendo l’ordine delle sommatorie nel doppio sommatorio, e riconoscendo che i termini \\(\\beta_i\\) sono costanti rispetto a \\(p\\), possiamo riscrivere la somma come:\n\\[\n\\sum_{i=1}^{I}\\sum_{p=1}^{P} u_{pi} \\cdot \\beta_i = \\sum_{i=1}^{I} c_i \\cdot \\beta_i,\n\\]\ndove \\(c_i\\) rappresenta la somma totale delle risposte corrette all’item \\(i\\) da parte di tutti i partecipanti. Sostituendo nella formula finale, otteniamo:\n\\[\n\\text{Pr}(U = u | \\theta, \\beta) = \\frac{\\exp\\{\\sum_{p=1}^{P} r_p \\cdot \\theta_p - \\sum_{i=1}^{I} c_i \\cdot \\beta_i\\}}{\\prod_{p=1}^{P} \\prod_{i=1}^{I} [1 + \\exp(\\theta_p - \\beta_i)]},\n\\]\nche rappresenta la probabilità congiunta desiderata.\n\n\n66.4.5.4 Considerazioni sulla Validità dell’Assunzione di Indipendenza\nBisogna valutare criticamente l’assunzione che le risposte di diversi partecipanti siano indipendenti. Per esempio, questa assunzione non è valida quando un partecipante copia le risposte da un altro. L’indipendenza locale è cruciale per applicare correttamente il modello di Rasch e per semplificare i calcoli delle probabilità congiunte.\n\n\n66.4.5.5 Implicazioni del Teorema di Andersen\nIl modello di Rasch e la sua formulazione si basano su questa indipendenza e su altre piccole assunzioni, come dimostrato dal teorema di Andersen. Verificare l’applicabilità di queste assunzioni è essenziale prima di implementare il modello in situazioni pratiche.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_assumptions.html#unidimensionalità-nel-modello-di-rasch",
    "href": "chapters/irt/03_assumptions.html#unidimensionalità-nel-modello-di-rasch",
    "title": "66  Assunzioni e Proprietà del Modello di Rasch",
    "section": "66.5 Unidimensionalità nel Modello di Rasch",
    "text": "66.5 Unidimensionalità nel Modello di Rasch\n\n66.5.1 Concetto di Unidimensionalità\nIl modello di Rasch presuppone unidimensionalità, ovvero assume che esista una singola dimensione latente che ordina le persone secondo le loro abilità. Nel contesto dei test psicometrici, un test è considerato unidimensionale quando misura esclusivamente un’unica abilità specifica. Ad esempio, un test di matematica puro misurerebbe solo l’abilità matematica. Al contrario, test come il SAT, che valutano sia competenze matematiche che verbali, non sono unidimensionali perché ogni sezione misura una diversa dimensione di abilità.\n\n\n66.5.2 Implicazioni dell’Unidimensionalità\nUn test ideale unidimensionale assegna a ciascun partecipante un singolo valore di abilità, riflettendo precisamente la competenza nella dimensione target del test. Tuttavia, la presenza di più dimensioni latenti può causare problemi come il Funzionamento Differenziale degli Item (DIF). Il DIF si verifica quando la difficoltà di specifici item varia tra diversi gruppi di candidati, indipendentemente dalle loro abilità nella dimensione misurata dal test.\n\n\n66.5.3 Esempio di DIF e Multidimensionalità\nConsideriamo un test di matematica dove non solo la competenza matematica (la dimensione primaria) è rilevante, ma anche le abilità linguistiche (una dimensione secondaria) influenzano i risultati. Se due gruppi di candidati differiscono significativamente nelle loro abilità linguistiche, questa dimensione secondaria potrebbe distorcere i risultati del test, rendendo alcuni item più facili o difficili per un gruppo rispetto all’altro. Questo fenomeno evidenzia il DIF, dove gli item sono influenzati da fattori non direttamente pertinenti all’abilità che il test intende misurare.\n\n\n66.5.4 Rilevazione e Impatto del DIF\nI test statistici per rilevare il DIF devono considerare la possibile presenza di multidimensionalità. Un test che non isoli adeguatamente la dimensione d’interesse può portare a valutazioni errate o ingiuste dei candidati. La presenza di DIF suggerisce che il test potrebbe non essere calibrato equamente per tutti i partecipanti o che alcuni item siano parziali verso determinati gruppi.\n\n\n66.5.5 Considerazioni su Costrutti Psicologici Complessi\nMolti costrutti psicologici, come l’intelligenza, sono intrinsecamente multidimensionali. Ad esempio, il modello di intelligenza di Carroll (1993) propone una struttura gerarchica con categorie come l’intelligenza fluida e quella cristallizzata, dimostrando la complessità e la multidimensionalità del costrutto.\n\n\n66.5.6 Estensioni Multidimensionali del Modello di Rasch\nSebbene il modello di Rasch classico sia limitato alla misurazione di una singola dimensione, sono state sviluppate estensioni che possono gestire e incorporare dimensioni multiple. Queste versioni multidimensionali del modello di Rasch permettono una valutazione simultanea di più dimensioni, fornendo un’analisi più completa di costrutti psicologici complessi.\n\n\n66.5.7 Conclusione\nL’unidimensionalità è una presunzione centrale nel modello di Rasch, essenziale per la validità dei test che mirano a misurare specifiche abilità. Tuttavia, la realtà dei costrutti psicologici spesso richiede un approccio più sfumato che tenga conto delle loro nature multidimensionali. Le estensioni multidimensionali del modello di Rasch rappresentano un adattamento cruciale per l’analisi di tali costrutti complessi, migliorando la precisione e l’equità delle valutazioni psicometriche.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_assumptions.html#scala-di-misurazione-nel-modello-di-rasch",
    "href": "chapters/irt/03_assumptions.html#scala-di-misurazione-nel-modello-di-rasch",
    "title": "66  Assunzioni e Proprietà del Modello di Rasch",
    "section": "66.6 Scala di Misurazione nel Modello di Rasch",
    "text": "66.6 Scala di Misurazione nel Modello di Rasch\n\n66.6.1 Definizione di Misurazione secondo Stevens\nSecondo Stevens (1946), la misurazione è definita come “l’assegnazione di numeri a oggetti o eventi secondo regole”. Questa definizione, ampiamente riconosciuta in psicologia, abbraccia tanto la misurazione fisica (es. l’uso di un righello per misurare una distanza) quanto quella psicologica (es. l’uso di paradigmi per valutare la percezione del volume sonoro).\n\n\n66.6.2 Classificazione delle Scale di Misurazione\nStevens identifica quattro categorie principali di scale di misurazione: 1. Scale Nominali: Numeri usati esclusivamente come etichette (es. numeri sulle maglie dei giocatori di calcio). 2. Scale Ordinali: Numeri utilizzati per stabilire un ordine (es. classifiche olimpiche). 3. Scale di Intervallo: Misurano distanze relative senza un punto zero naturale (es. gradi Celsius e Fahrenheit). 4. Scale di Rapporto: Simili alle scale di intervallo ma con un punto zero assoluto (es. lunghezza in metri o peso in chilogrammi).\n\n\n66.6.3 Applicazione nel Modello di Rasch\nNel modello di Rasch, la conversione delle misure di abilità da una unità a un’altra (da “Unità A” a “Unità B”) è analoga al cambio di unità di temperatura. Per esempio, ogni abilità \\(\\theta_p\\) può essere trasformata in \\(\\theta_p' = \\theta_p - b\\) e ogni difficoltà dell’item \\(\\beta_i\\) in \\(\\beta_i' = \\beta_i - b\\), mantenendo inalterate le probabilità di una risposta corretta.\n\n\n66.6.4 Scalabilità e Ricalibrazione\nConsideriamo un’ulteriore riscalatura da “Unità A” a “Unità C”, dove \\(\\theta\\) diventa \\(\\theta'' = (1/a) \\cdot \\theta\\) e \\(\\beta\\) diventa \\(\\beta'' = (1/a) \\cdot \\beta\\). Questa trasformazione riduce le logit di un fattore \\(1/a\\). Adattando la funzione logistica \\(f(x)\\) in modo che l’argomento sia scalato da \\(a\\), \\(f(a \\cdot x)\\), manteniamo invariate le probabilità di una risposta corretta tra le Unità A e C.\n\n\n66.6.5 Implicazioni della Scala di Misurazione\nQueste trasformazioni dimostrano che nel modello di Rasch gli item e le persone possono essere misurati su una scala di intervallo, ma il punto zero e la scala non sono intrinsecamente definiti. In pratica, questo implica la necessità di stabilire un punto zero e una scala per la funzione di risposta degli item, ad esempio fissando la difficoltà del primo item a zero, normalizzando la somma delle difficoltà a zero, o bilanciando la somma delle abilità a zero. La scala è comunemente impostata a 1 nel modello di Rasch, corrispondente alla standardizzazione della pendenza degli item.\n\n\n66.6.6 Discussione Futura\nNel prossimo capitolo dedicato alla stima dei parametri, esploreremo ulteriormente come selezionare e applicare queste convenzioni per calibrare efficacemente il modello di Rasch, garantendo che le misurazioni riflettano accuratamente le abilità e le difficoltà all’interno del contesto testato.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_assumptions.html#oggettività-specifica-nei-test-psicometrici",
    "href": "chapters/irt/03_assumptions.html#oggettività-specifica-nei-test-psicometrici",
    "title": "66  Assunzioni e Proprietà del Modello di Rasch",
    "section": "66.7 Oggettività Specifica nei Test Psicometrici",
    "text": "66.7 Oggettività Specifica nei Test Psicometrici\n\n66.7.1 Definizione e Importanza dell’Oggettività Specifica\nL’oggettività specifica è un concetto fondamentale nei test psicometrici, volto a garantire l’equità nei confronti tra individui. Essa assicura che i confronti tra le persone siano determinati unicamente dalle loro abilità e non dagli specifici item utilizzati nel test. Questo principio implica che se un individuo ha una probabilità maggiore di rispondere correttamente a un item rispetto a un altro, allora questa superiorità dovrebbe manifestarsi uniformemente su tutti gli item del test. Analogamente, se un item risulta più facile per una persona, dovrebbe risultare più facile per chiunque altro.\n\n\n66.7.2 Verifica dell’Oggettività Specifica\nUn metodo per controllare se un modello rispetta l’oggettività specifica è osservare le Curve di Caratteristica dell’Item (ICC). Nel modello di Rasch, l’oggettività specifica è soddisfatta se le ICC per diversi item non si incrociano, indicando che le probabilità di risposta corretta sono funzioni consistenti dell’abilità, indipendentemente dagli item specifici.\n\n\n66.7.3 Formalizzazione Algebrica dell’Oggettività Specifica\nSecondo Irtel (1996), l’oggettività specifica può essere definita più rigorosamente attraverso il concetto di rapporto di probabilità. Per due persone $ p $ e $ q $ e un item $ i $, definiamo $ O_{pi} $ come la probabilità che la persona $ p $ risponda correttamente all’item $ i $. L’oggettività specifica richiede che il rapporto di probabilità tra le persone sia costante attraverso tutti gli item del test:\n\\[\n\\frac{O_{p1}}{O_{q1}} = \\dots = \\frac{O_{pI}}{O_{qI}}\n\\]\nQuesto implica che per ogni coppia di item $ i $ e $ j $, il rapporto di probabilità tra due persone per l’item $ i $ sia uguale a quello per l’item $ j $, ossia:\n\\[\n\\frac{O_{pi}}{O_{qi}} = \\frac{O_{pj}}{O_{qj}}\n\\]\nCiò equivale matematicamente a che $ O_{pi} O_{qj} = O_{pj} O_{qi} $.\n\n\n66.7.4 Applicazione Pratica: Esempio di Oggettività Specifica\nPer illustrare, consideriamo Marco e Cora che completano un test di venti item. Se le probabilità (e di conseguenza le quote) di Marco risolvere il primo item sono 1:4 e quelle di Cora sono 1:1, l’oggettività specifica impone che il rapporto di 4:1 tra le quote di Cora e Marco debba rimanere costante per tutti i 20 item del test.\n\n\n66.7.5 Limitazioni e Implicazioni\nNonostante l’oggettività specifica fornisca un criterio rigoroso per la coerenza dei test, il suo uso può essere limitato da interpretazioni errate. Ad esempio, definire l’oggettività specifica come “indipendenza del campione” può portare a conclusioni erronee riguardo alla trasferibilità di un test tra gruppi diversi, come banchieri d’investimento e ingegneri del software, che potrebbero interpretare gli item in modo differente.\n\n\n66.7.6 Considerazioni Finali\nL’oggettività specifica non è una proprietà che può essere universalmente garantita in tutti i contesti. È un’ipotesi di lavoro che deve essere continuamente verificata attraverso dati empirici. Rasch stesso ha sottolineato l’importanza di testare ripetutamente questa proprietà ogni volta che si raccolgono nuovi dati. Questa attenzione costante ai dettagli e alle variabili del contesto è cruciale per mantenere l’integrità e l’affidabilità dei test psicometrici.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/03_assumptions.html#considerazioni-conclusive",
    "href": "chapters/irt/03_assumptions.html#considerazioni-conclusive",
    "title": "66  Assunzioni e Proprietà del Modello di Rasch",
    "section": "66.8 Considerazioni conclusive",
    "text": "66.8 Considerazioni conclusive\nIl modello di Rasch si basa su tre fondamentali assunzioni che ne garantiscono la validità e l’applicabilità: unidimensionalità, monotonicità e indipendenza locale. La violazione di queste assunzioni può indicare la necessità di ricorrere a metodologie più complesse o a modelli alternativi per un’analisi accurata dei dati. Questo può richiedere un esame più approfondito dei dati o l’adozione di modelli statistici avanzati capaci di gestire la complessità delle informazioni raccolte.\nUn aspetto distintivo del modello di Rasch è la sua capacità di quantificare la difficoltà degli item indipendentemente dalle abilità dei partecipanti, grazie all’uso della stima di massima verosimiglianza condizionale. Questo approccio garantisce che la difficoltà di ciascun item venga determinata basandosi unicamente sulle risposte specifiche a quell’item, in maniera indipendente dal livello complessivo di abilità dei rispondenti.\nQuesto principio di oggettività specifica è simile al concetto di invarianza in analisi di regressione, dove le caratteristiche di una linea di regressione (come pendenza e intercetta) non variano a seconda del campione analizzato. Analogamente, nel modello di Rasch, i parametri di difficoltà degli item restano costanti e non sono influenzati dalle competenze generali dei partecipanti, rendendo le valutazioni di difficoltà degli item stabili e affidabili, indipendentemente dalla varietà o dal livello di abilità del campione di rispondenti.\nL’oggettività specifica è particolarmente preziosa poiché elimina la necessità di selezionare campioni normati o rappresentativi della popolazione generale per la calibrazione degli item. Quasi qualsiasi gruppo di persone, purché con una varietà sufficiente nelle risposte, può essere utilizzato per stabilire la difficoltà degli item. Questo approccio si contrappone a quello dei test tradizionali, dove spesso è necessario un campione rappresentativo per sviluppare tabelle normative basate sulle percentuali di risposte corrette.\nIn conclusione, il modello di Rasch offre un framework robusto per la misurazione psicometrica, pur richiedendo una verifica attenta delle sue assunzioni fondamentali per assicurare risultati di misurazione accurati e equi.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Assunzioni e Proprietà del Modello di Rasch</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_estimation.html",
    "href": "chapters/irt/04_estimation.html",
    "title": "67  Stima",
    "section": "",
    "text": "67.1 Introduzione\nPer applicare il modello di Rasch nella ricerca pratica, è fondamentale sapere come stimare i suoi parametri utilizzando dati empirici. In questa sezione, verranno presentati diversi approcci per stimare i parametri del modello di Rasch a partire dai dati dei test osservati. Tutti questi metodi permettono di stimare sia i parametri degli item che quelli delle persone, ma differiscono nel modo in cui lo fanno.\nDue approcci – la massima verosimiglianza congiunta e l’inferenza bayesiana – stimano contemporaneamente i parametri degli item e delle persone. Gli altri due – la massima verosimiglianza condizionale e la massima verosimiglianza marginale – separano le stime, con i parametri delle persone calcolati in un secondo passaggio.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_estimation.html#la-funzione-di-verosimiglianza",
    "href": "chapters/irt/04_estimation.html#la-funzione-di-verosimiglianza",
    "title": "67  Stima",
    "section": "67.2 La Funzione di Verosimiglianza",
    "text": "67.2 La Funzione di Verosimiglianza\nLa stima dei parametri nel modello di Rasch si basa sulla funzione di verosimiglianza, che esprime la probabilità di osservare i dati disponibili dato un insieme di parametri del modello ancora sconosciuti. Nel modello di Rasch, \\(U_{pi}\\) rappresenta la risposta (corretta o errata) fornita dalla persona \\(p\\) all’item \\(i\\), dove una risposta corretta è codificata come 1, mentre una risposta errata è codificata come 0. La probabilità condizionale che la persona \\(p\\) dia una risposta specifica \\(u_{pi}\\) all’item \\(i\\), data la propria abilità \\(\\theta_p\\) e la difficoltà dell’item \\(\\beta_i\\), è data dalla seguente formula:\n\\[\nL_{upi}(\\theta_p, \\beta_i) = \\text{Pr}(U_{pi} = u_{pi} | \\theta_p, \\beta_i) = \\frac{\\exp\\{u_{pi} \\cdot (\\theta_p - \\beta_i)\\}}{1 + \\exp(\\theta_p - \\beta_i)}.\n\\tag{67.1}\\]\nQuesta espressione calcola la probabilità di osservare la risposta specifica \\(u_{pi}\\), in funzione della differenza tra l’abilità della persona (\\(\\theta_p\\)) e la difficoltà dell’item (\\(\\beta_i\\)). Se \\(\\theta_p &gt; \\beta_i\\), la probabilità di una risposta corretta (\\(u_{pi} = 1\\)) sarà elevata; al contrario, se \\(\\theta_p &lt; \\beta_i\\), la probabilità di una risposta corretta sarà bassa.\nPer ottenere la verosimiglianza complessiva delle risposte fornite da una persona \\(p\\) a tutti gli item del test (da \\(i = 1\\) a \\(I\\)), si moltiplicano le probabilità condizionali di tutte le sue risposte. La funzione di verosimiglianza totale è espressa come:\n\\[\nL_{up}(\\theta_p, \\beta) = \\prod_{i=1}^{I} \\frac{\\exp\\{u_{pi} \\cdot (\\theta_p - \\beta_i)\\}}{1 + \\exp(\\theta_p - \\beta_i)},\n\\]\nche può essere riscritta per maggiore chiarezza come:\n\\[\nL_{up}(\\theta_p, \\beta) = \\frac{\\exp(r_p \\cdot \\theta_p - \\sum_{i=1}^{I} u_{pi} \\cdot \\beta_i)}{\\prod_{i=1}^{I} [1 + \\exp(\\theta_p - \\beta_i)]}, \\tag{1}\n\\]\ndove \\(r_p = \\sum_{i=1}^{I} u_{pi}\\) è il punteggio grezzo (numero di risposte corrette) della persona \\(p\\).\n\n67.2.1 Importanza della Funzione di Verosimiglianza\nL’Equazione 67.1 rappresenta la base comune per tutti i metodi di stima dei parametri nel modello di Rasch. Tuttavia, il metodo scelto per stimare i parametri influenzerà il modo in cui l’abilità delle persone (\\(\\theta_p\\)) e la difficoltà degli item (\\(\\beta_i\\)) vengono calcolate e interpretate:\n\nStima simultanea: Alcuni metodi, come la massima verosimiglianza congiunta e l’inferenza bayesiana, stimano simultaneamente \\(\\theta_p\\) e \\(\\beta_i\\).\nStima separata: Altri metodi, come la massima verosimiglianza condizionale e la massima verosimiglianza marginale, stimano \\(\\beta_i\\) in un primo passaggio, per poi derivare \\(\\theta_p\\).\n\nOgni approccio introduce assunzioni specifiche che influenzano le proprietà delle stime e la loro applicabilità in diversi contesti.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_estimation.html#stima-dei-parametri-nel-modello-di-rasch",
    "href": "chapters/irt/04_estimation.html#stima-dei-parametri-nel-modello-di-rasch",
    "title": "67  Stima",
    "section": "67.3 Stima dei Parametri nel Modello di Rasch",
    "text": "67.3 Stima dei Parametri nel Modello di Rasch\n\n67.3.1 Stima della Massima Verosimiglianza Congiunta\nLa stima della massima verosimiglianza congiunta (JML) mira a determinare simultaneamente i parametri delle persone (\\(\\theta_p\\)) e degli item (\\(\\beta_i\\)) che massimizzano la probabilità complessiva dei dati osservati, come descritto nella funzione di verosimiglianza del modello di Rasch. Questo approccio identifica il set di parametri più probabili che potrebbero aver generato il dataset analizzato.\nPunti di forza:\n\nMetodo diretto e intuitivo, che utilizza tutta l’informazione disponibile nei dati osservati.\n\nLimitazioni:\n\nStime inconsistenti: Nonostante la semplicità del metodo, JML non garantisce stime consistenti dei parametri degli item, anche con campioni di grandi dimensioni. Questo limita la sua affidabilità, specialmente in contesti che richiedono alta precisione e robustezza nelle stime.\nBias intrinseco: Le stime delle abilità (\\(\\theta_p\\)) e delle difficoltà (\\(\\beta_i\\)) possono essere influenzate l’una dall’altra, causando errori sistematici.\n\nImplementazione in R:\n\nJML è implementato nel pacchetto TAM, attraverso la funzione tam.jml(). Sebbene disponibile, il suo utilizzo è sconsigliato in analisi avanzate o quando la consistenza delle stime è critica.\n\n\n\n\n67.3.2 Stima della Massima Verosimiglianza Condizionale\nLa stima della massima verosimiglianza condizionale (CML) affronta le limitazioni della JML separando la stima dei parametri degli item da quella delle persone. Questo approccio procede in due fasi:\n\nStima dei parametri degli item:\n\nLa CML utilizza le statistiche sufficienti delle persone (ad esempio, i punteggi grezzi \\(r_p = \\sum u_{pi}\\)) per isolare i parametri degli item. In questa fase, le abilità delle persone (\\(\\theta_p\\)) non sono direttamente considerate, evitando il bias congiunto.\n\nStima dei parametri delle persone:\n\nUna volta stimati i parametri degli item (\\(\\beta_i\\)), si procede alla stima delle abilità (\\(\\theta_p\\)) basandosi sui dati individuali e sulle difficoltà stimate.\n\n\nVantaggi:\n\nFornisce stime consistenti dei parametri degli item.\nEvita il problema del bias associato alla stima simultanea di JML.\n\nLimitazioni:\n\nL’accuratezza dei parametri delle persone dipende dalla precisione delle stime degli item nella prima fase.\n\nImplementazione in R:\n\nLa CML è implementata nel pacchetto eRm tramite la funzione RM(), che consente di stimare i parametri degli item in modo robusto e separato.\n\n\n\n\n67.3.3 Stima della Massima Verosimiglianza Marginale\nLa stima della massima verosimiglianza marginale (MML) rappresenta un approccio avanzato che considera le abilità delle persone come una variabile casuale seguendo una distribuzione ipotizzata, tipicamente normale. Questo metodo differisce dalla CML trattando i parametri delle abilità (\\(\\theta_p\\)) come effetti casuali anziché fissi, e li integra nella funzione di verosimiglianza complessiva.\n\n67.3.3.1 Come funziona:\n\nDistribuzione marginale delle abilità:\n\nLa MML assume che le abilità (\\(\\theta_p\\)) siano distribuite nella popolazione secondo una distribuzione nota (ad esempio, una normale standard). Invece di stimare direttamente \\(\\theta_p\\), il metodo stima i parametri degli item (\\(\\beta_i\\)) tenendo conto di questa distribuzione.\n\nScoring individuale:\n\nDopo aver stimato i parametri degli item, si calcolano i punteggi individuali (\\(\\theta_p\\)) basandosi sulle risposte e sui parametri stimati.\n\n\n\n\n67.3.3.2 Vantaggi:\n\nProduce stime più precise e realistiche dei parametri degli item rispetto alla JML.\nÈ particolarmente utile quando le abilità nella popolazione seguono una distribuzione continua e ipotizzabile.\n\n\n\n67.3.3.3 Limitazioni:\n\nLa validità delle stime dipende dalla correttezza dell’assunzione sulla distribuzione delle abilità (\\(\\theta_p\\)).\n\n\n\n67.3.3.4 Implementazione in R:\n\nLa MML è supportata dai pacchetti mirt e TAM. Ad esempio:\n\nFunzioni come mirt() in mirt permettono stime flessibili con distribuzioni marginali specificabili.\nAnche ltm (sebbene non più attivamente sviluppato) offre strumenti per la stima marginale.\n\n\n\n\n\n\n67.3.4 Confronto tra i Metodi\n\n\n\n\n\n\n\n\n\nMetodo\nCaratteristiche principali\nPro\nContro\n\n\n\n\nJML (Massima Verosimiglianza Congiunta)\nStima simultanea di \\(\\theta_p\\) e \\(\\beta_i\\).\nIntuitivo e diretto.\nStime inconsistenti; bias congiunto.\n\n\nCML (Massima Verosimiglianza Condizionale)\nStima separata in due fasi: prima \\(\\beta_i\\), poi \\(\\theta_p\\).\nStime consistenti per \\(\\beta_i\\); evita il bias.\nDipende dall’accuratezza delle stime iniziali degli item.\n\n\nMML (Massima Verosimiglianza Marginale)\nIntegra una distribuzione marginale per \\(\\theta_p\\); tratta \\(\\theta_p\\) come effetti casuali.\nStime realistiche e robuste; considera la distribuzione della popolazione.\nDipende dall’assunzione sulla distribuzione delle abilità.\n\n\n\nIn conclusione, ogni metodo presenta vantaggi e svantaggi che lo rendono più o meno adatto a specifici contesti di analisi. La JML è utile per analisi preliminari o semplici, ma è limitata dalla mancanza di consistenza. La CML e la MML offrono stime più robuste e realistiche, con la MML che si distingue per la sua flessibilità nell’incorporare distribuzioni di popolazione.\n\nEsercizio 67.1 Consideriamo ora la procedura di stima del livello di abilità \\(\\theta\\) di un individuo nel modello di Rasch attraverso l’uso della massima verosimiglianza marginale. La procedura per stimare la posizione di un individuo, dato un particolare pattern di risposte, può essere formulata con i seguenti passaggi.\n\nConsideriamo un determinato pattern di risposta. Per esempio, il pattern “11000” indica che un particolare individuo ha fornito due risposte corrette seguite da tre errate a cinque item, con un totale di \\(X = 2\\) risposte corrette.\nCalcoliamo le probabilità per ogni risposta. Utilizziamo l’Equazione 65.1 per calcolare la probabilità di ciascuna risposta nel pattern, in base a un dato livello di abilità \\(\\theta\\).\nDeterminiamo la probabilità del pattern di risposta. Questo passaggio si basa sull’assunzione di indipendenza condizionale (ovvero, per un dato \\(\\theta\\), le risposte sono indipendenti l’una dall’altra). Questa assunzione ci permette di applicare la regola di moltiplicazione per eventi indipendenti alle probabilità degli item per ottenere la probabilità complessiva del pattern di risposta per un dato \\(\\theta\\).\nRipetiamo i calcoli per diversi valori di \\(\\theta\\). Ripetiamo i passaggi 1 e 2 per una serie di valori di \\(\\theta\\). Nel nostro esempio, il range di \\(\\theta\\) va da \\(-3\\) a \\(3\\).\nDeterminiamo il valore di \\(\\theta\\) con la massima verosimiglianza. L’ultimo passaggio consiste nel determinare quale valore di \\(\\theta\\) tra quelli calcolati nel passaggio 3 abbia la più alta verosimiglianza di produrre il pattern “11000”. Per fare questo scegliamo il valore \\(\\theta\\) per cui la verosimiglianza è massima.\n\nDi seguito, esaminiamo uno script in R che implementa questa procedura.\n\n# Definiamo il pattern di risposta\nresponse_pattern &lt;- c(1, 1, 0, 0, 0)\n\n# Range di valori di theta da esplorare\ntheta_values &lt;- seq(-3, 3, by = 0.01)\n\n# Funzione per calcolare la probabilità di un singolo pattern di risposta\ncalculate_probability &lt;- function(theta, pattern) {\n    correct_probs &lt;- exp(theta) / (1 + exp(theta))\n    item_probs &lt;- ifelse(pattern == 1, correct_probs, 1 - correct_probs)\n    prod(item_probs)\n}\n# Per semplicità, assumiamo che il parametro di difficoltà (beta) sia zero per tutti gli item.\n\n# Calcoliamo le probabilità per ogni valore di theta. Usiamo sapply per applicare \n# la funzione calculate_probability a ciascun valore di theta nel range specificato.\nprobabilities &lt;- sapply(theta_values, calculate_probability, pattern = response_pattern)\n\n# Identifichiamo il valore di theta con la massima verosimiglianza\nbest_theta &lt;- theta_values[which.max(probabilities)]\n\nprint(paste(\"Valore di theta calcolato con la massima verosimiglianza:\", best_theta))\n\n[1] \"Valore di theta calcolato con la massima verosimiglianza: -0.41\"\n\n\nQuesto script calcola la probabilità di ottenere il pattern di risposta “11000” per cinque item per un dato intervallo di valori di \\(\\theta\\) e identifica il valore di \\(\\theta\\) che massimizza questa probabilità. Si noti che il modello di Rasch prevede che tutti gli item abbiano la stessa discriminazione, quindi non è necessario specificare un parametro di discriminazione per ogni item. Abbiamo assunto inoltre che la difficoltà di tutti gli item sia uguale a zero.\nLa verosimiglianza di un pattern di risposta di un singolo rispondente a diversi item può essere rappresentata simbolicamente nel modo seguente. Se consideriamo \\(x\\) come il pattern di risposta di un rispondente (ad esempio, \\(x = 11000\\) indica che il rispondente ha risposto correttamente ai primi due item e ha dato risposte sbagliate agli ultimi tre), la verosimiglianza del vettore di risposta \\(x_i\\) della persona \\(i\\) è espressa come:\n\\[\n\\begin{equation}\nL(x_i) = \\prod_{j=1}^{L} p_{ij},\n\\end{equation}\n\\]\ndove \\(p_{ij} = p(x_{ij} = 1 \\mid \\theta_i, \\alpha_j, \\delta_j)\\) rappresenta la probabilità che la persona \\(i\\), con un livello di abilità \\(\\theta_i\\), risponda correttamente all’item \\(j\\). In questa formula, \\(\\alpha_j\\) è il parametro di discriminazione dell’item \\(j\\) e \\(\\delta_j\\) è il suo parametro di difficoltà. Il parametro \\(\\alpha_j\\) indica quanto bene l’item \\(j\\) è in grado di discriminare tra rispondenti di diversi livelli di abilità, mentre \\(\\delta_j\\) rappresenta il livello di abilità per cui la probabilità di una risposta corretta è del 50%. Il prodotto è calcolato su tutti gli \\(L\\) item a cui il rispondente ha risposto, e il simbolo \\(\\prod\\) rappresenta il prodotto di tutte queste probabilità individuali.\nIl calcolo diretto della verosimiglianza può diventare problematico all’aumentare del numero di item, poiché il prodotto di molteplici probabilità può risultare in valori molto piccoli, difficili da gestire con precisione in calcoli numerici. Pertanto, è spesso più pratico lavorare con la trasformazione logaritmica naturale della verosimiglianza, ovvero \\(\\log_e(L(x_i))\\) o \\(\\ln(L(x_i))\\). Questa trasformazione converte il prodotto in una somma, come segue:\n\\[\n\\begin{equation}\n\\ln L(x_i) = \\sum_{j=1}^{L} \\ln(p_{ij}).\n\\end{equation}\n\\]\nL’uso del logaritmo naturale trasforma quindi la verosimiglianza in una somma di logaritmi, semplificando il calcolo e riducendo i problemi di rappresentazione numerica nei calcoli complessi.\n\n# Definizione del pattern di risposta\nresponse_pattern &lt;- c(1, 1, 0, 0, 0)\n\n# Range di valori di theta da esplorare\ntheta_values &lt;- seq(-3, 3, by = 0.1)\n\n# Calcolo della log-verosimiglianza per ogni valore di theta\nlog_likelihoods &lt;- numeric(length(theta_values))\nfor (i in seq_along(theta_values)) {\n    theta &lt;- theta_values[i]\n    log_item_probs &lt;- numeric(length(response_pattern))\n\n    # Calcolo delle probabilità logaritmiche individuali per ogni item nel pattern\n    for (j in seq_along(response_pattern)) {\n        prob_correct &lt;- exp(theta) / (1 + exp(theta))\n        prob &lt;- ifelse(response_pattern[j] == 1, prob_correct, 1 - prob_correct)\n        log_item_probs[j] &lt;- log(prob)\n    }\n\n    # Calcolo della log-verosimiglianza\n    log_likelihoods[i] &lt;- sum(log_item_probs)\n}\n\n# Creazione di un dataframe per il plotting\nplot_data &lt;- data.frame(theta = theta_values, log_likelihood = log_likelihoods)\n\n# Rappresentazione grafica della log-verosimiglianza\nggplot(plot_data, aes(x = theta, y = log_likelihood)) +\n    geom_line() +\n    labs(\n        x = expression(theta), y = \"Log-likelihood\",\n        title = \"Log-likelihood Function for Response Pattern 11000\"\n    )",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_estimation.html#errore-standard-della-stima-e-informazione-dellitem",
    "href": "chapters/irt/04_estimation.html#errore-standard-della-stima-e-informazione-dellitem",
    "title": "67  Stima",
    "section": "67.4 Errore Standard della Stima e Informazione dell’Item",
    "text": "67.4 Errore Standard della Stima e Informazione dell’Item\nNel modello di Rasch, l’Errore Standard della Stima (EES) è un indicatore chiave che quantifica l’incertezza associata alla stima del livello di abilità di un individuo (\\(\\theta\\)). L’EES è fondamentale perché fornisce una misura della precisione con cui la stima di \\(\\theta\\) riflette l’abilità reale del rispondente. Un EES più basso indica una stima più precisa, mentre un EES più alto segnala una maggiore incertezza.\n\n67.4.1 Calcolo dell’EES\nL’EES è determinato dall’informazione totale dell’item a un dato livello di abilità \\(\\theta\\), indicata con \\(I(\\theta)\\). L’EES è definito come l’inverso della radice quadrata di \\(I(\\theta)\\):\n\\[\n\\text{EES}(\\theta) = \\frac{1}{\\sqrt{I(\\theta)}},\n\\]\ndove \\(I(\\theta)\\) rappresenta l’informazione totale accumulata dagli item del test a quel livello di abilità.\n\n\n67.4.2 Informazione dell’Item\nL’informazione dell’item misura il contributo di ciascun item alla precisione della stima di \\(\\theta\\). Per un dato livello di abilità, l’informazione fornita da un singolo item dipende dalla probabilità che il rispondente dia una risposta corretta (\\(p_{ij}\\)) e dalla probabilità di una risposta errata (\\(1 - p_{ij}\\)). La formula per calcolare l’informazione totale degli item è:\n\\[\nI(\\theta) = \\sum_{j=1}^{L} p_{ij}(1 - p_{ij}),\n\\]\ndove:\n\n\\(L\\) è il numero totale di item del test.\n\\(p_{ij}\\) è la probabilità che una persona con abilità \\(\\theta\\) risponda correttamente all’item \\(j\\).\n\nL’informazione fornita da un singolo item raggiunge il suo massimo quando la difficoltà dell’item (\\(\\delta_j\\)) è uguale al livello di abilità del rispondente (\\(\\theta\\)). In questa condizione, l’item discrimina al meglio tra rispondenti con livelli di abilità leggermente superiori o inferiori a \\(\\delta_j\\).\n\n\n67.4.3 Relazione tra Informazione e Precisione\n\nMassima informazione, minima incertezza: Quando \\(I(\\theta)\\) è alta, l’EES (\\(\\text{EES}(\\theta)\\)) è basso, indicando una stima precisa.\nBassa informazione, alta incertezza: Quando \\(I(\\theta)\\) è bassa, l’EES è alto, segnalando una maggiore incertezza nella stima di \\(\\theta\\).\n\nQuesta relazione evidenzia l’importanza di progettare test con item che siano informativi per il range di abilità di interesse.\n\n\n67.4.4 Curva di Informazione dell’Item\nL’informazione dell’item varia a seconda del livello di abilità del rispondente. Per visualizzare questa relazione, si traccia la curva di informazione dell’item, che rappresenta l’informazione fornita da un singolo item in funzione di \\(\\theta\\). Alcune caratteristiche della curva:\n\nHa una forma a campana.\nRaggiunge il picco quando \\(\\theta = \\delta_j\\), ossia quando l’abilità del rispondente corrisponde alla difficoltà dell’item.\nLarghezza e altezza della curva dipendono dalla discriminazione dell’item (nel modello Rasch, fissata a 1).\n\nLa somma delle curve di informazione dei singoli item produce la curva di informazione totale del test, che mostra la precisione complessiva del test a diversi livelli di abilità.\n\n\n67.4.5 Applicazioni pratiche\n\nProgettazione del test: La conoscenza dell’informazione degli item aiuta a creare test che siano più informativi per specifici livelli di abilità, riducendo l’EES per i range di interesse.\nInterpretazione dei risultati: L’EES permette di stimare intervalli di confidenza per \\(\\theta\\), fornendo una misura della precisione della stima:\n\\[\n\\text{Intervallo di confidenza per } \\theta = \\theta \\pm 1.96 \\cdot \\text{EES}(\\theta).\n\\]\n\nL’analisi dell’informazione dell’item e del test è quindi essenziale per garantire che le misurazioni ottenute siano affidabili e utili per l’interpretazione e il confronto delle abilità.\n\nEsercizio 67.2 Utilizzando il modello di Rasch, possiamo calcolare le probabilità di risposta corretta per diversi valori di abilità e, di conseguenza, la Funzione Informativa dell’Item (Item Information Function, IIF):\n\n# Definizione di un range di abilità\ntheta &lt;- seq(-4, 4, by = 0.1)\n\n# Definizione di un parametro di difficoltà dell'item\nbeta &lt;- 0\n\n# Calcolo delle probabilità di risposta corretta per ciascun valore di abilità usando la funzione logistica\nprob_correct &lt;- exp(theta - beta) / (1 + exp(theta - beta))\n\n# Calcolo dell'informazione dell'item\nitem_info &lt;- prob_correct * (1 - prob_correct)\n\n# Creazione della prima grafica (ICC)\nplot(theta, prob_correct,\n    type = \"l\", col = \"blue\", lwd = 2,\n    xlab = \"Abilita' theta\", ylab = \"Probabilita' di Risposta Corretta\",\n    main = \"Curva Caratteristica dell'Item (ICC) e Informazione dell'Item\"\n)\n\n# Aggiunta di un secondo asse y per l'informazione\npar(new = TRUE)\nplot(theta, item_info,\n    type = \"l\", col = \"red\", lwd = 2,\n    xlab = \"\", ylab = \"\", axes = FALSE, ann = FALSE\n)\n\n# Aggiungere l'asse y di destra per l'informazione\naxis(side = 4, at = pretty(range(item_info)))\nmtext(\"Informazione\", side = 4, line = 3)\n\n# Aggiunta della legenda\nlegend(\"topright\",\n    legend = c(\"ICC\", \"Informazione\"),\n    col = c(\"blue\", \"red\"), lty = 1, cex = 0.8\n)\n\n\n\n\n\n\n\n\nQuesta rappresentazione grafica in R mostra come l’informazione vari in funzione del livello di abilità. In generale, l’informazione è massima quando l’abilità dell’esaminando è vicina alla difficoltà dell’item e diminuisce man mano che ci si allontana da questo punto.\nIl concetto di informazione in IRT è fondamentale sia per la costruzione del test sia per la sua interpretazione. Indica quanto efficacemente ciascun item misura l’abilità a vari livelli e aiuta a determinare quali item sono più informativi per la stima dell’abilità degli esaminandi. Inoltre, fornisce indicazioni sulla precisione con cui l’abilità degli esaminandi può essere stimata a vari punti lungo la scala di abilità.\n\n\nEsercizio 67.3 Per dimostrare come calcolare la TIF in \\(\\mathsf{R}\\), possiamo estendere l’esempio precedente includendo più item e sommando le loro informazioni:\n\n# Definizione di parametri di difficoltà per diversi item\nbeta_items &lt;- c(-1, 0, 1) # Esempio di tre item con difficoltà diverse\n\n# Calcolo dell'informazione per ogni item e somma per ottenere la TIF\ntest_info &lt;- rep(0, length(theta))\nfor (beta in beta_items) {\n    prob_correct &lt;- exp(theta - beta) / (1 + exp(theta - beta))\n    item_info &lt;- prob_correct * (1 - prob_correct)\n    test_info &lt;- test_info + item_info\n}\n\n# Creazione del grafico della TIF\nplot(theta, test_info,\n    type = \"l\", col = \"blue\", lwd = 2,\n    xlab = \"Abilità theta\", ylab = \"Informazione del Test\",\n    main = \"Funzione di Informazione del Test (TIF)\"\n)\n\n\n\n\n\n\n\n\nIn questo esempio, calcoliamo e sommiamo le informazioni di tre item con diverse difficoltà per visualizzare la TIF di un test ipotetico. La TIF mostra in modo chiaro come il test nel suo insieme stima l’abilità degli esaminandi a vari livelli, fornendo così indicazioni preziose sulla costruzione e sull’utilizzo ottimale del test in diversi contesti.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_estimation.html#stima-dellabilità",
    "href": "chapters/irt/04_estimation.html#stima-dellabilità",
    "title": "67  Stima",
    "section": "67.5 Stima dell’Abilità",
    "text": "67.5 Stima dell’Abilità\nNel contesto dell’IRT, la stima dell’abilità di un esaminando (\\(\\theta\\)) viene effettuata utilizzando metodi iterativi, come la massima verosimiglianza, che sfruttano i dati del test e i parametri degli item. Questo processo consente di stimare il livello di abilità in modo personalizzato, tenendo conto del pattern di risposte specifico di ciascun esaminando.\n\n67.5.1 Procedura di Stima dell’Abilità\n\nPunto di partenza:\n\nLa stima inizia con un’ipotesi iniziale o un valore a priori per l’abilità dell’esaminando. Questo valore può essere scelto in base a considerazioni teoriche (ad esempio, \\(\\theta = 0\\), corrispondente alla media presunta dell’abilità) o determinato da informazioni preliminari.\n\nUtilizzo dei parametri degli item:\n\nI parametri noti degli item (ad esempio, difficoltà (_i) e discriminazione (a_i)) vengono utilizzati per calcolare la probabilità che l’esaminando risponda correttamente a ciascun item in base al livello di abilità iniziale ipotizzato. Questa probabilità è calcolata attraverso la funzione di risposta dell’item (IRF).\n\nIterazione per aggiustare la stima:\n\nIl livello di abilità viene aggiornato iterativamente. L’obiettivo di ogni iterazione è migliorare la corrispondenza tra le probabilità previste di risposta corretta (basate sul livello di abilità stimato) e il pattern effettivo di risposte fornite dall’esaminando.\nQuesto processo continua fino a quando le modifiche alla stima di \\(\\theta\\) diventano trascurabili, indicando che è stato raggiunto un punto di convergenza. Il risultato finale è una stima stabile e affidabile dell’abilità.\n\nStima personalizzata:\n\nIl processo viene ripetuto per ciascun esaminando, assicurando che ogni stima di \\(\\theta\\) sia basata esclusivamente sulle sue risposte.\n\n\n\n\n67.5.2 Metodi alternativi di stima\n\nStima simultanea:\n\nIn alternativa alla stima iterativa individuale, esistono approcci che stimano simultaneamente i livelli di abilità di tutti gli esaminandi. Questi metodi sono particolarmente utili in presenza di un ampio campione, ottimizzando il processo di calcolo.\n\nStima Bayesiana:\n\nLa stima bayesiana combina i dati del test con una distribuzione a priori sull’abilità (\\(\\theta\\)) per ottenere una stima posteriore. Questo approccio è particolarmente utile quando il numero di item è limitato o le risposte sono incomplete.\n\n\n\n\n67.5.3 Importanza della Stima dell’Abilità\nLa stima dell’abilità in IRT è fondamentale per due motivi principali:\n\nValutazione personalizzata:\n\nPermette di misurare l’abilità di ciascun esaminando in maniera individualizzata, considerando le interazioni specifiche tra il rispondente e gli item. Questa personalizzazione rende la stima più accurata rispetto ai punteggi grezzi, che non tengono conto delle caratteristiche degli item.\n\nAnalisi mirate:\n\nPoiché la stima dell’abilità è direttamente legata ai parametri degli item, consente di condurre analisi dettagliate sull’efficacia del test (ad esempio, quali item sono più informativi per specifici livelli di abilità) e sulle caratteristiche dei rispondenti.\n\n\nIn conclusione, la stima dell’abilità in IRT è un processo iterativo che utilizza i parametri degli item e il pattern di risposte individuali per fornire stime accurate e personalizzate del livello di abilità di ciascun esaminando. Grazie alla sua precisione, questa metodologia rappresenta una componente essenziale dell’IRT, sia per la valutazione degli esaminandi sia per l’ottimizzazione dei test.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_estimation.html#stima-bayesiana",
    "href": "chapters/irt/04_estimation.html#stima-bayesiana",
    "title": "67  Stima",
    "section": "67.6 Stima Bayesiana",
    "text": "67.6 Stima Bayesiana\nLa stima bayesiana sta diventando un metodo sempre più popolare per stimare i parametri del modello di Rasch. Come la stima della massima verosimiglianza congiunta, la stima bayesiana stima simultaneamente sia i parametri delle persone che quelli degli item. Tuttavia, mentre la stima della massima verosimiglianza congiunta trova i valori di \\(\\theta\\) e \\(\\beta\\) massimizzando la verosimiglianza congiunta, la stima bayesiana utilizza la regola di Bayes per trovare la densità a posteriori, \\(f(\\theta,\\beta \\mid u)\\).\nNel modello di Rasch, la regola di Bayes afferma che:\n\\[\nf(\\theta,\\beta \\mid u) = \\frac{\\text{Pr}(u \\mid \\theta,\\beta)f(\\theta,\\beta)}{\\text{Pr}(u)}.\n\\]\nIl primo termine nel numeratore, \\(\\text{Pr}(u \\mid \\theta, \\beta)\\), è la verosimiglianza congiunta. Il secondo è la distribuzione a priori congiunta per \\(\\theta\\) e \\(\\beta\\). Il denominatore è la probabilità media dei dati osservati rispetto alla distribuzione a priori congiunta.\nA differenza della stima della massima verosimiglianza, che si concentra sulla massimizzazione della verosimiglianza, la stima bayesiana integra le informazioni a priori con i dati osservati. La regola di Bayes combina la verosimiglianza dei dati osservati (la probabilità di osservare i dati dati i parametri) con la distribuzione a priori (le nostre credenze sui parametri prima di osservare i dati) per produrre una distribuzione a posteriori (le nostre credenze aggiornate sui parametri dopo aver osservato i dati). La densità a posteriori \\(f(\\theta,\\beta \\mid u)\\) ci fornisce una stima completa dei parametri, considerando sia i dati osservati sia le informazioni a priori.\nIn pratica, la stima bayesiana fornisce un approccio flessibile e informativo alla stima dei parametri nel modello di Rasch, consentendo l’integrazione di conoscenze pregresse e osservazioni attuali.\n\n67.6.1 Implementazione\nEsaminiamo un’applicazione della stima Bayesiana usando il linguaggio probabilistico Stan. Il modello di Rasch è implementato nel file rasch_model.stan utilizzando le distribuzioni a priori specificate da Debelak et al. (2022).\n\nstan_file &lt;- \"../../code/rasch_model.stan\"\nmod &lt;- cmdstan_model(stan_file)\nmod$print()\n\ndata {\n  int&lt;lower=1&gt; num_person;\n  int&lt;lower=1&gt; num_item;\n  array[num_person, num_item] int&lt;lower=0, upper=1&gt; U;\n}\nparameters {\n  vector[num_person] theta;\n  vector[num_item] beta;\n  real mu_beta;\n  real&lt;lower=0&gt; sigma2_theta;\n  real&lt;lower=0&gt; sigma2_beta;\n}\ntransformed parameters {\n  array[num_person, num_item] real&lt;lower=0, upper=1&gt; prob_solve;\n  for (p in 1:num_person) \n    for (i in 1:num_item) \n      prob_solve[p, i] = inv_logit(theta[p] - beta[i]);\n}\nmodel {\n  for (p in 1:num_person) \n    for (i in 1:num_item) \n      U[p, i] ~ bernoulli(prob_solve[p, i]);\n  theta ~ normal(0, sqrt(sigma2_theta));\n  beta ~ normal(mu_beta, sqrt(sigma2_beta));\n  sigma2_theta ~ inv_chi_square(0.5);\n  sigma2_beta ~ inv_chi_square(0.5);\n}\n\n\n\n\nNella presente implementazione bayesiana del modello di Rasch, le sezioni “transformed parameters” e “model” hanno un ruolo centrale nel definire come i dati vengono processati e come il modello viene applicato. Vediamo dettagliatamente ciascuna sezione:\n\n67.6.1.1 Sezione Transformed Parameters\nNella sezione transformed parameters, viene definita la trasformazione dei parametri di base (i parametri theta per le abilità delle persone e beta per la difficoltà degli item) in una probabilità di risposta corretta per ogni coppia persona-item. Qui viene usata la funzione logistica inversa per convertire la differenza tra l’abilità della persona e la difficoltà dell’item in una probabilità:\ntransformed parameters {\n  array[num_person, num_item] real&lt;lower=0, upper=1&gt; prob_solve;\n  for (p in 1:num_person) \n    for (i in 1:num_item) \n      prob_solve[p, i] = inv_logit(theta[p] - beta[i]);\n}\nQuesta trasformazione serve a mappare la differenza tra l’abilità della persona (theta[p]) e la difficoltà dell’item (beta[i]) in un intervallo di probabilità tra 0 e 1. La funzione inv_logit è comunemente usata per questo scopo, essendo la funzione logistica inversa.\n\n\n67.6.1.2 Sezione Model\nNella sezione model, vengono definite le distribuzioni di probabilità per i dati osservati e i parametri del modello, che sono essenziali per la stima bayesiana. Questa parte del codice descrive come i dati sono generati, supponendo il modello di Rasch:\nmodel {\n  for (p in 1:num_person) \n    for (i in 1:num_item) \n      U[p, i] ~ bernoulli(prob_solve[p, i]);\n  theta ~ normal(0, sqrt(sigma2_theta));\n  beta ~ normal(mu_beta, sqrt(sigma2_beta));\n  sigma2_theta ~ inv_chi_square(0.5);\n  sigma2_beta ~ inv_chi_square(0.5);\n}\n\nU[p, i] ~ bernoulli(prob_solve[p, i]): ogni risposta U[p, i], che indica se la persona p ha risposto correttamente all’item i, segue una distribuzione di Bernoulli dove la probabilità di successo è data da prob_solve[p, i]. Questa è la vera verosimiglianza del modello, che collega i dati osservati alle probabilità calcolate tramite il modello logistico.\ntheta ~ normal(0, sqrt(sigma2_theta)) e beta ~ normal(mu_beta, sqrt(sigma2_beta)): le distribuzioni a priori per i parametri theta e beta sono normali. Questo significa che, in assenza di dati, si assume che queste variabili si distribuiscano normalmente con una media di 0 per theta e mu_beta per beta, e una deviazione standard derivata dai parametri di varianza sigma2_theta e sigma2_beta.\nsigma2_theta ~ inv_chi_square(0.5) e sigma2_beta ~ inv_chi_square(0.5): le varianze sigma2_theta e sigma2_beta hanno distribuzioni a priori che seguono una distribuzione chi quadrato inversa con parametro di forma 0.5. Questa è una scelta comune per imporre una distribuzione non informativa (vaga) sui parametri di scala.\n\nIn conclusione, la sezione transformed parameters calcola le probabilità di risposta corretta basate sui parametri di abilità e difficoltà, mentre la sezione model specifica come questi parametri e le risposte osservate interagiscono secondo il modello di Rasch, definendo così la struttura della verosimiglianza e delle priorità nel contesto bayesiano.\nCompiliamo il modello usando CmdStan:\n\nmod$compile()\n\nDefiniamo i dati nel formato appropriato per Stan:\n\ndata(data.fims.Aus.Jpn.scored, package = \"TAM\")\npeople &lt;- 1:400\nresponses &lt;- data.fims.Aus.Jpn.scored[people, 2:15]\nresponses &lt;- as.matrix(sapply(responses, as.integer))\ncolnames(responses) &lt;- gsub(\"M1PTI\", \"I\", colnames(responses))\n\nstan_data &lt;- list(\n    num_person = nrow(responses),\n    num_item = ncol(responses),\n    U = responses\n)\n\n\n    $num_person\n        400\n    $num_item\n        14\n    $U\n        \n\nA matrix: 400 x 14 of type int\n\n\nI1\nI2\nI3\nI6\nI7\nI11\nI12\nI14\nI17\nI18\nI19\nI21\nI22\nI23\n\n\n\n\n1\n0\n1\n1\n0\n1\n0\n0\n1\n0\n0\n0\n0\n1\n\n\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n1\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\n\n1\n1\n1\n1\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n1\n1\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n1\n\n\n1\n1\n1\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n\n\n0\n0\n1\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n0\n\n\n1\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n1\n\n\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n\n\n0\n1\n1\n0\n0\n1\n0\n0\n0\n1\n0\n0\n1\n1\n\n\n1\n1\n1\n1\n0\n1\n1\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n1\n1\n0\n1\n0\n1\n1\n1\n0\n0\n0\n0\n\n\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n0\n1\n1\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n1\n\n\n0\n0\n1\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n1\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n\n\n1\n1\n1\n1\n0\n1\n0\n1\n1\n0\n0\n0\n0\n0\n\n\n1\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n1\n0\n0\n1\n0\n1\n0\n1\n0\n0\n0\n1\n\n\n1\n1\n1\n0\n0\n1\n0\n1\n0\n1\n0\n0\n0\n1\n\n\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n1\n1\n1\n1\n0\n1\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n0\n1\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n1\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n0\n1\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n\n\n1\n0\n1\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n1\n\n\n0\n0\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n0\n\n\n1\n1\n1\n1\n0\n1\n0\n1\n1\n0\n0\n0\n0\n1\n\n\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n\n\n1\n1\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n0\n1\n1\n0\n1\n1\n1\n1\n0\n1\n0\n1\n0\n1\n\n\n0\n0\n1\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n1\n\n\n1\n1\n1\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n0\n0\n1\n0\n1\n1\n0\n1\n0\n1\n0\n0\n0\n1\n\n\n0\n0\n1\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n\n\n0\n1\n1\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n1\n\n\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n1\n\n\n1\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n0\n1\n1\n0\n0\n1\n0\n1\n0\n1\n0\n0\n0\n1\n\n\n1\n1\n1\n1\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n\n\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n\n\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n1\n\n\n1\n1\n1\n0\n0\n0\n1\n1\n0\n1\n0\n0\n0\n1\n\n\n1\n1\n1\n1\n0\n1\n0\n0\n0\n1\n0\n1\n0\n1\n\n\n1\n1\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n1\n1\n\n\n1\n1\n1\n1\n0\n1\n0\n1\n0\n1\n0\n0\n1\n1\n\n\n1\n1\n1\n1\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n\n\n1\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n1\n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n1\n\n\n1\n1\n1\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n1\n\n\n1\n1\n1\n1\n0\n1\n0\n0\n1\n1\n0\n0\n0\n1\n\n\n0\n1\n1\n1\n0\n1\n0\n0\n1\n0\n1\n0\n0\n1\n\n\n1\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n1\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n\n\n1\n1\n1\n0\n0\n1\n0\n1\n0\n1\n0\n0\n0\n1\n\n\n1\n1\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n\n\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\n\nEseguiamo il campionamento MCMC per ottenere la distribuzione a posteriori dei parametri.\n\nfit &lt;- mod$sample(\n    data = stan_data,\n    chains = 4, # Number of MCMC chains\n    parallel_chains = 2, # Number of chains to run in parallel \n    iter_warmup = 2000, # Number of warmup iterations per chain\n    iter_sampling = 2000, # Number of sampling iterations per chain\n    seed = 1234 # Set a seed for reproducibility\n)\n\nEsaminiamo le tracce per due parametri.\n\nfit_draws &lt;- fit$draws() # extract the posterior draws\nmcmc_trace(fit_draws, pars = c(\"beta[1]\"))\n\n\n\n\n\n\n\n\n\nmcmc_trace(fit_draws, pars = c(\"theta[1]\"))\n\n\n\n\n\n\n\n\nFocalizziamoci sulla stima dei parametri degli item.\n\nparameters &lt;- c(\n    \"beta[1]\", \"beta[2]\", \"beta[3]\", \"beta[4]\", \"beta[5]\",\n    \"beta[6]\", \"beta[7]\", \"beta[8]\", \"beta[9]\",\"beta[10]\",\n    \"beta[11]\", \"beta[12]\", \"beta[13]\", \"beta[14]\"\n)\n\nEsaminiamo la statistica rhat.\n\nrhats &lt;- rhat(fit_draws, pars = parameters)\nmcmc_rhat(rhats)\n\n\n\n\n\n\n\n\nEsaminiamo l’effect ratio:\n\neff_ratio &lt;- neff_ratio(fit, pars = parameters)\neff_ratio \n\nbeta[1]1.13228653874956beta[2]1.18893586285432beta[3]1.16206445337895beta[4]1.09833035160828beta[5]1.35717661914907beta[6]1.15952811419412beta[7]1.17595353070278beta[8]1.08208811843576beta[9]1.30070203631086beta[10]1.20775869658246beta[11]1.26276161662907beta[12]1.2345210973256beta[13]1.39244422596835beta[14]1.30555561451952\n\n\n\nmcmc_neff(eff_ratio)\n\n\n\n\n\n\n\n\nEsaminiamo l’autocorrelazione.\n\nmcmc_acf(fit_draws, pars = parameters)\n\n\n\n\n\n\n\n\nOtteniamo le statistiche riassuntive delle distribuzioni a posteriori dei parametri degli item.\n\nfit$summary(\n    variables = parameters,\n    posterior::default_summary_measures(),\n    extra_quantiles = ~ posterior::quantile2(., probs = c(.0275, .975))\n)\n\n\nA draws_summary: 14 x 9\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nq2.75\nq97.5\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nbeta[1]\n-1.0979\n-1.0981\n0.129\n0.126\n-1.311\n-0.892\n-1.351\n-0.849\n\n\nbeta[2]\n-1.2447\n-1.2443\n0.130\n0.130\n-1.458\n-1.033\n-1.496\n-0.997\n\n\nbeta[3]\n-2.0287\n-2.0246\n0.158\n0.156\n-2.295\n-1.769\n-2.337\n-1.729\n\n\nbeta[4]\n-0.0478\n-0.0489\n0.120\n0.121\n-0.241\n0.149\n-0.274\n0.191\n\n\nbeta[5]\n2.5117\n2.5087\n0.183\n0.185\n2.219\n2.819\n2.176\n2.885\n\n\nbeta[6]\n-1.2447\n-1.2440\n0.132\n0.131\n-1.463\n-1.029\n-1.505\n-0.989\n\n\nbeta[7]\n0.8112\n0.8099\n0.124\n0.124\n0.610\n1.013\n0.576\n1.055\n\n\nbeta[8]\n-0.4933\n-0.4928\n0.120\n0.119\n-0.688\n-0.294\n-0.723\n-0.263\n\n\nbeta[9]\n1.3228\n1.3214\n0.135\n0.138\n1.107\n1.546\n1.071\n1.583\n\n\nbeta[10]\n-0.3948\n-0.3951\n0.120\n0.121\n-0.592\n-0.196\n-0.622\n-0.158\n\n\nbeta[11]\n2.0529\n2.0529\n0.158\n0.159\n1.796\n2.313\n1.754\n2.366\n\n\nbeta[12]\n1.7469\n1.7445\n0.148\n0.146\n1.505\n1.993\n1.465\n2.041\n\n\nbeta[13]\n2.3963\n2.3938\n0.172\n0.172\n2.117\n2.686\n2.069\n2.736\n\n\nbeta[14]\n-1.9200\n-1.9184\n0.152\n0.148\n-2.175\n-1.674\n-2.223\n-1.629\n\n\n\n\n\nI risultati ottenuti replicano quelli riportati da Debelak et al. (2022).",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_estimation.html#grandezza-del-campione",
    "href": "chapters/irt/04_estimation.html#grandezza-del-campione",
    "title": "67  Stima",
    "section": "67.7 Grandezza del Campione",
    "text": "67.7 Grandezza del Campione\nLa stima dei parametri degli item basata su un campione osservato di risposte è spesso definita come la calibrazione degli item. Generalmente, un campione di calibrazione più ampio consente una stima più accurata dei parametri degli item, sebbene altri fattori influenzino anch’essi l’accuratezza della stima. Ad esempio, la difficoltà di un item può essere stimata con maggiore precisione se l’item non è né troppo facile né troppo difficile per il campione di partecipanti al test. Pertanto, i fattori che influenzano l’accuratezza della stima includono l’allineamento e la forma delle distribuzioni dei parametri degli item e delle persone, il numero di item e la tecnica di stima utilizzata.\nDiverse pubblicazioni hanno affrontato la questione della dimensione del campione tipicamente necessaria per lavorare con il modello di Rasch e come questa sia influenzata da questi e altri fattori. Ad esempio, De Ayala (2009) fornisce la linea guida generale che un campione di calibrazione dovrebbe contenere almeno diverse centinaia di rispondenti e cita, tra le altre referenze, un articolo precedente di Wright (1977) che afferma che un campione di calibrazione di 500 sarebbe più che adeguato. De Ayala (2009) suggerisce anche che 250 o più rispondenti sono necessari per adattare un modello di Partial Credit. Poiché il modello di Partial Credit è una generalizzazione del modello di Rasch con più parametri degli item, ciò implica che la dimensione del campione suggerita di 250 dovrebbe essere sufficiente anche per adattare un modello di Rasch. Studi più recenti hanno indagato l’applicazione del modello di Rasch con dimensioni del campione di soli 100 rispondenti (ad esempio, Steinfeld & Robitzsch, 2021; Suárez-Falcón & Glas, 2003). Tali linee guida non devono essere interpretate come regole fisse, ma solo come indicazioni generali in quanto una dimensione del campione adeguata dipende dalle condizioni e dagli obiettivi dell’analisi.\nUn metodo più elaborato per determinare la dimensione del campione necessaria è l’analisi della potenza statistica. Qui, l’accuratezza della stima desiderata o il rischio di falsi positivi e falsi negativi devono essere formalizzati prima dell’analisi. La dimensione del campione necessaria viene quindi determinata in base a queste considerazioni.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/04_estimation.html#session-info",
    "href": "chapters/irt/04_estimation.html#session-info",
    "title": "67  Stima",
    "section": "67.8 Session Info",
    "text": "67.8 Session Info\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats4    grid      stats     graphics  grDevices utils     datasets \n[8] methods   base     \n\nother attached packages:\n [1] effectsize_0.8.9    rsvg_2.6.1          rstan_2.32.6       \n [4] StanHeaders_2.32.10 posterior_1.6.0     cmdstanr_0.8.1.9000\n [7] ggmirt_0.1.0        TAM_4.2-21          CDM_8.2-6          \n[10] mvtnorm_1.3-2       mirt_1.43           lattice_0.22-6     \n[13] latex2exp_0.9.6     MASS_7.3-61         viridis_0.6.5      \n[16] viridisLite_0.4.2   ggpubr_0.6.0        ggExtra_0.10.1     \n[19] gridExtra_2.3       patchwork_1.3.0     bayesplot_1.11.1   \n[22] semTools_0.5-6      semPlot_1.1.6       lavaan_0.6-19      \n[25] psych_2.4.6.26      scales_1.3.0        markdown_1.13      \n[28] knitr_1.49          lubridate_1.9.3     forcats_1.0.0      \n[31] stringr_1.5.1       dplyr_1.1.4         purrr_1.0.2        \n[34] readr_2.1.5         tidyr_1.3.1         tibble_3.2.1       \n[37] ggplot2_3.5.1       tidyverse_2.0.0     here_1.0.1         \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.2        later_1.3.2          pbdZMQ_0.3-13       \n  [4] R.oo_1.27.0          datawizard_0.13.0    XML_3.99-0.17       \n  [7] rpart_4.1.23         lifecycle_1.0.4      rstatix_0.7.2       \n [10] rprojroot_2.0.4      processx_3.8.4       globals_0.16.3      \n [13] insight_0.20.5       rockchalk_1.8.157    backports_1.5.0     \n [16] magrittr_2.0.3       openxlsx_4.2.7.1     Hmisc_5.2-0         \n [19] rmarkdown_2.29       httpuv_1.6.15        qgraph_1.9.8        \n [22] zip_2.3.1            pkgbuild_1.4.5       sessioninfo_1.2.2   \n [25] pbapply_1.7-2        minqa_1.2.8          audio_0.1-11        \n [28] multcomp_1.4-26      abind_1.4-8          quadprog_1.5-8      \n [31] R.utils_2.12.3       tensorA_0.36.2.1     nnet_7.3-19         \n [34] TH.data_1.1-2        sandwich_3.1-1       inline_0.3.19       \n [37] listenv_0.9.1        testthat_3.2.1.1     openintro_2.5.0     \n [40] RPushbullet_0.3.4    vegan_2.6-8          arm_1.14-4          \n [43] airports_0.1.0       parallelly_1.39.0    permute_0.9-7       \n [46] codetools_0.2-20     tidyselect_1.2.1     farver_2.1.2        \n [49] lme4_1.1-35.5        matrixStats_1.4.1    base64enc_0.1-3     \n [52] jsonlite_1.8.9       polycor_0.8-1        progressr_0.15.0    \n [55] Formula_1.2-5        survival_3.7-0       emmeans_1.10.5      \n [58] tools_4.4.2          snow_0.4-4           Rcpp_1.0.13-1       \n [61] glue_1.8.0           mnormt_2.1.1         admisc_0.36         \n [64] xfun_0.49            mgcv_1.9-1           distributional_0.5.0\n [67] IRdisplay_1.1        loo_2.8.0            withr_3.0.2         \n [70] beepr_2.0            fastmap_1.2.0        boot_1.3-31         \n [73] fansi_1.0.6          digest_0.6.37        mi_1.1              \n [76] timechange_0.3.0     R6_2.5.1             mime_0.12           \n [79] estimability_1.5.1   colorspace_2.1-1     Cairo_1.6-2         \n [82] gtools_3.9.5         jpeg_0.1-10          R.methodsS3_1.8.2   \n [85] utf8_1.2.4           generics_0.1.3       data.table_1.16.2   \n [88] corpcor_1.6.10       usdata_0.3.1         SimDesign_2.17.1    \n [91] htmlwidgets_1.6.4    parameters_0.23.0    pkgconfig_2.0.3     \n [94] sem_3.1-16           gtable_0.3.6         brio_1.1.5          \n [97] htmltools_0.5.8.1    carData_3.0-5        png_0.1-8           \n[100] rstudioapi_0.17.1    tzdb_0.4.0           reshape2_1.4.4      \n[103] uuid_1.2-1           coda_0.19-4.1        checkmate_2.3.2     \n[106] nlme_3.1-166         curl_6.0.1           nloptr_2.1.1        \n[109] repr_1.1.7           zoo_1.8-12           parallel_4.4.2      \n[112] miniUI_0.1.1.1       foreign_0.8-87       pillar_1.9.0        \n[115] vctrs_0.6.5          promises_1.3.0       car_3.1-3           \n[118] OpenMx_2.21.13       xtable_1.8-4         Deriv_4.1.6         \n[121] cluster_2.1.6        dcurver_0.9.2        GPArotation_2024.3-1\n[124] htmlTable_2.4.3      evaluate_1.0.1       pbivnorm_0.6.0      \n[127] cli_3.6.3            kutils_1.73          compiler_4.4.2      \n[130] rlang_1.1.4          crayon_1.5.3         future.apply_1.11.3 \n[133] ggsignif_0.6.4       labeling_0.4.3       fdrtool_1.2.18      \n[136] ps_1.8.1             plyr_1.8.9           stringi_1.8.4       \n[139] QuickJSR_1.4.0       munsell_0.5.1        lisrelToR_0.3       \n[142] bayestestR_0.15.0    V8_6.0.0             pacman_0.5.1        \n[145] Matrix_1.7-1         IRkernel_1.3.2       hms_1.1.3           \n[148] glasso_1.11          future_1.34.0        shiny_1.9.1         \n[151] igraph_2.1.1         broom_1.0.7          RcppParallel_5.1.9  \n[154] cherryblossom_0.1.0 \n\n\n\n\n\n\nDebelak, R., Strobl, C., & Zeigenfuse, M. D. (2022). An introduction to the rasch model with examples in r. CRC Press.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Stima</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_1pl_2pl_3pl.html",
    "href": "chapters/irt/05_1pl_2pl_3pl.html",
    "title": "68  Modelli 1PL, 2PL e 3PL",
    "section": "",
    "text": "68.1 Introduzione\nNell’ambito dei modelli IRT, il modello di Rasch impone i vincoli più restrittivi. Tali vincoli possono essere rilassati progressivamente definendo quelli che vengono chiamati modelli 1PL, 2PL e 3PL.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Modelli 1PL, 2PL e 3PL</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_1pl_2pl_3pl.html#un-esempio-pratico",
    "href": "chapters/irt/05_1pl_2pl_3pl.html#un-esempio-pratico",
    "title": "68  Modelli 1PL, 2PL e 3PL",
    "section": "68.2 Un Esempio Pratico",
    "text": "68.2 Un Esempio Pratico\nIn questo capitolo, utilizzeremo nuovamente i dati che abbiamo esaminato in precedenza nel Capitolo 67.\n\ndata(data.fims.Aus.Jpn.scored, package = \"TAM\")\nfims &lt;- data.fims.Aus.Jpn.scored\n\nEsaminiamo le risposte dei primi 400 partecipanti. Per facilitare la manipolazione dei dati, cambiamo il nome delle colonne.\n\nresponses &lt;- fims[1:400, 2:15]\ncolnames(responses) &lt;- gsub(\"M1PTI\", \"I\", colnames(responses))\nglimpse(responses)\n\nRows: 400\nColumns: 14\n$ I1  &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,~\n$ I2  &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,~\n$ I3  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,~\n$ I6  &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,~\n$ I7  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ I11 &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,~\n$ I12 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ I14 &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,~\n$ I17 &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,~\n$ I18 &lt;dbl&gt; 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,~\n$ I19 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ I21 &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,~\n$ I22 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,~\n$ I23 &lt;dbl&gt; 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,~\n\n\n\ngender &lt;- as.factor(fims$SEX[1:400])\nlevels(gender) &lt;- c(\"male\", \"female\")",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Modelli 1PL, 2PL e 3PL</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_1pl_2pl_3pl.html#modello-1pl",
    "href": "chapters/irt/05_1pl_2pl_3pl.html#modello-1pl",
    "title": "68  Modelli 1PL, 2PL e 3PL",
    "section": "68.3 Modello 1PL",
    "text": "68.3 Modello 1PL\nIl modello ad un parametro logistico (1PL) può essere espresso matematicamente come:\n\\[\nP(X_i = 1 \\mid \\theta_v, \\alpha, \\delta_i) = \\frac{\\exp(\\alpha(\\theta_v - \\delta_i))}{1 + \\exp(\\alpha(\\theta_v - \\delta_i))} = \\frac{1}{1 + \\exp(-\\alpha(\\theta_v - \\delta_i))}, \\tag{1}\n\\]\ndove:\n\n\\(\\theta_v\\): livello di abilità del rispondente \\(v\\),\n\\(\\delta_i\\): parametro di difficoltà dell’item \\(i\\),\n\\(\\alpha\\): parametro di discriminazione dell’item.\n\nNel modello 1PL, \\(\\alpha\\) è fisso e uguale per tutti gli item, il che implica che la capacità di discriminazione è costante. L’assenza di un pedice per \\(\\alpha\\) riflette questa assunzione.\n\n68.3.1 Interpretazione del parametro \\(\\alpha\\)\nIl parametro \\(\\alpha\\) è direttamente collegato alla pendenza della curva caratteristica dell’item (ICC) e definisce quanto bene un item discrimina tra individui con diversi livelli di abilità \\(\\theta\\). Un \\(\\alpha\\) più grande produce una curva ICC più ripida, indicando una maggiore capacità dell’item di differenziare tra rispondenti con abilità vicine alla difficoltà dell’item (\\(\\delta_i\\)).\nNel modello 1PL:\n\nGli item con \\(\\alpha = 0\\) non discriminano e non forniscono informazioni utili.\nGli item con \\(\\alpha &gt; 0\\) hanno potere discriminatorio proporzionale al valore di \\(\\alpha\\).\n\n\nEsercizio 68.1 Supponiamo di avere tre item (\\(i_1, i_2, i_3\\)), tutti con difficoltà \\(\\delta = 0\\), e tre valori di discriminazione: \\(\\alpha_1 = 0.0\\), \\(\\alpha_2 = 1.0\\) e \\(\\alpha_3 = 2.0\\). Consideriamo due rispondenti: rispondente A con \\(\\theta_A = -1\\) e rispondente B con \\(\\theta_B = 1\\).\nItem con \\(\\alpha = 0.0\\). la curva ICC per l’item \\(i_1\\) è orizzontale (\\(\\alpha = 0.0\\)), indicando che la probabilità di risposta corretta è costante e pari a \\(0.5\\) per tutti i livelli di \\(\\theta\\):\n\\[\nP(X_i = 1 \\mid \\theta, \\delta) = 0.5.\n\\]\nIn questo caso, l’item non discrimina tra rispondenti con abilità diverse e non fornisce alcuna informazione utile.\nItem con \\(\\alpha = 1.0\\). Con \\(\\alpha = 1.0\\), le probabilità di risposta corretta per i rispondenti A e B sono calcolate come:\n\\[\nP(X_i = 1 \\mid \\theta, \\delta) = \\frac{1}{1 + \\exp(-(\\theta - \\delta))}.\n\\]\n\nPer \\(\\theta_A = -1\\): \\(P(X_i = 1) = 0.2689\\),\nPer \\(\\theta_B = 1\\): \\(P(X_i = 1) = 0.7311\\).\n\nL’item ora discrimina meglio tra i due rispondenti, producendo probabilità diverse.\nItem con \\(\\alpha = 2.0\\). Con \\(\\alpha = 2.0\\), la curva ICC è più ripida. Le probabilità di risposta corretta sono:\n\\[\nP(X_i = 1 \\mid \\theta, \\delta) = \\frac{1}{1 + \\exp(-2(\\theta - \\delta))}.\n\\]\n\nPer \\(\\theta_A = -1\\): \\(P(X_i = 1) = 0.1192\\),\nPer \\(\\theta_B = 1\\): \\(P(X_i = 1) = 0.8808\\).\n\nLa maggiore ripidità della curva ICC riflette una capacità superiore di discriminare tra rispondenti con abilità diverse.\nIn conclusione, un aumento di \\(\\alpha\\) produce una curva ICC più ripida, migliorando la capacità dell’item di distinguere tra rispondenti con abilità vicine alla sua difficoltà. Al contrario, una diminuzione di \\(\\alpha\\) rende la curva ICC più piatta, riducendo la capacità discriminatoria dell’item.\n\n\n\n68.3.2 Modello di Rasch e Modello 1PL\nIl modello 1PL e il modello di Rasch condividono l’assunzione di un parametro di discriminazione (\\(\\alpha\\)) costante per tutti gli item, pur consentendo che gli item differiscano in termini di difficoltà (\\(\\delta_i\\)). Tuttavia, nel modello di Rasch, \\(\\alpha\\) è fissato a 1.0, mentre nel modello 1PL, \\(\\alpha\\) può assumere un valore costante diverso da 1.0. Nonostante questa differenza, i due modelli sono matematicamente equivalenti: i parametri stimati in uno possono essere trasformati nell’altro tramite una riscalatura appropriata.\n\n68.3.2.1 Differenze concettuali tra modello di Rasch e modello 1PL\nOltre agli aspetti matematici, i due modelli differiscono nella filosofia sottostante:\n\nIl modello 1PL è orientato a ottimizzare l’adattamento ai dati osservati, cercando di descrivere il dataset nel miglior modo possibile all’interno della struttura del modello.\nIl modello di Rasch, invece, viene utilizzato come strumento per costruire la variabile di interesse, assumendo che il modello definisca una misura oggettiva della latente. Questo approccio si concentra sullo sviluppo di uno strumento di misurazione valido, piuttosto che sull’adattamento ai dati.\n\nSecondo questa prospettiva filosofica, il modello di Rasch non è solo un mezzo per analizzare i dati, ma uno standard per progettare e costruire strumenti di misura. Questa visione è stata approfondita da studiosi come Andrich (1988), Wilson (2005), Wright (1984), Wright & Masters (1982) e Wright & Stone (1979).\n\n\n68.3.2.2 Un’analogia con le scienze fisiche\nLa filosofia del modello di Rasch può essere paragonata alla misurazione di grandezze fisiche, come il tempo. Ad esempio:\n\nLa misurazione del tempo si basa su un processo ripetitivo (ad esempio, l’oscillazione di un pendolo), che definisce unità costanti e uniformi.\nAnalogamente, nel modello di Rasch, l’unità di misura è il logit, ossia la distanza sul continuum latente che corrisponde a un aumento del rapporto di successo di un fattore pari alla costante trascendentale \\(e\\).\n\nQuesta unità è costante e consente di misurare la variabile latente con una metrica stabile, proprio come avviene nelle scienze fisiche.\n\n\n68.3.2.3 Terminologia adottata\nPer semplificare la discussione, useremo il termine modello 1PL per riferirci sia al caso in cui \\(\\alpha = 1.0\\) (il modello di Rasch), sia al caso in cui \\(\\alpha\\) assume un altro valore costante. Tuttavia:\n\nQuando ci riferiamo specificamente al modello di Rasch, intendiamo la situazione in cui \\(\\alpha = 1.0\\).\nInoltre, l’uso del termine “modello di Rasch” implica una visione filosofica secondo cui il modello è uno standard per la costruzione della variabile di interesse, piuttosto che un semplice strumento di adattamento ai dati.\n\n\nIn sintesi, il modello di Rasch e il modello 1PL possono essere considerati varianti di una stessa formulazione matematica, ma differiscono nel loro utilizzo e nella loro interpretazione concettuale. Il modello di Rasch si distingue per il suo ruolo nella costruzione di misure oggettive e per l’importanza attribuita alla definizione di una metrica latente stabile.\n\nEsercizio 68.2 In \\(\\mathsf{R}\\), il modello di Rasch si implementa nel modo seguente:\n\nmirt_rm &lt;- mirt(responses, 1, \"Rasch\", verbose = FALSE)\n\nIl modello 1PL si implementa nel modo seguente:\n\nmirt_1pl &lt;- mirt(responses, 1, \"1PL\", verbose = FALSE)\n\nConfrontiamo i due modelli:\n\nanova(mirt_rm, mirt_1pl)\n\n\nA mirt_df: 2 x 8\n\n\n\nAIC\nSABIC\nHQ\nBIC\nlogLik\nX2\ndf\np\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nmirt_rm\n5663\n5675\n5687\n5723\n-2816\nNA\nNA\nNA\n\n\nmirt_1pl\n5662\n5673\n5684\n5718\n-2817\n-0.705\n-1\nNaN\n\n\n\n\n\nI modelli mirt_rm e mirt_1pl sono praticamente equivalenti in termini di adattamento ai dati. Il modello mirt_1pl mostra lievi miglioramenti nei criteri di informazione (AIC e BIC), ma la differenza è minima.\n\n\nEsercizio 68.3 In questo esercizio, esploreremo il concetto di discriminazione degli item utilizzando Curve Caratteristiche dell’Item (ICC) per due item con discriminazioni diverse. L’obiettivo è evidenziare come il parametro di discriminazione (\\(\\beta\\)) influisca sulla capacità di un item di distinguere tra rispondenti con diversi livelli di abilità (\\(\\theta\\)).\nLa funzione sigmoide rappresenta l’ICC, calcolando la probabilità che un rispondente con abilità \\(\\theta\\) risponda correttamente a un item con discriminazione \\(\\beta\\) e difficoltà fissata a zero (\\(\\delta = 0\\)):\n\nsigmoid &lt;- function(x, beta) {\n    1 / (1 + exp(-beta * (x - 0)))\n}\n\nConsideriamo due item con discriminazioni diverse:\n\nItem 1: Discriminazione inferiore (\\(\\beta = 1\\)),\nItem 2: Discriminazione superiore (\\(\\beta = 2\\)).\n\nUtilizziamo una sequenza di valori di abilità (\\(\\theta\\)) da -2 a 2 per calcolare le probabilità previste per ciascun item.\n\n# Sequenza di livelli di abilità\nabilities &lt;- seq(-2, 2, length.out = 100)\n\n# Probabilità per gli item\nprobabilities_low &lt;- sigmoid(abilities, beta = 1)\nprobabilities_high &lt;- sigmoid(abilities, beta = 2)\n\nTracciamo le ICC per i due item, evidenziando come la curva diventi più ripida all’aumentare del parametro di discriminazione:\n\n# Creazione del dataframe per ggplot\ndata &lt;- data.frame(\n    Ability = rep(abilities, 2),\n    Probability = c(probabilities_low, probabilities_high),\n    Item = rep(c(\"Discriminazione Inferiore (beta=1)\", \"Discriminazione Superiore (beta=2)\"), each = 100)\n)\n\n# Creazione del grafico\nggplot(data, aes(x = Ability, y = Probability, color = Item)) +\n    geom_line() +\n    scale_color_manual(values = c(\"blue\", \"orange\")) +\n    ggtitle(\"Curve Caratteristiche dell'Item (ICC)\\n per Diversi Valori di Discriminazione\") +\n    xlab(\"Abilita' (theta)\") +\n    ylab(\"Probabilita' di Risposta Corretta\") +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nCalcoliamo la probabilità di risposta corretta per due livelli di abilità (\\(\\theta = -0.5\\) e \\(\\theta = 0.5\\)) per entrambi gli item:\n\nsigmoid(-0.5, 1) # Discriminazione inferiore\nsigmoid(0.5, 1)\n\nsigmoid(-0.5, 2) # Discriminazione superiore\nsigmoid(0.5, 2)\n\n0.377540668798145\n\n\n0.622459331201855\n\n\n0.268941421369995\n\n\n0.731058578630005\n\n\nRisultati:\n\nItem con \\(\\beta = 1\\):\n\n\\(P(\\theta = -0.5) = 0.38\\), \\(P(\\theta = 0.5) = 0.62\\)\nDifferenza: \\(0.62 - 0.38 = 0.24\\)\n\nItem con \\(\\beta = 2\\):\n\n\\(P(\\theta = -0.5) = 0.27\\), \\(P(\\theta = 0.5) = 0.73\\)\nDifferenza: \\(0.73 - 0.27 = 0.46\\)\n\n\nInterpretazione\n\nPer l’item con discriminazione inferiore (\\(\\beta = 1\\)), la differenza di probabilità tra i due rispondenti è minore, e la curva ICC è più piatta.\nPer l’item con discriminazione superiore (\\(\\beta = 2\\)), la differenza di probabilità è maggiore, e la curva ICC è più ripida. Questo indica che l’item è più sensibile alle differenze nelle abilità dei rispondenti.\n\nIl parametro di discriminazione può essere interpretato come una misura della pendenza della curva caratteristica dell’item (ICC) al centro, dove la curva assume un andamento quasi lineare. La pendenza è direttamente proporzionale al valore del parametro di discriminazione:\n\nPer un item con \\(\\beta = 1\\), la pendenza approssimata è \\(\\frac{0.24}{1} = 0.24\\),\nPer un item con \\(\\beta = 2\\), la pendenza approssimata è \\(\\frac{0.46}{1} = 0.46\\).\n\nQuesta relazione evidenzia come un valore maggiore di \\(\\beta\\) corrisponda a una curva ICC più ripida e a una maggiore capacità dell’item di distinguere tra rispondenti con abilità vicine.\nConfronto con il Modello di Rasch\nNel modello di Rasch, il parametro di discriminazione è fisso a \\(\\beta = 1\\), il che implica che tutti gli item abbiano la stessa pendenza. Questo semplifica il modello, ma potrebbe non catturare differenze reali nella capacità discriminatoria degli item.\nIn conclusione, il parametro di discriminazione determina quanto efficacemente un item può distinguere tra rispondenti con abilità simili. Item con una maggiore discriminazione (\\(\\beta\\)) presentano curve ICC più ripide e sono più sensibili alle differenze tra i livelli di abilità.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Modelli 1PL, 2PL e 3PL</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_1pl_2pl_3pl.html#modello-2pl",
    "href": "chapters/irt/05_1pl_2pl_3pl.html#modello-2pl",
    "title": "68  Modelli 1PL, 2PL e 3PL",
    "section": "68.4 Modello 2PL",
    "text": "68.4 Modello 2PL\nIl modello 2PL (Modello IRT a due parametri) è un’estensione del modello 1PL che consente alle curve caratteristiche degli item (ICC) di avere pendenze diverse. Questo significa che, a differenza del modello 1PL e del modello di Rasch, le curve degli item non sono necessariamente parallele tra loro.\nNel modello 2PL, ogni item è descritto da due parametri:\n\nParametro di difficoltà (\\(b\\)): Rappresenta il livello di abilità necessario affinché la probabilità di risposta corretta sia pari al 50%.\nParametro di discriminazione (\\(a\\)): Controlla la pendenza della curva ICC, cioè la capacità dell’item di distinguere tra rispondenti con abilità diverse.\n\nLa formula per le curve caratteristiche degli item nel modello 2PL è:\n\\[\nP(X_i = 1 \\mid \\theta, a_i, b_i) = \\frac{1}{1 + \\exp(-a_i (\\theta - b_i))},\n\\]\ndove:\n\n\\(\\theta\\) è il livello di abilità del rispondente,\n\\(a_i\\) è il parametro di discriminazione dell’item \\(i\\),\n\\(b_i\\) è il parametro di difficoltà dell’item \\(i\\).\n\n\nEsercizio 68.4 Utilizziamo il pacchetto mirt per adattare il modello 2PL ai dati. La funzione mirt() permette di stimare i parametri del modello specificando la struttura 2PL:\n\nmirt_2pl &lt;- mirt(responses, 1, \"2PL\")\n\nIteration: 14, Log-Lik: -2759.601, Max-Change: 0.00009\n\n\nUna volta adattato il modello, possiamo esaminare le curve ICC utilizzando il comando plot():\n\nplot(mirt_2pl, type = \"trace\")\n\n\n\n\n\n\n\n\nSe preferiamo visualizzare tutte le ICC in un singolo grafico (senza suddividere per item), possiamo aggiungere l’opzione facet_items = FALSE:\n\nplot(mirt_2pl, type = \"trace\", facet_items = FALSE)\n\n\n\n\n\n\n\n\nUtilizziamo la funzione coef() per ottenere le stime dei parametri degli item:\n\ncoef(mirt_2pl, IRTpars = TRUE, simplify = TRUE)\n\n\n    $items\n        \n\nA matrix: 14 x 4 of type dbl\n\n\n\na\nb\ng\nu\n\n\n\n\nI1\n1.147\n-1.0215\n0\n1\n\n\nI2\n1.769\n-0.9107\n0\n1\n\n\nI3\n1.372\n-1.6798\n0\n1\n\n\nI6\n1.479\n-0.0425\n0\n1\n\n\nI7\n1.071\n2.4414\n0\n1\n\n\nI11\n1.594\n-0.9570\n0\n1\n\n\nI12\n0.703\n1.0789\n0\n1\n\n\nI14\n0.771\n-0.6124\n0\n1\n\n\nI17\n0.707\n1.7577\n0\n1\n\n\nI18\n0.750\n-0.5019\n0\n1\n\n\nI19\n1.831\n1.4590\n0\n1\n\n\nI21\n-0.214\n-7.0761\n0\n1\n\n\nI22\n0.277\n7.6569\n0\n1\n\n\nI23\n1.521\n-1.5034\n0\n1\n\n\n\n\n\n    $means\n        F1: 0\n    $cov\n        \n\nA matrix: 1 x 1 of type dbl\n\n\n\nF1\n\n\n\n\nF1\n1\n\n\n\n\n\n\n\n\nLe stime includono:\n\n\\(a\\): parametro di discriminazione,\n\\(b\\): parametro di difficoltà,\n\\(g\\) e \\(u\\): parametri opzionali per la pseudo-chance e il limite superiore (non rilevanti per il modello 2PL).\n\nConfronto tra Modello 1PL e Modello 2PL\nPer verificare quale modello si adatta meglio ai dati, possiamo confrontare il modello 1PL (\\(\\alpha\\) fisso per tutti gli item) con il modello 2PL (\\(\\alpha\\) variabile) utilizzando la funzione anova():\n\nanova(mirt_rm, mirt_2pl)\n\n\nA mirt_df: 2 x 8\n\n\n\nAIC\nSABIC\nHQ\nBIC\nlogLik\nX2\ndf\np\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nmirt_rm\n5663\n5675\n5687\n5723\n-2816\nNA\nNA\nNA\n\n\nmirt_2pl\n5575\n5598\n5619\n5687\n-2760\n114\n13\n0\n\n\n\n\n\n\nIl modello 2PL ha valori di AIC, BIC e log-likelihood migliori rispetto al modello 1PL, indicando un adattamento più accurato ai dati.\nIl test di \\(X^2\\) e il p-value (\\(p = 0\\)) suggeriscono che le differenze tra i modelli sono significative: il modello 2PL spiega meglio la variazione nei dati.\nQuesto risultato implica che gli item differiscono non solo per la difficoltà (\\(b\\)), ma anche per la discriminazione (\\(a\\)).\n\nIn conclusione, il modello 2PL supera il modello 1PL in termini di adattamento ai dati quando gli item hanno diverse capacità di discriminazione.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Modelli 1PL, 2PL e 3PL</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_1pl_2pl_3pl.html#modello-3pl",
    "href": "chapters/irt/05_1pl_2pl_3pl.html#modello-3pl",
    "title": "68  Modelli 1PL, 2PL e 3PL",
    "section": "68.5 Modello 3PL",
    "text": "68.5 Modello 3PL\nIl modello IRT a tre parametri (3PL) è una generalizzazione del modello 2PL che introduce un terzo parametro, noto come guessing (\\(g\\)), per tener conto della probabilità di indovinare una risposta corretta. Questo parametro è particolarmente utile nei test a scelta multipla, dove un rispondente con abilità latente molto bassa potrebbe comunque rispondere correttamente a un item semplicemente per caso.\nFormula del modello 3PL\nLa probabilità di risposta corretta è definita come:\n\\[\nP(X_i = 1 \\mid \\theta, a_i, b_i, g_i) = g_i + (1 - g_i) \\frac{1}{1 + \\exp(-a_i (\\theta - b_i))},\n\\]\ndove:\n\n\\(\\theta\\): livello di abilità del rispondente,\n\\(a_i\\): parametro di discriminazione dell’item \\(i\\),\n\\(b_i\\): parametro di difficoltà dell’item \\(i\\),\n\\(g_i\\): parametro di guessing, che rappresenta il limite inferiore della curva ICC.\n\nCaratteristiche principali del modello 3PL\n\nParametro di guessing (\\(g\\)):\n\nIntroduce un asintoto inferiore maggiore di zero nella curva ICC.\nAd esempio, un item con \\(g = 0.25\\) implica che anche i rispondenti con abilità molto bassa (\\(\\theta \\to -\\infty\\)) abbiano una probabilità di risposta corretta pari almeno al 25%, corrispondente alla probabilità di indovinare in un test a scelta multipla con quattro opzioni.\n\nFlessibilità rispetto al modello 2PL:\n\nIl modello 3PL può rappresentare meglio il comportamento degli item, includendo la possibilità di indovinare le risposte.\nLa presenza del parametro \\(g\\) rende il modello più complesso e richiede un numero maggiore di dati per ottenere stime affidabili.\n\nCurve caratteristiche degli item (ICC):\n\nCon il parametro \\(g\\), la probabilità di risposta corretta non scende mai al di sotto del valore \\(g\\), anche per i rispondenti con abilità molto bassa.\n\n\n\nEsercizio 68.5 Utilizziamo il pacchetto mirt per stimare i parametri del modello 3PL:\n\nmirt_3pl &lt;- mirt(responses, 1, \"3PL\")\n\nIteration: 41, Log-Lik: -2740.210, Max-Change: 0.00009\n\n\nLe curve ICC possono essere visualizzate con il comando:\n\nplot(mirt_3pl, type = \"trace\", facet_items = FALSE)\n\n\n\n\n\n\n\n\nUtilizziamo la funzione coef() per ottenere le stime dei parametri degli item (\\(a\\), \\(b\\), \\(g\\)):\n\ncoef(mirt_3pl, IRTpars = TRUE, simplify = TRUE)\n\n\n    $items\n        \n\nA matrix: 14 x 4 of type dbl\n\n\n\na\nb\ng\nu\n\n\n\n\nI1\n1.410\n-0.49321\n2.44e-01\n1\n\n\nI2\n2.667\n-0.40358\n2.91e-01\n1\n\n\nI3\n1.445\n-1.63253\n1.22e-04\n1\n\n\nI6\n1.514\n0.00861\n1.95e-02\n1\n\n\nI7\n1.163\n2.29703\n1.18e-05\n1\n\n\nI11\n1.487\n-0.98362\n1.45e-05\n1\n\n\nI12\n2.131\n1.27506\n2.07e-01\n1\n\n\nI14\n0.805\n-0.58574\n6.14e-05\n1\n\n\nI17\n3.076\n1.31998\n1.38e-01\n1\n\n\nI18\n0.724\n-0.51032\n4.10e-05\n1\n\n\nI19\n1.844\n1.44861\n3.54e-06\n1\n\n\nI21\n-5.264\n-2.27062\n1.67e-01\n1\n\n\nI22\n9.179\n1.98999\n8.97e-02\n1\n\n\nI23\n1.625\n-1.45227\n4.40e-05\n1\n\n\n\n\n\n    $means\n        F1: 0\n    $cov\n        \n\nA matrix: 1 x 1 of type dbl\n\n\n\nF1\n\n\n\n\nF1\n1\n\n\n\n\n\n\n\n\nConfronto tra modelli 2PL e 3PL\n\nanova(mirt_2pl, mirt_3pl)\n\n\nA mirt_df: 2 x 8\n\n\n\nAIC\nSABIC\nHQ\nBIC\nlogLik\nX2\ndf\np\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nmirt_2pl\n5575\n5598\n5619\n5687\n-2760\nNA\nNA\nNA\n\n\nmirt_3pl\n5564\n5599\n5631\n5732\n-2740\n38.8\n14\n0.000394\n\n\n\n\n\n\nIl modello 3PL presenta un AIC inferiore rispetto al modello 2PL, suggerendo un miglior adattamento ai dati.\nTuttavia, il BIC penalizza maggiormente la complessità del modello, favorendo leggermente il modello 2PL.\nLa significatività del test \\(X^2\\) (\\(p = 0.0004\\)) indica che il modello 3PL offre un miglioramento significativo rispetto al modello 2PL.\n\nValutazione della bontà dell’adattamento\nPer verificare se il modello 3PL rappresenta adeguatamente i dati, utilizziamo la statistica \\(M2\\):\n\nM2(mirt_3pl)\n\n\nA data.frame: 1 x 9\n\n\n\nM2\ndf\np\nRMSEA\nRMSEA_5\nRMSEA_95\nSRMSR\nTLI\nCFI\n\n\n\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nstats\n76.1\n63\n0.125\n0.0228\n0\n0.0394\n0.0453\n0.975\n0.983\n\n\n\n\n\n\nIl valore \\(p = 0.125\\) indica che il modello 3PL non può essere rifiutato come rappresentazione adeguata dei dati.\nIl RMSEA inferiore a 0.05 (limite superiore: 0.039) suggerisce un buon adattamento.\n\nAdattamento degli item\nUtilizziamo la funzione itemfit() per calcolare statistiche che valutano quanto bene ciascun item si adatta al modello:\n\nitemfit(mirt_3pl, \"infit\", method = \"ML\") # infit and outfit stats\n\n\nA mirt_df: 14 x 5\n\n\nitem\noutfit\nz.outfit\ninfit\nz.infit\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nI1\n1.005\n0.0842\n0.983\n-0.273\n\n\nI2\n0.865\n-0.3667\n0.873\n-2.016\n\n\nI3\n0.876\n-0.4039\n0.909\n-0.940\n\n\nI6\n0.981\n-0.1451\n0.916\n-1.503\n\n\nI7\n0.783\n-0.7974\n0.896\n-0.899\n\n\nI11\n1.462\n2.3832\n0.886\n-1.567\n\n\nI12\n0.945\n-0.9293\n0.942\n-1.157\n\n\nI14\n0.967\n-0.5649\n1.009\n0.223\n\n\nI17\n0.784\n-2.4566\n0.807\n-2.709\n\n\nI18\n1.025\n0.5391\n1.013\n0.349\n\n\nI19\n0.590\n-1.3929\n0.816\n-2.165\n\n\nI21\n0.915\n-0.9834\n0.917\n-0.963\n\n\nI22\n0.860\n-0.9783\n0.844\n-1.160\n\n\nI23\n0.778\n-0.7361\n0.872\n-1.370\n\n\n\n\n\n\nL’output riporta i valori di infit e outfit per ciascun item del modello 3PL, insieme ai relativi z-score (\\(z.\\text{infit}\\) e \\(z.\\text{outfit}\\)), che indicano la distanza standardizzata tra i valori osservati e attesi.\n\nValori attesi per infit e outfit:\n\nBuon adattamento: Valori compresi tra 0.7 e 1.3 indicano un buon adattamento dell’item al modello.\nProblemi di adattamento:\n\nValori inferiori a 0.7: L’item è troppo prevedibile o ridondante.\nValori superiori a 1.3: L’item presenta risposte inaspettate rispetto al modello.\n\n\nInterpretazione degli z-score:\n\n\\(|z| \\leq 2\\): L’adattamento è accettabile.\n\\(|z| &gt; 2\\): L’item potrebbe non adattarsi bene al modello.\n\n\nAnalisi dei risultati 1. Item con buon adattamento (infit e outfit accettabili):\n\nI1:\n\nOutfit: \\(1.005\\) (ottimo),\nInfit: \\(0.983\\) (ottimo),\n\\(z.\\text{outfit} = 0.084\\), \\(z.\\text{infit} = -0.273\\): L’item si adatta bene.\n\nI3, I6, I14, I18, I21:\n\nValori di infit e outfit compresi tra 0.7 e 1.3, con \\(|z|\\) accettabili.\n\n\n\nItem con infit e outfit problematici:\n\nI11:\n\nOutfit: \\(1.462\\) (superiore a 1.3),\n\\(z.\\text{outfit} = 2.383\\): Indica risposte inaspettate per rispondenti con livelli di abilità lontani dalla difficoltà dell’item.\nInfit: \\(0.886\\) (accettabile), ma \\(z.\\text{infit} = -1.567\\): Non critico, ma da monitorare.\n\nI17:\n\nOutfit: \\(0.784\\) (accettabile),\nInfit: \\(0.807\\), ma \\(z.\\text{infit} = -2.709\\): Indica discrepanze significative per rispondenti con abilità vicine alla difficoltà dell’item.\n\n\nItem con adattamento troppo prevedibile (infit e outfit &lt; 0.7):\n\nNessun item ha valori inferiori a 0.7, quindi non ci sono problemi di prevedibilità o ridondanza.\n\nItem con discrepanze significative da monitorare:\n\nI19:\n\nOutfit: \\(0.590\\) (inferiore a 0.7, troppo prevedibile),\nInfit: \\(0.816\\), con \\(z.\\text{infit} = -2.165\\): L’item potrebbe essere ridondante o eccessivamente facile.\n\n\n\nRaccomandazioni\n\nLa maggior parte degli item si adatta bene al modello, con valori di infit e outfit entro l’intervallo accettabile.\nVerificare il contenuto e il formato degli item I11 e I17. Potrebbero contenere ambiguità o comportarsi in modo non previsto per alcuni livelli di abilità.\nL’item I19 potrebbe essere troppo facile o ridondante. Considerare la revisione o la rimozione.\nPer gli item con discrepanze significative (\\(|z| &gt; 2\\)), si può considerare un’analisi qualitativa e, se necessario, modificare o rimuovere gli item per migliorare l’affidabilità complessiva del test.\n\nIn sintesi, il test mostra un buon adattamento generale al modello 3PL, ma alcuni item richiedono attenzione per garantire una misurazione più precisa e affidabile.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Modelli 1PL, 2PL e 3PL</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_1pl_2pl_3pl.html#invarianza-di-gruppo-dei-parametri-degli-item-nella-irt",
    "href": "chapters/irt/05_1pl_2pl_3pl.html#invarianza-di-gruppo-dei-parametri-degli-item-nella-irt",
    "title": "68  Modelli 1PL, 2PL e 3PL",
    "section": "68.6 Invarianza di Gruppo dei Parametri degli Item nella IRT",
    "text": "68.6 Invarianza di Gruppo dei Parametri degli Item nella IRT\nUna delle proprietà distintive della IRT è l’invarianza di gruppo dei parametri degli item, che garantisce che i parametri di un item (difficoltà, discriminazione, guessing) siano intrinseci all’item stesso, indipendentemente dal livello di abilità della popolazione esaminata. Questo principio può essere descritto nei seguenti termini:\nPrincipio di Invarianza\nConsideriamo due gruppi di esaminandi estratti dalla stessa popolazione:\n\nGruppo 1: Livelli di abilità compresi tra -3 e -1 (media = -2).\nGruppo 2: Livelli di abilità compresi tra +1 e +3 (media = +2).\n\nAttraverso la stima a massima verosimiglianza:\n\nSi calcola la proporzione di risposte corrette per ciascun livello di abilità in entrambi i gruppi.\nSi adatta una curva caratteristica dell’item (ICC) ai dati, ottenendo stime dei parametri come \\(a = 1.27\\) (discriminazione) e \\(b = 0.39\\) (difficoltà).\n\nIl punto chiave è che queste stime rimangono costanti:\n\nGli stessi valori di \\(a\\) e \\(b\\) sarebbero ottenuti analizzando esclusivamente i dati del Gruppo 1 o del Gruppo 2.\nQuesto dimostra che i parametri sono una proprietà dell’item stesso e non dipendono dalla distribuzione di abilità del gruppo esaminato.\n\n\nEsercizio 68.6 Il seguente script R visualizza l’invarianza dei parametri tra due gruppi con abilità diverse utilizzando una curva ICC:\n\ngroupinv &lt;- function(mdl, t1l, t1u, t2l, t2u) {\n    if (missing(t1l)) t1l &lt;- -3\n    if (missing(t1u)) t1u &lt;- -1\n    if (missing(t2l)) t2l &lt;- 1\n    if (missing(t2u)) t2u &lt;- 3\n    theta &lt;- seq(-3, 3, .1875)\n    f &lt;- rep(21, length(theta))\n    wb &lt;- round(runif(1, -3, 3), 2)\n    wa &lt;- round(runif(1, 0.2, 2.8), 2)\n    wc &lt;- round(runif(1, 0, .35), 2)\n    if (mdl == 1 | mdl == 2) {\n        wc &lt;- 0\n    }\n    if (mdl == 1) {\n        wa &lt;- 1\n    }\n    for (g in 1:length(theta)) {\n        P &lt;- wc + (1 - wc) / (1 + exp(-wa * (theta - wb)))\n    }\n    p &lt;- rbinom(length(theta), f, P) / f\n    lowerg1 &lt;- 0\n    for (g in 1:length(theta)) {\n        if (theta[g] &lt;= t1l) {\n            lowerg1 &lt;- lowerg1 + 1\n        }\n    }\n    upperg1 &lt;- 0\n    for (g in 1:length(theta)) {\n        if (theta[g] &lt;= t1u) {\n            upperg1 &lt;- upperg1 + 1\n        }\n    }\n    theta1 &lt;- theta[lowerg1:upperg1]\n    p1 &lt;- p[lowerg1:upperg1]\n    lowerg2 &lt;- 0\n    for (g in 1:length(theta)) {\n        if (theta[g] &lt;= t2l) {\n            lowerg2 &lt;- lowerg2 + 1\n        }\n    }\n    upperg2 &lt;- 0\n    for (g in 1:length(theta)) {\n        if (theta[g] &lt;= t2u) {\n            upperg2 &lt;- upperg2 + 1\n        }\n    }\n    theta2 &lt;- theta[lowerg2:upperg2]\n    p2 &lt;- p[lowerg2:upperg2]\n    theta12 &lt;- c(theta1, theta2)\n    p12 &lt;- c(p1, p2)\n    par(lab = c(7, 5, 3))\n    plot(theta12, p12,\n        xlim = c(-3, 3), ylim = c(0, 1),\n        xlab = \"Ability\", ylab = \"Probability of Correct Response\"\n    )\n    if (mdl == 1) {\n        maintext &lt;- paste(\"Pooled Groups\", \"\\n\", \"b=\", wb)\n    }\n    if (mdl == 2) {\n        maintext &lt;- paste(\"Pooled Groups\", \"\\n\", \"a=\", wa, \"b=\", wb)\n    }\n    if (mdl == 3) {\n        maintext &lt;- paste(\n            \"Pooled Groups\", \"\\n\",\n            \"a=\", wa, \"b=\", wb, \"c=\", wc\n        )\n    }\n    par(new = \"T\")\n    plot(theta, P,\n        xlim = c(-3, 3), ylim = c(0, 1), type = \"l\",\n        xlab = \"\", ylab = \"\", main = maintext\n    )\n}\n\n\nset.seed(1)\ngroupinv(1, -3, -1, 1, 3)\n\n\n\n\n\n\n\n\nQuesto genera due segmenti di una curva caratteristica dell’item, uno per ciascun gruppo. Le stime dei parametri restano coerenti per entrambi i gruppi, confermando l’invarianza di gruppo.\n\n\n68.6.1 Confronto con la Teoria Classica dei Test\nNella Teoria Classica dei Test (TCT), la difficoltà di un item viene calcolata come la proporzione complessiva di risposte corrette. Questo valore è fortemente influenzato dal livello medio di abilità del gruppo esaminato, rendendo la difficoltà dell’item dipendente dalla popolazione. Al contrario, nella Teoria della Risposta all’Item (IRT), la difficoltà di un item (( b )) è una proprietà intrinseca e indipendente dalla popolazione che risponde al test. Questa caratteristica rende la IRT particolarmente adatta per confronti tra popolazioni diverse.\nTuttavia, l’invarianza dei parametri degli item presenta alcune limitazioni. Sebbene teoricamente i parametri siano invarianti, le stime empiriche possono variare a causa dell’errore campionario. La precisione di queste stime dipende dalla dimensione del campione e dalla varianza dei dati. Inoltre, l’invarianza è valida solo se l’item misura lo stesso tratto latente in entrambi i gruppi. Se un item viene utilizzato per valutare un tratto diverso o in contesti non comparabili, l’invarianza dei parametri non può essere garantita.\nLa curva caratteristica dell’item (ICC) rappresenta la probabilità di risposta corretta in funzione dell’abilità latente. L’invarianza implica che la forma e i parametri della curva non siano influenzati dalla distribuzione delle abilità all’interno della popolazione. Questo significa che anche analizzando solo un segmento limitato della curva, è possibile stimare accuratamente i parametri di un item, confermando la robustezza della IRT rispetto alla distribuzione degli esaminandi.\nIn conclusione, l’invarianza di gruppo dei parametri degli item è una caratteristica fondamentale della IRT, che consente di confrontare i risultati tra popolazioni diverse senza essere condizionati dalle differenze nei livelli di abilità. Questo principio garantisce che i parametri siano intrinseci all’item e non dipendano dal gruppo esaminato. Tuttavia, bisogna considerare che le stime empiriche sono soggette a errore campionario e che l’invarianza è valida solo quando l’item è utilizzato per misurare lo stesso tratto latente in popolazioni comparabili.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Modelli 1PL, 2PL e 3PL</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_1pl_2pl_3pl.html#considerazioni-conclusive",
    "href": "chapters/irt/05_1pl_2pl_3pl.html#considerazioni-conclusive",
    "title": "68  Modelli 1PL, 2PL e 3PL",
    "section": "68.7 Considerazioni Conclusive",
    "text": "68.7 Considerazioni Conclusive\nIl pacchetto mirt (Chalmers, 2021) offre funzionalità per l’adattamento di una varietà di modelli IRT, inclusi i modelli Rasch, 2PL e 3PL, nonché diversi modelli per risposte politomiche utilizzando la massima verosimiglianza marginale. In questo capitolo, abbiamo esplorato come sia possibile confrontare i modelli 1PL, 2PL e 3PL, che sono stati adattati ai dati usando mirt, attraverso il test del rapporto di verosimiglianza e gli indici di informazione.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Modelli 1PL, 2PL e 3PL</span>"
    ]
  },
  {
    "objectID": "chapters/irt/05_1pl_2pl_3pl.html#session-info",
    "href": "chapters/irt/05_1pl_2pl_3pl.html#session-info",
    "title": "68  Modelli 1PL, 2PL e 3PL",
    "section": "68.8 Session Info",
    "text": "68.8 Session Info\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] grid      stats4    stats     graphics  grDevices utils     datasets \n[8] methods   base     \n\nother attached packages:\n [1] latex2exp_0.9.6   psychotools_0.7-4 ggmirt_0.1.0      TAM_4.2-21       \n [5] CDM_8.2-6         mvtnorm_1.3-2     mirt_1.43         lattice_0.22-6   \n [9] eRm_1.0-6         MASS_7.3-61       viridis_0.6.5     viridisLite_0.4.2\n[13] ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0  \n[17] bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19    \n[21] psych_2.4.6.26    scales_1.3.0      markdown_1.13     knitr_1.49       \n[25] lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n[29] purrr_1.0.2       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n[33] tidyverse_2.0.0   here_1.0.1        ggplot2_3.5.1    \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.2        later_1.3.2          pbdZMQ_0.3-13       \n  [4] R.oo_1.27.0          XML_3.99-0.17        rpart_4.1.23        \n  [7] lifecycle_1.0.4      rstatix_0.7.2        rprojroot_2.0.4     \n [10] globals_0.16.3       rockchalk_1.8.157    backports_1.5.0     \n [13] magrittr_2.0.3       openxlsx_4.2.7.1     Hmisc_5.2-0         \n [16] rmarkdown_2.29       httpuv_1.6.15        qgraph_1.9.8        \n [19] zip_2.3.1            sessioninfo_1.2.2    pbapply_1.7-2       \n [22] minqa_1.2.8          audio_0.1-11         multcomp_1.4-26     \n [25] abind_1.4-8          quadprog_1.5-8       R.utils_2.12.3      \n [28] nnet_7.3-19          TH.data_1.1-2        sandwich_3.1-1      \n [31] listenv_0.9.1        testthat_3.2.1.1     RPushbullet_0.3.4   \n [34] openintro_2.5.0      vegan_2.6-8          arm_1.14-4          \n [37] airports_0.1.0       parallelly_1.39.0    permute_0.9-7       \n [40] codetools_0.2-20     tidyselect_1.2.1     farver_2.1.2        \n [43] lme4_1.1-35.5        base64enc_0.1-3      jsonlite_1.8.9      \n [46] polycor_0.8-1        progressr_0.15.0     Formula_1.2-5       \n [49] survival_3.7-0       emmeans_1.10.5       tools_4.4.2         \n [52] snow_0.4-4           Rcpp_1.0.13-1        glue_1.8.0          \n [55] mnormt_2.1.1         admisc_0.36          xfun_0.49           \n [58] mgcv_1.9-1           IRdisplay_1.1        withr_3.0.2         \n [61] beepr_2.0            fastmap_1.2.0        boot_1.3-31         \n [64] fansi_1.0.6          digest_0.6.37        mi_1.1              \n [67] timechange_0.3.0     R6_2.5.1             mime_0.12           \n [70] estimability_1.5.1   colorspace_2.1-1     Cairo_1.6-2         \n [73] gtools_3.9.5         jpeg_0.1-10          R.methodsS3_1.8.2   \n [76] utf8_1.2.4           generics_0.1.3       data.table_1.16.2   \n [79] corpcor_1.6.10       usdata_0.3.1         SimDesign_2.17.1    \n [82] htmlwidgets_1.6.4    pkgconfig_2.0.3      sem_3.1-16          \n [85] gtable_0.3.6         brio_1.1.5           htmltools_0.5.8.1   \n [88] carData_3.0-5        png_0.1-8            rstudioapi_0.17.1   \n [91] tzdb_0.4.0           reshape2_1.4.4       uuid_1.2-1          \n [94] curl_6.0.1           coda_0.19-4.1        checkmate_2.3.2     \n [97] nlme_3.1-166         nloptr_2.1.1         repr_1.1.7          \n[100] zoo_1.8-12           parallel_4.4.2       miniUI_0.1.1.1      \n[103] foreign_0.8-87       pillar_1.9.0         vctrs_0.6.5         \n[106] promises_1.3.0       car_3.1-3            OpenMx_2.21.13      \n[109] xtable_1.8-4         Deriv_4.1.6          cluster_2.1.6       \n[112] dcurver_0.9.2        GPArotation_2024.3-1 htmlTable_2.4.3     \n[115] evaluate_1.0.1       pbivnorm_0.6.0       cli_3.6.3           \n[118] kutils_1.73          compiler_4.4.2       rlang_1.1.4         \n[121] crayon_1.5.3         future.apply_1.11.3  ggsignif_0.6.4      \n[124] labeling_0.4.3       fdrtool_1.2.18       plyr_1.8.9          \n[127] stringi_1.8.4        munsell_0.5.1        lisrelToR_0.3       \n[130] pacman_0.5.1         Matrix_1.7-1         IRkernel_1.3.2      \n[133] hms_1.1.3            glasso_1.11          future_1.34.0       \n[136] shiny_1.9.1          igraph_2.1.1         broom_1.0.7         \n[139] RcppParallel_5.1.9   cherryblossom_0.1.0",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Modelli 1PL, 2PL e 3PL</span>"
    ]
  },
  {
    "objectID": "chapters/irt/06_implementation.html",
    "href": "chapters/irt/06_implementation.html",
    "title": "69  Implementazione",
    "section": "",
    "text": "69.1 Introduzione\nIn questo capitolo esamineremo il tutorial di Debelak et al. (2022).",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/06_implementation.html#un-esempio-pratico",
    "href": "chapters/irt/06_implementation.html#un-esempio-pratico",
    "title": "69  Implementazione",
    "section": "69.2 Un esempio pratico",
    "text": "69.2 Un esempio pratico\nIl set di dati data.fims.Aus.Jpn.scored contiene le risposte valutate per un sottoinsieme di item da parte di studenti australiani e giapponesi nello studio “First International Mathematics Study” (FIMS, Husén, 1967).\n\ndata(data.fims.Aus.Jpn.scored, package = \"TAM\")\nfims &lt;- data.fims.Aus.Jpn.scored\n\n\nglimpse(fims)\n\nRows: 6,371\nColumns: 16\n$ SEX     &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~\n$ M1PTI1  &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1~\n$ M1PTI2  &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1~\n$ M1PTI3  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1~\n$ M1PTI6  &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0~\n$ M1PTI7  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~\n$ M1PTI11 &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1~\n$ M1PTI12 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0~\n$ M1PTI14 &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1~\n$ M1PTI17 &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0~\n$ M1PTI18 &lt;dbl&gt; 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1~\n$ M1PTI19 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~\n$ M1PTI21 &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0~\n$ M1PTI22 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~\n$ M1PTI23 &lt;dbl&gt; 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1~\n$ country &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~\n\n\nOltre alle risposte sui 14 item di matematica, il data set contiene anche informazioni sul genere del partecipate e sul paese d’origine.\n\nfims$SEX &lt;- as.factor(fims$SEX)\nlevels(fims$SEX) &lt;- c(\"male\", \"female\")\nfims$country &lt;- as.factor(fims$country)\nlevels(fims$country) &lt;- c(\"Australia\", \"Japan\")\n\n\nsummary(fims[, c(\"SEX\", \"country\")])\n\n     SEX            country    \n male  :3319   Australia:4320  \n female:3052   Japan    :2051  \n\n\nEsaminiamo le risposte dei primi 400 partecipanti. Con le seguenti istruzioni, per facilitare la manipolazione dei dati, cambiamo il nome delle colonne.\n\nresponses &lt;- fims[1:400, 2:15]\ncolnames(responses) &lt;- gsub(\"M1PTI\", \"I\", colnames(responses))\nglimpse(responses)\n\nRows: 400\nColumns: 14\n$ I1  &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,~\n$ I2  &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,~\n$ I3  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,~\n$ I6  &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,~\n$ I7  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ I11 &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,~\n$ I12 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ I14 &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,~\n$ I17 &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,~\n$ I18 &lt;dbl&gt; 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,~\n$ I19 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\n$ I21 &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,~\n$ I22 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,~\n$ I23 &lt;dbl&gt; 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,~\n\n\n\ngender &lt;- as.factor(fims$SEX[1:400])\nlevels(gender) &lt;- c(\"male\", \"female\")",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/06_implementation.html#modello-di-rasch",
    "href": "chapters/irt/06_implementation.html#modello-di-rasch",
    "title": "69  Implementazione",
    "section": "69.3 Modello di Rasch",
    "text": "69.3 Modello di Rasch\nUn’analisi IRT può essere paragonata a un’analisi fattoriale. Dopo avere adattato il modello di Rasch ai dati usando mirt(), possiamo usare la funzione summary() per ottenere quella che viene definita “soluzione fattoriale”, che include i carichi fattoriali (F1) e le comunalità (h2). Le comunalità, essendo carichi fattoriali al quadrato, sono interpretate come la varianza spiegata in un item dal tratto latente. Nel caso presente, tutti gli item hanno una relazione sostanziale (saturazioni \\(\\approx\\) .50) con il tratto latente, indicando che il tratto latente è un buon indicatore della varianza osservata in quegli item. Questo suggerisce che il tratto latente è in grado di spiegare una porzione almento moderata della varianza nei punteggi degli item.\n\nmirt_rm &lt;- mirt(responses, 1, itemtype = \"Rasch\", verbose = FALSE)\nsummary(mirt_rm)\n\n       F1    h2\nI1  0.488 0.238\nI2  0.488 0.238\nI3  0.488 0.238\nI6  0.488 0.238\nI7  0.488 0.238\nI11 0.488 0.238\nI12 0.488 0.238\nI14 0.488 0.238\nI17 0.488 0.238\nI18 0.488 0.238\nI19 0.488 0.238\nI21 0.488 0.238\nI22 0.488 0.238\nI23 0.488 0.238\n\nSS loadings:  3.33 \nProportion Var:  0.238 \n\nFactor correlations: \n\n   F1\nF1  1\n\n\nNell’IRT, tuttavia, siamo generalmente più interessati ai parametri di discriminazione e difficoltà. Questi parametri possono essere estratti dall’oggetto creato da mirt() nel seguente modo:\n\nparams_rm &lt;- coef(mirt_rm, IRTpars = TRUE, simplify = TRUE)\nround(params_rm$items, 2) # g = c = guessing parameter\n\n\nA matrix: 14 x 4 of type dbl\n\n\n\na\nb\ng\nu\n\n\n\n\nI1\n1\n-1.10\n0\n1\n\n\nI2\n1\n-1.25\n0\n1\n\n\nI3\n1\n-2.04\n0\n1\n\n\nI6\n1\n-0.05\n0\n1\n\n\nI7\n1\n2.53\n0\n1\n\n\nI11\n1\n-1.25\n0\n1\n\n\nI12\n1\n0.81\n0\n1\n\n\nI14\n1\n-0.50\n0\n1\n\n\nI17\n1\n1.33\n0\n1\n\n\nI18\n1\n-0.40\n0\n1\n\n\nI19\n1\n2.06\n0\n1\n\n\nI21\n1\n1.75\n0\n1\n\n\nI22\n1\n2.41\n0\n1\n\n\nI23\n1\n-1.93\n0\n1\n\n\n\n\n\n\n\\(a\\) (Discriminazione): Il parametro \\(a\\) (discriminazione) rappresenta la pendenza delle curve caratteristiche degli item (ICC - Item Characteristic Curves). Una pendenza elevata (valore alto di \\(a\\)) indica che l’item è molto efficace nel distinguere tra individui con livelli diversi del tratto latente (ad esempio, abilità). Questo significa che piccole variazioni nel tratto latente portano a grandi cambiamenti nella probabilità di rispondere correttamente all’item. Una pendenza bassa (valore basso di \\(a\\)) suggerisce che l’item non è altrettanto efficace nel discriminare tra livelli diversi del tratto latente. In questo caso, anche ampie variazioni nel tratto latente comportano solo piccoli cambiamenti nella probabilità di risposta corretta. Nel modello di Rasch si assume che tutti gli item abbiano la stessa pendenza (o potere discriminante), e quindi tutti i valori di \\(a\\) sono fissati allo stesso valore (ovvero 1).\n\\(b\\) (Difficoltà): Rappresenta il livello di abilità a cui un rispondente ha il 50% di probabilità di rispondere correttamente all’item. Un valore positivo indica un item più difficile (richiede un livello di abilità superiore per rispondere correttamente), mentre un valore negativo indica un item più facile. Ad esempio, I7 ha un valore di difficoltà di 2.53, il che significa che è relativamente difficile, mentre I3, con un valore di -2.04, è relativamente facile.\n\\(g\\) (Probabilità di Indovinare): In questo modello, la probabilità di indovinare è impostata a zero per tutti gli item, il che è coerente con il modello di Rasch, dove non si considera la possibilità di indovinare correttamente un item per caso.\n\nAdattiamo ora ai dati il modello di Rasch con la funzione eRm::RM()\n\nrm_sum0 &lt;- eRm::RM(responses)\n\n\nsummary(rm_sum0)\n\n\nResults of RM estimation: \n\nCall:  eRm::RM(X = responses) \n\nConditional log-likelihood: -1887 \nNumber of iterations: 23 \nNumber of parameters: 13 \n\nItem (Category) Difficulty Parameters (eta): with 0.95 CI:\n    Estimate Std. Error lower CI upper CI\nI2    -1.420      0.121   -1.658   -1.183\nI3    -2.210      0.145   -2.494   -1.926\nI6    -0.215      0.108   -0.426   -0.004\nI7     2.364      0.170    2.031    2.697\nI11   -1.420      0.121   -1.658   -1.183\nI12    0.642      0.113    0.422    0.863\nI14   -0.663      0.110   -0.879   -0.448\nI17    1.152      0.122    0.913    1.391\nI18   -0.565      0.109   -0.778   -0.351\nI19    1.889      0.146    1.602    2.175\nI21    1.578      0.134    1.315    1.841\nI22    2.244      0.163    1.925    2.564\nI23   -2.103      0.141   -2.379   -1.827\n\nItem Easiness Parameters (beta) with 0.95 CI:\n         Estimate Std. Error lower CI upper CI\nbeta I1     1.273      0.118    1.041    1.504\nbeta I2     1.420      0.121    1.183    1.658\nbeta I3     2.210      0.145    1.926    2.494\nbeta I6     0.215      0.108    0.004    0.426\nbeta I7    -2.364      0.170   -2.697   -2.031\nbeta I11    1.420      0.121    1.183    1.658\nbeta I12   -0.642      0.113   -0.863   -0.422\nbeta I14    0.663      0.110    0.448    0.879\nbeta I17   -1.152      0.122   -1.391   -0.913\nbeta I18    0.565      0.109    0.351    0.778\nbeta I19   -1.889      0.146   -2.175   -1.602\nbeta I21   -1.578      0.134   -1.841   -1.315\nbeta I22   -2.244      0.163   -2.564   -1.925\nbeta I23    2.103      0.141    1.827    2.379\n\n\n\nLa funzione RM() impone un vincolo sulle stime dei parametri di difficoltà degli item. Questo vincolo è che la media di questi parametri (beta) sia zero. Questo approccio è noto come “parametrizzazione ancorata” o “centrata”. Il vantaggio di questa parametrizzazione è che posiziona la scala di difficoltà degli item in un punto di riferimento fisso, facilitando il confronto tra diversi set di item o tra diverse applicazioni dello stesso test.\nVerifichiamo.\n\ncoef(rm_sum0) |&gt; print()\n\n beta I1  beta I2  beta I3  beta I6  beta I7 beta I11 beta I12 beta I14 \n   1.273    1.420    2.210    0.215   -2.364    1.420   -0.642    0.663 \nbeta I17 beta I18 beta I19 beta I21 beta I22 beta I23 \n  -1.152    0.565   -1.889   -1.578   -2.244    2.103 \n\n\n\nsum(rm_sum0$betapar)\n\n8.88178419700125e-16\n\n\nNella parametrizzazione utilizzata da mirt(), i parametri di difficoltà vengono invece stimati senza un vincolo sulla loro media. Questo può portare a stime dei parametri di difficoltà che differiscono da quelle ottenute tramite RM(). Questa libertà nella stima dei parametri permette una flessibilità maggiore, specialmente in modelli IRT complessi o multidimensionali, ma può rendere più complesso il confronto diretto tra set di item o test differenti.\nDalla soluzione prodotta da eRm::RM() possiamo estrarre le stime sia nei termini della facilità che della difficoltà degli item.\n\ntab &lt;- data.frame(\n    item_score = colSums(responses),\n    easiness = coef(rm_sum0),\n    difficulty = -coef(rm_sum0)\n)\ntab[order(tab$item_score), ]\n\n\nA data.frame: 14 x 3\n\n\n\nitem_score\neasiness\ndifficulty\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nI7\n40\n-2.364\n2.364\n\n\nI22\n44\n-2.244\n2.244\n\n\nI19\n58\n-1.889\n1.889\n\n\nI21\n73\n-1.578\n1.578\n\n\nI17\n98\n-1.152\n1.152\n\n\nI12\n134\n-0.642\n0.642\n\n\nI6\n204\n0.215\n-0.215\n\n\nI18\n233\n0.565\n-0.565\n\n\nI14\n241\n0.663\n-0.663\n\n\nI1\n287\n1.273\n-1.273\n\n\nI2\n297\n1.420\n-1.420\n\n\nI11\n297\n1.420\n-1.420\n\n\nI23\n336\n2.103\n-2.103\n\n\nI3\n341\n2.210\n-2.210\n\n\n\n\n\nIn alterativa, possiamo usare il pacchetto TAM. Come nel caso di mirt, anche in questo caso viene usata una procedura di stima di massima verosimiglianza marginale.\n\ntam_rm &lt;- tam.mml(responses)\n\n....................................................\nProcessing Data      2024-11-21 16:52:43.253148 \n    * Response Data: 400 Persons and  14 Items \n    * Numerical integration with 21 nodes\n    * Created Design Matrices   ( 2024-11-21 16:52:43.267069 )\n    * Calculated Sufficient Statistics   ( 2024-11-21 16:52:43.277864 )\n....................................................\nIteration 1     2024-11-21 16:52:43.283241\nE Step\nM Step Intercepts   |----\n  Deviance = 5667.9246\n  Maximum item intercept parameter change: 0.315\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 0.123\n....................................................\nIteration 2     2024-11-21 16:52:43.294375\nE Step\nM Step Intercepts   |--\n  Deviance = 5633.0417 | Absolute change: 34.9 | Relative change: 0.00619\n  Maximum item intercept parameter change: 0.00328\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 0.00796\n....................................................\nIteration 3     2024-11-21 16:52:43.301458\nE Step\nM Step Intercepts   |--\n  Deviance = 5633.0057 | Absolute change: 0.036 | Relative change: 6.39e-06\n  Maximum item intercept parameter change: 0.0022\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 0.00584\n....................................................\nIteration 4     2024-11-21 16:52:43.311792\nE Step\nM Step Intercepts   |--\n  Deviance = 5632.9887 | Absolute change: 0.017 | Relative change: 3.02e-06\n  Maximum item intercept parameter change: 0.00151\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 0.00406\n....................................................\nIteration 5     2024-11-21 16:52:43.320987\nE Step\nM Step Intercepts   |--\n  Deviance = 5632.9805 | Absolute change: 0.0081 | Relative change: 1.44e-06\n  Maximum item intercept parameter change: 0.00104\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 0.00281\n....................................................\nIteration 6     2024-11-21 16:52:43.326444\nE Step\nM Step Intercepts   |--\n  Deviance = 5632.9767 | Absolute change: 0.0039 | Relative change: 6.8e-07\n  Maximum item intercept parameter change: 0.00071\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 0.00194\n....................................................\nIteration 7     2024-11-21 16:52:43.335553\nE Step\nM Step Intercepts   |--\n  Deviance = 5632.9749 | Absolute change: 0.0018 | Relative change: 3.2e-07\n  Maximum item intercept parameter change: 0.000486\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 0.00134\n....................................................\nIteration 8     2024-11-21 16:52:43.34728\nE Step\nM Step Intercepts   |--\n  Deviance = 5632.974 | Absolute change: 9e-04 | Relative change: 1.5e-07\n  Maximum item intercept parameter change: 0.000333\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 0.000925\n....................................................\nIteration 9     2024-11-21 16:52:43.353418\nE Step\nM Step Intercepts   |--\n  Deviance = 5632.9736 | Absolute change: 4e-04 | Relative change: 7e-08\n  Maximum item intercept parameter change: 0.000228\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 0.000637\n....................................................\nIteration 10     2024-11-21 16:52:43.365543\nE Step\nM Step Intercepts   |--\n  Deviance = 5632.9734 | Absolute change: 2e-04 | Relative change: 3e-08\n  Maximum item intercept parameter change: 0.000156\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 0.000439\n....................................................\nIteration 11     2024-11-21 16:52:43.374954\nE Step\nM Step Intercepts   |--\n  Deviance = 5632.9733 | Absolute change: 1e-04 | Relative change: 2e-08\n  Maximum item intercept parameter change: 0.000107\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 0.000302\n....................................................\nIteration 12     2024-11-21 16:52:43.406924\nE Step\nM Step Intercepts   |-\n  Deviance = 5632.9733 | Absolute change: 0 | Relative change: 1e-08\n  Maximum item intercept parameter change: 7.3e-05\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 0.000208\n....................................................\nIteration 13     2024-11-21 16:52:43.420703\nE Step\nM Step Intercepts   |-\n  Deviance = 5632.9732 | Absolute change: 0 | Relative change: 0\n  Maximum item intercept parameter change: 5e-05\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 0.000143\n....................................................\nIteration 14     2024-11-21 16:52:43.425746\nE Step\nM Step Intercepts   |-\n  Deviance = 5632.9732 | Absolute change: 0 | Relative change: 0\n  Maximum item intercept parameter change: 3.4e-05\n  Maximum item slope parameter change: 0\n  Maximum regression parameter change: 0\n  Maximum variance parameter change: 9.8e-05\n....................................................\nItem Parameters\n   xsi.index xsi.label    est\n1          1        I1 -1.103\n2          2        I2 -1.250\n3          3        I3 -2.042\n4          4        I6 -0.048\n5          5        I7  2.531\n6          6       I11 -1.250\n7          7       I12  0.813\n8          8       I14 -0.495\n9          9       I17  1.326\n10        10       I18 -0.397\n11        11       I19  2.064\n12        12       I21  1.754\n13        13       I22  2.414\n14        14       I23 -1.935\n...................................\nRegression Coefficients\n     [,1]\n[1,]    0\n\nVariance:\n       [,1]\n[1,] 0.9037\n\n\nEAP Reliability:\n[1] 0.656\n\n-----------------------------\nStart:  2024-11-21 16:52:43.251286\nEnd:  2024-11-21 16:52:43.481495 \nTime difference of 0.23 secs\n\n\n\nPossiamo ispezionare le stime dei parametri con\n\ntam_rm$item\n\n\nA data.frame: 14 x 6\n\n\n\nitem\nN\nM\nxsi.item\nAXsi_.Cat1\nB.Cat1.Dim1\n\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nI1\nI1\n400\n0.718\n-1.103\n-1.103\n1\n\n\nI2\nI2\n400\n0.743\n-1.250\n-1.250\n1\n\n\nI3\nI3\n400\n0.853\n-2.042\n-2.042\n1\n\n\nI6\nI6\n400\n0.510\n-0.048\n-0.048\n1\n\n\nI7\nI7\n400\n0.100\n2.531\n2.531\n1\n\n\nI11\nI11\n400\n0.743\n-1.250\n-1.250\n1\n\n\nI12\nI12\n400\n0.335\n0.813\n0.813\n1\n\n\nI14\nI14\n400\n0.603\n-0.495\n-0.495\n1\n\n\nI17\nI17\n400\n0.245\n1.326\n1.326\n1\n\n\nI18\nI18\n400\n0.583\n-0.397\n-0.397\n1\n\n\nI19\nI19\n400\n0.145\n2.064\n2.064\n1\n\n\nI21\nI21\n400\n0.182\n1.755\n1.755\n1\n\n\nI22\nI22\n400\n0.110\n2.415\n2.415\n1\n\n\nI23\nI23\n400\n0.840\n-1.935\n-1.935\n1\n\n\n\n\n\nLe colonne di questo output possono essere interpretate come segue:\n\nitem indica il nome dell’item.\nN indica il numero di candidati che hanno risposto a ciascun item. In questo caso, tutti i 400 candidati hanno risposto a ogni item.\nM è la media delle risposte a ciascun item. Nel caso di un item con una media alta ciò significa che a tale item è stata fornita uba risposta corretta da una alta percentuale di candidati.\nxsi.item per il modello di Rasch è il parametro di difficoltà dell’item. Gli item con valori alti tendono ad essere più difficili.\nAXsi.Cat1 ripete la difficoltà dell’item per il modello di Rasch, ma permetterebbe l’inclusione di una matrice di design A, che non abbiamo usato qui. Per i modelli politomici, l’output includerà parametri dell’item per più di una categoria.\nB.Cat1.Dim1 è il parametro di discriminazione o pendenza dell’item. Per il modello di Rasch, la pendenza è 1 per ogni item.\n\nPossiamo mostrare solo la difficoltà e l’errore standard con:\n\ntam_rm$xsi\n\n\nA data.frame: 14 x 2\n\n\n\nxsi\nse.xsi\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nI1\n-1.103\n0.120\n\n\nI2\n-1.250\n0.123\n\n\nI3\n-2.042\n0.149\n\n\nI6\n-0.048\n0.109\n\n\nI7\n2.531\n0.174\n\n\nI11\n-1.250\n0.123\n\n\nI12\n0.813\n0.115\n\n\nI14\n-0.495\n0.111\n\n\nI17\n1.326\n0.125\n\n\nI18\n-0.397\n0.111\n\n\nI19\n2.064\n0.150\n\n\nI21\n1.755\n0.138\n\n\nI22\n2.415\n0.168\n\n\nI23\n-1.935\n0.145\n\n\n\n\n\nLa parametrizzazione classica IRT si ottiene con:\n\ntam_rm$item_irt\n\n\nA data.frame: 14 x 3\n\n\nitem\nalpha\nbeta\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nI1\n1\n-1.103\n\n\nI2\n1\n-1.250\n\n\nI3\n1\n-2.042\n\n\nI6\n1\n-0.048\n\n\nI7\n1\n2.531\n\n\nI11\n1\n-1.250\n\n\nI12\n1\n0.813\n\n\nI14\n1\n-0.495\n\n\nI17\n1\n1.326\n\n\nI18\n1\n-0.397\n\n\nI19\n1\n2.064\n\n\nI21\n1\n1.755\n\n\nI22\n1\n2.415\n\n\nI23\n1\n-1.935",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/06_implementation.html#valutazione-del-test",
    "href": "chapters/irt/06_implementation.html#valutazione-del-test",
    "title": "69  Implementazione",
    "section": "69.4 Valutazione del Test",
    "text": "69.4 Valutazione del Test\nIl primo strumento per la valutazione dei test che vorremmo presentare sono i metodi grafici. Il primo di questi, la mappa persona-item, mostra se il campione di persone copre l’intera gamma degli item e viceversa. Il secondo approccio, che consiste nel confrontare le ICC (Curve Caratteristiche dell’Item) teoriche ed empiriche, può aiutare a rilevare gli item che non si adattano bene. Il terzo, il test grafico, è un test visivo per il Funzionamento Differenziale degli Item (DIF).\n\n69.4.1 Mappa Persona-Item\nOltre alla valutazione complessiva dell’adattamento del modello, ci possiamo chiedere quanto bene gli item coprono l’intervallo dell’abilità latente. A tale domanda si può rispondere creando una “Mappa Item-Persona” (noto anche come Wright Map; person−item map). Questa visualizzazione inizia tracciando la distribuzione dell’abilità latente nel campione studiato. Successivamente, tracciamo anche la difficoltà di ciascun item sulla stessa scala di theta. Allineando entrambi i grafici, possiamo vedere quanto bene gli item coprono l’abilità latente. Per poter stimare con precisione i parametri degli item a partire dal campione delle persone e viceversa, le difficoltà degli item dovrebbero coprire l’intera gamma delle abilità delle persone e viceversa.\nLa mappa persona-item si ottiene usando la funzione itempersonMap()\n\nitempersonMap(mirt_rm)\n\n\n\n\n\n\n\n\nLa parte superiore della mappa persona-item mostra un istogramma delle stime dei parametri di abilità, mentre la parte inferiore mostra le stime delle difficoltà per ciascun item del test. Per ogni item, la stima della difficoltà è indicata dalla posizione del punto sulla linea tratteggiata corrispondente a quell’item. Ad esempio, la difficoltà stimata per l’item 1 corrisponde alla posizione del punto sulla linea tratteggiata più in alto. La mappa persona-item offre un controllo visivo di coerenza per le stime del nostro modello IRT (Teoria della Risposta all’Item). Le stime delle abilità sono più accurate quando cadono nel mezzo della distribuzione dei parametri degli item e viceversa. Pertanto, idealmente, l’istogramma delle abilità e le stime delle difficoltà dovrebbero essere centrate sullo stesso punto e mostrare un’ampia sovrapposizione. Nel nostro test, sembra essere questo il caso.\nIn alternativa, possiamo usare la funzione plotPImap() di eRm.\n\n\n69.4.2 ICC Empiriche\nLe Curve Caratteristiche degli Item (ICC) descrivono la relazione teorica tra l’abilità dei partecipanti al test e la probabilità di una risposta corretta che ci aspettiamo sotto il modello di Rasch per una data difficoltà. La ICC attesa per un item può essere tracciata dopo che la sua difficoltà è stata stimata. Oltre alle probabilità attese di una risposta corretta illustrate dall’ICC, possiamo anche tracciare le frequenze relative empiriche di una risposta corretta. Queste frequenze relative empiriche sono indicate nella figura come punti e vengono chiamate ICC empiriche.\nUsando eRm possimo generare le ECC empiriche nel modo seguente.\n\nplotICC(\n    rm_sum0, \n    item.subset = \"all\",\n    empICC = list(\"raw\"), \n    empCI = list()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLe ICC empiriche sono rappresentate dai singoli punti, mentre la ICC attesa sotto il modello di Rasch è indicata dalla linea liscia. Dalle figure precedenti, per gli item 12 e 14 notiamo che in generale la forma dell’ICC empirica è molto ben allineata con l’ICC attesa, ma per l’item 12 l’ICC empirica mostra valori sopra zero anche per le abilità più basse a sinistra della dimensione latente. Questo potrebbe indicare una tendenza al tentativo di indovinare (guessing). Per l’item 19, l’ICC empirica appare più ripida dell’ICC attesa sotto il modello di Rasch. Mostra un salto molto più pronunciato tra la prima metà approssimativa dei punti e i punti rimanenti. Per l’item 21, al contrario, l’ICC empirica è molto più piatta rispetto a quella attesa. Confronteremo la nostra impressione visiva con le statistiche di adattamento degli item per questi item di seguito.\nÈ possibile visualizzare le ICC attese di tutti gli item del test in un unico grafico utilizzando la funzione plotjointICC(). Questo grafico ci permette di esaminare come la difficoltà influenzi la probabilità che un candidato risponda correttamente a un item. Ricordiamo che la difficoltà di un item è definita come il livello di abilità in cui una persona ha una probabilità del 50% di rispondere correttamente all’item. Abbiamo aggiunto una linea tratteggiata orizzontale alla probabilità di 0.5 usando il comando segments. Il punto in cui un’ICC interseca questa linea rappresenta la sua difficoltà. Questo ci permette di leggere facilmente le difficoltà relative degli item dal grafico. Spostandosi da sinistra a destra, il primo ICC intersecato dalla linea orizzontale corrisponde all’item meno difficile (in questo caso l’item 3, seguito da vicino dall’item 23, come indicato nell’ordine degli item nella legenda), e l’ultimo ICC intersecato dalla linea orizzontale è per l’item più difficile (in questo caso l’item 7).\nDa notare che le ICC attese nella figura sono parallele per definizione. Il modello di Rasch assume che le ICC siano parallele, quindi produrrà sempre ICC teoriche o attese parallele, anche quando gli item hanno in realtà pendenze o tassi di guessing diversi, come abbiamo visto in precedenza per le ICC empiriche.\n\neRm::plotjointICC(rm_sum0, cex = 0.7)\nsegments(-2.5, 0.5, 4, 0.5, lty = 2)\n\n\n\n\n\n\n\n\nIn alternativa, possiamo generare le ICC usando il pacchetto mirt:\n\nmirt_rm &lt;- mirt(responses, 1, \"Rasch\")\nplot(mirt_rm, type = \"trace\")\n\nIteration: 16, Log-Lik: -2816.487, Max-Change: 0.00008\n\n\n\n\n\n\n\n\n\nLe curve caratteristiche degli item offrono un quadro dettagliato e visuale di come ciascun item del test si comporta attraverso diversi livelli dell’abilità latente. Per esempio:\n\nVisualizzazione della Difficoltà e della Discriminazione:\n\nSupponiamo di avere un item che mostra una curva con una ripida salita in un punto specifico della scala di abilità. Questo indica che l’item ha una difficoltà concentrata attorno a quel punto e che discrimina efficacemente tra rispondenti con abilità appena al di sotto e al di sopra di quel livello.\nAl contrario, una curva più graduale suggerisce che l’item è meno discriminante, con una variazione più ampia nella probabilità di risposta corretta a seconda del livello di abilità.\n\nIdentificazione di Lacune nella Valutazione:\n\nVisualizzando le curve di più item, possiamo identificare se ci sono lacune nella copertura dell’abilità latente. Ad esempio, se tutti gli item hanno curve che si concentrano su livelli di abilità bassi, potrebbe esserci una mancanza di item difficili per misurare l’abilità ad alti livelli.\nInoltre, se le curve degli item si sovrappongono eccessivamente, potrebbe indicare ridondanza tra gli item, suggerendo che alcuni di essi non aggiungono informazioni uniche alla valutazione.\n\nConfronto tra Diversi Tipi di Item:\n\nPer esempio, gli item progettati per misurare concetti di base potrebbero avere curve che mostrano alta probabilità di risposta corretta anche a livelli di abilità bassi.\nAl contrario, item progettati per essere più impegnativi potrebbero mostrare probabilità elevate di risposta corretta solo a livelli di abilità più alti.\n\n\n\n\n69.4.3 Test Grafico\nIl test grafico del modello, basato sui principi di Rasch (1960), è un metodo intuitivo per valutare l’invarianza degli item in un test, confrontando i parametri degli item stimati per due gruppi di persone. Affinché il modello di Rasch sia considerato valido, è necessario che le stime dei parametri degli item per i diversi gruppi concordino, fino a una trasformazione lineare. In termini pratici, ciò si traduce nel fatto che, quando visualizzate in un grafico, le stime dei parametri degli item dei due gruppi dovrebbero allinearsi lungo una linea retta.\nPer complementare questa analisi, possiamo ricorrere al test del rapporto di verosimiglianza di Andersen (1973), un approccio ben consolidato per verificare l’adeguatezza del modello di Rasch nel rappresentare il comportamento dei partecipanti ai test. Il test di Andersen valuta se le stime dei parametri degli item rimangono consistenti tra diversi gruppi di partecipanti. Se i parametri degli item stimati individualmente per ciascun gruppo differiscono significativamente, ciò indica che il modello di Rasch potrebbe non essere un’adeguata rappresentazione del comportamento osservato nei test.\nA differenza del test grafico, il test del rapporto di verosimiglianza confronta il massimo della verosimiglianza condizionata sotto il modello di Rasch con il massimo della verosimiglianza condizionata quando i parametri degli item possono variare tra i gruppi. Questa metodologia offre un’indicazione di quanto efficacemente ciascun modello rappresenti il comportamento dei partecipanti.\nIl test del rapporto di verosimiglianza utilizza la statistica di test \\(T = −2 \\cdot log(LR)\\), che ha una distribuzione campionaria approssimativamente \\(\\chi^2\\) per campioni grandi. Valori del rapporto di verosimiglianza inferiori a 1, o valori elevati di T, suggeriscono una violazione del modello di Rasch.\nIl test di Andersen è implementato nel pacchetto eRm in R, offrendo uno strumento utile per l’analisi. Tuttavia, è importante notare che un risultato non significativo in questo test non può essere interpretato automaticamente come supporto per il modello di Rasch, specialmente se il modello più generale non descrive adeguatamente i dati. Inoltre, la capacità di rilevare differenze tra i gruppi specificati dipende dall’effettiva diversità dei parametri del modello tra questi gruppi. Sono stati messi a punto approcci più flessibili per rilevare le differenze nei parametri.\n\nlrt_mean_split &lt;- LRtest(rm_sum0, splitcr = \"mean\")\nlrt_mean_split\n\n\nAndersen LR-test: \nLR-value: 79.7 \nChi-square df: 13 \np-value:  0 \n\n\nL’output di questo test mostra una violazione significativa del modello di Rasch al livello \\(\\alpha\\) = 0.05.\n\nplotGOF(\n    lrt_mean_split,\n    tlab = \"item\", pos = 1,\n    main = \"Difficulty by Score (with Item Names)\",\n    conf = list(gamma = 0.95, col = 1)\n)\n\n\n\n\n\n\n\n\nOra possiamo tracciare le stime delle difficoltà di ciascun gruppo utilizzando la funzione plotGOF() per creare il test grafico. La funzione plotGOF() prende il risultato di LRtest() e traccia le stime dei parametri degli item per i due gruppi. Per facilitare la valutazione visiva, plotGOF() può opzionalmente etichettare gli item e aggiungere ellissi di confidenza.\nPer creare il grafico per il test grafico basato sulla divisione media, possiamo procedere in questo modo: ogni piccolo cerchio nella Figura mostra le stime delle difficoltà per un singolo item. La coordinata x di un cerchio indica la sua stima di difficoltà per i partecipanti al test con punteggi sotto la media e la sua coordinata y indica la stima di difficoltà per i partecipanti al test con punteggi sopra la media. La linea y = x è fornita come riferimento, poiché i punti che cadono su questa linea avrebbero la stessa stima in entrambi i gruppi. La distanza tra qualsiasi punto e la linea di riferimento y = x indica quanto le stime differiscono tra i due gruppi. Indica anche la direzione di questa differenza. Gli item sotto la linea sono più difficili per i partecipanti al test con punteggi sotto la media, mentre gli item sopra la linea sono più difficili per i partecipanti al test con punteggi sopra la media.\nGli assi orizzontali e verticali mostrano intervalli di confidenza per le stime per ciascun gruppo di partecipanti al test. La larghezza di ciascun intervallo di confidenza è determinata dall’elemento gamma della lista fornita a conf. L’impostazione predefinita gamma = .95 produce intervalli di confidenza al 95% per ciascun asse dell’ellisse. Quando un’ellisse di confidenza non incrocia la linea di riferimento, l’item rispettivo è diagnosticato come mostrante un significativo DIF.\nLa figura indica che gli item 2, 6, 21 e 22 differiscono significativamente tra le persone con punteggi sopra e sotto la media, poiché le loro ellissi di confidenza non incrociano la linea di riferimento. Gli item 21 e 22 sono più difficili per le persone con punteggi pari o superiori alla media, mentre gli item 2 e 6 sono più difficili per le persone con punteggi sotto la media. Tali violazioni del modello possono verificarsi quando le ICC osservate differiscono dalle ICC attese sotto il modello di Rasch per i partecipanti al test con abilità basse e alte. Questo può accadere, ad esempio, se è presente il tentativo di indovinare (guessing), o se la pendenza è più ripida o meno ripida di quanto previsto dal modello di Rasch.\nPossiamo anche fornire all’argomento splitcr una variabile che divide i partecipanti al test in gruppi. Ad esempio, possiamo testare se i parametri degli item differiscono in base al genere passando un vettore contenente le appartenenze di gruppo come argomento splitcr.\n\nlrt_gender &lt;- LRtest(rm_sum0, splitcr = gender)\nlrt_gender\n\n\nAndersen LR-test: \nLR-value: 33 \nChi-square df: 13 \np-value:  0.002 \n\n\nCome nel test precedente, anche il Test del Rapporto di Verosimiglianza (LRT) per il genere indica una violazione significativa del modello di Rasch al livello α = 0.05.\n\nplotGOF(\n    lrt_gender,\n    tlab = \"item\", pos = 1,\n    main = \"Difficulty by Score (with Item Names)\",\n    conf = list(gamma = 0.95, col = 1)\n)\n\n\n\n\n\n\n\n\nLa figura indica che gli item 2, 7 e 21 differiscono tra partecipanti al test femminili e maschili. Gli item 2 e 7 sono più difficili per i partecipanti femminili, mentre l’item 21 è più difficile per i partecipanti maschili.\n\n\n69.4.4 Test di Wald\nLe impostazioni del test del rapporto di verosimiglianza di Andersen (1973) e del test di Wald sono molto simili. Entrambi i test si basano sull’idea che il modello di Rasch sia un modello ragionevole per i dati dei test solo se i parametri degli item stimati non variano sistematicamente tra gruppi di persone. In entrambi i test, consideriamo le stime dei parametri degli item per ciascun gruppo di persone. A differenza del test del rapporto di verosimiglianza, tuttavia, il test di Wald confronta direttamente le stime dei parametri degli item dei gruppi. In sostanza, il test di Wald calcola la differenza tra la stima del primo gruppo della difficoltà dell’item i, β̂(1)i, e quella del secondo gruppo, β̂(2)i. Questa differenza viene divisa per il suo errore standard per tenere conto del fatto che tutte le stime sono soggette a rumore. Questo porta alla statistica di test per l’item i:\n\\[\nT_i = \\frac{\\hat{\\beta}^{(1)}_i - \\hat{\\beta}^{(2)}_i}{\\sqrt{se(\\hat{\\beta}^{(1)}_i)^2 + se(\\hat{\\beta}^{(2)}_i)^2}},\n\\]\ndove $ se(^{(1)}_i) $ e $ se(^{(2)}_i) $ indicano rispettivamente gli errori standard di $ ^{(1)}_i $ e $ ^{(2)}_i $.\nPer campioni di grandi dimensioni, $ T_i $ approssimativamente segue una distribuzione normale standard sotto l’ipotesi nulla che il vero parametro dell’item sia lo stesso per entrambi i gruppi. Valori estremi di $ T_i $ sono improbabili sotto la distribuzione normale. Quindi, un valore estremo di $ T_i $, con un piccolo valore p, indica che l’item i viola il modello di Rasch.\nEseguiamo il test con R:\n\nWaldtest(rm_sum0, splitcr = \"mean\")\n\n\nWald test on item level (z-values):\n\n         z-statistic p-value\nbeta I1       -0.514   0.607\nbeta I2       -3.328   0.001\nbeta I3       -0.838   0.402\nbeta I6       -2.555   0.011\nbeta I7        0.210   0.834\nbeta I11      -1.773   0.076\nbeta I12       1.562   0.118\nbeta I14       1.821   0.069\nbeta I17       1.550   0.121\nbeta I18       0.333   0.739\nbeta I19      -1.827   0.068\nbeta I21       5.768   0.000\nbeta I22       4.106   0.000\nbeta I23      -1.560   0.119\n\n\nQuesti test indicano nuovamente che gli item 2, 6, 21 e 22 differiscono significativamente tra i partecipanti al test con punteggi sopra e sotto la media.\nPossiamo anche eseguire il test per la differenza tra maschi e femmine:\n\nWaldtest(rm_sum0, splitcr = gender)\n\n\nWald test on item level (z-values):\n\n         z-statistic p-value\nbeta I1       -1.727   0.084\nbeta I2        2.543   0.011\nbeta I3       -1.020   0.308\nbeta I6        0.067   0.946\nbeta I7        3.089   0.002\nbeta I11      -1.978   0.048\nbeta I12      -0.861   0.389\nbeta I14      -0.673   0.501\nbeta I17       0.815   0.415\nbeta I18      -0.493   0.622\nbeta I19       0.583   0.560\nbeta I21      -2.305   0.021\nbeta I22      -0.030   0.976\nbeta I23      -1.019   0.308\n\n\nI risultati qui concordano in gran parte anche con la figura precedente. In linea con il test grafico, il test di Wald indica che gli item 2, 7 e 21 differiscono tra i gruppi.\n\n\n69.4.5 Ancoraggio\nL’ancoraggio è una procedura cruciale quando si confrontano le stime dei parametri degli item tra diversi gruppi, un passo fondamentale in test come il Wald e in metodi grafici. Tale processo necessita di particolare attenzione perché implica la restrizione di alcuni parametri degli item per allineare le scale latenti tra i gruppi. Ad esempio, fissare il parametro del primo item a zero in entrambi i gruppi crea un punto di riferimento comune, ma anche limitazioni.\nLa scelta degli item di ancoraggio è delicata: fissare un parametro in entrambi i gruppi significa non poter più valutare la differenza per quell’item specifico. La selezione dovrebbe essere guidata da un’attenta analisi dei dati e da considerazioni teoriche. Approcci guidati dai dati sono stati proposti per identificare item invarianti o escludere quelli con DIF, processo noto come purificazione. Tuttavia, occorre cautela: anche metodi ben progettati possono portare a conclusioni errate se gli item di ancoraggio scelti sono inappropriati.\nIn pratica, spesso si adotta una restrizione in cui la somma dei parametri degli item è zero per tutti i gruppi. Questo approccio, adottato da pacchetti software come eRm e difR in R, si basa sull’assunzione che eventuali DIF si annullino su tutti gli item. Ma se questa assunzione non è valida, o se l’ancoraggio include item con DIF, potremmo incorrere in errori interpretativi.\nIn sintesi, l’ancoraggio è una strategia potente ma che richiede un’attenta considerazione e un’analisi critica. È fondamentale non solo selezionare gli item di ancoraggio adeguati ma anche interpretare i risultati con una comprensione chiara delle ipotesi e delle potenziali limitazioni del metodo scelto.\n\nresp &lt;- as.matrix(responses)\nanchortest(\n    resp ~ gender,\n    class = \"constant\",\n    select = \"MPT\"\n)\n\nAnchor items:\nrespI23, respI3, respI22, respI12\n\nFinal DIF tests:\n\n     Simultaneous Tests for General Linear Hypotheses\n\nLinear Hypotheses:\n             Estimate Std. Error z value Pr(&gt;|z|)   \nrespI1 == 0   0.21972    0.29124    0.75   0.4506   \nrespI2 == 0  -0.83344    0.29343   -2.84   0.0045 **\nrespI3 == 0   0.10283    0.26853    0.38   0.7018   \nrespI6 == 0  -0.21887    0.27110   -0.81   0.4195   \nrespI7 == 0  -1.79695    0.57084   -3.15   0.0016 **\nrespI11 == 0  0.29599    0.29835    0.99   0.3212   \nrespI12 == 0 -0.00299    0.22722   -0.01   0.9895   \nrespI14 == 0 -0.05134    0.27413   -0.19   0.8514   \nrespI17 == 0 -0.41504    0.30706   -1.35   0.1765   \nrespI18 == 0 -0.09294    0.27284   -0.34   0.7334   \nrespI19 == 0 -0.38657    0.36100   -1.07   0.2842   \nrespI21 == 0  0.42520    0.32052    1.33   0.1846   \nrespI22 == 0 -0.19346    0.30037   -0.64   0.5195   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Univariate p values reported)\n\n\n\nanchortest(\n    resp ~ gender,\n    class = \"forward\",\n    select = \"MTT\"\n)\n\nAnchor items:\nrespI23, respI12, respI18, respI14, respI6, respI1, respI11,\nrespI19, respI17\n\nFinal DIF tests:\n\n     Simultaneous Tests for General Linear Hypotheses\n\nLinear Hypotheses:\n             Estimate Std. Error z value Pr(&gt;|z|)   \nrespI1 == 0    0.2818     0.2362    1.19   0.2328   \nrespI2 == 0   -0.7714     0.2660   -2.90   0.0037 **\nrespI3 == 0    0.1649     0.3229    0.51   0.6097   \nrespI6 == 0   -0.1568     0.2157   -0.73   0.4671   \nrespI7 == 0   -1.7349     0.5574   -3.11   0.0019 **\nrespI11 == 0   0.3580     0.2432    1.47   0.1410   \nrespI12 == 0   0.0591     0.2262    0.26   0.7940   \nrespI14 == 0   0.0107     0.2188    0.05   0.9610   \nrespI17 == 0  -0.3530     0.2514   -1.40   0.1603   \nrespI18 == 0  -0.0309     0.2175   -0.14   0.8871   \nrespI19 == 0  -0.3245     0.3030   -1.07   0.2841   \nrespI21 == 0   0.4872     0.2952    1.65   0.0989 . \nrespI22 == 0  -0.1314     0.3717   -0.35   0.7237   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Univariate p values reported)\n\n\n\nanchortest(\n    resp ~ gender,\n    select = \"Gini\"\n)\n\nAnchor items:\nrespI23\n\nFinal DIF tests:\n\n     Simultaneous Tests for General Linear Hypotheses\n\nLinear Hypotheses:\n             Estimate Std. Error z value Pr(&gt;|z|)   \nrespI1 == 0   0.12610    0.38737    0.33   0.7448   \nrespI2 == 0  -0.92706    0.38904   -2.38   0.0172 * \nrespI3 == 0   0.00921    0.42718    0.02   0.9828   \nrespI6 == 0  -0.31249    0.37656   -0.83   0.4066   \nrespI7 == 0  -1.89057    0.63210   -2.99   0.0028 **\nrespI11 == 0  0.20238    0.39228    0.52   0.6059   \nrespI12 == 0 -0.09661    0.38679   -0.25   0.8028   \nrespI14 == 0 -0.14496    0.37695   -0.38   0.7006   \nrespI17 == 0 -0.50866    0.40738   -1.25   0.2118   \nrespI18 == 0 -0.18656    0.37641   -0.50   0.6202   \nrespI19 == 0 -0.48019    0.45080   -1.07   0.2868   \nrespI21 == 0  0.33158    0.41809    0.79   0.4277   \nrespI22 == 0 -0.28708    0.47621   -0.60   0.5466   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Univariate p values reported)\n\n\nGli output di R della funzione anchortest() elencano gli item di ancoraggio selezionati dai rispettivi approcci di selezione dell’ancoraggio, oltre ai risultati del test di Wald basati su questi item di ancoraggio. Tutti e tre gli approcci portano a risultati in cui solo gli item 2 e 7 mostrano DIF per genere, mentre il test grafico e il test di Wald in eRm hanno identificato anche l’item 21 e l’item 11 (al limite) come aventi DIF.\nRiesaminando il test grafico nella figura precedente, notiamo che gli item 2 e 7 mostrano DIF nella stessa direzione (sopra la diagonale), mentre gli item 21 e 11 sono orientati nella direzione opposta (sotto la diagonale) e in misura minore.\nConsiderando questi risultati nel loro insieme, si può concludere che potrebbe essere presente un DIF non bilanciato e che la diagonale usata nella Figura 6.4 non è ideale per valutare gli item. Per illustrare ciò, tracciamo manualmente una linea di riferimento alternativa attraverso la posizione dell’item 23, che è stato selezionato come item di ancoraggio (primario) dai tre approcci presentati in psychotools, utilizzando il comando abline.\n\n plotGOF(\n    lrt_gender,\n    tlab = \"item\", pos = 1,\n    main = \"Difficulty by Gender (with Item Names)\",\n    conf = list(gamma = 0.95, col = 1)\n)\nabline(-0.3, 1, lty=2)\n\n\n\n\n\n\n\n\nCome si può vedere nella figura risultante, basandoci sulla linea di riferimento alternativa, non troviamo più DIF negli item 11 e 21, ma gli item 2 e 7 mostrano ancora più chiaramente un DIF.\nPer questo set di dati, la stessa conclusione viene raggiunta in eRm quando si utilizza la funzione stepwiseIt(), che esegue diversi test di Wald e ad ogni passo esclude l’item singolo con la statistica di test più grande.\n\nstepwiseIt(rm_sum0, criterion = list(\"Waldtest\", gender))\n\nEliminated item - Step 1: I7\nEliminated item - Step 2: I2\n\n\n\nResults for stepwise item elimination:\nNumber of steps: 2 \nCriterion: Waldtest\n\n           z-statistic p-value\nStep 1: I7        3.09   0.002\nStep 2: I2        3.06   0.002\n\n\nUtilizzando questo metodo, dopo l’esclusione degli item 7 e 2, che presentavano il DIF più marcato, non si rilevano più differenze significative nei test degli item rimanenti. Per visualizzare meglio questo processo, immaginiamo la figura precedente: inizialmente, la linea di riferimento corrisponde alla diagonale solida. Tuttavia, dopo aver eliminato l’item 7, questa linea si sposta verso quella tratteggiata nel secondo passaggio e, rimuovendo poi l’item 2, si allinea o si avvicina molto alla linea tratteggiata nel terzo passaggio. Di conseguenza, gli item restanti non mostrano più un DIF significativo.\nIn sintesi, mentre i test grafici e di Wald basati sulla restrizione della somma zero possono risultare ingannevoli in presenza di un DIF non bilanciato, l’impiego di metodi di ancoraggio avanzati e l’approccio di eliminazione graduale degli item possono offrire una visione più accurata e dettagliata della situazione.\n\n\n69.4.6 Rimozione di item\nSe questa analisi facesse parte della costruzione di un test reale, gli item che mostrano DIF (o altre anomalie nelle analisi successive) dovrebbero essere attentamente esaminati da esperti di contenuto per decidere se modificarli o rimuoverli dal test. Nella discussione seguente, tuttavia, non rimuoveremo gli item perché desideriamo mantenere il set di dati completo. Tuttavia, se si desiderasse rimuovere alcuni item (ovvero colonne) dal set di dati, ciò potrebbe essere fatto con i seguenti comandi.\n\nresponses_removeDIFitems &lt;- responses[, -which(colnames(responses) %in% c(\"I2\", \"I7\"))]\ncolnames(responses_removeDIFitems)\n\n\n'I1''I3''I6''I11''I12''I14''I17''I18''I19''I21''I22''I23'\n\n\nDopo aver rimosso degli item, l’intero processo dovrebbe ricominciare da capo, rifacendo il modello di Rasch e indagando sugli item rimanenti.\n\n\n69.4.7 Test di Martin-Löf\nNella sezione precedente, abbiamo visto che il test del rapporto di verosimiglianza di Andersen (1973) verifica l’ipotesi che i parametri degli item siano invarianti per vari gruppi di persone. Una ipotesi correlata riguarda l’invarianza dei parametri delle persone per diversi gruppi di item.\nQui, la domanda fondamentale è se diversi gruppi di item misurino tratti latenti differenti. Ciò rappresenterebbe una violazione del modello di Rasch, il quale implica un singolo tratto latente alla base di tutti gli item. Se questo tipo di violazione del modello viene rilevato, un modello IRT multidimensionale potrebbe essere più appropriato.\nUn metodo comune per valutare la dimensionalità in generale è l’analisi fattoriale esplorativa. Qui invece descriveremo il test di Martin-Löf che affronta l’ipotesi alternativa secondo cui gruppi di item misurano tratti latenti differenti ed è disponibile nel pacchetto eRm. Come il test del rapporto di verosimiglianza di Andersen, questo test si basa sul confronto di due verosimiglianze condizionate. La prima verosimiglianza condizionata Lu(r,β) è quella del modello di Rasch. La seconda verosimiglianza condizionata Lu(r1, r2, β) è nuovamente quella di un modello più generale che ora permette diversi parametri di persona per specifici gruppi di item. I gruppi di item devono essere definiti prima dell’analisi, il che può essere fatto in base alle loro difficoltà (cioè, testiamo item facili contro difficili) o in base a diverse dimensioni latenti che si sospetta siano misurate dai gruppi di item (cioè, il gruppo di item 1 è sospettato di misurare una dimensione latente diversa rispetto al gruppo di item 2). Se la seconda verosimiglianza è maggiore, ciò indica una violazione del modello di Rasch (analogamente al test del rapporto di verosimiglianza di Andersen).\nIl test di Martin-Löf è spesso descritto come un test per la unidimensionalità. Certi tipi di multidimensionalità possono anche manifestarsi come DIF. Per questa ragione, i test che mirano a rilevare il DIF, possono anche essere sensibili a certe violazioni della unidimensionalità.\n\nmloef_median &lt;- MLoef(rm_sum0, splitcr = \"median\")\nmloef_median\n\n\nMartin-Loef-Test (split criterion: median)\nLR-value: 67.083 \nChi-square df: 48 \np-value: 0.036 \n\n\nOtteniamo un valore p inferiore a 0.05. Ciò indica che le stime dei parametri delle persone ottenute dagli item facili e difficili differiscono in modo significativo, ovvero, una violazione del modello di Rasch.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/06_implementation.html#item-e-person-fit",
    "href": "chapters/irt/06_implementation.html#item-e-person-fit",
    "title": "69  Implementazione",
    "section": "69.5 Item e Person Fit",
    "text": "69.5 Item e Person Fit\n\n69.5.1 Tests e Statistiche di Bontà di Adattamento\nIn questa sezione esaminiamo una varietà di metodi per valutare l’adattamento dei dati di risposta agli item e al modello di Rasch. Alcuni di questi metodi sono test statistici formali, mentre altri sono statistiche descrittive per le quali sono stati suggeriti nella letteratura dei limiti critici empirici. Vedremo anche che esistono approcci per valutare l’adattamento a livello dell’intero test psicologico, così come approcci focalizzati sulla valutazione dell’adattamento di singoli item o individui.\n\n\n69.5.2 Test di Bontà di Adattamento χ2 e G2\nNella valutazione del modello di Rasch, esaminiamo due classi principali di test di bontà di adattamento: il test χ2 e il test G2, entrambi noti nell’analisi delle tabelle di contingenza. A differenza dei test del rapporto di verosimiglianza o dei test di Martin-Löf, il test χ2 non confronta l’adattamento relativo di due modelli. Piuttosto, esso valuta quanto accuratamente i modelli di risposta previsti dal modello di Rasch corrispondano ai modelli di risposta osservati. Questo avviene attraverso il confronto tra il numero di partecipanti che mostrano ciascun modello di risposta osservato e il numero previsto dal modello di Rasch.\nIl principio dei test di bontà di adattamento χ2 per il modello di Rasch è basato sull’analisi di tutti i possibili modelli di risposta (combinazioni di 0 e 1 per risposte errate e corrette). Definiamo Ou come il numero osservato di partecipanti con il modello di risposta u e Eu come il numero previsto sotto il modello di Rasch. La statistica del test χ2 è data da:\n\\[ T = \\sum_{u} \\frac{(O_u - E_u)^2}{E_u} \\]\nIn questa formula, le differenze tra osservazioni e previsioni sono elevate al quadrato e poi ponderate inversamente rispetto alla frequenza attesa. In campioni di grandi dimensioni, T segue approssimativamente una distribuzione χ2, se il modello di Rasch è appropriato. Valori alti di T indicano una cattiva adattazione del modello.\nTuttavia, il test χ2 richiede che ogni modello di risposta abbia una frequenza attesa sufficientemente alta, una condizione spesso non soddisfatta in test con molti item. In questi casi, il test χ2 non segue una distribuzione χ2 sotto l’ipotesi nulla, rendendolo poco pratico. Una soluzione potrebbe essere quella di raggruppare i modelli di risposta per aumentare le frequenze attese.\nParallelamente, la statistica del rapporto di verosimiglianza G2, anch’essa derivante dall’analisi dei dati categoriali, è calcolata come:\n\\[ G^2 = 2 \\sum_{u} O_u \\log \\left( \\frac{O_u}{E_u} \\right) \\]\nG2 confronta le frequenze osservate con quelle attese, anziché le verosimiglianze di due modelli. Se le frequenze attese sono vicine a quelle osservate, il rapporto \\(\\frac{O_u}{E_u}\\) si avvicina a 1, rendendo il logaritmo naturale \\(\\log\\left(\\frac{O_u}{E_u}\\right)\\) vicino a 0 e la statistica G2 tende a 0, indicando un buon adattamento. Anche G2 segue una distribuzione χ2 se il modello di Rasch è appropriato. Tuttavia, proprio come per il test χ2, G2 è praticabile solo con grandi frequenze attese, limitandone l’uso effettivo. Nonostante ciò, G2 è importante da comprendere poiché molte altre statistiche di test si basano su di esso.\n\n\n69.5.3 Statistica M2\nLa statistica M2, sviluppata da Maydeu-Olivares e Joe (2006), affronta il problema dei modelli di risposta rari che possono complicare i test χ2. Invece di confrontare le frequenze di interi modelli di risposta, la statistica M2 utilizza le informazioni provenienti dagli item individuali e dalle coppie di item. Specificatamente, confronta: 1. Le frequenze attese e osservate delle risposte corrette agli item individuali. 2. Le frequenze attese e osservate delle risposte corrette a entrambi gli item in una coppia di item.\nPer esempio, con due item, confronterebbe le frequenze osservate e attese per una risposta corretta al primo item, al secondo item e ad entrambi gli item insieme. Questo approccio è simile all’analisi delle tabelle di frequenza per le coppie di item. La statistica M2, come il test di bontà di adattamento χ2, implica un cattivo adattamento tra i dati e il modello di Rasch se produce un valore elevato o, equivalentemente, un valore p piccolo. Senza violazione del modello, la statistica M2 segue approssimativamente una distribuzione χ2 con gradi di libertà calcolati come $ k - d $, dove $ k $ è il numero di frequenze confrontate e $ d $ è il numero di parametri liberi del modello.\n\n\n69.5.4 Errore Quadratico Medio di Approssimazione (RMSEA)\nIl RMSEA deriva dalla statistica M2. Utilizza i gradi di libertà (nuovamente $ k - d $) e la dimensione del campione $ P $ per calcolare il valore RMSEA. La formula per il RMSEA è:\n\\[ \\text{RMSEA} = \\sqrt{\\frac{M2 - df}{P \\cdot df}} \\]\nValori di RMSEA vicini a 0 generalmente indicano un buon adattamento del modello ai dati. Sebbene non esistano linee guida universalmente accettate per interpretare il RMSEA, un valore intorno a 0,05 è spesso considerato indicativo di un buon adattamento del modello.\n\n\n69.5.5 Residuo Quadratico Medio Standardizzato (SRMSR)\nSRMSR è un’altra statistica di adattamento complessivo che confronta le correlazioni o le covarianze osservate tra tutte le coppie di item con quelle previste sotto il modello di Rasch (o un altro modello della teoria della risposta agli item). Valori vicini a 0 suggeriscono un buon adattamento del modello. Maydeu-Olivares (2013) raccomanda l’uso di un valore di soglia di 0.05 per SRMSR, simile al RMSEA.\nNel complesso, queste statistiche (M2, RMSEA e SRMSR) sono utili per valutare l’adattamento di un modello, come il modello di Rasch, a un dato insieme di dati di risposta agli item. Forniscono diverse prospettive attraverso le quali la congruenza tra i dati e il modello teorico può essere valutata, ognuna con il suo focus unico e metodo di calcolo.\n\nfit_rasch &lt;- mirt(responses, 1, itemtype = \"Rasch\", verbose = FALSE)\nfit_rasch\n\n\nCall:\nmirt(data = responses, model = 1, itemtype = \"Rasch\", verbose = FALSE)\n\nFull-information item factor analysis with 1 factor(s).\nConverged within 1e-04 tolerance after 16 EM iterations.\nmirt version: 1.43 \nM-step optimizer: nlminb \nEM acceleration: Ramsay \nNumber of rectangular quadrature: 61\nLatent density type: Gaussian \n\nLog-likelihood = -2816\nEstimated parameters: 15 \nAIC = 5663\nBIC = 5723; SABIC = 5675\nG2 (16368) = 1319, p = 1\nRMSEA = 0, CFI = NaN, TLI = NaN\n\n\n\nM2(fit_rasch)\n\n\nA data.frame: 1 x 9\n\n\n\nM2\ndf\np\nRMSEA\nRMSEA_5\nRMSEA_95\nSRMSR\nTLI\nCFI\n\n\n\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nstats\n277.5505\n90\n0\n0.0722689\n0.06261658\n0.08191791\n0.09413757\n0.7494602\n0.7522134\n\n\n\n\n\nLa statistica M2 è alta e significativa, indicando che ci sono differenze preoccupanti tra il modello e i dati. Questo è ulteriormente supportato da un RMSEA troppo alto e da un CFA e TLI lontani da 1.\nRicordiamo il significato degli indici RMSEA, CFA e TLI.\nRMSEA (Root Mean Square Error of Approximation): - Il RMSEA è una misura di adattamento che valuta quanto bene un modello si adatta ai dati a livello di popolazione. - Un valore basso di RMSEA indica un buon adattamento, suggerendo che il modello approssima bene la realtà. - Generalmente, un RMSEA inferiore a 0.05 o 0.06 è considerato indicativo di un ottimo adattamento del modello.\nCFA (Comparative Fit Index): - Il CFA è un indice relativo di bontà di adattamento che confronta il modello specificato con un modello nullo o di base. - Valori più vicini a 1 indicano un adattamento migliore. Un CFA superiore a 0.90 o 0.95 è spesso considerato indicativo di un buon adattamento.\nTLI (Tucker-Lewis Index): - Simile al CFA, il TLI è un altro indice relativo di adattamento che tiene conto della complessità del modello. - Anche per il TLI, valori più vicini a 1 indicano un adattamento migliore. Valori superiori a 0.90 o 0.95 sono generalmente considerati buoni.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/06_implementation.html#valutare-ladattamento-degli-item",
    "href": "chapters/irt/06_implementation.html#valutare-ladattamento-degli-item",
    "title": "69  Implementazione",
    "section": "69.6 Valutare l’Adattamento degli Item",
    "text": "69.6 Valutare l’Adattamento degli Item\nTuttavia, nell’IRT, ci interessiamo maggiormente agli indici di adattamento degli item e delle persone. L’IRT ci consente di valutare quanto bene ogni item si adatti al modello e se i pattern di risposta individuali sono allineati con il modello.\nIniziamo con l’addattamento agli item. Sono stati proposti diversi indici per valutare l’adattamento degli item e possiamo utilizzare la funzione itemfit() per ottenere una varietà di questi indici. Di default, riceviamo l’S_X2 di Orlando e Thissen (2000) con i corrispondenti gradi di libertà (dfs), RMSEA e valori p. Questo test dovrebbe risultare non significativo per indicare un buon adattamento dell’item. Come vediamo qui sotto, diversi item mostra un cattivo adattamento.\n\nitemfit(fit_rasch)\n\n\nA mirt_df: 14 x 5\n\n\nitem\nS_X2\ndf.S_X2\nRMSEA.S_X2\np.S_X2\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nI1\n4.15\n7\n0.0000\n7.62e-01\n\n\nI2\n19.33\n7\n0.0665\n7.20e-03\n\n\nI3\n5.53\n7\n0.0000\n5.96e-01\n\n\nI6\n14.09\n8\n0.0437\n7.96e-02\n\n\nI7\n8.98\n7\n0.0266\n2.54e-01\n\n\nI11\n23.36\n7\n0.0765\n1.47e-03\n\n\nI12\n17.67\n7\n0.0618\n1.35e-02\n\n\nI14\n9.79\n7\n0.0316\n2.01e-01\n\n\nI17\n35.13\n7\n0.1004\n1.06e-05\n\n\nI18\n3.70\n7\n0.0000\n8.14e-01\n\n\nI19\n20.93\n7\n0.0706\n3.87e-03\n\n\nI21\n86.54\n7\n0.1688\n6.33e-16\n\n\nI22\n73.46\n7\n0.1543\n2.95e-13\n\n\nI23\n7.61\n7\n0.0148\n3.68e-01\n\n\n\n\n\n\n69.6.1 Statistiche di Infit e Outfit\nNella sezione precedente abbiamo discusso i test χ2 e M2, basati sul confronto tra le frequenze osservate e quelle attese secondo il modello di Rasch. Le statistiche di adattamento presentate di seguito si basano su un approccio simile, utilizzando i residui di Rasch. Questi sono le differenze tra le risposte osservate (vale a dire, le risposte 0 o 1 per gli item dicotomici) e i loro valori attesi (cioè, le probabilità predette di una risposta corretta secondo il modello di Rasch). Tipicamente, questi valori attesi vengono calcolati in base alle stime dei parametri degli item e delle persone.\nGeneralmente, quando c’è un buon adattamento tra i dati e il modello, si può prevedere che i residui siano piccoli. Pertanto, è naturale che i residui di Rasch possano essere utilizzati per valutare l’adattamento del modello di Rasch. Vedremo che nell’analisi di Rasch non solo i casi in cui i residui sono più grandi del previsto possono essere motivo di preoccupazione, ma anche quelli in cui i residui sono più piccoli del previsto.\nUn approccio comune per verificare l’adattamento di singoli item usando i residui di Rasch consiste nel calcolare le statistiche di infit e outfit. Descriveremo i passaggi per calcolare queste statistiche prima di affrontarne l’interpretazione. Ci concentreremo sul caso in cui queste statistiche vengono calcolate per singoli item.\n\n69.6.1.1 Outfit\nLa funzione principale della statistica di outfit è quella di quantificare in che misura le risposte dei partecipanti si allontanano dalle previsioni del modello. Questo indice si calcola attraverso diversi passaggi, che mirano a stabilire la misura in cui le risposte individuali si discostano dalle aspettative teoriche.\n1. Definizione dei Residui di Rasch: Inizialmente, per ogni partecipante e per ciascun item del test, si calcola il residuo di Rasch. Un residuo è essenzialmente la differenza tra la risposta osservata di un individuo a un determinato item e la risposta prevista da quel partecipante per lo stesso item. La risposta prevista è calcolata sulla base della probabilità, fornita dal modello di Rasch, che il partecipante risponda correttamente all’item. Ad esempio, se il modello prevede che un partecipante abbia il 40% di probabilità di rispondere correttamente a un item e il partecipante risponde effettivamente correttamente, il residuo corrispondente sarà $ 1 - 0.40 = 0.60 $.\n2. Standardizzazione dei Residui di Rasch: Successivamente, questi residui vengono standardizzati. La standardizzazione implica l’adeguamento dei residui in modo che abbiano una media di zero e una varianza di uno. Ciò permette di confrontare i residui in maniera uniforme, indipendentemente dalle caratteristiche specifiche degli item o dei partecipanti.\n3. Calcolo dello Z-Score: Per ciascun residuo, si calcola lo z-score standardizzato, $ Z_{si} $, utilizzando la formula:\n\\[\n   Z_{si} = \\frac{X_{si} - E(X_{si})}{\\sqrt{Var(X_{si})}},\n   \\]\ndove $ Z_{si} $ rappresenta lo z-score del residuo per il partecipante $ s $ all’item $ i $, $ X_{si} $ è la risposta osservata, $ E(X_{si}) $ è la risposta attesa (basata sulla probabilità di una risposta corretta secondo il modello di Rasch), e $ Var(X_{si}) $ è la varianza della risposta attesa.\n4. Calcolo della Statistica di Outfit: Per calcolare la statistica di outfit mean square (MSQ) per un specifico item, si seguono questi passaggi: - Si elevano al quadrato gli z-score standardizzati di ogni partecipante per l’item in questione. - Si sommano tutti questi valori quadrati. - Si divide la somma ottenuta per il numero totale dei partecipanti.\nLa formula risultante per la statistica di outfit MSQ per l’item $ i $ è la seguente:\n\\[\n   \\text{Outfit MSQ}_i = \\frac{\\sum_{p=1}^{P} Z_{pi}^2}{P}.\n   \\]\nQuesta procedura fornisce una misura dell’adattamento delle risposte degli individui all’item specifico, rispetto alle previsioni del modello di Rasch. Un valore di MSQ significativamente alto o basso può indicare potenziali discrepanze tra le risposte osservate e quelle previste, suggerendo la necessità di ulteriori analisi o revisioni del modello o degli item del test.\nSecondo Wright e Masters (1990), questa statistica ha un valore atteso di 1 sotto il modello di Rasch. Valori superiori a 1 indicano residui di Rasch più grandi del previsto secondo il modello di Rasch, e quindi una possibile violazione del modello. Tali item vengono anche detti mostrare un underfit. Valori inferiori a 1 indicano che i residui sono inferiori al previsto. Ciò è considerato indicare un overfit delle risposte al modello di Rasch. In questo contesto, overfit significa che la deviazione tra i valori attesi e i dati empirici è minore del previsto.\nPossiamo inoltre ottenere una statistica di mean square pesata e standardizzata per ciascun item, tipicamente denotata da ti. Siano \\(\\sqrt[3]{\\text{MSQ}_i}\\) e sd(MSQ_i) il cubo radice e la deviazione standard attesa di Outfit MSQ_i, rispettivamente. Allora la statistica standardizzata ti è\n\\[\n\\text{Outfit ti} = \\left( \\sqrt[3]{\\text{MSQ}_i} - 1 \\right) \\left( \\frac{3}{\\text{sd(MSQ}_i)} \\right) + \\left( \\frac{\\text{sd(MSQ}_i)}{3} \\right).\n\\]\nQuesta statistica standardizzata ti è spesso presentata nei risultati del software in aggiunta alla statistica MSQ.\nItem che mostrano underfit e overfit possono anche essere identificati approssimativamente usando le loro ICC empiriche, come abbiamo già visto in precedenza. Gli item che mostrano underfit hanno ICC empiriche più piatte di quelle previste sotto il modello di Rasch. Gli item che mostrano overfit hanno ICC empiriche più ripide del previsto.\n\n\n69.6.1.2 Infit\nL’indice di infit è un altro indice critico nel modello di Rasch. A differenza dell’outfit, che è più influenzato da risposte casuali o outlier, l’infit è più sensibile alle risposte che sono incoerenti con il pattern generale del modello. L’infit è calcolato come una media ponderata dei residui standardizzati, dove i pesi sono inversamente proporzionali alla varianza degli item. Questo rende l’infit particolarmente utile per identificare problemi di adattamento del modello legati alla consistenza interna delle risposte.\nLa statistica di infit MSQ, come quella di outfit, serve a valutare l’adattamento delle risposte individuali rispetto alle aspettative teoriche del modello. Tuttavia, la statistica di infit differisce dall’outfit per il modo in cui tratta i residui.\n1. Ponderazione dei Residui di Rasch: Nella statistica di infit, i residui di Rasch delle risposte individuali vengono ponderati in base alla loro varianza attesa sotto il modello di Rasch. Ciò significa che i residui con varianze minori (che tendono a verificarsi quando c’è una grande distanza tra le abilità dei rispondenti e la difficoltà degli item) hanno un impatto relativamente minore sulla statistica di infit rispetto a quelli con varianze maggiori.\n2. Riduzione dell’Impatto degli Outlier: Questo approccio di ponderazione rende la statistica di infit meno sensibile agli outlier rispetto all’outfit. In altre parole, mentre la statistica di outfit è influenzata in maniera più uniforme da tutte le deviazioni dalle aspettative del modello, l’infit dà maggiore peso alle deviazioni che sono meno estreme o più prevedibili data la struttura del modello.\n3. Formula per la Statistica di Infit MSQ: La formula per calcolare l’Infit MSQ per un dato item $ i $ è la seguente:\n\\[\n   \\text{Infit MSQ}_i = \\frac{\\sum_{p=1}^{P} W_{pi} Z_{pi}^2}{\\sum_{p=1}^{P} W_{pi}},\n   \\]\ndove: - $ Z_{pi} $ rappresenta il residuo di Rasch standardizzato per il rispondente $ p $ all’item $ i $. - $ W_{pi} $ è la varianza attesa del residuo $ Z_{pi} $ sotto il modello di Rasch. - $ P $ è il numero totale dei rispondenti.\n4. Standardizzazione della Statistica Infit: Come per l’outfit, è anche possibile calcolare una versione standardizzata dell’Infit MSQ per ogni item. Questa versione standardizzata, nota come statistica Infit t, consente di confrontare più facilmente l’adattamento degli item in diverse situazioni o in diversi test, normalizzando i valori su una scala comune.\nIn sintesi, la statistica di infit MSQ offre un modo ponderato per valutare l’adattamento delle risposte ai singoli item in un test basato sul modello di Rasch, tenendo conto della varianza attesa delle risposte. Questo la rende particolarmente utile per identificare i casi in cui le risposte si discostano dalle previsioni del modello in modi meno estremi o più in linea con la struttura del modello stesso.\n\n\n69.6.1.3 Soglie\nPer entrambi i valori MSQ e t delle statistiche di infit e outfit, sono stati proposti vari valori di soglia. Bond e Fox (2007) e Engelhard (2013) menzionano valori di soglia di -2 e 2 per le statistiche t, mentre Paek e Cole (2020) suggeriscono -3 e 3. Analogamente, Bond e Fox (2007) danno 0.75 e 1.3 come valori di soglia per le statistiche MSQ, mentre DeMars (2010) menziona 0.6 e 1.5 come possibili alternative. Desjardins e Bulut (2018), d’altra parte, si oppongono all’uso di valori di soglia specifici per queste statistiche.\nPossiamo calcolare le statistiche infit e oputfit degli item usando il pacchetto eRm:\n\nrm_sum0 &lt;- RM(responses)\neRm::itemfit(person.parameter(rm_sum0))\n\n\nItemfit Statistics: \n    Chisq  df p-value Outfit MSQ Infit MSQ Outfit t Infit t Discrim\nI1    325 397   0.996      0.818     0.904   -1.750  -1.666   0.418\nI2    274 397   1.000      0.688     0.809   -2.928  -3.271   0.520\nI3    289 397   1.000      0.727     0.869   -1.574  -1.505   0.371\nI6    334 397   0.991      0.838     0.860   -2.317  -3.257   0.505\nI7    273 397   1.000      0.686     0.838   -1.328  -1.423   0.279\nI11   333 397   0.992      0.836     0.816   -1.432  -3.143   0.473\nI12   459 397   0.018      1.152     0.972    1.538  -0.538   0.321\nI14   396 397   0.508      0.994     1.023   -0.043   0.492   0.320\nI17   524 397   0.000      1.317     0.936    2.290  -1.031   0.280\nI18   433 397   0.104      1.088     1.019    1.121   0.424   0.314\nI19   227 397   1.000      0.569     0.750   -2.579  -2.999   0.453\nI21   906 397   0.000      2.276     1.246    5.846   2.986  -0.108\nI22   728 397   0.000      1.829     0.985    2.918  -0.105   0.059\nI23   276 397   1.000      0.693     0.852   -1.918  -1.812   0.412\n\n\nIn alternativa, è possibile usare la funzione mirt del pacchetto mirt:\n\nmirt_rm &lt;- mirt(responses, 1, \"Rasch\")\nmirt::itemfit(mirt_rm, fit_stats = \"infit\", method = \"ML\")\n\nIteration: 16, Log-Lik: -2816.487, Max-Change: 0.00008\n\n\n\nA mirt_df: 14 x 5\n\n\nitem\noutfit\nz.outfit\ninfit\nz.infit\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nI1\n0.814\n-1.683\n0.904\n-1.665\n\n\nI2\n0.685\n-2.793\n0.809\n-3.271\n\n\nI3\n0.724\n-1.510\n0.870\n-1.493\n\n\nI6\n0.834\n-2.272\n0.860\n-3.258\n\n\nI7\n0.682\n-1.357\n0.831\n-1.504\n\n\nI11\n0.832\n-1.381\n0.816\n-3.143\n\n\nI12\n1.148\n1.484\n0.971\n-0.554\n\n\nI14\n0.990\n-0.086\n1.023\n0.494\n\n\nI17\n1.315\n2.276\n0.935\n-1.040\n\n\nI18\n1.083\n1.011\n1.019\n0.426\n\n\nI19\n0.569\n-2.585\n0.748\n-3.014\n\n\nI21\n2.279\n5.852\n1.244\n2.960\n\n\nI22\n1.823\n2.911\n0.978\n-0.165\n\n\nI23\n0.690\n-1.831\n0.853\n-1.804\n\n\n\n\n\nLa tabella risultante inizia con le statistiche del test di adattamento χ2 approssimativo, i suoi gradi di libertà e i valori di p risultanti. Se il modello di Rasch è valido, la statistica di test risultante può essere approssimativamente descritta da una distribuzione χ2, il che porta ai valori di p presentati.\nLe colonne seguenti presentano le statistiche MSQ e t di infit e outfit. Per le statistiche MSQ di infit e outfit, valori vicini a 1 indicano un buon adattamento del modello, mentre per le statistiche t di infit e outfit, valori vicini a 0 indicano un buon adattamento. Valori più alti indicano che le risposte sono più casuali di quanto previsto dal modello di Rasch, segnalando un sottoadattamento (underfit); valori più bassi indicano che le risposte sono meno casuali del previsto, segnalando un sovradattamento (overfit).\nSeguendo una delle linee guida proposte, esamineremo ulteriormente quegli item i cui valori t di infit o outfit sono inferiori a -2 o superiori a 2 (ma esistono linee guida alternative). Troviamo che per gli item 2, 6, 11 e 19, almeno un valore t è inferiore a -2, indicando un sovradattamento. Per l’item 19 ciò è supportato dal fatto che la ICC empirica ha una pendenza più ripida rispetto alla ICC attesa.\nPer gli item 17, 21 e 22, invece, almeno un valore t per le statistiche di infit e outfit è superiore a 2, indicando un sottoadattamento. Questo è nuovamente in linea con l’esame delle ICC, dove abbiamo riscontrato che la ICC empirica per l’item 21 ha una pendenza inferiore rispetto alla ICC attesa.\n\nitemfitPlot(mirt_rm)\n\n\n\n\n\n\n\n\n\n\n\n69.6.2 Valutare l’Adattamento delle Persone\nPossiamo generare le stesse misure di adattamento per ogni persona per valutare quanto bene i pattern di risposta di ciascuno si allineano con il modello. Ragioniamo in questo modo: se una persona con un alto valore di \\(\\theta\\) (cioè alta abilità latente) non risponde correttamente a un item facile, questa persona non si adatta bene al modello. Al contrario, se una persona con bassa abilità risponde correttamente a una domanda molto difficile, anche questo non è conforme al modello. Nella pratica, è probabile che ci saranno alcune persone che non si adattano bene al modello. Tuttavia, finché il numero di rispondenti non conformi è basso, la situazione è accettabile. Di solito, ci concentriamo nuovamente sulle statistiche di infit e outfit. Se meno del 5% dei rispondenti presenta valori di infit e outfit superiori o inferiori a 1.96 e -1.96, possiamo considerare il modello adeguato.\nStimiamo gli indici infit e outfit delle persone usando eRm:\n\neRm::personfit(person.parameter(rm_sum0))\n\n\nPersonfit Statistics: \n     Chisq df p-value Outfit MSQ Infit MSQ Outfit t Infit t\n1    13.19 13   0.433      0.942     1.080     0.07    0.35\n2     7.00 13   0.902      0.500     0.744    -0.44   -0.85\n3     7.56 13   0.871      0.540     0.799    -0.37   -0.62\n4    13.65 13   0.399      0.975     1.112     0.14    0.44\n5     4.02 13   0.991      0.287     0.354    -1.81   -2.20\n6    22.05 13   0.055      1.575     1.304     0.96    1.01\n7    22.82 13   0.044      1.630     1.623     1.02    1.83\n8    12.94 13   0.453      0.924     0.746     0.00   -0.66\n9    71.92 13   0.000      5.137     1.376     2.32    1.11\n10   22.51 13   0.048      1.608     1.065     1.12    0.30\n11    8.24 13   0.827      0.589     0.786    -0.83   -0.52\n12   17.93 13   0.160      1.280     0.916     0.67   -0.10\n13    5.63 13   0.959      0.402     0.667    -0.33   -1.01\n14   18.08 13   0.154      1.291     0.950     0.62   -0.07\n15    9.43 13   0.740      0.673     0.890    -0.50   -0.24\n16   49.15 13   0.000      3.511     1.562     1.53    1.20\n17   16.14 13   0.242      1.153     1.094     0.46    0.37\n18    7.40 13   0.880      0.529     0.785    -0.39   -0.68\n19    4.70 13   0.981      0.336     0.424    -1.70   -1.96\n20    4.70 13   0.981      0.336     0.424    -1.70   -1.96\n21   45.74 13   0.000      3.267     1.221     1.99    0.79\n22    5.44 13   0.964      0.389     0.501    -1.49   -1.60\n23    6.69 13   0.917      0.478     0.812    -0.21   -0.49\n24    9.04 13   0.770      0.646     0.957    -0.19   -0.05\n25   11.94 13   0.533      0.853     1.032    -0.01    0.21\n26    8.35 13   0.820      0.596     0.851    -0.49   -0.42\n27   28.14 13   0.009      2.010     1.853     1.40    2.35\n28    7.92 13   0.849      0.566     0.731    -0.84   -0.66\n29   65.84 13   0.000      4.703     1.677     1.83    1.38\n30    8.30 13   0.824      0.593     0.752    -0.70   -0.71\n31    6.58 13   0.922      0.470     0.660    -0.78   -1.18\n32   11.04 13   0.607      0.789     1.055    -0.12    0.28\n33   10.08 13   0.687      0.720     0.908    -0.39   -0.18\n34   13.55 13   0.406      0.968     1.375     0.25    1.23\n35   23.62 13   0.035      1.687     1.170     1.23    0.60\n36   10.89 13   0.620      0.778     0.863    -0.33   -0.28\n37    6.06 13   0.944      0.433     0.585    -1.15   -1.38\n38   28.19 13   0.009      2.014     1.700     1.77    1.73\n39   13.75 13   0.392      0.982     1.271     0.36    0.85\n40   34.16 13   0.001      2.440     1.211     1.50    0.76\n41   71.92 13   0.000      5.137     1.376     2.32    1.11\n42   10.85 13   0.624      0.775     0.840    -0.31   -0.32\n43    4.02 13   0.991      0.287     0.354    -1.81   -2.20\n44    6.98 13   0.903      0.499     0.661    -1.04   -0.90\n45   68.33 13   0.000      4.881     1.024     2.24    0.18\n46   15.79 13   0.260      1.128     1.141     0.41    0.55\n47   21.71 13   0.060      1.551     1.756     0.86    1.83\n48   13.78 13   0.390      0.984     1.094     0.25    0.37\n49   15.99 13   0.250      1.142     1.334     0.60    0.81\n50   10.99 13   0.612      0.785     1.008    -0.24    0.13\n51   12.66 13   0.475      0.904     1.053    -0.04    0.27\n52    4.17 13   0.989      0.298     0.804     0.40    0.00\n53    7.53 13   0.873      0.538     0.972    -0.17    0.04\n54    5.03 13   0.975      0.359     0.491    -1.08   -1.99\n55   11.32 13   0.584      0.809     1.061    -0.08    0.30\n56   42.85 13   0.000      3.061     1.148     1.39    0.46\n57   30.25 13   0.004      2.161     1.909     1.95    2.13\n58   17.43 13   0.180      1.245     1.137     0.57    0.47\n59    5.67 13   0.957      0.405     0.521    -1.43   -1.51\n60    4.09 13   0.990      0.292     0.390    -1.43   -2.02\n61   16.04 13   0.247      1.145     1.125     0.43    0.44\n62   18.20 13   0.150      1.300     1.095     0.65    0.37\n63   19.94 13   0.097      1.425     1.438     0.90    1.15\n64   37.69 13   0.000      2.692     2.222     1.74    2.65\n65   24.53 13   0.027      1.752     1.618     1.21    1.51\n66   12.17 13   0.513      0.870     0.973     0.09    0.04\n67   19.62 13   0.105      1.402     1.199     0.84    0.68\n68   12.57 13   0.482      0.898     0.877    -0.05   -0.24\n69   18.67 13   0.134      1.334     1.365     0.74    1.10\n70    7.60 13   0.868      0.543     0.755    -0.60   -0.78\n71    9.72 13   0.716      0.695     0.926    -0.50   -0.08\n72    5.03 13   0.975      0.359     0.491    -1.08   -1.99\n73    7.08 13   0.898      0.506     0.637    -1.02   -0.98\n74    7.60 13   0.868      0.543     0.755    -0.60   -0.78\n75    4.22 13   0.989      0.301     0.456    -0.98   -1.79\n76    5.67 13   0.957      0.405     0.521    -1.43   -1.51\n77    9.06 13   0.768      0.647     0.829    -0.38   -0.50\n78    8.27 13   0.826      0.591     0.796    -0.58   -0.46\n79   11.04 13   0.607      0.789     1.055    -0.12    0.28\n80    5.83 13   0.952      0.416     0.549    -1.31   -1.31\n81   10.48 13   0.654      0.749     0.917    -0.24   -0.10\n82   10.89 13   0.620      0.778     1.038    -0.04    0.22\n83   10.09 13   0.687      0.721     0.868    -0.30   -0.24\n84    6.77 13   0.914      0.483     0.619    -1.09   -1.05\n85    4.02 13   0.991      0.287     0.354    -1.81   -2.20\n86    6.47 13   0.927      0.462     0.649    -0.80   -1.22\n87    8.13 13   0.835      0.581     0.875    -0.36   -0.24\n88   12.39 13   0.496      0.885     0.903    -0.06   -0.14\n89    5.67 13   0.957      0.405     0.521    -1.43   -1.51\n90    5.12 13   0.972      0.366     0.513    -1.18   -1.46\n91    5.36 13   0.966      0.383     0.507    -1.32   -1.73\n92    4.09 13   0.990      0.292     0.390    -1.43   -2.02\n93    6.58 13   0.922      0.470     0.660    -0.78   -1.18\n94   12.09 13   0.520      0.864     0.995    -0.08    0.09\n95    5.63 13   0.959      0.402     0.667    -0.33   -1.01\n96   24.69 13   0.025      1.764     1.076     1.33    0.34\n97   15.12 13   0.300      1.080     1.025     0.32    0.19\n98    9.84 13   0.707      0.703     0.918    -0.43   -0.15\n99   10.89 13   0.620      0.778     0.863    -0.33   -0.28\n100   7.27 13   0.888      0.519     0.764    -0.41   -0.76\n101   8.27 13   0.826      0.591     0.796    -0.58   -0.46\n102  13.28 13   0.427      0.948     0.998     0.06    0.11\n103  20.67 13   0.080      1.476     1.181     0.98    0.58\n104  24.87 13   0.024      1.777     1.370     1.40    1.00\n105   6.58 13   0.922      0.470     0.660    -0.78   -1.18\n106   5.03 13   0.975      0.359     0.491    -1.08   -1.99\n107   5.16 13   0.972      0.368     0.485    -1.37   -1.84\n108  22.38 13   0.050      1.599     1.205     1.16    0.64\n109   4.02 13   0.991      0.287     0.354    -1.81   -2.20\n110   4.70 13   0.981      0.336     0.424    -1.70   -1.96\n111   5.03 13   0.975      0.359     0.491    -1.08   -1.99\n112   6.94 13   0.905      0.496     0.675    -0.81   -0.85\n113   5.36 13   0.966      0.383     0.507    -1.32   -1.73\n114   9.88 13   0.703      0.706     0.844    -0.51   -0.34\n115  18.18 13   0.151      1.298     1.534     0.63    1.62\n116   4.35 13   0.987      0.310     0.634    -0.11   -0.77\n117  10.40 13   0.661      0.743     1.095    -0.05    0.41\n118  26.27 13   0.016      1.876     1.226     1.46    0.75\n119   5.83 13   0.952      0.416     0.549    -1.31   -1.31\n120  40.54 13   0.000      2.896     1.718     1.51    1.87\n121   5.44 13   0.964      0.389     0.501    -1.49   -1.60\n122  28.23 13   0.008      2.016     1.666     1.41    1.93\n123   5.64 13   0.958      0.403     0.735    -0.38   -0.69\n124   5.67 13   0.957      0.405     0.521    -1.43   -1.51\n125  38.77 13   0.000      2.769     1.219     1.46    0.72\n126  45.62 13   0.000      3.258     1.202     1.98    0.73\n127 102.53 13   0.000      7.324     1.711     2.35    1.44\n128  40.52 13   0.000      2.895     1.439     1.51    1.26\n129   7.30 13   0.886      0.521     0.779    -0.40   -0.70\n130  26.98 13   0.013      1.927     1.304     1.52    0.95\n131   8.84 13   0.785      0.632     0.892    -0.42   -0.27\n132   9.43 13   0.740      0.673     0.890    -0.50   -0.24\n133  27.41 13   0.011      1.957     1.442     1.35    1.38\n134  19.83 13   0.099      1.417     1.647     0.72    1.92\n135  25.52 13   0.020      1.823     1.546     1.22    1.64\n136  11.12 13   0.601      0.794     1.081    -0.15    0.33\n137  17.81 13   0.165      1.272     1.014     0.67    0.16\n138  16.06 13   0.246      1.147     1.220     0.45    0.67\n139   5.74 13   0.955      0.410     0.644    -0.70   -1.01\n140   5.12 13   0.972      0.366     0.513    -1.18   -1.46\n141   7.08 13   0.898      0.506     0.637    -1.02   -0.98\n142  11.50 13   0.569      0.822     1.160     0.06    0.61\n143  10.95 13   0.615      0.782     0.932    -0.18   -0.06\n144   6.06 13   0.944      0.433     0.585    -1.15   -1.38\n145   4.92 13   0.977      0.351     0.633    -0.47   -1.06\n146   5.28 13   0.968      0.377     0.630    -0.37   -1.16\n147  24.05 13   0.031      1.718     1.470     1.37    1.26\n148  15.12 13   0.300      1.080     1.025     0.32    0.19\n149   5.44 13   0.964      0.389     0.501    -1.49   -1.60\n150   5.16 13   0.972      0.368     0.485    -1.37   -1.84\n151  11.64 13   0.558      0.831     0.940    -0.20   -0.05\n152   8.67 13   0.797      0.619     0.812    -0.44   -0.56\n153   6.26 13   0.936      0.447     0.628    -0.83   -1.32\n154  15.00 13   0.307      1.071     1.263     0.31    0.77\n155  17.01 13   0.199      1.215     1.418     0.55    1.23\n156   7.27 13   0.888      0.519     0.764    -0.41   -0.76\n157  12.37 13   0.498      0.883     1.136    -0.04    0.50\n158   8.92 13   0.779      0.637     0.938    -0.21   -0.11\n159   9.20 13   0.758      0.657     0.810    -0.59   -0.41\n160   5.44 13   0.964      0.389     0.501    -1.49   -1.60\n161  27.20 13   0.012      1.943     1.221     1.16    0.79\n162   5.12 13   0.972      0.366     0.513    -1.18   -1.46\n163  58.61 13   0.000      4.186     1.621     2.44    1.86\n164  10.70 13   0.636      0.764     0.896    -0.16   -0.26\n165  41.12 13   0.000      2.937     1.378     1.53    1.12\n166   5.16 13   0.972      0.368     0.485    -1.37   -1.84\n167  58.08 13   0.000      4.149     1.313     1.56    0.62\n168  17.86 13   0.163      1.276     1.502     0.60    1.54\n169 146.26 13   0.000     10.447     1.338     2.27    0.65\n170  21.25 13   0.068      1.518     1.420     1.00    1.24\n171  26.86 13   0.013      1.919     1.962     1.31    2.59\n172   4.22 13   0.989      0.301     0.456    -0.98   -1.79\n173   9.22 13   0.756      0.659     1.019     0.02    0.17\n174   7.86 13   0.853      0.561     0.817    -0.10   -0.47\n175  11.45 13   0.573      0.818     0.834    -0.21   -0.34\n176   7.47 13   0.877      0.533     0.799    -0.13   -0.53\n177   4.22 13   0.989      0.301     0.456    -0.98   -1.79\n178  10.70 13   0.636      0.764     0.896    -0.16   -0.26\n179  20.35 13   0.087      1.453     1.320     0.91    0.99\n180 123.64 13   0.000      8.832     2.158     3.23    2.72\n181   8.28 13   0.825      0.592     1.068     0.61    0.35\n182   8.86 13   0.784      0.633     0.808    -0.41   -0.58\n183  48.85 13   0.000      3.489     1.539     2.10    1.65\n184   4.09 13   0.990      0.292     0.390    -1.43   -2.02\n185  10.98 13   0.613      0.784     1.153     0.01    0.59\n186   4.22 13   0.989      0.301     0.456    -0.98   -1.79\n187   6.77 13   0.914      0.483     0.619    -1.09   -1.05\n188  46.01 13   0.000      3.287     1.243     2.00    0.86\n189  36.36 13   0.001      2.597     1.751     1.60    2.16\n190  20.04 13   0.094      1.432     1.448     0.88    1.30\n191   5.16 13   0.972      0.368     0.485    -1.37   -1.84\n192  58.33 13   0.000      4.167     1.599     2.43    1.80\n193   7.14 13   0.895      0.510     0.663    -0.92   -1.06\n194   5.67 13   0.957      0.405     0.521    -1.43   -1.51\n195  11.73 13   0.550      0.838     0.857    -0.16   -0.27\n196  13.87 13   0.383      0.991     0.924     0.14   -0.10\n197   9.06 13   0.768      0.647     0.829    -0.38   -0.50\n198   4.28 13   0.988      0.306     0.544    -0.56   -1.41\n199   5.74 13   0.955      0.410     0.644    -0.70   -1.01\n200  10.58 13   0.646      0.755     0.746    -0.35   -0.61\n201  17.32 13   0.185      1.237     1.268     0.60    0.78\n202   6.26 13   0.936      0.447     0.628    -0.83   -1.32\n203   9.81 13   0.710      0.700     0.767    -0.43   -0.66\n204   6.26 13   0.936      0.447     0.628    -0.83   -1.32\n205  10.48 13   0.654      0.749     0.917    -0.24   -0.10\n206   9.17 13   0.760      0.655     1.086    -0.01    0.35\n207  16.66 13   0.215      1.190     1.409     0.52    1.13\n208  11.76 13   0.548      0.840     0.980    -0.07    0.07\n209   5.83 13   0.952      0.416     0.549    -1.31   -1.31\n210  14.94 13   0.311      1.067     1.217     0.35    0.68\n211   5.35 13   0.967      0.382     0.570    -0.68   -1.62\n212   5.44 13   0.964      0.389     0.501    -1.49   -1.60\n213   5.67 13   0.957      0.405     0.521    -1.43   -1.51\n214   5.12 13   0.972      0.366     0.513    -1.18   -1.46\n215   5.64 13   0.958      0.403     0.735    -0.38   -0.69\n216   6.77 13   0.914      0.483     0.619    -1.09   -1.05\n217   5.78 13   0.954      0.413     0.860    -0.01   -0.18\n218  12.70 13   0.471      0.907     0.921    -0.01   -0.09\n219   5.92 13   0.949      0.423     0.747    -0.34   -0.65\n220   4.70 13   0.981      0.336     0.424    -1.70   -1.96\n221  27.07 13   0.012      1.934     1.200     1.15    0.73\n222   7.44 13   0.878      0.532     0.713    -0.86   -0.86\n223   5.63 13   0.959      0.402     0.667    -0.33   -1.01\n224   4.22 13   0.989      0.301     0.456    -0.98   -1.79\n225   9.13 13   0.763      0.652     0.859    -0.44   -0.27\n226  14.02 13   0.372      1.001     1.074     0.17    0.32\n227  10.95 13   0.615      0.782     0.932    -0.18   -0.06\n228  13.78 13   0.390      0.984     1.094     0.25    0.37\n229   5.67 13   0.957      0.405     0.521    -1.43   -1.51\n230   8.26 13   0.826      0.590     0.741    -0.77   -0.63\n231   6.77 13   0.914      0.483     0.619    -1.09   -1.05\n232  10.66 13   0.639      0.762     0.843    -0.37   -0.34\n233  62.46 13   0.000      4.461     2.648     3.13    3.89\n234   8.74 13   0.793      0.624     0.755    -0.50   -0.59\n235   7.86 13   0.853      0.561     0.817    -0.10   -0.47\n236   7.92 13   0.849      0.566     0.731    -0.84   -0.66\n237   7.92 13   0.849      0.566     0.731    -0.84   -0.66\n238   4.37 13   0.987      0.312     0.865     0.40    0.07\n239  18.15 13   0.152      1.296     1.223     0.92    0.53\n240  33.12 13   0.002      2.366     1.889     1.72    2.43\n241   4.02 13   0.991      0.287     0.354    -1.81   -2.20\n242   8.79 13   0.788      0.628     0.782    -0.71   -0.54\n243   7.44 13   0.878      0.532     0.713    -0.86   -0.86\n244   8.23 13   0.828      0.588     0.749    -0.78   -0.60\n245   5.63 13   0.959      0.402     0.667    -0.33   -1.01\n246   4.22 13   0.989      0.301     0.456    -0.98   -1.79\n247  22.64 13   0.046      1.617     0.967     1.00   -0.01\n248  16.01 13   0.249      1.144     1.334     0.44    0.96\n249   4.02 13   0.991      0.287     0.354    -1.81   -2.20\n250  18.84 13   0.128      1.346     1.182     0.65    0.62\n251   4.35 13   0.987      0.310     0.634    -0.11   -0.77\n252   9.20 13   0.758      0.657     0.810    -0.59   -0.41\n253   6.47 13   0.927      0.462     0.649    -0.80   -1.22\n254   9.30 13   0.750      0.664     0.822    -0.62   -0.41\n255   5.67 13   0.957      0.405     0.521    -1.43   -1.51\n256  18.16 13   0.152      1.297     1.362     0.70    0.99\n257  12.84 13   0.460      0.917     1.162     0.07    0.53\n258   5.36 13   0.966      0.383     0.507    -1.32   -1.73\n259   6.91 13   0.907      0.494     0.910     0.11   -0.05\n260   7.50 13   0.875      0.536     0.710    -0.98   -0.78\n261   6.98 13   0.903      0.499     0.661    -1.04   -0.90\n262   4.02 13   0.991      0.287     0.354    -1.81   -2.20\n263   7.78 13   0.857      0.556     1.094     0.58    0.37\n264   8.74 13   0.793      0.624     0.755    -0.50   -0.59\n265   9.30 13   0.750      0.664     1.038    -0.21    0.23\n266   8.48 13   0.811      0.606     0.901    -0.26   -0.24\n267   4.22 13   0.989      0.301     0.456    -0.98   -1.79\n268  20.66 13   0.080      1.476     1.214     0.84    0.76\n269   6.47 13   0.927      0.462     0.649    -0.80   -1.22\n270   8.37 13   0.819      0.598     0.829    -0.27   -0.51\n271  23.03 13   0.041      1.645     1.150     0.95    0.52\n272   8.67 13   0.798      0.619     0.756    -0.51   -0.58\n273   7.85 13   0.853      0.561     0.740    -0.78   -0.76\n274  19.06 13   0.121      1.362     1.210     0.66    0.70\n275   8.23 13   0.828      0.588     0.749    -0.78   -0.60\n276  14.98 13   0.309      1.070     1.236     0.31    0.78\n277  12.38 13   0.497      0.884     1.034    -0.08    0.21\n278   4.92 13   0.977      0.351     0.633    -0.47   -1.06\n279   5.12 13   0.972      0.366     0.513    -1.18   -1.46\n280  11.27 13   0.589      0.805     1.035     0.00    0.22\n281  30.64 13   0.004      2.189     1.642     1.39    1.61\n282  16.13 13   0.242      1.152     1.025     0.45    0.19\n283  10.80 13   0.628      0.771     1.136    -0.01    0.54\n284   5.44 13   0.964      0.389     0.501    -1.49   -1.60\n285  17.34 13   0.184      1.238     1.623     0.57    1.67\n286   5.89 13   0.950      0.421     0.769    -0.35   -0.58\n287  32.26 13   0.002      2.304     1.363     1.77    0.99\n288  23.82 13   0.033      1.701     1.277     1.00    0.82\n289   4.02 13   0.991      0.287     0.354    -1.81   -2.20\n290   9.69 13   0.719      0.692     0.728    -0.50   -0.67\n291  11.57 13   0.563      0.826     0.991    -0.21    0.09\n292  15.01 13   0.307      1.072     1.001     0.32    0.13\n293  12.04 13   0.524      0.860     0.736    -0.14   -0.69\n294   8.46 13   0.812      0.605     0.790    -0.47   -0.64\n296   5.74 13   0.955      0.410     0.644    -0.70   -1.01\n297  15.67 13   0.267      1.119     1.279     0.41    0.83\n298   7.78 13   0.857      0.556     1.094     0.58    0.37\n299  15.52 13   0.276      1.109     1.298     0.38    0.85\n300  32.03 13   0.002      2.288     1.795     2.03    1.84\n301   5.03 13   0.975      0.359     0.491    -1.08   -1.99\n302   5.36 13   0.966      0.383     0.507    -1.32   -1.73\n303  10.58 13   0.646      0.755     0.746    -0.35   -0.61\n304  12.08 13   0.521      0.863     0.975    -0.08    0.03\n305  14.77 13   0.322      1.055     0.934     0.27   -0.07\n306  18.87 13   0.127      1.348     1.290     0.78    0.83\n308  14.15 13   0.363      1.011     1.091     0.28    0.37\n309   5.74 13   0.955      0.410     0.644    -0.70   -1.01\n310  18.16 13   0.152      1.297     1.087     0.72    0.36\n311   5.35 13   0.967      0.382     0.570    -0.68   -1.62\n312  16.04 13   0.247      1.145     1.125     0.43    0.44\n313  23.03 13   0.041      1.645     1.150     0.95    0.52\n314   6.94 13   0.905      0.496     0.675    -0.81   -0.85\n315  11.31 13   0.584      0.808     1.024    -0.23    0.19\n316   7.47 13   0.876      0.534     0.633    -0.99   -1.06\n317  24.89 13   0.024      1.778     1.098     1.34    0.40\n318  18.18 13   0.151      1.298     0.811     0.70   -0.41\n319   6.02 13   0.945      0.430     0.903     0.01   -0.08\n320  13.67 13   0.398      0.976     1.072     0.11    0.32\n321   7.50 13   0.875      0.536     0.710    -0.98   -0.78\n322   8.74 13   0.793      0.624     0.755    -0.50   -0.59\n323  13.75 13   0.392      0.982     1.201     0.18    0.63\n324  28.43 13   0.008      2.031     1.448     1.03    1.04\n325   6.90 13   0.907      0.493     0.768    -0.53   -0.58\n326  15.66 13   0.268      1.119     1.228     0.39    0.69\n327   8.74 13   0.793      0.624     0.755    -0.50   -0.59\n328   4.02 13   0.991      0.287     0.354    -1.81   -2.20\n329   9.69 13   0.719      0.692     0.728    -0.50   -0.67\n330   5.44 13   0.964      0.389     0.501    -1.49   -1.60\n331   4.09 13   0.990      0.292     0.390    -1.43   -2.02\n332   7.44 13   0.878      0.532     0.713    -0.86   -0.86\n333   5.12 13   0.972      0.366     0.513    -1.18   -1.46\n334   8.05 13   0.840      0.575     0.762    -0.74   -0.68\n335  31.38 13   0.003      2.242     1.672     1.97    1.61\n336   4.70 13   0.981      0.336     0.424    -1.70   -1.96\n337  10.03 13   0.691      0.717     0.928    -0.13   -0.08\n338  11.78 13   0.546      0.841     1.183     0.09    0.68\n339  11.27 13   0.589      0.805     1.035     0.00    0.22\n340  30.47 13   0.004      2.176     1.461     1.65    1.20\n341   4.09 13   0.990      0.292     0.390    -1.43   -2.02\n342  17.01 13   0.199      1.215     1.125     0.54    0.45\n343   5.12 13   0.972      0.366     0.513    -1.18   -1.46\n344  12.13 13   0.517      0.866     0.920    -0.10   -0.09\n345  12.26 13   0.507      0.876     0.905     0.10   -0.15\n346  12.94 13   0.453      0.924     0.746     0.00   -0.66\n347  21.89 13   0.057      1.564     1.668     0.81    1.67\n348  13.52 13   0.409      0.965     0.934     0.10   -0.05\n349   5.74 13   0.955      0.410     0.644    -0.70   -1.01\n350  12.70 13   0.471      0.907     0.921    -0.01   -0.09\n351  23.70 13   0.034      1.693     1.714     1.14    1.69\n352   7.47 13   0.876      0.534     0.633    -0.99   -1.06\n353  26.39 13   0.015      1.885     1.588     1.60    1.51\n354  15.31 13   0.288      1.094     1.203     0.35    0.69\n355   4.02 13   0.991      0.287     0.354    -1.81   -2.20\n356   7.14 13   0.895      0.510     0.663    -0.92   -1.06\n357  13.63 13   0.401      0.973     1.200     0.11    0.64\n358   5.44 13   0.964      0.389     0.501    -1.49   -1.60\n359  18.07 13   0.155      1.291     1.289     0.62    0.97\n360  11.94 13   0.533      0.853     1.032    -0.01    0.21\n361   8.30 13   0.824      0.593     0.752    -0.70   -0.71\n362  21.69 13   0.060      1.549     1.504     1.09    1.28\n363   8.67 13   0.797      0.619     0.812    -0.44   -0.56\n364   8.35 13   0.820      0.596     0.851    -0.49   -0.42\n365  17.60 13   0.173      1.257     1.041     0.65    0.23\n366  20.61 13   0.081      1.472     1.003     0.97    0.13\n367   8.28 13   0.825      0.592     1.068     0.61    0.35\n368   4.57 13   0.984      0.326     0.848     0.42    0.06\n369  63.80 13   0.000      4.557     1.160     1.79    0.49\n370  38.91 13   0.000      2.779     1.507     1.46    1.42\n371  20.97 13   0.073      1.498     1.488     0.91    1.25\n372  31.49 13   0.003      2.249     1.150     1.62    0.57\n373   5.16 13   0.972      0.368     0.485    -1.37   -1.84\n374   7.47 13   0.877      0.533     0.799    -0.13   -0.53\n375  26.27 13   0.016      1.876     1.226     1.46    0.75\n376   7.27 13   0.888      0.519     0.764    -0.41   -0.76\n377   8.75 13   0.792      0.625     0.921    -0.23   -0.17\n378   6.39 13   0.931      0.456     0.873     0.07   -0.14\n379  19.96 13   0.096      1.426     1.151     0.79    0.58\n380  11.67 13   0.555      0.833     1.198     0.21    0.66\n381   9.06 13   0.769      0.647     0.959    -0.19   -0.04\n382   6.74 13   0.915      0.481     0.635    -1.01   -1.17\n383   5.67 13   0.957      0.405     0.521    -1.43   -1.51\n384  46.62 13   0.000      3.330     1.814     1.70    2.07\n385  27.07 13   0.012      1.934     1.200     1.15    0.73\n386   9.88 13   0.703      0.706     0.844    -0.51   -0.34\n387   9.51 13   0.733      0.679     0.828    -0.53   -0.36\n388  20.99 13   0.073      1.499     1.349     1.05    1.00\n389   8.21 13   0.829      0.587     0.737    -0.59   -0.64\n390   5.67 13   0.957      0.405     0.521    -1.43   -1.51\n391   7.40 13   0.880      0.529     0.785    -0.39   -0.68\n392   4.02 13   0.991      0.287     0.354    -1.81   -2.20\n393   9.02 13   0.771      0.645     0.802    -0.67   -0.47\n394   8.23 13   0.828      0.588     0.749    -0.78   -0.60\n395  19.67 13   0.104      1.405     1.465     0.90    1.25\n396   8.66 13   0.798      0.619     0.865    -0.24   -0.38\n397   5.36 13   0.966      0.383     0.507    -1.32   -1.73\n398   4.70 13   0.981      0.336     0.424    -1.70   -1.96\n399  14.35 13   0.350      1.025     1.217     0.26    0.77\n400   5.28 13   0.968      0.377     0.630    -0.37   -1.16\n\n\nCome per le statistiche di infit e outfit per i singoli item, individui con valori di t superiori a 2 mostrano un comportamento di risposta più casuale rispetto a quanto previsto dal modello di Rasch. Questo può indicare, ad esempio, comportamenti di risposta basati su supposizioni o scarsa attenzione. I modelli di risposta che portano a valori di t inferiori a -2 indicano un comportamento di risposta più deterministico rispetto a quello atteso. In questo esempio, le persone identificate con il numero 5 e 43 mostrano questo comportamento.\nOtteniamo le stime infit e outfit per le persone con mirt:\n\nhead(personfit(mirt_rm))\n\n\nA data.frame: 6 x 5\n\n\n\noutfit\nz.outfit\ninfit\nz.infit\nZh\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n0.931\n0.0419\n1.090\n0.372\n-0.142\n\n\n2\n0.510\n-0.6496\n0.728\n-0.898\n0.889\n\n\n3\n0.548\n-0.5660\n0.782\n-0.682\n0.751\n\n\n4\n0.996\n0.1686\n1.111\n0.430\n-0.232\n\n\n5\n0.289\n-1.8874\n0.352\n-2.239\n1.654\n\n\n6\n1.465\n0.8848\n1.315\n1.010\n-1.002\n\n\n\n\n\n\npersonfit(mirt_rm) %&gt;%\n    summarize(\n        infit.outside = prop.table(table(z.infit &gt; 1.96 | z.infit &lt; -1.96)),\n        outfit.outside = prop.table(table(z.outfit &gt; 1.96 | z.outfit &lt; -1.96))\n    ) # lower row = non-fitting people\n\n\nA data.frame: 2 x 2\n\n\ninfit.outside\noutfit.outside\n\n\n&lt;table[1d]&gt;\n&lt;table[1d]&gt;\n\n\n\n\n0.9175\n0.98\n\n\n0.0825\n0.02\n\n\n\n\n\n\npersonfitPlot(mirt_rm)\n\n\n\n\n\n\n\n\nIn conclusione, nel caso dei dati in esame, meno del 5% dei rispondenti mostra valori di outfit che eccedono la soglia di 1.96 o che sono inferiori a -1.96. Invece, l’8% dei rispondenti mostra valori di infit che eccedono la soglia di 1.96 o che sono inferiori a -1.96. Questi risultati suggeriscono che il modello di Rasch non è del tutto coerente con i dati esaminati.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/06_implementation.html#curva-di-informazione-dellitem",
    "href": "chapters/irt/06_implementation.html#curva-di-informazione-dellitem",
    "title": "69  Implementazione",
    "section": "69.7 Curva di Informazione dell’Item",
    "text": "69.7 Curva di Informazione dell’Item\nUn altro modo per valutare la qualità di ciascun item è tramite la creazione delle cosiddette curve di informazione degli item. L’informazione è un concetto statistico che si riferisce alla capacità di un item di stimare con precisione i punteggi su theta. L’informazione a livello di item chiarisce quanto bene ogni item contribuisca alla precisione nella stima dei punteggi, con livelli più elevati di informazione che portano a stime dei punteggi più accurate.\nPer esempio:\n\nUn item con un’elevata informazione sarà molto utile per discriminare tra rispondenti con diversi livelli di abilità latente attorno a un certo punto della scala di theta. Questo significa che l’item fornisce dati affidabili e significativi sulla capacità o conoscenza che si sta misurando.\nAl contrario, un item con bassa informazione non aggiunge molto alla precisione della stima del punteggio. Questo potrebbe accadere se l’item è troppo facile o troppo difficile per la maggior parte dei rispondenti, o se non è strettamente correlato al tratto latente che si sta cercando di misurare.\n\nLa posizione delle Curve Caratteristiche degli Item (ICC) determina le regioni sul tratto latente dove ciascun item fornisce il massimo di informazione. Questo viene illustrato tramite il grafico dell’informazione dell’item.\n\nplotINFO(rm_sum0, type = \"item\", legpos = FALSE)\n\n\n\n\n\n\n\n\nQui vediamo che alcuni item forniscono maggiori informazioni sui livelli più bassi di \\(\\theta\\), altri a livelli medi di \\(\\theta\\) e altri ancora ai livelli alti di \\(\\theta\\).\n\n69.7.1 Informazione del Test\nIl concetto di “informazione” può essere applicato anche all’intera scala del test. La Test Information Curve (TIC) è una rappresentazione grafica che mostra quanta informazione un test fornisce a diversi livelli di abilità latente (\\(\\theta\\)). L’informazione è una misura della precisione con cui il test stima l’abilità di un individuo\nIn questo caso, osserviamo che la scala è molto efficace nel stimare i punteggi di theta tra -2 e 3, ma presenta una minore precisione nella stima dei punteggi di theta agli estremi. In altre parole, il test fornisce stime accurate per una vasta gamma di abilità medie e leggermente superiori alla media, ma diventa meno affidabile per valutare abilità molto basse o molto elevate.\nQuesta osservazione ha importanti implicazioni pratiche:\n\nValutazione Ottimale per la Maggior Parte dei Rispondenti: La scala è particolarmente adatta per valutare rispondenti il cui livello di abilità si trova all’interno dell’intervallo in cui il test è più informativo (-2 a 4).\nLimiti nella Valutazione degli Estremi: Per rispondenti con abilità molto al di sotto di -2 o molto al di sopra di 4, il test potrebbe non fornire stime di abilità così precise. Questo significa che per questi individui, il test potrebbe non essere in grado di discriminare efficacemente tra diversi livelli di abilità.\n\nLe curve di informazione del test aiutano a identificare dove il test è più efficace e dove potrebbe aver bisogno di miglioramenti o aggiustamenti, come l’aggiunta di item più difficili o più facili per estendere la sua precisione ai livelli estremi di abilità. Questa analisi consente di ottimizzare il test per una valutazione più accurata su tutta la gamma di abilità latente che si intende misurare.\nIl grafico dell’informazione del test può essere generato utilizzando eRm::plotINFO:\n\neRm::plotINFO(rm_sum0, type = \"test\")\n\n\n\n\n\n\n\n\nOppure possiamo usare l’output di mirt:\n\nplot(mirt_rm, type = \"info\")",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/06_implementation.html#errore-standard-di-misurazione-del-test",
    "href": "chapters/irt/06_implementation.html#errore-standard-di-misurazione-del-test",
    "title": "69  Implementazione",
    "section": "69.8 Errore Standard di Misurazione del Test",
    "text": "69.8 Errore Standard di Misurazione del Test\nIl Test Standard Error of Measurement (SEM) è una misura della precisione con cui un test stima il livello di abilità latente (\\(\\theta\\)) per una persona. In altre parole, rappresenta l’incertezza associata alla stima di \\(\\theta\\). Il grafico prodotto da plot(raschModel, type = \"SE\") mostra come il SEM varia in funzione del livello di abilità latente (\\(\\theta\\)).\n\nIl SEM è una stima dell’errore standard nella misurazione di \\(\\theta\\).\nÈ inversamente proporzionale alla quantità di informazione fornita dal test a un dato livello di \\(\\theta\\):\n\n\\[\nSEM(\\theta) = \\frac{1}{\\sqrt{\\text{Informazione}(\\theta)}}\n\\]\n\nIl SEM è espresso nella stessa scala di \\(\\theta\\).\nUn valore più basso del SEM implica una stima più precisa di \\(\\theta\\).\nIl SEM non è costante: varia in base al livello di \\(\\theta\\), riflettendo il fatto che il test è più informativo per alcune abilità rispetto ad altre.\n\n\nplot(mirt_rm, type = \"SE\")\n\n\n\n\n\n\n\n\nNel modello Rasch, il SEM dipende dalla distribuzione dei parametri di difficoltà (\\(b\\)) degli item:\n\nIl SEM minimo si verifica intorno ai valori di \\(\\theta\\) che corrispondono ai parametri di difficoltà (\\(b\\)) degli item.\nUn SEM alto si verifica a valori di \\(\\theta\\) lontani dal range dei parametri di difficoltà (\\(b\\)), poiché il test non discrimina bene a quei livelli di abilità.\n\nIn conclusione, il grafico del SEM ci aiuta a identificare i punti di forza e di debolezza del test in termini di precisione della stima di \\(\\theta\\). Il SEM ci permette di capire in quali range di \\(\\theta\\) il test fornisce stime più affidabili.\nLa funzione testInfoPlot() fornisce il grafico del SEM insieme alla curva di informazione del test:\n\ntestInfoPlot(mirt_rm, adj_factor = 2)\n\n\n\n\n\n\n\n\nL’informazione del test è maggiore attorno allo zero e, di conseguenza, gli errori standard aumentano allontanandosi dallo zero.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/06_implementation.html#stima-dei-parametri-delle-persone",
    "href": "chapters/irt/06_implementation.html#stima-dei-parametri-delle-persone",
    "title": "69  Implementazione",
    "section": "69.9 Stima dei Parametri delle Persone",
    "text": "69.9 Stima dei Parametri delle Persone\nLa stima dei parametri delle persone ottenuta con il metodo di massima verosimiglianza si ottiene nel modo seguente:\n\ntheta &lt;- eRm::person.parameter(rm_sum0)\ntheta\n\n\nPerson Parameters:\n\n Raw Score Estimate Std.Error\n         1  -3.3992     1.085\n         2  -2.5218     0.832\n         3  -1.9160     0.737\n         4  -1.4093     0.692\n         5  -0.9452     0.673\n         6  -0.4965     0.668\n         7  -0.0474     0.673\n         8   0.4118     0.683\n         9   0.8878     0.698\n        10   1.3892     0.720\n        11   1.9351     0.762\n        12   2.5766     0.851\n        13   3.4832     1.097\n        14   4.4549        NA\n\n\nDa notare che questa tabella non mostra una stima per ogni persona. La stima dell’abilità di una persona dipende unicamente dal numero di item a cui ha risposto correttamente. Questo significa che dobbiamo calcolare una stima dell’abilità per ogni possibile punteggio totale (indicato come “punteggi grezzi” nella tabella) e possiamo assegnare tale stima a ciascuna persona che ottiene quel punteggio. Ad esempio, stimiamo che l’abilità di una persona che risponde correttamente a dieci item sia circa 1.39.\nVediamo che le stime dell’abilità aumentano con il punteggio grezzo. Questo ha senso, poiché un candidato ha maggiori probabilità di rispondere correttamente a un item se la sua abilità supera la difficoltà di quell’item. Più item vengono risposti correttamente, più è probabile che l’abilità del candidato sia elevata. Inoltre, vediamo che l’errore standard aumenta con la distanza da zero, come era prevedibile dalla mappa persona-item o dalle curve di informazione degli item e del test, dove abbiamo visto che la maggior parte degli item si trova intorno allo zero.\nLa mancanza di un errore standard per i candidati che rispondono correttamente a tutti i 14 item potrebbe lasciarci perplessi. La ragione di tale mancanza è che non esiste una stima di massima verosimiglianza per questo punteggio perfetto. Per gestire questo, la funzione person.parameter() utilizza un metodo chiamato interpolazione spline per produrre una stima dell’abilità, ma la procedura non fornisce stime dell’errore. Lo stesso sarebbe vero per i candidati che risolvono correttamente 0 item, ma in questo campione non si è verificato un punteggio di zero.\nPossiamo ottenere informazioni sulle stime dell’abilità dei singoli candidati utilizzando la funzione summary(), cioè,\n\nsummary(theta)\n\n\nEstimation of Ability Parameters\n\nCollapsed log-likelihood: -76.3 \nNumber of iterations: 10 \nNumber of parameters: 13 \n\nML estimated ability parameters (without spline interpolated values): \n          Estimate Std. Err.   2.5 %  97.5 %\ntheta 1    -0.4965     0.668 -1.8067  0.8137\ntheta 2    -1.4093     0.692 -2.7663 -0.0522\ntheta 3    -1.4093     0.692 -2.7663 -0.0522\ntheta 4    -0.4965     0.668 -1.8067  0.8137\ntheta 5     0.4118     0.683 -0.9270  1.7506\ntheta 6    -0.9452     0.673 -2.2645  0.3741\ntheta 7    -0.9452     0.673 -2.2645  0.3741\ntheta 8    -0.0474     0.673 -1.3664  1.2716\ntheta 9    -1.9160     0.737 -3.3596 -0.4724\ntheta 10   -0.4965     0.668 -1.8067  0.8137\ntheta 11   -0.0474     0.673 -1.3664  1.2716\ntheta 12    0.4118     0.683 -0.9270  1.7506\ntheta 13   -1.9160     0.737 -3.3596 -0.4724\ntheta 14   -0.9452     0.673 -2.2645  0.3741\ntheta 15   -0.4965     0.668 -1.8067  0.8137\ntheta 16   -2.5218     0.832 -4.1521 -0.8915\ntheta 17   -0.0474     0.673 -1.3664  1.2716\ntheta 18   -1.4093     0.692 -2.7663 -0.0522\ntheta 19   -0.0474     0.673 -1.3664  1.2716\ntheta 20   -0.0474     0.673 -1.3664  1.2716\ntheta 21   -1.4093     0.692 -2.7663 -0.0522\ntheta 22   -0.0474     0.673 -1.3664  1.2716\ntheta 23   -1.9160     0.737 -3.3596 -0.4724\ntheta 24   -1.4093     0.692 -2.7663 -0.0522\ntheta 25   -0.9452     0.673 -2.2645  0.3741\ntheta 26   -0.9452     0.673 -2.2645  0.3741\ntheta 27   -0.9452     0.673 -2.2645  0.3741\ntheta 28    0.4118     0.683 -0.9270  1.7506\ntheta 29   -2.5218     0.832 -4.1521 -0.8915\ntheta 30   -0.4965     0.668 -1.8067  0.8137\ntheta 31   -0.9452     0.673 -2.2645  0.3741\ntheta 32   -0.9452     0.673 -2.2645  0.3741\ntheta 33   -0.4965     0.668 -1.8067  0.8137\ntheta 34   -1.4093     0.692 -2.7663 -0.0522\ntheta 35   -0.4965     0.668 -1.8067  0.8137\ntheta 36   -0.0474     0.673 -1.3664  1.2716\ntheta 37   -0.4965     0.668 -1.8067  0.8137\ntheta 38   -0.0474     0.673 -1.3664  1.2716\ntheta 39   -1.9160     0.737 -3.3596 -0.4724\ntheta 40   -1.4093     0.692 -2.7663 -0.0522\ntheta 41   -1.9160     0.737 -3.3596 -0.4724\ntheta 42    0.4118     0.683 -0.9270  1.7506\ntheta 43    0.4118     0.683 -0.9270  1.7506\ntheta 44    0.4118     0.683 -0.9270  1.7506\ntheta 45   -1.9160     0.737 -3.3596 -0.4724\ntheta 46   -0.9452     0.673 -2.2645  0.3741\ntheta 47    1.3892     0.720 -0.0226  2.8010\ntheta 48    1.3892     0.720 -0.0226  2.8010\ntheta 49   -2.5218     0.832 -4.1521 -0.8915\ntheta 50   -0.4965     0.668 -1.8067  0.8137\ntheta 51   -0.0474     0.673 -1.3664  1.2716\ntheta 52   -3.3992     1.085 -5.5254 -1.2730\ntheta 53    1.9351     0.762  0.4415  3.4287\ntheta 54   -0.9452     0.673 -2.2645  0.3741\ntheta 55   -0.9452     0.673 -2.2645  0.3741\ntheta 56   -2.5218     0.832 -4.1521 -0.8915\ntheta 57   -0.0474     0.673 -1.3664  1.2716\ntheta 58    0.8878     0.698 -0.4796  2.2552\ntheta 59   -0.0474     0.673 -1.3664  1.2716\ntheta 60    0.8878     0.698 -0.4796  2.2552\ntheta 61    0.8878     0.698 -0.4796  2.2552\ntheta 62    0.8878     0.698 -0.4796  2.2552\ntheta 63    0.4118     0.683 -0.9270  1.7506\ntheta 64    1.3892     0.720 -0.0226  2.8010\ntheta 65    0.8878     0.698 -0.4796  2.2552\ntheta 66    1.3892     0.720 -0.0226  2.8010\ntheta 67   -0.4965     0.668 -1.8067  0.8137\ntheta 68   -0.0474     0.673 -1.3664  1.2716\ntheta 69   -0.4965     0.668 -1.8067  0.8137\ntheta 70   -0.9452     0.673 -2.2645  0.3741\ntheta 71    0.4118     0.683 -0.9270  1.7506\ntheta 72   -0.9452     0.673 -2.2645  0.3741\ntheta 73    0.4118     0.683 -0.9270  1.7506\ntheta 74   -0.9452     0.673 -2.2645  0.3741\ntheta 75    1.3892     0.720 -0.0226  2.8010\ntheta 76   -0.0474     0.673 -1.3664  1.2716\ntheta 77   -0.9452     0.673 -2.2645  0.3741\ntheta 78    0.8878     0.698 -0.4796  2.2552\ntheta 79   -0.9452     0.673 -2.2645  0.3741\ntheta 80    0.4118     0.683 -0.9270  1.7506\ntheta 81    0.8878     0.698 -0.4796  2.2552\ntheta 82    1.3892     0.720 -0.0226  2.8010\ntheta 83    0.8878     0.698 -0.4796  2.2552\ntheta 84    0.4118     0.683 -0.9270  1.7506\ntheta 85    0.4118     0.683 -0.9270  1.7506\ntheta 86   -0.9452     0.673 -2.2645  0.3741\ntheta 87    1.3892     0.720 -0.0226  2.8010\ntheta 88    0.4118     0.683 -0.9270  1.7506\ntheta 89   -0.0474     0.673 -1.3664  1.2716\ntheta 90    0.8878     0.698 -0.4796  2.2552\ntheta 91   -0.4965     0.668 -1.8067  0.8137\ntheta 92    0.8878     0.698 -0.4796  2.2552\ntheta 93   -0.9452     0.673 -2.2645  0.3741\ntheta 94   -0.4965     0.668 -1.8067  0.8137\ntheta 95   -1.9160     0.737 -3.3596 -0.4724\ntheta 96   -0.4965     0.668 -1.8067  0.8137\ntheta 97   -0.4965     0.668 -1.8067  0.8137\ntheta 98   -0.4965     0.668 -1.8067  0.8137\ntheta 99   -0.0474     0.673 -1.3664  1.2716\ntheta 100  -1.4093     0.692 -2.7663 -0.0522\ntheta 101   0.8878     0.698 -0.4796  2.2552\ntheta 102  -0.0474     0.673 -1.3664  1.2716\ntheta 103   0.4118     0.683 -0.9270  1.7506\ntheta 104   0.4118     0.683 -0.9270  1.7506\ntheta 105  -0.9452     0.673 -2.2645  0.3741\ntheta 106  -0.9452     0.673 -2.2645  0.3741\ntheta 107  -0.4965     0.668 -1.8067  0.8137\ntheta 108   0.4118     0.683 -0.9270  1.7506\ntheta 109   0.4118     0.683 -0.9270  1.7506\ntheta 110  -0.0474     0.673 -1.3664  1.2716\ntheta 111  -0.9452     0.673 -2.2645  0.3741\ntheta 112   0.8878     0.698 -0.4796  2.2552\ntheta 113  -0.4965     0.668 -1.8067  0.8137\ntheta 114  -0.0474     0.673 -1.3664  1.2716\ntheta 115  -0.9452     0.673 -2.2645  0.3741\ntheta 116  -2.5218     0.832 -4.1521 -0.8915\ntheta 117  -1.4093     0.692 -2.7663 -0.0522\ntheta 118  -0.4965     0.668 -1.8067  0.8137\ntheta 119   0.4118     0.683 -0.9270  1.7506\ntheta 120  -1.9160     0.737 -3.3596 -0.4724\ntheta 121  -0.0474     0.673 -1.3664  1.2716\ntheta 122  -0.9452     0.673 -2.2645  0.3741\ntheta 123   1.9351     0.762  0.4415  3.4287\ntheta 124  -0.0474     0.673 -1.3664  1.2716\ntheta 125  -1.9160     0.737 -3.3596 -0.4724\ntheta 126  -1.4093     0.692 -2.7663 -0.0522\ntheta 127  -2.5218     0.832 -4.1521 -0.8915\ntheta 128  -1.9160     0.737 -3.3596 -0.4724\ntheta 129  -1.4093     0.692 -2.7663 -0.0522\ntheta 130  -0.4965     0.668 -1.8067  0.8137\ntheta 131  -0.9452     0.673 -2.2645  0.3741\ntheta 132  -0.4965     0.668 -1.8067  0.8137\ntheta 133  -0.9452     0.673 -2.2645  0.3741\ntheta 134  -1.4093     0.692 -2.7663 -0.0522\ntheta 135  -0.9452     0.673 -2.2645  0.3741\ntheta 136   0.8878     0.698 -0.4796  2.2552\ntheta 137  -0.0474     0.673 -1.3664  1.2716\ntheta 138   0.4118     0.683 -0.9270  1.7506\ntheta 139   1.3892     0.720 -0.0226  2.8010\ntheta 140   0.8878     0.698 -0.4796  2.2552\ntheta 141   0.4118     0.683 -0.9270  1.7506\ntheta 142  -1.4093     0.692 -2.7663 -0.0522\ntheta 143   0.8878     0.698 -0.4796  2.2552\ntheta 144  -0.4965     0.668 -1.8067  0.8137\ntheta 145   1.9351     0.762  0.4415  3.4287\ntheta 146  -1.9160     0.737 -3.3596 -0.4724\ntheta 147  -0.0474     0.673 -1.3664  1.2716\ntheta 148  -0.4965     0.668 -1.8067  0.8137\ntheta 149  -0.0474     0.673 -1.3664  1.2716\ntheta 150  -0.4965     0.668 -1.8067  0.8137\ntheta 151  -0.0474     0.673 -1.3664  1.2716\ntheta 152  -0.9452     0.673 -2.2645  0.3741\ntheta 153  -0.9452     0.673 -2.2645  0.3741\ntheta 154   0.4118     0.683 -0.9270  1.7506\ntheta 155  -0.4965     0.668 -1.8067  0.8137\ntheta 156  -1.4093     0.692 -2.7663 -0.0522\ntheta 157  -0.4965     0.668 -1.8067  0.8137\ntheta 158  -1.4093     0.692 -2.7663 -0.0522\ntheta 159   0.4118     0.683 -0.9270  1.7506\ntheta 160  -0.0474     0.673 -1.3664  1.2716\ntheta 161  -1.4093     0.692 -2.7663 -0.0522\ntheta 162   0.8878     0.698 -0.4796  2.2552\ntheta 163  -1.4093     0.692 -2.7663 -0.0522\ntheta 164  -0.9452     0.673 -2.2645  0.3741\ntheta 165  -1.9160     0.737 -3.3596 -0.4724\ntheta 166  -0.4965     0.668 -1.8067  0.8137\ntheta 167  -3.3992     1.085 -5.5254 -1.2730\ntheta 168  -0.9452     0.673 -2.2645  0.3741\ntheta 169  -3.3992     1.085 -5.5254 -1.2730\ntheta 170  -0.4965     0.668 -1.8067  0.8137\ntheta 171  -0.9452     0.673 -2.2645  0.3741\ntheta 172   1.3892     0.720 -0.0226  2.8010\ntheta 173  -1.9160     0.737 -3.3596 -0.4724\ntheta 174  -1.9160     0.737 -3.3596 -0.4724\ntheta 175   0.4118     0.683 -0.9270  1.7506\ntheta 176  -1.9160     0.737 -3.3596 -0.4724\ntheta 177   1.3892     0.720 -0.0226  2.8010\ntheta 178  -0.9452     0.673 -2.2645  0.3741\ntheta 179  -0.4965     0.668 -1.8067  0.8137\ntheta 180  -1.9160     0.737 -3.3596 -0.4724\ntheta 181  -3.3992     1.085 -5.5254 -1.2730\ntheta 182  -0.9452     0.673 -2.2645  0.3741\ntheta 183  -1.4093     0.692 -2.7663 -0.0522\ntheta 184   0.8878     0.698 -0.4796  2.2552\ntheta 185  -1.4093     0.692 -2.7663 -0.0522\ntheta 186   1.3892     0.720 -0.0226  2.8010\ntheta 187   0.4118     0.683 -0.9270  1.7506\ntheta 188  -1.4093     0.692 -2.7663 -0.0522\ntheta 189  -1.4093     0.692 -2.7663 -0.0522\ntheta 190  -0.4965     0.668 -1.8067  0.8137\ntheta 191  -0.4965     0.668 -1.8067  0.8137\ntheta 192  -1.4093     0.692 -2.7663 -0.0522\ntheta 193  -0.4965     0.668 -1.8067  0.8137\ntheta 194  -0.0474     0.673 -1.3664  1.2716\ntheta 195   0.4118     0.683 -0.9270  1.7506\ntheta 196  -0.0474     0.673 -1.3664  1.2716\ntheta 197  -0.9452     0.673 -2.2645  0.3741\ntheta 198   1.9351     0.762  0.4415  3.4287\ntheta 199   1.3892     0.720 -0.0226  2.8010\ntheta 200   0.4118     0.683 -0.9270  1.7506\ntheta 201   0.4118     0.683 -0.9270  1.7506\ntheta 202  -0.9452     0.673 -2.2645  0.3741\ntheta 203  -0.4965     0.668 -1.8067  0.8137\ntheta 204  -0.9452     0.673 -2.2645  0.3741\ntheta 205   0.8878     0.698 -0.4796  2.2552\ntheta 206   1.9351     0.762  0.4415  3.4287\ntheta 207   1.9351     0.762  0.4415  3.4287\ntheta 208   0.8878     0.698 -0.4796  2.2552\ntheta 209   0.4118     0.683 -0.9270  1.7506\ntheta 210   1.3892     0.720 -0.0226  2.8010\ntheta 211  -1.4093     0.692 -2.7663 -0.0522\ntheta 212  -0.0474     0.673 -1.3664  1.2716\ntheta 213  -0.0474     0.673 -1.3664  1.2716\ntheta 214   0.8878     0.698 -0.4796  2.2552\ntheta 215   1.9351     0.762  0.4415  3.4287\ntheta 216   0.4118     0.683 -0.9270  1.7506\ntheta 217   2.5766     0.851  0.9081  4.2451\ntheta 218   0.4118     0.683 -0.9270  1.7506\ntheta 219   1.9351     0.762  0.4415  3.4287\ntheta 220  -0.0474     0.673 -1.3664  1.2716\ntheta 221  -1.4093     0.692 -2.7663 -0.0522\ntheta 222  -0.4965     0.668 -1.8067  0.8137\ntheta 223  -1.9160     0.737 -3.3596 -0.4724\ntheta 224   1.3892     0.720 -0.0226  2.8010\ntheta 225   0.8878     0.698 -0.4796  2.2552\ntheta 226  -0.0474     0.673 -1.3664  1.2716\ntheta 227   0.8878     0.698 -0.4796  2.2552\ntheta 228   1.3892     0.720 -0.0226  2.8010\ntheta 229  -0.0474     0.673 -1.3664  1.2716\ntheta 230   0.4118     0.683 -0.9270  1.7506\ntheta 231   0.4118     0.683 -0.9270  1.7506\ntheta 232  -0.0474     0.673 -1.3664  1.2716\ntheta 233  -0.9452     0.673 -2.2645  0.3741\ntheta 234   0.8878     0.698 -0.4796  2.2552\ntheta 235  -1.9160     0.737 -3.3596 -0.4724\ntheta 236   0.4118     0.683 -0.9270  1.7506\ntheta 237   0.4118     0.683 -0.9270  1.7506\ntheta 238   3.4832     1.097  1.3336  5.6329\ntheta 239  -3.3992     1.085 -5.5254 -1.2730\ntheta 240  -0.9452     0.673 -2.2645  0.3741\ntheta 241   0.4118     0.683 -0.9270  1.7506\ntheta 242  -0.0474     0.673 -1.3664  1.2716\ntheta 243  -0.4965     0.668 -1.8067  0.8137\ntheta 244   0.4118     0.683 -0.9270  1.7506\ntheta 245  -1.9160     0.737 -3.3596 -0.4724\ntheta 246   1.3892     0.720 -0.0226  2.8010\ntheta 247  -0.9452     0.673 -2.2645  0.3741\ntheta 248  -0.0474     0.673 -1.3664  1.2716\ntheta 249   0.4118     0.683 -0.9270  1.7506\ntheta 250  -1.9160     0.737 -3.3596 -0.4724\ntheta 251  -2.5218     0.832 -4.1521 -0.8915\ntheta 252   0.4118     0.683 -0.9270  1.7506\ntheta 253  -0.9452     0.673 -2.2645  0.3741\ntheta 254  -0.0474     0.673 -1.3664  1.2716\ntheta 255  -0.0474     0.673 -1.3664  1.2716\ntheta 256   0.4118     0.683 -0.9270  1.7506\ntheta 257   0.8878     0.698 -0.4796  2.2552\ntheta 258  -0.4965     0.668 -1.8067  0.8137\ntheta 259  -2.5218     0.832 -4.1521 -0.8915\ntheta 260  -0.0474     0.673 -1.3664  1.2716\ntheta 261   0.4118     0.683 -0.9270  1.7506\ntheta 262   0.4118     0.683 -0.9270  1.7506\ntheta 263   3.4832     1.097  1.3336  5.6329\ntheta 264   0.8878     0.698 -0.4796  2.2552\ntheta 265   1.3892     0.720 -0.0226  2.8010\ntheta 266  -1.4093     0.692 -2.7663 -0.0522\ntheta 267   1.3892     0.720 -0.0226  2.8010\ntheta 268  -0.9452     0.673 -2.2645  0.3741\ntheta 269  -0.9452     0.673 -2.2645  0.3741\ntheta 270  -1.4093     0.692 -2.7663 -0.0522\ntheta 271   1.3892     0.720 -0.0226  2.8010\ntheta 272   0.8878     0.698 -0.4796  2.2552\ntheta 273  -0.4965     0.668 -1.8067  0.8137\ntheta 274  -1.9160     0.737 -3.3596 -0.4724\ntheta 275   0.4118     0.683 -0.9270  1.7506\ntheta 276  -0.4965     0.668 -1.8067  0.8137\ntheta 277  -0.0474     0.673 -1.3664  1.2716\ntheta 278   1.9351     0.762  0.4415  3.4287\ntheta 279   0.8878     0.698 -0.4796  2.2552\ntheta 280   1.3892     0.720 -0.0226  2.8010\ntheta 281   1.3892     0.720 -0.0226  2.8010\ntheta 282   0.4118     0.683 -0.9270  1.7506\ntheta 283  -1.4093     0.692 -2.7663 -0.0522\ntheta 284  -0.0474     0.673 -1.3664  1.2716\ntheta 285  -1.9160     0.737 -3.3596 -0.4724\ntheta 286   1.9351     0.762  0.4415  3.4287\ntheta 287   0.8878     0.698 -0.4796  2.2552\ntheta 288   1.3892     0.720 -0.0226  2.8010\ntheta 289   0.4118     0.683 -0.9270  1.7506\ntheta 290   0.4118     0.683 -0.9270  1.7506\ntheta 291  -0.0474     0.673 -1.3664  1.2716\ntheta 292   0.8878     0.698 -0.4796  2.2552\ntheta 293  -0.0474     0.673 -1.3664  1.2716\ntheta 294  -0.9452     0.673 -2.2645  0.3741\ntheta 296   1.3892     0.720 -0.0226  2.8010\ntheta 297   1.3892     0.720 -0.0226  2.8010\ntheta 298   3.4832     1.097  1.3336  5.6329\ntheta 299   0.8878     0.698 -0.4796  2.2552\ntheta 300   0.4118     0.683 -0.9270  1.7506\ntheta 301  -0.9452     0.673 -2.2645  0.3741\ntheta 302  -0.4965     0.668 -1.8067  0.8137\ntheta 303   0.4118     0.683 -0.9270  1.7506\ntheta 304  -0.4965     0.668 -1.8067  0.8137\ntheta 305  -0.0474     0.673 -1.3664  1.2716\ntheta 306   0.4118     0.683 -0.9270  1.7506\ntheta 308   1.3892     0.720 -0.0226  2.8010\ntheta 309   1.3892     0.720 -0.0226  2.8010\ntheta 310  -0.0474     0.673 -1.3664  1.2716\ntheta 311  -1.4093     0.692 -2.7663 -0.0522\ntheta 312   0.8878     0.698 -0.4796  2.2552\ntheta 313   1.3892     0.720 -0.0226  2.8010\ntheta 314   0.8878     0.698 -0.4796  2.2552\ntheta 315   0.4118     0.683 -0.9270  1.7506\ntheta 316  -0.0474     0.673 -1.3664  1.2716\ntheta 317  -0.4965     0.668 -1.8067  0.8137\ntheta 318   0.4118     0.683 -0.9270  1.7506\ntheta 319   2.5766     0.851  0.9081  4.2451\ntheta 320  -0.0474     0.673 -1.3664  1.2716\ntheta 321  -0.0474     0.673 -1.3664  1.2716\ntheta 322   0.8878     0.698 -0.4796  2.2552\ntheta 323   0.8878     0.698 -0.4796  2.2552\ntheta 324   2.5766     0.851  0.9081  4.2451\ntheta 325   1.3892     0.720 -0.0226  2.8010\ntheta 326   0.4118     0.683 -0.9270  1.7506\ntheta 327   0.8878     0.698 -0.4796  2.2552\ntheta 328   0.4118     0.683 -0.9270  1.7506\ntheta 329   0.4118     0.683 -0.9270  1.7506\ntheta 330  -0.0474     0.673 -1.3664  1.2716\ntheta 331   0.8878     0.698 -0.4796  2.2552\ntheta 332  -0.4965     0.668 -1.8067  0.8137\ntheta 333   0.8878     0.698 -0.4796  2.2552\ntheta 334  -0.4965     0.668 -1.8067  0.8137\ntheta 335   0.4118     0.683 -0.9270  1.7506\ntheta 336  -0.0474     0.673 -1.3664  1.2716\ntheta 337   1.3892     0.720 -0.0226  2.8010\ntheta 338  -1.4093     0.692 -2.7663 -0.0522\ntheta 339   1.3892     0.720 -0.0226  2.8010\ntheta 340   0.8878     0.698 -0.4796  2.2552\ntheta 341   0.8878     0.698 -0.4796  2.2552\ntheta 342   1.9351     0.762  0.4415  3.4287\ntheta 343   0.8878     0.698 -0.4796  2.2552\ntheta 344   0.4118     0.683 -0.9270  1.7506\ntheta 345   1.3892     0.720 -0.0226  2.8010\ntheta 346  -0.0474     0.673 -1.3664  1.2716\ntheta 347   1.9351     0.762  0.4415  3.4287\ntheta 348   0.4118     0.683 -0.9270  1.7506\ntheta 349   1.3892     0.720 -0.0226  2.8010\ntheta 350   0.4118     0.683 -0.9270  1.7506\ntheta 351   0.8878     0.698 -0.4796  2.2552\ntheta 352  -0.0474     0.673 -1.3664  1.2716\ntheta 353  -0.0474     0.673 -1.3664  1.2716\ntheta 354  -0.4965     0.668 -1.8067  0.8137\ntheta 355   0.4118     0.683 -0.9270  1.7506\ntheta 356  -0.4965     0.668 -1.8067  0.8137\ntheta 357  -0.0474     0.673 -1.3664  1.2716\ntheta 358  -0.0474     0.673 -1.3664  1.2716\ntheta 359  -0.9452     0.673 -2.2645  0.3741\ntheta 360  -0.9452     0.673 -2.2645  0.3741\ntheta 361  -0.4965     0.668 -1.8067  0.8137\ntheta 362   0.4118     0.683 -0.9270  1.7506\ntheta 363  -0.9452     0.673 -2.2645  0.3741\ntheta 364  -0.9452     0.673 -2.2645  0.3741\ntheta 365  -0.0474     0.673 -1.3664  1.2716\ntheta 366   0.4118     0.683 -0.9270  1.7506\ntheta 367  -3.3992     1.085 -5.5254 -1.2730\ntheta 368  -3.3992     1.085 -5.5254 -1.2730\ntheta 369  -2.5218     0.832 -4.1521 -0.8915\ntheta 370  -1.9160     0.737 -3.3596 -0.4724\ntheta 371   0.8878     0.698 -0.4796  2.2552\ntheta 372  -0.9452     0.673 -2.2645  0.3741\ntheta 373  -0.4965     0.668 -1.8067  0.8137\ntheta 374  -1.9160     0.737 -3.3596 -0.4724\ntheta 375  -0.4965     0.668 -1.8067  0.8137\ntheta 376  -1.4093     0.692 -2.7663 -0.0522\ntheta 377  -1.4093     0.692 -2.7663 -0.0522\ntheta 378  -2.5218     0.832 -4.1521 -0.8915\ntheta 379  -0.9452     0.673 -2.2645  0.3741\ntheta 380  -1.9160     0.737 -3.3596 -0.4724\ntheta 381  -1.4093     0.692 -2.7663 -0.0522\ntheta 382  -0.4965     0.668 -1.8067  0.8137\ntheta 383  -0.0474     0.673 -1.3664  1.2716\ntheta 384  -1.9160     0.737 -3.3596 -0.4724\ntheta 385  -1.4093     0.692 -2.7663 -0.0522\ntheta 386  -0.0474     0.673 -1.3664  1.2716\ntheta 387   0.4118     0.683 -0.9270  1.7506\ntheta 388  -0.0474     0.673 -1.3664  1.2716\ntheta 389   0.8878     0.698 -0.4796  2.2552\ntheta 390  -0.0474     0.673 -1.3664  1.2716\ntheta 391  -1.4093     0.692 -2.7663 -0.0522\ntheta 392   0.4118     0.683 -0.9270  1.7506\ntheta 393  -0.0474     0.673 -1.3664  1.2716\ntheta 394   0.4118     0.683 -0.9270  1.7506\ntheta 395  -0.0474     0.673 -1.3664  1.2716\ntheta 396  -1.4093     0.692 -2.7663 -0.0522\ntheta 397  -0.4965     0.668 -1.8067  0.8137\ntheta 398  -0.0474     0.673 -1.3664  1.2716\ntheta 399  -0.9452     0.673 -2.2645  0.3741\ntheta 400  -1.9160     0.737 -3.3596 -0.4724\n\n\nL’output – che qui abbiamo di nuovo troncato per risparmiare spazio – contiene stime ed errori standard, insieme ai limiti inferiori (2.5%) e superiori (97.5%) degli intervalli di confidenza al 95%, per tutti i candidati che hanno risposto correttamente fino a 13 item. I candidati che hanno risposto correttamente a tutti i 14 item (o a nessun item) sono omessi da questo output. Possiamo anche notare che alcuni candidati, ad esempio il secondo e il terzo, ricevono esattamente la stessa stima di abilità ed errore standard. Questo è dovuto al fatto che hanno lavorato sullo stesso set di item e hanno ottenuto lo stesso punteggio totale. In alternativa, possiamo utilizzare il comando coef(theta) per ottenere solo la stima dell’abilità per ciascun candidato.\nPossiamo anche stimare l’abilità dei candidati utilizzando la funzione mirt::fscores(). Per i modelli unidimensionali, gli argomenti più importanti di fscores() sono object e method. L’argomento object accetta il risultato della funzione mirt(). L’argomento method indica quale metodo utilizzare per stimare i parametri della persona. Per impostazione predefinita, method=\"EAP\", il che indica che il parametro della persona dovrebbe essere stimato utilizzando il metodo expected a posteriori (EAP). Possiamo calcolare le stime EAP per il modello di Rasch e stampare le sue prime sei voci inserendo:\n\ntheta_eap &lt;- fscores(mirt_rm)\nhead(theta_eap)\n\n\nA matrix: 6 x 1 of type dbl\n\n\nF1\n\n\n\n\n-0.218\n\n\n-0.828\n\n\n-0.828\n\n\n-0.218\n\n\n0.391\n\n\n-0.521\n\n\n\n\n\nPer impostazione predefinita mirt mostra solo le stime puntuali, ma è possibile aggiungere gli errori standard tramite l’opzione full.scores.SE = TRUE alla funzione fscores(). Gli errori standard dovrebbero essere esaminati prima di interpretare o riportare le stime dei parametri della persona.\nLa funzione fscores() fornisce anche stimatori di massima verosimiglianza (ML), massimo a posteriori (MAP) e likelihood ponderata (WLE). Ora confrontiamo i quattro tipi di stime dei parametri della persona fornite da mirt. Gli stimatori ML, MAP e WLE possono essere calcolati inserendo\n\ntheta_ml &lt;- fscores(mirt_rm, method = \"ML\", max_theta = 30)\ntheta_map &lt;- fscores(mirt_rm, method = \"MAP\")\ntheta_wle &lt;- fscores(mirt_rm, method = \"WLE\")\n\n\nests &lt;- cbind(theta_eap, theta_ml, theta_map, theta_wle)\ncolnames(ests) &lt;- c(\"EAP\", \"ML\", \"MAP\", \"WLE\")\npairs(ests, xlim = c(-3, 3), ylim = c(-3, 3))",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/06_implementation.html#affidabilità-condizionale",
    "href": "chapters/irt/06_implementation.html#affidabilità-condizionale",
    "title": "69  Implementazione",
    "section": "69.10 Affidabilità Condizionale",
    "text": "69.10 Affidabilità Condizionale\nIl concetto di affidabilità varia tra la CTT e la IRT. Nell’IRT, possiamo calcolare l’affidabilità condizionale, ossia l’affidabilità della scala a diversi livelli di theta.\n\nNella CTT, l’affidabilità è solitamente considerata come una proprietà fissa del test, indipendentemente dal livello di abilità dei rispondenti. Si misura spesso attraverso il coefficiente alfa di Cronbach o metodi simili.\nNell’IRT, invece, l’affidabilità è vista come una proprietà variabile che dipende dal livello di theta del rispondente. A diversi livelli di theta, la precisione con cui il test misura l’abilità può variare significativamente.\n\nL’affidabilità condizionale fornisce una misura più specifica e dettagliata di quanto affidabilmente un test misura l’abilità a diversi livelli di \\(\\theta\\).\n\nconRelPlot(mirt_rm)\n\n\n\n\n\n\n\n\n\nplot(mirt_rm, type = \"rxx\")\n\n\n\n\n\n\n\n\nNel caso presente, - a livelli medi di \\(\\theta\\): Il test mostra una buona affidabilità, indicando che è in grado di distinguere con precisione tra rispondenti con abilità medie. - agli estremi di \\(\\theta\\): Il test mostra un’affidabilità più bassa, suggerendo che non è altrettanto efficace nel distinguere tra livelli di abilità molto alti o molto bassi.\nIn sostanza, l’affidabilità condizionale nell’IRT ci fornisce una comprensione più dettagliata di dove il test funziona bene e dove potrebbe richiedere miglioramenti per valutare con precisione l’abilità su tutta la gamma di theta.\nÈ comunque possibile calcolare un singolo valore di attendibilità:\n\nmarginal_rxx(mirt_rm)\n\n0.698151005130639\n\n\n\nIl valore riportato (\\(r_{xx} = 0.698\\)) indica che circa il 70% della varianza osservata nei punteggi stimati è attribuibile al punteggio vero (\\(\\theta\\)), mentre il restante 30% è dovuto all’errore di misura. Questo valore suggerisce che il test stima l’abilità latente \\(\\theta\\) con una precisione accettabile.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/06_implementation.html#curva-caratteristica-del-test",
    "href": "chapters/irt/06_implementation.html#curva-caratteristica-del-test",
    "title": "69  Implementazione",
    "section": "69.11 Curva Caratteristica del Test",
    "text": "69.11 Curva Caratteristica del Test\nUna proprietà aggiuntiva di un modello IRT è che il punteggio complessivo delle risposte corrette (la somma dei punteggi per le risposte corrette) risulta essere una stima efficace del tratto latente sottostante. Un grafico della cosiddetta curva caratteristica della scala (scale characteristic curve) permette di valutare visivamente questo aspetto tracciando la relazione tra theta e il punteggio di risposte corrette.\n\nQuesto tipo di grafico mostra come il punteggio totale delle risposte corrette si correla con il livello di abilità latente (theta) stimato dal modello IRT.\nAd esempio, se la curva mostra che punteggi più alti di risposte corrette corrispondono sistematicamente a livelli più alti di theta e viceversa, ciò indica che il punteggio totale è un buon indicatore del tratto latente.\nAl contrario, se la curva non mostra una relazione chiara o lineare tra punteggio totale e theta, ciò potrebbe suggerire che il punteggio totale non cattura completamente la complessità o le sfumature del tratto latente.\n\nIn sintesi, la curva caratteristica della scala fornisce una rappresentazione visiva di come il punteggio totale di risposte corrette rifletta l’abilità latente misurata dal test, offrendo una visione utile per valutare l’efficacia del punteggio totale come indicatore del tratto latente in questione.\n\nscaleCharPlot(mirt_rm)\n\n\n\n\n\n\n\n\n\nplot(mirt_rm, type = \"score\")\n\n\n\n\n\n\n\n\nQuesta curva di solito assume la forma di una S, poiché la relazione è più forte nel range medio di theta e meno precisa agli estremi (come già visto nella curva di informazione del test).\nPossiamo ovviamente testare anche questo con una semplice correlazione. Per prima cosa, estraiamo il punteggio latente IRT utilizzando la funzione fscores(). Quindi lo correliamo con il punteggio di risposte corrette.\n\nscore &lt;- fscores(mirt_rm)\nsumscore &lt;- rowSums(responses)\ncor.test(score, sumscore)\n\n\n    Pearson's product-moment correlation\n\ndata:  score and sumscore\nt = 1097, df = 398, p-value &lt;2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 1 1\nsample estimates:\ncor \n  1 \n\n\nNel caso presente, la correlazione è quasi perfetta.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/06_implementation.html#considerazioni-conclusive",
    "href": "chapters/irt/06_implementation.html#considerazioni-conclusive",
    "title": "69  Implementazione",
    "section": "69.12 Considerazioni Conclusive",
    "text": "69.12 Considerazioni Conclusive\nTradizionalmente, il punteggio totale ottenuto in un test psicologico è stato considerato come la misura più efficace dell’abilità o della predisposizione di una persona rispetto a un certo tratto di personalità. Tuttavia, la dipendenza del punteggio totale dalla difficoltà degli item presenta limitazioni significative. Ad esempio, due persone possono ottenere lo stesso punteggio totale rispondendo in modo diverso a item di varia difficoltà, il che non riflette accuratamente le loro abilità reali.\nNella Teoria Classica dei Test (CTT), l’enfasi è posta sul punteggio totale, ma questa prospettiva ignora le variazioni nella difficoltà degli item e assume che gli errori di misurazione si annullino reciprocamente attraverso la procedura di sommazione. Tuttavia, la CTT è limitata dalla sua assunzione di varianze di errore uniformi per tutti i rispondenti, dall’aspettativa di errori di misurazione nulli e dalla focalizzazione esclusiva sui punteggi totali, senza considerare l’adattamento di item e persone.\nAl contrario, la Teoria della Risposta all’Item (IRT) cambia il focus dai punteggi totali alle risposte a ciascun item, sfruttando le caratteristiche degli item. L’IRT descrive come attributi come abilità, atteggiamento o personalità, insieme alle caratteristiche degli item, influenzino la probabilità di fornire una risposta. Il Modello di Rasch, una forma semplice di IRT per risposte binarie, stabilisce una relazione diretta tra la probabilità di una risposta corretta e il livello di abilità del rispondente.\nLa stima dell’abilità in IRT non dipende dagli specifici item somministrati, permettendo di confrontare i risultati tra gruppi diversi con lo stesso set di item. Inoltre, la qualità degli item è valutata indipendentemente dal campione di rispondenti, rendendo le proprietà degli item costanti tra diversi gruppi con varie abilità.\nL’IRT supera i limiti della CTT stimando congiuntamente le proprietà degli item e il livello di abilità dei rispondenti. Le caratteristiche degli item diventano indipendenti dal campione di individui utilizzato per costruire il test, permettendo la creazione di insiemi di item equivalenti per misurare abilità latenti. Questo approccio offre maggiore precisione e affidabilità nelle misurazioni, assicurando la comparabilità tra diversi gruppi di individui. In conclusione, l’IRT rappresenta un metodo statistico avanzato e versatile per una valutazione più accurata e affidabile di tratti e abilità in contesti psicometrici.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/irt/06_implementation.html#session-info",
    "href": "chapters/irt/06_implementation.html#session-info",
    "title": "69  Implementazione",
    "section": "69.13 Session Info",
    "text": "69.13 Session Info\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] grid      stats4    stats     graphics  grDevices utils     datasets \n[8] methods   base     \n\nother attached packages:\n [1] psychotools_0.7-4 ggmirt_0.1.0      TAM_4.2-21        CDM_8.2-6        \n [5] mvtnorm_1.3-2     mirt_1.43         lattice_0.22-6    eRm_1.0-6        \n [9] MASS_7.3-61       viridis_0.6.5     viridisLite_0.4.2 ggpubr_0.6.0     \n[13] ggExtra_0.10.1    gridExtra_2.3     patchwork_1.3.0   bayesplot_1.11.1 \n[17] semTools_0.5-6    semPlot_1.1.6     lavaan_0.6-19     psych_2.4.6.26   \n[21] scales_1.3.0      markdown_1.13     knitr_1.49        lubridate_1.9.3  \n[25] forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2      \n[29] readr_2.1.5       tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1    \n[33] tidyverse_2.0.0   here_1.0.1       \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.2        later_1.3.2          pbdZMQ_0.3-13       \n  [4] R.oo_1.27.0          XML_3.99-0.17        rpart_4.1.23        \n  [7] lifecycle_1.0.4      rstatix_0.7.2        rprojroot_2.0.4     \n [10] globals_0.16.3       rockchalk_1.8.157    backports_1.5.0     \n [13] magrittr_2.0.3       openxlsx_4.2.7.1     Hmisc_5.2-0         \n [16] rmarkdown_2.29       httpuv_1.6.15        qgraph_1.9.8        \n [19] zip_2.3.1            sessioninfo_1.2.2    cowplot_1.1.3       \n [22] RColorBrewer_1.1-3   pbapply_1.7-2        minqa_1.2.8         \n [25] audio_0.1-11         multcomp_1.4-26      abind_1.4-8         \n [28] quadprog_1.5-8       R.utils_2.12.3       nnet_7.3-19         \n [31] TH.data_1.1-2        sandwich_3.1-1       listenv_0.9.1       \n [34] testthat_3.2.1.1     openintro_2.5.0      RPushbullet_0.3.4   \n [37] vegan_2.6-8          arm_1.14-4           airports_0.1.0      \n [40] parallelly_1.39.0    permute_0.9-7        codetools_0.2-20    \n [43] tidyselect_1.2.1     farver_2.1.2         lme4_1.1-35.5       \n [46] base64enc_0.1-3      jsonlite_1.8.9       polycor_0.8-1       \n [49] progressr_0.15.0     Formula_1.2-5        survival_3.7-0      \n [52] emmeans_1.10.5       tools_4.4.2          snow_0.4-4          \n [55] Rcpp_1.0.13-1        glue_1.8.0           mnormt_2.1.1        \n [58] admisc_0.36          xfun_0.49            mgcv_1.9-1          \n [61] IRdisplay_1.1        withr_3.0.2          beepr_2.0           \n [64] fastmap_1.2.0        latticeExtra_0.6-30  boot_1.3-31         \n [67] fansi_1.0.6          digest_0.6.37        mi_1.1              \n [70] timechange_0.3.0     R6_2.5.1             mime_0.12           \n [73] estimability_1.5.1   colorspace_2.1-1     Cairo_1.6-2         \n [76] gtools_3.9.5         jpeg_0.1-10          R.methodsS3_1.8.2   \n [79] utf8_1.2.4           generics_0.1.3       data.table_1.16.2   \n [82] corpcor_1.6.10       usdata_0.3.1         SimDesign_2.17.1    \n [85] htmlwidgets_1.6.4    pkgconfig_2.0.3      sem_3.1-16          \n [88] gtable_0.3.6         brio_1.1.5           htmltools_0.5.8.1   \n [91] carData_3.0-5        png_0.1-8            rstudioapi_0.17.1   \n [94] tzdb_0.4.0           reshape2_1.4.4       uuid_1.2-1          \n [97] coda_0.19-4.1        checkmate_2.3.2      nlme_3.1-166        \n[100] curl_6.0.1           nloptr_2.1.1         repr_1.1.7          \n[103] zoo_1.8-12           parallel_4.4.2       miniUI_0.1.1.1      \n[106] foreign_0.8-87       pillar_1.9.0         vctrs_0.6.5         \n[109] promises_1.3.0       car_3.1-3            OpenMx_2.21.13      \n[112] xtable_1.8-4         Deriv_4.1.6          cluster_2.1.6       \n[115] dcurver_0.9.2        GPArotation_2024.3-1 htmlTable_2.4.3     \n[118] evaluate_1.0.1       pbivnorm_0.6.0       cli_3.6.3           \n[121] kutils_1.73          compiler_4.4.2       rlang_1.1.4         \n[124] crayon_1.5.3         future.apply_1.11.3  ggsignif_0.6.4      \n[127] labeling_0.4.3       fdrtool_1.2.18       interp_1.1-6        \n[130] plyr_1.8.9           stringi_1.8.4        deldir_2.0-4        \n[133] munsell_0.5.1        lisrelToR_0.3        pacman_0.5.1        \n[136] Matrix_1.7-1         IRkernel_1.3.2       hms_1.1.3           \n[139] glasso_1.11          future_1.34.0        shiny_1.9.1         \n[142] igraph_2.1.1         broom_1.0.7          RcppParallel_5.1.9  \n[145] cherryblossom_0.1.0 \n\n\n\n\n\n\nDebelak, R., Strobl, C., & Zeigenfuse, M. D. (2022). An introduction to the rasch model with examples in r. CRC Press.",
    "crumbs": [
      "IRT",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>Implementazione</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/01_lgm_intro.html",
    "href": "chapters/lgm/01_lgm_intro.html",
    "title": "70  Curve di crescita latente",
    "section": "",
    "text": "70.1 Introduzione\nQuando si vuole studiare come le persone cambiano nel tempo, è necessario raccogliere dati longitudinali, ovvero misurazioni ripetute sullo stesso gruppo di individui. A differenza delle analisi tradizionali che confrontano le persone in un unico momento, i dati longitudinali permettono di tracciare le traiettorie individuali di cambiamento.\nLe caratteristiche distintive di questi dati, come la presenza di variazione sia tra gli individui sia all’interno degli stessi e la dipendenza tra le osservazioni ripetute, richiedono l’utilizzo di modelli statistici specifici.\nI modelli a curve di crescita latente (LGCM) sono stati sviluppati proprio per affrontare queste complessità. Questi modelli consentono di modellare la crescita e il cambiamento nel tempo, tenendo conto sia delle differenze individuali nelle traiettorie di sviluppo sia degli effetti di variabili esterne. Ad esempio, in uno studio sullo sviluppo cognitivo, un LGCM può essere utilizzato per analizzare come il quoziente intellettivo cambia dall’infanzia all’età adulta, tenendo conto di fattori come l’ambiente familiare e l’istruzione.\nUno dei principali vantaggi degli LGCM è la loro flessibilità. Possono essere applicati a una vasta gamma di dati e possono essere utilizzati per rispondere a diverse domande di ricerca. Inoltre, gli LGCM permettono di identificare i periodi di vita in cui il cambiamento è più rapido o più lento, e di valutare l’impatto di interventi specifici sulle traiettorie di sviluppo.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/01_lgm_intro.html#concettualizzazioni-del-tempo-in-studi-longitudinali",
    "href": "chapters/lgm/01_lgm_intro.html#concettualizzazioni-del-tempo-in-studi-longitudinali",
    "title": "70  Curve di crescita latente",
    "section": "70.2 Concettualizzazioni del Tempo in Studi Longitudinali",
    "text": "70.2 Concettualizzazioni del Tempo in Studi Longitudinali\nNegli studi longitudinali, ci sono principalmente cinque modalità di concettualizzare e analizzare il trascorrere del tempo:\n\nDisegno Trasversale (Cross-Sectional Design):\n\nQuesto metodo studia gruppi di persone di età diverse, ma in un unico momento temporale, senza misurazioni ripetute sugli stessi individui.\nAd esempio, si potrebbe confrontare le capacità cognitive di bambini di 6, 8 e 10 anni in un unico momento.\nQuesto approccio permette di ottenere dati preliminari e valutare le relazioni tra variabili, ma non può descrivere i processi evolutivi nel tempo, poiché le differenze tra gruppi di età potrebbero dipendere sia da fattori di sviluppo che da differenze tra coorti.\n\nDisegno Longitudinale di Singola Coorte:\n\nPrevede misurazioni ripetute sugli stessi individui nel tempo.\nAd esempio, si potrebbero valutare le stesse persone a 6, 8 e 10 anni per studiare il loro sviluppo cognitivo.\nQuesto permette di analizzare i cambiamenti intra-individuali nel tempo utilizzando modelli di panel, modelli di curva di crescita o altri modelli di cambiamento.\nI modelli di panel esaminano variazioni in sequenze di misurazioni, mentre i modelli di curva di crescita analizzano la variabilità nel cambiamento individuale.\n\nDisegno Cross-Sequenziale:\n\nCombina un disegno trasversale iniziale con una successione di misurazioni longitudinali.\nAd esempio, si potrebbero valutare gruppi di bambini di 6, 8 e 10 anni, e poi seguirli nel tempo con misurazioni successive.\nQuesto approccio permette di studiare sia gli effetti legati all’età che le differenze tra coorti, anche se può essere più complesso separare questi fattori.\n\nDisegno Sequenziale di Coorte:\n\nAvvia uno studio longitudinale con gruppi (coorti) di partecipanti della stessa età.\nOgni nuova coorte attraversa la stessa fascia di età nel tempo.\nAd esempio, si potrebbero valutare gruppi di bambini di 6 anni, 8 anni e 10 anni, seguendoli negli anni successivi.\nQuesto design aiuta a distinguere gli effetti legati all’età da quelli dovuti alle differenze tra coorti.\n\nDisegno Sequenziale Temporale:\n\nMeno comune, ma utile per separare gli effetti legati all’età da quelli legati al tempo di misurazione.\nMantiene invariata la fascia di età dei partecipanti, ma valuta nuove e vecchie coorti in diversi momenti temporali.\nAd esempio, si potrebbero valutare gruppi di bambini di 6-8 anni in diversi anni, per distinguere i cambiamenti legati all’età da quelli legati al passare del tempo.\nQuesto disegno però non permette di separare gli effetti di coorte dall’interazione tra età e tempo di misurazione.\n\n\nIn sintesi, ciascuno di questi approcci offre vantaggi e svantaggi nel comprendere l’impatto dell’età, delle coorti e del tempo di misurazione sui fenomeni di interesse negli studi longitudinali. La scelta del design dipende dagli obiettivi specifici della ricerca.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/01_lgm_intro.html#validità",
    "href": "chapters/lgm/01_lgm_intro.html#validità",
    "title": "70  Curve di crescita latente",
    "section": "70.3 Validità",
    "text": "70.3 Validità\nRischi per la validità includono la regressione verso la media, gli effetti del retest, effetti di selezione, attrito selettivo e effetti di strumentazione.\n\nLa regressione verso la media indica che i punteggi estremi tendono a spostarsi verso la media nelle misurazioni successive. È un fenomeno di inaffidabilità nelle misure ripetute e può essere mitigato utilizzando modelli SEM a variabili latenti.\nGli effetti del retest emergono quando una misura è sensibile all’esposizione ripetuta. Questi effetti possono essere stimati e corretti assegnando casualmente ai partecipanti la ricezione o meno di una misurazione, o utilizzando protocolli di mancata risposta pianificata.\nGli effetti di selezione si verificano quando il piano di campionamento non fornisce un campione rappresentativo della popolazione di interesse. L’attrito selettivo si riferisce alla perdita di partecipanti correlata a specifiche caratteristiche del campione.\nGli effetti di strumentazione possono alterare le proprietà di misurazione del fenomeno studiato. Misure sensibili al cambiamento sono cruciali per rilevare i processi di cambiamento.\n\nIn sintesi, gli studi longitudinali affrontano diverse sfide di validità, che richiedono metodi sofisticati per la misurazione e l’analisi dei dati. È fondamentale considerare come varie forze, come la regressione verso la media e gli effetti di retest, possano influenzare i risultati, e come strumenti di misurazione adeguati possano catturare in modo efficace i cambiamenti nel tempo.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/01_lgm_intro.html#dati-mancanti",
    "href": "chapters/lgm/01_lgm_intro.html#dati-mancanti",
    "title": "70  Curve di crescita latente",
    "section": "70.4 Dati Mancanti",
    "text": "70.4 Dati Mancanti\nIl problema dei dati mancanti nei disegni longitudinali rappresenta una sfida cruciale nella ricerca, specialmente nei campi della psicologia e delle scienze sociali. I dati mancanti portano a due conseguenze principali: la perdita di potenza statistica e l’introduzione di bias.\nLa perdita di potenza si verifica perché la riduzione dei dati comporta una minore capacità di rilevare effetti reali. Nei disegni longitudinali, questa perdita è particolarmente critica, poiché le misure ripetute nel tempo sono essenziali per comprendere le dinamiche e i cambiamenti. Una riduzione nel numero di osservazioni può rendere difficile individuare tendenze significative o effetti degli interventi.\nIl bias si introduce quando i dati mancanti non sono distribuiti casualmente. Se la mancanza di dati è legata a caratteristiche specifiche dei soggetti, i risultati possono non essere più rappresentativi della popolazione originale, portando a conclusioni errate o fuorvianti.\nI metodi tradizionali per gestire i dati mancanti, come l’eliminazione dei casi o l’utilizzo dell’ultimo punto disponibile, possono peggiorare la situazione, aumentando sia la perdita di potenza sia il bias. Al contrario, le tecniche moderne mirano a preservare il più possibile la potenza del dataset originale e, se usate correttamente, possono ridurre il bias selettivo.\nTra le tecniche moderne per la gestione dei dati mancanti troviamo l’imputazione multipla, che crea più set completi di dati imputando i valori mancanti in modo da riflettere l’incertezza associata a tali valori. L’uso di variabili ausiliarie appropriate nel modello di analisi e nel processo di imputazione contribuisce a minimizzare il bias.\nLe variabili ausiliarie sono fondamentali: se scelte correttamente, possono spiegare il meccanismo dei dati mancanti e ridurre il bias. Se assenti o selezionate in modo inappropriato, i risultati dell’analisi possono rimanere distorti e le conclusioni dello studio risultare compromesse.\nUn approccio basato sui dati, come l’uso dell’imputazione multipla con MICE (Multiple Imputation by Chained Equations) o missForest, e l’inclusione di tutte le variabili disponibili (comprese informazioni potenzialmente non lineari), permette di rappresentare al meglio il meccanismo di mancanza dei dati. Questo approccio presuppone generalmente che le relazioni tra variabili siano lineari, ma consente anche di includere informazioni non lineari rilevanti. Successivamente, quando viene selezionato un sottoinsieme di variabili per l’analisi, l’effetto della gestione dei dati mancanti è mantenuto, aumentando la generalizzabilità delle analisi considerate le variabili incluse nel protocollo.\nIn sintesi, la gestione dei dati mancanti nei disegni longitudinali richiede un’attenta considerazione del meccanismo di mancanza e l’applicazione di tecniche moderne che possano mitigare la perdita di potenza e il bias. Questo è essenziale per garantire l’affidabilità e la validità dei risultati della ricerca.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/01_lgm_intro.html#domande-della-ricerca",
    "href": "chapters/lgm/01_lgm_intro.html#domande-della-ricerca",
    "title": "70  Curve di crescita latente",
    "section": "70.5 Domande della ricerca",
    "text": "70.5 Domande della ricerca\nL’analisi di dati longitudinali nella ricerca psicologica è particolarmente interessata a valutare quanto le persone cambino in un particolare aspetto o tratto nel tempo (cioè la crescita media), quanto questa crescita varia rispetto agli altri individui (cioè la varianza della crescita), e come le persone cambiano rispetto a se stesse nel tempo (cioè i modelli di crescita intra-individuale). I modelli di crescita media (LGM) descrivono i primi due di questi elementi chiave, mentre i modelli di punteggio di cambiamento latente (LCSM) descrivono il terzo (come i valori precedenti prevedono i valori successivi nel tempo all’interno della stessa persona).\nIn ambito di ricerca psicologica, l’analisi di dati longitudinali si concentra sullo studio di come le persone cambiano in un particolare tratto o aspetto nel tempo, sulla variazione di questo cambiamento rispetto ad altre persone e su come le persone cambiano rispetto a se stesse nel tempo. I modelli di crescita media (LGM) e quelli di punteggio di cambiamento latente (LCSM) vengono utilizzati per descrivere questi aspetti.\nGrimm et al. (2016) identificano cinque motivi principali per cui questi modelli vengono utilizzati.\n\nIn primo luogo, l’analisi longitudinale consente di identificare direttamente il cambiamento e la stabilità intra-individuale. Ciò significa che è possibile valutare in che modo specifici attributi dell’individuo cambiano o rimangono gli stessi nel tempo, attraverso la misurazione ripetuta della stessa persona.\nIn secondo luogo, l’analisi longitudinale consente di identificare le differenze interindividuali nel cambiamento intra-individuale, ovvero se diversi individui cambiano in modi diversi, in quantità o direzioni diverse o se passano da uno stadio all’altro in momenti diversi.\nIn terzo luogo, l’analisi longitudinale consente di analizzare le interrelazioni nel cambiamento comportamentale, ovvero come i cambiamenti in una variabile influenzino i cambiamenti in un’altra variabile.\nIn quarto luogo, l’analisi longitudinale consente di analizzare le cause del cambiamento intra-individuale, ovvero di identificare i fattori e/o i meccanismi variabili nel tempo che influenzano i cambiamenti intra-individuali.\nInfine, in quinto luogo, l’analisi longitudinale consente di analizzare le cause delle differenze interindividuali nel cambiamento intra-individuale, ovvero di identificare le variabili invarianti nel tempo che sono correlate a specifici aspetti del cambiamento all’interno della persona, come le caratteristiche demografiche, gli interventi sperimentali e le caratteristiche dei contesti degli individui.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/01_lgm_intro.html#riflessioni-conclusive",
    "href": "chapters/lgm/01_lgm_intro.html#riflessioni-conclusive",
    "title": "70  Curve di crescita latente",
    "section": "70.6 Riflessioni Conclusive",
    "text": "70.6 Riflessioni Conclusive\nQuesto capitolo ha introdotto le peculiarità dei dati longitudinali e l’approccio necessario per studiare il cambiamento umano nel tempo. L’uso di disegni e modelli specifici, dalle curve di crescita latente alle tecniche di imputazione per i dati mancanti, è cruciale per comprendere come individui e gruppi evolvano nel tempo.\nGli studi longitudinali richiedono una gestione attenta delle complessità statistiche per evitare che i risultati siano distorti o di difficile generalizzazione. In particolare, il controllo di variabili di confondimento e la corretta modellazione della crescita intra- e interindividuale evidenziano quanto la qualità dei risultati dipenda dalla precisione nella progettazione dello studio e nella selezione delle metodologie analitiche.\nLa metodologia longitudinale ci ricorda che il cambiamento umano è complesso e multiforme. Sebbene ogni modello rappresenti un tentativo di cogliere questa complessità, nessun approccio è esaustivo: per cogliere appieno le sfumature del cambiamento, potrebbe essere necessario combinare più metodologie, integrando i punti di forza di ciascun approccio.\n\n\n\n\nGrimm, K. J., Ram, N., & Estabrook, R. (2016). Growth modeling: Structural equation and multilevel modeling approaches. Guilford Publications.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/02_lgm_prelims.html",
    "href": "chapters/lgm/02_lgm_prelims.html",
    "title": "71  Considerazioni Preliminari",
    "section": "",
    "text": "71.1 Strutture dei dati\nTradizionalmente, gli studi longitudinali venivano condotti con un numero relativamente basso di valutazioni ripetute (meno di 8) e un numero elevato di individui (più di 200). Tuttavia, i progressi nelle teorie statistica, che includono l’utilizzo di modelli non lineari, e nella tecnologia di raccolta dati, come i sondaggi basati sul web e gli smartphone, hanno notevolmente ampliato le possibilità di raccogliere e analizzare dati longitudinali. In particolare, Grimm et al. (2016) hanno discusso l’applicazione di modelli di crescita latente a dati longitudinali di grandi dimensioni, comprendenti fino a 50,000 individui e 1,000 valutazioni ripetute.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Considerazioni Preliminari</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/02_lgm_prelims.html#strutture-dei-dati",
    "href": "chapters/lgm/02_lgm_prelims.html#strutture-dei-dati",
    "title": "71  Considerazioni Preliminari",
    "section": "",
    "text": "71.1.1 Formato Long e Wide\nI dati longitudinali tipicamente si presentano in due forme: long e wide. Nel formato long, la descrizione del tempo è sulle righe; nel formato wide le variabili relative ad ogni occasione temporale sono organizzate in colonne. È possibile trasformere i dati dal formato long in formato wide e viceversa usando le funzioni R pivot_wider() e pivot_longer(). La sintassi è spiegata nella pagina web tidyr.\nPer fare un esempio, esaminiamo il cambiamento nel rendimento in matematica dei bambini durante la scuola elementare e media utilizzando il set di dati NLSY-CYA. Questi dati sono stati analizzati da Grimm et al. (2016) e possono essere utilizzati per illustrare i concetti relativi alle analisi di cambiamento longitudinale.\nIniziamo a leggere i dati in R.\n\n# set filepath for data file\nfilepath &lt;- \"https://raw.githubusercontent.com/LRI-2/Data/main/GrowthModeling/nlsy_math_long_R.dat\"\n\n# read in the text data file using the url() function\ndat &lt;- read.table(\n  file = url(filepath),\n  na.strings = \".\"\n) # indicates the missing data designator\n# copy data with new name\nnlsy_math_long &lt;- dat\n\n# Add names the columns of the data set\nnames(nlsy_math_long) &lt;- c(\n  \"id\", \"female\", \"lb_wght\",\n  \"anti_k1\", \"math\", \"grade\",\n  \"occ\", \"age\", \"men\",\n  \"spring\", \"anti\"\n)\n\n# view the first few observations in the data set\nhead(nlsy_math_long) |&gt; \nprint()\n\n    id female lb_wght anti_k1 math grade occ age men spring anti\n1  201      1       0       0   38     3   2 111   0      1    0\n2  201      1       0       0   55     5   3 135   1      1    0\n3  303      1       0       1   26     2   2 121   0      1    2\n4  303      1       0       1   33     5   3 145   0      1    2\n5 2702      0       0       0   56     2   2 100  NA      1    0\n6 2702      0       0       0   58     4   3 125  NA      1    2\n\n\nI dati sono qui forniti nel formato long.\nContiamo il numero di partecipanti.\n\nnlsy_math_long |&gt;\n  distinct(id) |&gt;\n  count() |&gt; \n  print()\n\n    n\n1 932\n\n\nCon pivot_wider possiamo trasformare i dati in formato wide.\n\nnlsy_math_wide &lt;- nlsy_math_long |&gt; \n  pivot_wider(names_from = grade, values_from = math)\n\nnlsy_math_wide |&gt;\n  head() |&gt; \n  print()\n\n# A tibble: 6 x 16\n     id female lb_wght anti_k1   occ   age   men spring  anti   `3`\n  &lt;int&gt;  &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1   201      1       0       0     2   111     0      1     0    38\n2   201      1       0       0     3   135     1      1     0    NA\n3   303      1       0       1     2   121     0      1     2    NA\n4   303      1       0       1     3   145     0      1     2    NA\n5  2702      0       0       0     2   100    NA      1     0    NA\n6  2702      0       0       0     3   125    NA      1     2    NA\n# i 6 more variables: `5` &lt;int&gt;, `2` &lt;int&gt;, `4` &lt;int&gt;, `8` &lt;int&gt;,\n#   `6` &lt;int&gt;, `7` &lt;int&gt;",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Considerazioni Preliminari</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/02_lgm_prelims.html#visualizzazione-dei-dati-longitudinali",
    "href": "chapters/lgm/02_lgm_prelims.html#visualizzazione-dei-dati-longitudinali",
    "title": "71  Considerazioni Preliminari",
    "section": "71.2 Visualizzazione dei dati longitudinali",
    "text": "71.2 Visualizzazione dei dati longitudinali\nCome in qualsiasi analisi statistica, è importante esaminare attentamente i dati. Ciò include la produzione di sia riepiloghi quantitativi che visualizzazioni. Per fare un esempio di visualizzazione di dati longitudinali, esaminiamo il cambiamento nel rendimento in matematica dei bambini durante la scuola elementare e media utilizzando il set di dati NLSY-CYA (si veda Grimm et al., 2016). Da questi dati, selezioniamo solo il grado scolastico, il codice identificativo e il punteggio di matematica.\n\nnlsy_math_only_long &lt;- nlsy_math_long |&gt;\n    dplyr::select(id, grade, math)\n\nLe traiettorie di cambiamento intra-individuale possono essere prodotte nel modo seguente.\n\nnlsy_math_long |&gt; # data set\n  ggplot(aes(x = grade, y = math, group = id)) + # setting variables\n  geom_point(size = .5) + # adding points to plot\n  geom_line(alpha=0.2) + # adding lines to plot\n  # setting the x-axis with breaks and labels\n  scale_x_continuous(\n    limits = c(2, 8),\n    breaks = c(2, 3, 4, 5, 6, 7, 8),\n    name = \"Grade at Testing\"\n  ) +\n  # setting the y-axis with limits breaks and labels\n  scale_y_continuous(\n    limits = c(10, 90),\n    breaks = c(10, 30, 50, 70, 90),\n    name = \"PIAT Mathematics\"\n  )\n\n\n\n\n\n\n\n\nEsaminiamo le traiettorie di sviluppo dei primi 9 partecipanti.\n\nsubset_it &lt;- c(201, 303, 2702, 4303, 5002, 5005, 5701, 6102, 6801)\ntemp &lt;- nlsy_math_long[nlsy_math_long$id %in% subset_it, ]\n\ntemp |&gt;\n  ggplot(aes(x = grade, y = math)) +\n  geom_point() +\n  geom_line() +\n  # coord_cartesian(ylim = c(1, 4)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~id)",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Considerazioni Preliminari</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/02_lgm_prelims.html#data-screening",
    "href": "chapters/lgm/02_lgm_prelims.html#data-screening",
    "title": "71  Considerazioni Preliminari",
    "section": "71.3 Data screening",
    "text": "71.3 Data screening\nPrima di adattare i modelli di crescita ai dati longitudinali, è essenziale effettuare una valutazione preliminare dei dati e acquisire informazioni di base sulle variabili da utilizzare nell’analisi. Una delle prime fasi di questa valutazione preliminare è l’ispezione della distribuzione dei punteggi per ogni variabile, utilizzando le principali statistiche descrittive univariate, come la media, la mediana, la varianza (deviazione standard), l’asimmetria, la curtosi, il minimo, il massimo, l’intervallo e il numero di osservazioni per ogni variabile, in base alla metrica del tempo scelta. Le statistiche descrittive bivariate, come le correlazioni/covarianze e le tabelle di frequenza bivariate per variabili nominali o ordinali, possono fornire informazioni sui possibili schemi e relazioni non lineari, nonché sui potenziali valori anomali e codici errati.\nI dati longitudinali sono caratterizzati dall’ordinamento dei dati lungo una o più metriche del tempo (ad esempio, l’occasione di misurazione, l’età, la data, il tempo dall’evento, il numero di esposizioni, ecc.). È importante esaminare come la media, la varianza e il numero di casi disponibili cambiano attraverso le misure ripetute (ad esempio wght5, wght6, wght7). Va notato che la selezione della metrica del tempo influisce notevolmente sulla capacità di interpretare i risultati di qualsiasi modello di crescita specifico. Pertanto, durante la fase di selezione dei dati, è necessario considerare attentamente come varie proprietà dei dati longitudinali cambiano quando i dati sono organizzati in relazione a diverse metriche del tempo.\nPer i dati dell’esempio, le statistiche descrittive possono essere ottenute nel modo seguente.\n\ndescribe(nlsy_math_long) |&gt;\n    print()\n\n        vars    n      mean        sd median   trimmed       mad min\nid         1 2221 528449.15 327303.70 497403 515466.90 384144.63 201\nfemale     2 2221      0.49      0.50      0      0.49      0.00   0\nlb_wght    3 2221      0.08      0.27      0      0.00      0.00   0\nanti_k1    4 2221      1.42      1.50      1      1.19      1.48   0\nmath       5 2221     46.12     12.80     46     46.22     11.86  12\ngrade      6 2221      4.51      1.77      4      4.44      1.48   2\nocc        7 2221      2.84      0.79      3      2.77      1.48   2\nage        8 2221    126.90     22.06    126    126.28     25.20  82\nmen        9 1074      0.19      0.40      0      0.12      0.00   0\nspring    10 2221      0.65      0.48      1      0.69      0.00   0\nanti      11 2170      1.58      1.54      1      1.38      1.48   0\n            max   range  skew kurtosis      se\nid      1256601 1256400  0.30    -0.90 6945.07\nfemale        1       1  0.03    -2.00    0.01\nlb_wght       1       1  3.10     7.63    0.01\nanti_k1       8       8  1.14     1.14    0.03\nmath         81      69 -0.03    -0.18    0.27\ngrade         8       6  0.26    -0.92    0.04\nocc           5       3  0.55    -0.48    0.02\nage         175      93  0.19    -0.91    0.47\nmen           1       1  1.54     0.37    0.01\nspring        1       1 -0.63    -1.61    0.01\nanti          8       8  0.98     0.64    0.03\n\n\nEsaminiamo le statistiche descrittive bivariate.\n\n# Calcola la matrice di correlazione e arrotondala a 2 decimali\ncor_matrix &lt;- cor(nlsy_math_long, use = \"pairwise.complete.obs\") |&gt; round(2)\n\n# Imposta i valori al di sopra della diagonale principale a NA\ncor_matrix[!lower.tri(cor_matrix, diag = TRUE)] &lt;- NA\n\n# Stampa solo la matrice triangolare inferiore\nprint(cor_matrix, na.print = \"\")\n\n           id female lb_wght anti_k1  math grade  occ  age  men spring anti\nid       1.00                                                              \nfemale  -0.01   1.00                                                       \nlb_wght -0.01   0.06    1.00                                               \nanti_k1 -0.02  -0.09    0.03    1.00                                       \nmath    -0.22  -0.05   -0.03   -0.08  1.00                                 \ngrade   -0.01   0.00   -0.02   -0.03  0.59  1.00                           \nocc      0.01  -0.02   -0.03   -0.04  0.53  0.87 1.00                      \nage     -0.01  -0.04    0.01   -0.01  0.58  0.95 0.86 1.00                 \nmen     -0.02   0.02    0.04    0.01  0.30  0.62 0.57 0.64 1.00            \nspring  -0.11   0.04    0.03   -0.01  0.29  0.12 0.17 0.21 0.16   1.00     \nanti     0.01  -0.07    0.02    0.52 -0.05  0.04 0.04 0.06 0.13  -0.01    1\n\n\nScomponiamo i punteggi nelle componenti tra i soggetti ed entro i soggetti.\nEsaminiamo la distribuzione delle medie dei punteggi tra i soggetti.\n\ntmp &lt;- meanDecompose(math ~ id, data = nlsy_math_long)\n\n\nplot(\n    testDistribution(\n        tmp[[\"math by id\"]]$X,\n        extremevalues = \"theoretical\", ev.perc = .001\n    ),\n    varlab = \"Between Person Math Scores\"\n)\n\n\n\n\n\n\n\n\nEsaminiamo la distribuzione dei punteggi entro i soggetti.\n\nplot(\n    testDistribution(\n        tmp[[\"math by residual\"]]$X,\n        extremevalues = \"theoretical\", ev.perc = .001\n    ),\n    varlab = \"Within Person Math Scores\"\n)",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Considerazioni Preliminari</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/02_lgm_prelims.html#attendibilità",
    "href": "chapters/lgm/02_lgm_prelims.html#attendibilità",
    "title": "71  Considerazioni Preliminari",
    "section": "71.4 Attendibilità",
    "text": "71.4 Attendibilità\nPer garantire la validità delle analisi, è necessario valutare l’affidabilità degli strumenti di misurazione utilizzati. Ciò è particolarmente importante in analisi longitudinali, in cui si studiano i cambiamenti all’interno di ciascun individuo nel tempo. Una delle metodologie utilizzate per valutare l’affidabilità degli strumenti di misurazione è il calcolo dell’indice di affidabilità \\(\\omega\\) di McDonald per ogni momento temporale in cui si effettua la misurazione. Tuttavia, è importante sottolineare che questo indice non è equivalente alle misure di affidabilità degli indici di cambiamento longitudinale. Quest’ultima è una problematica complessa, soprattutto in disegni longitudinali intensivi, e costituisce un tema di discussione nella letteratura metodologica contemporanea.\nMentre l’affidabilità dei cambiamenti intra-individuali è difficile da stimare, l’affidabilità delle differenze inter-individuali può essere trovata facilmente. L’affidabilità del cambiamento inter-individuale è la proporzione di varianza nei punteggi osservati tra gli individui che può essere attribuita alla varianza nei punteggi veri piuttosto che alla varianza dell’errore:\n\\[\n\\frac{\\text{varianza di interesse}}{\\text{varianza di interesse} + \\text{varianza d'errore}},\n\\]\nNel caso delle misure di differenza individuale ottenute tramite Intensive Longitudinal Designs (ILD), per valutare l’affidabilità delle medie intra-individuali \\(\\bar{y}\\) (cioè la varianza “vera” delle medie tra i soggetti) si può utilizzare il coefficiente di correlazione intraclasse (ICC). In sostanza, l’ICC indica la proporzione di varianza totale che è attribuibile alle differenze tra gli individui rispetto alla varianza totale, che comprende sia le differenze tra gli individui che le differenze all’interno di ciascun individuo nel tempo. L’ICC viene calcolato come un rapporto tra varianze:\n\\[\n\\rho^2_{\\bar{y}} = \\frac{\\hat{\\tau}^2_{\\mu}}{\\hat{\\tau}^2_{\\mu} + \\hat{\\tau}^2_{\\varepsilon}}.\n\\]\nIl coefficiente di correlazione intraclasse (ICC) può essere stimato utilizzano un modello misto lineare con intercetta casuale che tiene conto della clusterizzazione dei dati, cioè del fatto che le osservazioni sono raggruppate in base ai soggetti. L’ICC viene calcolato come il rapporto tra la varianza tra le medie dei cluster di raggruppamento dei dati e la varianza totale, che comprende la varianza tra i cluster e la varianza all’interno dei cluster. In altre parole, l’ICC rappresenta la proporzione di varianza totale che è dovuta alle differenze tra i soggetti rispetto alla varianza totale delle misure ripetute. Questo indice è utile per valutare l’affidabilità delle misure ripetute e la loro utilità per lo studio delle differenze individuali.\nConsideriamo nuovamente i dati nlsy_math_long, che rappresentano il cambiamento nel rendimento in matematica dei bambini tra i gradi scolastici 2 e 8.\nSelezioniamo solo le variabili di interesse.\n\nnlsy_math_only_long &lt;- nlsy_math_long %&gt;%\n    dplyr::select(id, grade, math)\n\nnlsy_math_only_long |&gt;\n    head()|&gt; \n    print()\n\n    id grade math\n1  201     3   38\n2  201     5   55\n3  303     2   26\n4  303     5   33\n5 2702     2   56\n6 2702     4   58\n\n\nIl coefficiente ICC può essere trovato, ad esempio, mediante la funzione iccMixed specificando un raggruppamento dei dati nei termini dei soggetti.\n\niccMixed(\n  dv = \"math\",\n  id = c(\"id\"),\n  data = nlsy_math_long\n) |&gt;\n  print()\n\n        Var     Sigma       ICC\n     &lt;char&gt;     &lt;num&gt;     &lt;num&gt;\n1:       id  47.02036 0.2872289\n2: Residual 116.68307 0.7127711\n\n\nPer capire meglio il significato dell’ICC calcolato in precedenza per i dati nlsy_math_long, possiamo replicare lo stesso risultato utilizzando il modello misto lineare lmer, che tiene conto dell’effetto casuale del soggetto.\n\nm &lt;- lmer(math ~ 1 + (1 | id), data = nlsy_math_only_long)\n\nIl modello misto decompone la varianza totale in due componenti: la varianza che dipende dalla differenze tra le medie di ciascun soggetto (tra i soggetti, o varianza delle intercette individuali, \\(\\tau_{00}\\)) e la varianza che dipende dalle variazioni di ciascun soggetto attorno alla sua media.\nCalcoliamo la varianza totale dei punteggi di matematica.\n\nvar(nlsy_math_only_long$math) |&gt;\n    print()\n\n[1] 163.8379\n\n\nEsaminiamo ora la scomposizione della varianza eseguita dal modello misto. Si noti che la somma delle due componenti è uguale alla varianza totale.\n\nVarCorr(m) |&gt;\n    print()\n\n Groups   Name        Std.Dev.\n id       (Intercept)  6.8571 \n Residual             10.8020 \n\n\n\n6.8571^2 + 10.8020^2\n\n163.70302441\n\n\nIl coefficiente ICC è data dal rapporto tra la varianza attribuibile alla variazione tra le medie dei soggetti e la varianza totale.\n\n6.8571^2 / (6.8571^2 + 10.8020^2)\n\n0.287226339155697\n\n\nNel contesto dei modelli di crescita latente (LGM), l’ICC può essere usato per stimare l’affidabilità delle medie intra-individuali dei fattori latenti, ma non è adatto a valutare l’affidabilità delle variazioni intra-individuali nelle traiettorie di sviluppo. Per queste ultime, è necessario ricorrere a diverse misure di affidabilità.\nAd esempio, il coefficiente di affidabilità test-retest può essere utilizzato per stimare l’affidabilità intra-individuale delle traiettorie di sviluppo calcolate in due momenti distinti. Tuttavia, questo richiede l’assunzione che i punteggi veri restino invariati nel tempo. Un’alternativa è l’uso di forme parallele di test per misurare l’affidabilità delle traiettorie intra-individuali, anche se tali forme sono raramente disponibili.\nPer stimare l’affidabilità della componente sistematica della variazione intra-individuale nelle traiettorie di sviluppo, sono necessari metodi di stima specifici, ancora oggetto di dibattito nella letteratura metodologica.\nPrima di approfondire i modelli di crescita latente, è utile considerare l’analisi dei dati longitudinali tramite modelli misti. In R, queste analisi possono essere condotte con la funzione lmer del pacchetto lme4.\n\n71.4.1 Analisi con lmer\nLa funzione lmer() accetta i seguenti argomenti:\n\nformula: una formula lineare a due lati che descrive sia gli effetti fissi che gli effetti casuali del modello, con la risposta a sinistra dell’operatore ~ e i predittori e gli effetti casuali sulla destra dell’operatore ~.\ndata: Un data.frame, che deve essere nel cosiddetto formato “lungo”, con una singola riga per osservazione.\n\nIniziamo a descrivere la sintassi che consente la specificazione di un modello misto. Gli effetti fissi sono specificati come segue.\n\n\n\n\n\n\n\nFormula\nDescription\n\n\n\n\na + b\nmain effects of a and b (and no interaction)\n\n\na:b\nonly interaction of a and b (and no main effects)\n\n\na * b\nmain effects and interaction of a and b (expands to: a + b + a:b)\n\n\n(a+b+c)^2\nmain effects and two-way interactions, but no three-way interaction (expands to: a + b + c + a:b + b:c + a:c)\n\n\n(a+b)*c\nall main effects and pairwise interactions between c and a or b (expands to: a + b + c + a:c + b:c)\n\n\n0 + a\n0 suppresses the intercept resulting in a model that has one parameter per level of a (identical to: a - 1)\n\n\n\nGli effetti random vengono aggiunti alla formula tra parentesi (). All’interno di queste parentesi si fornisce sul lato sinistro di un segno condizionale | la specifica degli effetti casuali relativi alle pendenze individuali da includere nel modello. Sul lato destro di questo segno condizionale, si specifica il fattore di raggruppamento o i fattori di raggruppamento da cui dipendono questi effetti casuali. I fattori di raggruppamento devono essere di classe factor (cioè non possono essere variabili numeriche).\nGli effetti random vengono specificati come segue.\n\n\n\n\n\n\n\nFormula\nDescription\n\n\n\n\n(1\\|s)\nrandom intercepts for unique level of the factor s\n\n\n(1\\|s) + (1\\|i)\nrandom intercepts for each unique level of s and for each unique level of i\n\n\n(1\\|s/i)\nrandom intercepts for factor s and i, where the random effects for i are nested in s. This expands to (1\\|s) + (1\\|s:i) , i.e. a random intercept for each level of s, and each unique combination of the levels of s and i. Nested random effects are used in so-called multilevel models. For example, s might refer to schools, and i to classrooms within those schools.\n\n\n(a\\|s)\nrandom intercepts and random slopes for a, for each level of s. Correlations between the intercept and slope effects are also estimated – identical to (a*b\\|s))\n\n\n(a*b\\|s)\nrandom intercepts and slopes for a, b, and the a:b interaction, for each level of s. Correlations between all the random effects are estimated.\n\n\n(0+a\\|s)\nrandom slopes for a for each level of s, but no random intercepts\n\n\n(a\\|\\|s)\nrandom intercepts and random slopes for a, for each level of s, but no correlations between the random effects (i.e. they are set to 0). This expands to: (0+a\\|s) + (1\\|s)\n\n\n\n\n\n71.4.2 Formulazione del modello\nIn precedenza abbiamo descritto la variazione media tra gli individui mediante un modello misto ad intercetta casuale. È possibile estendere questo modello a casi più complessi, per esempio quello che assume una retta di regressione con pendenza ed intercetta diversa per ciascun soggetto. Per i dati nlsy_math_long possiamo specificare un tale modello in lmer usando la sintassi seguente.\n\nm &lt;- lmer(math ~ grade + (1 + grade | id), data = nlsy_math_long)\n\nIn un modello misto, i coefficienti delle rette di regressione di ciascun soggetto sono considerati come componenti casuali di una distribuzione di coefficienti relativi all’intercetta e alla pendenza complessive del gruppo. Questi coefficienti casuali possono essere modellati specificando una componente casuale (1 + grado | id). In questo tipo di modello, esiste una correlazione tra i parametri delle intercette e quelli delle pendenze individuali. Ciò significa che le componenti di varianza attribuibili ai vari effetti del modello (fissi e casuali) non sono più indipendenti e la varianza totale non può essere scomposta in componenti indipendenti.\nPer estrarre le componenti di varianza di un modello misto, è possibile utilizzare le funzioni fornite dal pacchetto insight. Ad esempio, nel caso dell’esempio presentato, i risultati possono essere ottenuti attraverso l’oggetto creato dalla funzione lmer.\n\ninsight::get_variance(m) |&gt;\n    print()\n\n$var.fixed\n[1] 58.94602\n\n$var.random\n[1] 70.66767\n\n$var.residual\n[1] 36.23643\n\n$var.distribution\n[1] 36.23643\n\n$var.dispersion\n[1] 0\n\n$var.intercept\n      id \n68.40554 \n\n$var.slope\n id.grade \n0.7391598 \n\n$cor.slope_intercept\n        id \n-0.2353175 \n\n\n\nUna descrizione visiva della varianza delle varie componenti del modello può essere ottenuta mediante la funzione modelDiagnostics del pacchetto JWileymisc.\n\nmd &lt;- JWileymisc::modelDiagnostics(m, ev.perc = .001)\nplot(md, ask = FALSE, ncol = 2, nrow = 3)\n\n\n\n\n\n\n\n\nUna descrizione dei parametri del modello può essere ottenuta nel modo seguente.\n\nmt &lt;- modelTest(m)\nnames(mt)\nAPAStyler(mt) |&gt;\n    print()\n\n\n'FixedEffects''RandomEffects''EffectSizes''OverallModel'\n\n\n                        Term                     Est           Type\n                      &lt;char&gt;                  &lt;char&gt;         &lt;char&gt;\n 1:              (Intercept) 26.59*** [25.66, 27.51]  Fixed Effects\n 2:                    grade  4.34*** [ 4.17,  4.51]  Fixed Effects\n 3: cor_grade.(Intercept)|id                   -0.24 Random Effects\n 4:        sd_(Intercept)|id                    8.27 Random Effects\n 5:              sd_grade|id                    0.86 Random Effects\n 6:                    sigma                    6.02 Random Effects\n 7:                 Model DF                       6  Overall Model\n 8:               N (Groups)                id (932)  Overall Model\n 9:         N (Observations)                    2221  Overall Model\n10:                   logLik                -7968.69  Overall Model\n11:                      AIC                15949.39  Overall Model\n12:                      BIC                15983.62  Overall Model\n13:              Marginal R2                    0.36  Overall Model\n14:              Marginal F2                    0.55  Overall Model\n15:           Conditional R2                    0.78  Overall Model\n16:           Conditional F2                    3.57  Overall Model\n17:   grade (Fixed + Random)     0.55/2.26, p &lt; .001   Effect Sizes\n18:           grade (Random)     0.01/0.09, p = .002   Effect Sizes\n\n\nLa varianza spiegata dal modello viene ottenuta nel modo seguente.\n\nmodelPerformance(m) |&gt;\n    print()\n\n$Performance\n    Model Estimator N_Obs N_Groups      AIC      BIC        LL  LLDF\n   &lt;char&gt;    &lt;char&gt; &lt;num&gt;   &lt;char&gt;    &lt;num&gt;    &lt;num&gt;     &lt;num&gt; &lt;num&gt;\n1: merMod      REML  2221 id (932) 15952.99 15987.22 -7970.494     6\n     Sigma MarginalR2 ConditionalR2 MarginalF2 ConditionalF2\n     &lt;num&gt;      &lt;num&gt;         &lt;num&gt;      &lt;num&gt;         &lt;num&gt;\n1: 6.01967  0.3553143      0.781476  0.5511433      3.576157\n\nattr(,\"class\")\n[1] \"modelPerformance.merMod\" \"modelPerformance\"       \n\n\n\n\n\n\nGrimm, K. J., Ram, N., & Estabrook, R. (2016). Growth modeling: Structural equation and multilevel modeling approaches. Guilford Publications.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Considerazioni Preliminari</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/06_lgm_mixed.html",
    "href": "chapters/lgm/06_lgm_mixed.html",
    "title": "72  LGM e modelli misti",
    "section": "",
    "text": "72.1 Introduzione\nI modelli di crescita latente (LGM, Latent Growth Models) rappresentano una forma specializzata di analisi fattoriale confermativa (CFA) che si focalizza sull’evoluzione dei costrutti nel tempo. Questi modelli si distinguono per la loro capacità di fissare le saturazioni fattoriali a valori predefiniti. Questo significa che si imposta in anticipo come ciascun fattore contribuisca alla varianza osservata nei dati raccolti in diversi momenti. In molti casi, la traiettoria di crescita nel tempo può essere descritta utilizzando una funzione lineare o quadratica, permettendo di modellare diversi tipi di evoluzioni, come un aumento costante o un cambiamento accelerato.\nUn aspetto centrale dei LGM è il concetto di fattori di crescita, che rappresentano le differenze individuali all’interno dei dati longitudinali. Questi fattori di crescita sono rappresentati da variabili latenti continue, denominate growth factors. In pratica, permettono di catturare e quantificare variazioni individuali nel modo in cui i soggetti cambiano nel tempo, ad esempio, in termini di sviluppo delle competenze o dell’andamento di un sintomo.\nPer facilitare una comprensione più approfondita dei modelli LGM, nel presente capitolo si propone un confronto con i modelli misti {cite:p}hoffman2022catching. Questo confronto mira a evidenziare le differenze e le somiglianze tra i due approcci, aiutando a discernere quando e perché scegliere un modello rispetto all’altro. Mentre i modelli misti possono essere utilizzati per analizzare dati gerarchici o nidificati, i modelli LGM si concentrano specificamente sull’analisi della traiettoria di crescita nel tempo, rendendoli particolarmente utili in studi longitudinali dove l’interesse primario è capire come un costrutto si sviluppa o cambia nel corso del tempo.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>LGM e modelli misti</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/06_lgm_mixed.html#modelli-misti",
    "href": "chapters/lgm/06_lgm_mixed.html#modelli-misti",
    "title": "72  LGM e modelli misti",
    "section": "72.2 Modelli misti",
    "text": "72.2 Modelli misti\nI modelli lineari generalizzati classici presuppongono l’indipendenza delle osservazioni, un’ipotesi che può non essere valida in contesti dove si effettuano misurazioni ripetute sullo stesso soggetto nel tempo, portando a osservazioni correlate. In questi scenari, si ricorre all’uso di modelli lineari a effetti misti (o modelli gerarchici lineari), che consentono di gestire adeguatamente la correlazione intragruppo.\nNei modelli a effetti misti, si realizza un equilibrio tra i due approcci estremi di modellazione: la modellazione aggregata (o pooled) e la modellazione separata (o per gruppo). La modellazione aggregata tratta l’insieme dei dati come un unico gruppo, ignorando le differenze intergruppo e potenzialmente perdendo informazioni rilevanti sulla variabilità tra i gruppi. La modellazione separata, d’altra parte, adatta un modello distinto per ciascun gruppo, potendo portare a sovrapparametrizzazione e ridotta capacità di generalizzazione.\nI modelli a effetti misti superano queste limitazioni integrando elementi sia della modellazione aggregata sia della modellazione separata. Ciò consente di considerare sia le differenze tra gruppi (variabilità intergruppo) sia l’informazione comune (variabilità intragruppo), migliorando la precisione delle stime dei parametri. Tradizionalmente, i dati rilevati all’interno di un soggetto sono classificati come dati di Livello 1, mentre i dati raccolti tra soggetti diversi sono definiti dati di Livello 2.\nAnalogamente ai modelli di regressione lineari tradizionali, i modelli a effetti misti includono un’intercetta fissa, coefficienti fissi per i predittori e un termine di errore per la deviazione tra i valori osservati e quelli predetti dal modello. Tuttavia, a differenza dei modelli lineari standard, in un modello a effetti misti, l’intercetta e i coefficienti dei predittori possono variare tra le unità di analisi, permettendo una maggiore flessibilità e adattabilità nel rappresentare la struttura dei dati. Questo approccio rende i modelli a effetti misti particolarmente adatti per l’analisi di dati longitudinali o gerarchici, dove è necessario tener conto della correlazione tra osservazioni all’interno dello stesso gruppo o soggetto.\nIl modello lineare multilivello, applicato all’analisi di dati strutturati in gruppi o cluster, permette una comprensione dettagliata della variazione sia all’interno dei gruppi (within-group variation) sia tra i gruppi (between-group variation). Formalmente, consideriamo un’unità statistica \\(i\\) all’interno di un gruppo \\(j\\) (dove \\(i = 1, ..., n_j\\) e \\(j = 1, ..., N\\)), per un totale di \\(N\\) gruppi ciascuno con numerosità \\(n_j\\).\nPer una variabile dipendente \\(Y\\), una variabile indipendente a livello individuale \\(x\\) e una variabile di gruppo \\(z\\), il modello si articola su due livelli. Il primo livello è rappresentato dalla seguente equazione lineare:\n\\[\nY_{ij} = \\beta_{0j} + \\beta_{1j}x_{ij} + \\varepsilon_{ij}.\n\\]\nQuesta equazione descrive la relazione tra \\(Y\\) e \\(x\\) per ogni unità \\(i\\) nel gruppo \\(j\\), dove \\(\\beta_{0j}\\) è l’intercetta e \\(\\beta_{1j}\\) la pendenza, specifiche per ciascun gruppo \\(j\\). Il termine d’errore \\(\\varepsilon_{ij}\\) è assunto normalmente distribuito con media zero e varianza costante \\(\\sigma^2\\).\nIl secondo livello del modello esplicita come l’intercetta \\(\\beta_{0j}\\) e la pendenza \\(\\beta_{1j}\\) varino tra i gruppi in relazione alla variabile di gruppo \\(z\\)\n\\[\n\\beta_{0j} = \\gamma_{00} + \\gamma_{01}z_j + U_{0j}\n\\]\n\\[\n\\beta_{1j} = \\gamma_{10} + \\gamma_{11}z_j + U_{1j}\n\\]\ndove:\n\n\\(\\gamma_{00}\\) rappresenta l’intercetta media a livello di gruppo. In altre parole, è il valore previsto di \\(Y\\) quando tutte le variabili indipendenti (\\(x\\) e \\(z\\)) sono pari a zero. È una sorta di “punto di partenza” medio per i vari gruppi nel modello.\n\\(\\gamma_{01}\\) è il coefficiente di pendenza che descrive come l’intercetta varia tra i gruppi in relazione alla variabile di gruppo \\(z\\). In pratica, \\(\\gamma_{01}\\) moltiplica la variabile di gruppo \\(z_j\\) (per ogni gruppo \\(j\\)) per quantificare l’influenza di questa variabile sulla variazione dell’intercetta tra i gruppi. Un valore positivo di \\(\\gamma_{01}\\) indica che un aumento in \\(z_j\\) è associato a un aumento dell’intercetta di \\(Y\\), mentre un valore negativo indica il contrario.\n\\(\\gamma_{10}\\) rappresenta il valore medio della pendenza della relazione tra \\(Y\\) e \\(x\\) attraverso tutti i gruppi, quando la variabile di gruppo \\(z\\) è zero. Indica come, in media, la variabile indipendente a livello individuale \\(x\\) si relaziona con \\(Y\\) nei diversi gruppi.\n\\(\\gamma_{11}\\) modella come la relazione (pendenza) tra \\(Y\\) e \\(x\\) varia tra i gruppi in funzione della variabile di gruppo \\(z\\). Analogamente a \\(\\gamma_{01}\\), questo coefficiente moltiplica \\(z_j\\) per mostrare l’effetto di \\(z\\) sulla pendenza di \\(Y\\) rispetto a \\(x\\) tra i gruppi. Se \\(\\gamma_{11}\\) è significativo, indica che l’effetto di \\(x\\) su \\(Y\\) non è costante tra i gruppi, ma varia in base al valore di \\(z\\).\n\nIn sintesi, questi coefficienti permettono di comprendere non solo come varia la relazione tra \\(Y\\) e \\(x\\) all’interno di ciascun gruppo (grazie a \\(\\gamma_{10}\\)), ma anche come questa relazione sia influenzata dalla variabile di gruppo \\(z\\) (mediante \\(\\gamma_{11}\\)). Allo stesso modo, essi illustrano come l’intercetta di \\(Y\\) varia tra i gruppi in base a \\(z\\) (\\(\\gamma_{01}\\)), oltre a fornire un valore di intercetta medio (\\(\\gamma_{00}\\)).\nQueste equazioni legano le variazioni di \\(\\beta_{0j}\\) e \\(\\beta_{1j}\\) tra i gruppi alla variabile \\(z\\). I termini \\(U_{0j}\\) e \\(U_{1j}\\) rappresentano l’errore a livello di gruppo, anch’essi assunti normalmente distribuiti con media zero e varianze costanti \\(\\tau_0^2\\) e \\(\\tau_1^2\\), rispettivamente, e indipendenti dall’errore a livello individuale \\(\\varepsilon_{ij}\\).\nIl modello multilivello permette così di analizzare come le caratteristiche di gruppo (come \\(z\\)) influenzano non solo l’intercetta (il livello di base di \\(Y\\)) ma anche la relazione tra \\(Y\\) e \\(x\\) (la pendenza). In altre parole, consente di esplorare come la relazione tra una variabile dipendente e indipendente possa cambiare da un gruppo all’altro.\nUn elemento chiave di questo approccio è il coefficiente di correlazione intragruppo \\(\\rho(Y \\mid x)\\), definito come:\n\\[\n\\rho(Y \\mid x) = \\frac{\\tau_0^2}{\\tau_0^2 + \\sigma^2}\n\\]\nIl coefficiente di correlazione intragruppo misura la proporzione della varianza totale di \\(Y\\) attribuibile alle differenze tra i gruppi. Un valore di \\(\\rho\\) vicino a 1 indica che la maggior parte della varianza di \\(Y\\) è spiegata dalle differenze tra i gruppi, mentre un valore vicino a 0 suggerisce che la varianza è prevalentemente dovuta a differenze all’interno dei singoli gruppi. Questo coefficiente fornisce quindi una misura quantitativa dell’importanza relativa delle variazioni tra e all’interno dei gruppi nel modello.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>LGM e modelli misti</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/06_lgm_mixed.html#simulare-effetti-casuali",
    "href": "chapters/lgm/06_lgm_mixed.html#simulare-effetti-casuali",
    "title": "72  LGM e modelli misti",
    "section": "72.3 Simulare effetti casuali",
    "text": "72.3 Simulare effetti casuali\nEsaminiamo con una simulazione una dimostrazione del funzionamento dei modelli misti. Ciò ci permetterà di meglio comprendere i modelli a crescita latente. Simuleremo dei dati bilanciati, con punteggi su quattro rilevazioni temporali per 500 individui (soggetti). Esamineremo il tasso di crescita (‘growth’) e consentiremo la presenza di intercette e pendenze specifiche per i diversi soggetti.\nLe istruzioni seguenti generano i dati (per i nostri scopi, non è importante capire i dettagli di questa porzione di codice).\n\nset.seed(12345)\nn &lt;- 500\ntimepoints &lt;- 4\ntime &lt;- rep(0:3, times = n)\nsubject &lt;- rep(1:n, each = 4)\n\nintercept &lt;- .5\nslope &lt;- .25\nrandomEffectsCorr &lt;- matrix(c(1, .2, .2, 1), ncol = 2)\n\nrandomEffects &lt;- MASS::mvrnorm(\n  n,\n  mu = c(0, 0), Sigma = randomEffectsCorr, empirical = T\n) %&gt;%\n  data.frame()\n\ncolnames(randomEffects) &lt;- c(\"Int\", \"Slope\")\n\nNella simulazione, abbiamo impostato gli effetti fissi, che comprendono l’intercetta e la pendenza della regressione lineare standard, ai valori di 0.5 e 0.25 rispettivamente. Inoltre, è stata simulata una correlazione di 0.2 tra l’intercetta e la pendenza che sono specifiche per ogni singolo soggetto. A causa di questa correlazione, i dati sono stati generati utilizzando una distribuzione normale multivariata. In questo contesto, abbiamo assegnato una varianza di 1 sia per l’intercetta sia per la pendenza.\nProcediamo ora con l’analisi dei dati risultanti dalla simulazione. Questo passaggio è fondamentale per comprendere le implicazioni dei parametri scelti nella simulazione e per verificare se i dati generati rispecchiano le aspettative teoriche stabilite inizialmente.\nI dati prodotti fino ad ora sono i seguenti:\n\ndata.frame(\n    Subject = subject, \n    time = time, \n    randomEffects[subject, ]\n) |&gt;\n    head(10)\n\n\nA data.frame: 10 x 4\n\n\n\nSubject\ntime\nInt\nSlope\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1\n0\n-1.3322902\n-0.9548087\n\n\n1.1\n1\n1\n-1.3322902\n-0.9548087\n\n\n1.2\n1\n2\n-1.3322902\n-0.9548087\n\n\n1.3\n1\n3\n-1.3322902\n-0.9548087\n\n\n2\n2\n0\n-2.1261548\n-1.7813625\n\n\n2.1\n2\n1\n-2.1261548\n-1.7813625\n\n\n2.2\n2\n2\n-2.1261548\n-1.7813625\n\n\n2.3\n2\n3\n-2.1261548\n-1.7813625\n\n\n3\n3\n0\n0.4606242\n0.3039838\n\n\n3.1\n3\n1\n0.4606242\n0.3039838\n\n\n\n\n\nPer generare la variabile target, procediamo sommando gli effetti casuali, precedentemente calcolati, all’intercetta globale e applichiamo un analogo procedimento alle pendenze. In aggiunta, introduciamo un rumore gaussiano ai dati, caratterizzato da una deviazione standard \\(\\sigma\\) pari a 0.5. Questa operazione ha lo scopo di aggiungere un livello di variabilità realistica e di incertezza ai dati, rendendo la simulazione più vicina a scenari osservati nella realtà pratica.\n\nset.seed(12345)\nsigma &lt;- .5\ny1 &lt;- \n  (intercept + randomEffects$Int[subject]) + # random intercepts\n  (slope + randomEffects$Slope[subject]) * time + # random slopes\n  rnorm(n * timepoints, mean = 0, sd = sigma) # noise\n\nd &lt;- data.frame(subject, time, y1)\n\n\nd |&gt;\n  head(10) \n\n\nA data.frame: 10 x 3\n\n\n\nsubject\ntime\ny1\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1\n0\n-0.5395258\n\n\n2\n1\n1\n-1.1823659\n\n\n3\n1\n2\n-2.2965593\n\n\n4\n1\n3\n-3.1734649\n\n\n5\n2\n0\n-1.3232110\n\n\n6\n2\n1\n-4.0664952\n\n\n7\n2\n2\n-4.3738304\n\n\n8\n2\n3\n-6.3583342\n\n\n9\n3\n0\n0.8185443\n\n\n10\n3\n1\n1.0549470\n\n\n\n\n\nIl grafico seguente mostra le rette di regressione per ciascuno dei 500 soggetti.\n\nggplot(d, aes(x = time, y = y1)) +\n  geom_path(aes(group = subject), alpha = .1) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nAdattiamo ai dati un modello misto utilizzando la funzione lmer del pacchetto lme4. Si noti che questo è un modello in cui solo le intercette sono consentite variare tra i cluster.\n\nm0 &lt;- lmer(y1 ~ 1 + (1 | subject), data = d)\n\nLe componenti di varianza del modello si estraggono con la funzione VarCorr().\n\nVarCorr(m0)\n\n Groups   Name        Std.Dev.\n subject  (Intercept) 1.8306  \n Residual             1.4352  \n\n\nCalcoliamo il coefficiente di correlazione intraclasse.\n\n1.8306^2 / (1.8306^2 + 1.4352^2)\n\n0.619323810990691\n\n\nLaddove\n\n(1.8306^2 + 1.4352^2)\n\n5.4108954\n\n\nè uguale alla varianza della variabile risposta\n\nvar(d$y1)\n\n5.40584388269803\n\n\nLo stesso risultato si ottiene utilizzando una funzione R per il calcolo della correlazione intraclasse.\n\nmultilevelTools::iccMixed(\n  dv = \"y1\",\n  id = c(\"subject\"),\n  data = d\n) |&gt;\n  print()\n\n        Var    Sigma       ICC\n     &lt;char&gt;    &lt;num&gt;     &lt;num&gt;\n1:  subject 3.350987 0.6193062\n2: Residual 2.059886 0.3806938\n\n\n\n0.6193062 + 0.3806938\n\n1\n\n\nEsaminiamo ora un modello in cui sia le intercette sia le pendenze variano tra i cluster.\n\nmix_mod &lt;- lmer(y1 ~ time + (1 + time | subject), data = d)\nsummary(mix_mod) |&gt;\n    print()\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y1 ~ time + (1 + time | subject)\n   Data: d\n\nREML criterion at convergence: 5881.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.03499 -0.46249  0.00414  0.48241  2.74992 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n subject  (Intercept) 1.0245   1.0122       \n          time        1.0301   1.0149   0.15\n Residual             0.2412   0.4911       \nNumber of obs: 2000, groups:  subject, 500\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.50159    0.04885  10.267\ntime         0.25157    0.04644   5.417\n\nCorrelation of Fixed Effects:\n     (Intr)\ntime 0.072 \n\n\nGli effetti fissi che abbiamo ottenuto (\\(\\alpha\\) = 0.50159, \\(\\beta\\) = 0.25157) sono simili ai valori che abbiamo impostato nella simulazione per l’intercetta e la pendenza globale.\n\nVarCorr(mix_mod)\n\n Groups   Name        Std.Dev. Corr \n subject  (Intercept) 1.01217       \n          time        1.01494  0.150\n Residual             0.49108       \n\n\nLe varianze degli effetti casuali stimati (\\(1.0122^2\\), \\(1.0149^2\\)) sono molto simili al valore impostato di 1 nella simulazione, la correlazione (0.15) è simile al valore impostato di 0.2 e la deviazione standard dei residui (0.4911) è simile al valore impostato di 0.5.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>LGM e modelli misti</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/06_lgm_mixed.html#modello-di-crescita-latente",
    "href": "chapters/lgm/06_lgm_mixed.html#modello-di-crescita-latente",
    "title": "72  LGM e modelli misti",
    "section": "72.4 Modello di crescita latente",
    "text": "72.4 Modello di crescita latente\n\nEsploriamo ora l’analisi degli stessi dati tramite un modello di crescita latente (LGM). I modelli LGM possono essere considerati come un’estensione dei modelli CFA, in cui si ipotizzano due fattori latenti principali: il primo è una variabile latente associata alle intercette casuali, cioè rappresenta la variazione delle intercette individuali dei partecipanti; il secondo è una variabile latente relativa alle pendenze casuali, che descrive la variazione delle pendenze individuali dei partecipanti. Queste intercette e pendenze si riferiscono alla linea di regressione che descrive per ciascun partecipante la relazione tra la variabile in esame e il tempo.\nIn quanto il modello mira a spiegare la relazione tra le medie dei punteggi dei partecipanti nel tempo, è necessario analizzare i dati grezzi piuttosto che la matrice di covarianza campionaria. Ciò significa utilizzare le osservazioni individuali per ciascun partecipante come input.\nPer l’analisi, utilizzeremo nuovamente il software lavaan, ma con una sintassi differente per poter fissare le saturazioni fattoriali a valori specifici, come richiesto dai vincoli del modello LGM. Di conseguenza, l’output che otterremo sarà diverso da quello dei modelli SEM standard, poiché i parametri relativi alle saturazioni fattoriali sono fissi e non stimati.\nPer il fattore che rappresenta le intercette, le saturazioni fattoriali sono impostate a 1. Questo valore può essere interpretato come l’equivalente della colonna dell’intercetta nella matrice \\(\\boldsymbol{X}\\) di un modello di regressione multipla.\nLe saturazioni per il fattore che definisce le pendenze casuali sono stabilite in base alla sequenza temporale delle misurazioni \\(y\\), con valori \\(\\lambda\\) che vanno da 0 a 3. Questi riflettono gli intervalli temporali delle misurazioni. Iniziare la codifica da 0 conferisce un significato interpretabile allo zero, analogamente ai valori che, nella matrice \\(\\boldsymbol{X}\\) di un modello di regressione multipla, corrisponderebbero alla colonna della pendenza.\nIl modello di crescita latente (LGM) espresso dalla formula\n\\[\ny_j = \\alpha_0 + \\alpha_1 \\lambda_j + \\zeta_{00} + \\zeta_{11} \\lambda_j + \\epsilon_j\n\\]\npuò essere messo in relazione con il modello lineare ad effetti misti precedentemente descritto attraverso la comprensione della struttura dei due modelli e delle loro componenti.\n\nInterpretazione dei Componenti del Modello LGM:\n\n$ _0 $: Rappresenta l’intercetta media del modello di crescita.\n$ _1 _j $: Rappresenta il tasso medio di crescita nel tempo, dove $ _j $ sono i valori che descrivono l’intervallo temporale delle misurazioni.\n$ _{00} $: Indica la variazione delle intercette individuali tra i soggetti rispetto all’intercetta media $ _0 $.\n$ _{11} _j $: Esprime la variazione nelle pendenze individuali (tassi di crescita) tra i soggetti rispetto al tasso medio di crescita $ _1 $.\n$ _j $: Rappresenta l’errore di misurazione per ogni singolo soggetto.\n\nConfronto con il Modello Lineare ad Effetti Misti:\n\nNel modello lineare ad effetti misti, si considerano sia effetti fissi (come l’intercetta e la pendenza media del modello) sia effetti casuali (variazione delle intercette e delle pendenze tra i soggetti). In maniera simile, il modello LGM considera l’intercetta media e il tasso medio di crescita (effetti fissi) e permette la variazione individuale in queste componenti (effetti casuali).\nLa componente $ {00} $ nel modello LGM è analoga alla variazione casuale delle intercette nel modello ad effetti misti, mentre $ {11} $ corrisponde alla variazione casuale delle pendenze.\nEntrambi i modelli permettono di analizzare dati strutturati longitudinalmente, offrendo la flessibilità di modellare non solo la tendenza generale (effetti fissi) ma anche la variazione individuale intorno a questa tendenza (effetti casuali).\n\n\nIn sintesi, il modello LGM può essere visto come un caso speciale o un’estensione del modello lineare ad effetti misti, con un’enfasi particolare sulla modellazione del cambiamento nel tempo e sulla relazione di questa dinamica con variabili latenti. Entrambi i modelli sono strumenti potenti nell’analisi di dati longitudinali, permettendo di esaminare sia la tendenza centrale sia la variabilità individuale all’interno dei dati.\nUn requisito degli LGM è che i dati devono essere forniti del formato wide (mentre per il precedente modello misto abbiamo usato il formato long), il che significa che ogni colonna rappresenta la variabile di esito in un diverso momento nel tempo. Si presume che ogni osservazione o riga sia indipendente dalle altre; le colonne mostrano invece una dipendenza temporale. Trasformiamo dunque i dati nel formato richiesto.\n\ndwide &lt;- d %&gt;%\n  spread(time, y1) %&gt;%\n  rename_at(vars(-subject), function(x) paste0(\"y\", x))\nhead(dwide)\n\n\nA data.frame: 6 x 5\n\n\n\nsubject\ny0\ny1\ny2\ny3\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1\n-0.5395258\n-1.1823659\n-2.2965593\n-3.173465\n\n\n2\n2\n-1.3232110\n-4.0664952\n-4.3738304\n-6.358334\n\n\n3\n3\n0.8185443\n1.0549470\n2.0104678\n3.531232\n\n\n4\n4\n0.4469440\n-0.3162615\n-1.7896354\n-1.843919\n\n\n5\n5\n1.8959902\n5.5259110\n9.6045869\n12.546123\n\n\n6\n6\n2.1829579\n1.6287374\n-0.3136214\n-1.660328\n\n\n\n\n\nIl modello misto che abbiamo descritto in precedenza corrisponde dunque ad un modello fattoriale con due variabili latenti: un fattore (\\(\\eta_0\\)) che rappresenta il “punteggio vero” delle intercette individuali e un fattore (\\(\\eta_1\\)) che rappresenta il “punteggio vero” delle pendenze delle rette di regressione per i singoli individui.\nNella sintassi di lavaan il modello diventa:\n\nmodel &lt;- \"\n    i =~ 1*y0 + 1*y1 + 1*y2 + 1*y3\n    s =~ 0*y0 + 1*y1 + 2*y2 + 3*y3\n\"\n\nPossiamo adattare il modello ai dati usando una funzione specifica di lavaan, ovvero growth, che può essere usata per questa classe di modelli.\n\ngrowth_curve_model &lt;- growth(model, data = dwide)\n\n\nsummary(growth_curve_model) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 41 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                                 4.212\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.519\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  i =~                                                \n    y0                1.000                           \n    y1                1.000                           \n    y2                1.000                           \n    y3                1.000                           \n  s =~                                                \n    y0                0.000                           \n    y1                1.000                           \n    y2                2.000                           \n    y3                3.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  i ~~                                                \n    s                 0.162    0.051    3.137    0.002\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    i                 0.501    0.049   10.263    0.000\n    s                 0.252    0.046    5.428    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .y0                0.268    0.042    6.356    0.000\n   .y1                0.237    0.022   10.713    0.000\n   .y2                0.209    0.029    7.262    0.000\n   .y3                0.299    0.066    4.556    0.000\n    i                 1.007    0.078   12.996    0.000\n    s                 1.021    0.068   14.953    0.000\n\n\n\n\ngrowth_curve_model |&gt;\n    semPaths(\n        style = \"ram\",\n        whatLabels = \"par\", edge.label.cex = .6,\n        label.prop = 0.9, edge.label.color = \"black\", rotation = 4,\n        equalizeManifests = FALSE, optimizeLatRes = TRUE,\n        node.width = 1.5,\n        edge.width = 0.5, shapeMan = \"rectangle\", shapeLat = \"ellipse\",\n        shapeInt = \"triangle\", sizeMan = 4, sizeInt = 2, sizeLat = 4,\n        curve = 2, unCol = \"#070b8c\"\n    )\n\n\n\n\n\n\n\n\nUsiamo l’oggetto creato da growth per creare un diagramma di percorso.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>LGM e modelli misti</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/06_lgm_mixed.html#comparazione-tra-modelli-a-effetti-misti-e-modelli-di-crescita-latente",
    "href": "chapters/lgm/06_lgm_mixed.html#comparazione-tra-modelli-a-effetti-misti-e-modelli-di-crescita-latente",
    "title": "72  LGM e modelli misti",
    "section": "72.5 Comparazione tra Modelli a Effetti Misti e Modelli di Crescita Latente",
    "text": "72.5 Comparazione tra Modelli a Effetti Misti e Modelli di Crescita Latente\nNell’output del metodo growth(), la sezione denominata Intercepts rappresenta in realtà gli effetti fissi all’interno del contesto di un modello a effetti misti:\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    i                 0.510    0.048   10.542    0.000\n    s                 0.234    0.046    5.133    0.000\nPuò apparire inizialmente non intuitivo riferirsi agli effetti fissi come ‘intercette’ in un modello a effetti misti. Tuttavia, questa terminologia diventa più chiara quando consideriamo la parametrizzazione del modello di crescita latente (LGM). In un modello LGM, ‘i’ rappresenta l’intercetta generale del modello (cioè, il punto di partenza medio per tutti i soggetti), mentre ‘s’ indica la pendenza media, ovvero il tasso di crescita o di cambiamento nel tempo.\nÈ interessante notare come le stime riportate qui siano molto vicine a quelle che si ottengono in un modello a effetti misti. Questa similitudine dimostra l’affinità tra i due approcci di modellazione: entrambi mirano a comprendere e quantificare sia gli effetti generali (come la tendenza media di crescita) sia le variazioni individuali all’interno di un insieme di dati longitudinali. In entrambi i casi, l’intercetta e la pendenza giocano ruoli cruciali nell’interpretazione dei modelli e nella comprensione di come i valori della variabile dipendente evolvano nel tempo.\n\nprint(fixef(mix_mod))\n\n(Intercept)        time \n  0.5015932   0.2515722 \n\n\nSi noti inoltre che le stime degli effetti fissi del modello misto sono identiche a quelle che vengono trovate usando un modello di regressione standard:\n\nlm(y1 ~ time, data = d)\n\n\nCall:\nlm(formula = y1 ~ time, data = d)\n\nCoefficients:\n(Intercept)         time  \n     0.5016       0.2516  \n\n\nConsideriamo ora le stime della varianza nel modello a crescita latente.\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  i ~~                                                \n    s                 0.220    0.050    4.371    0.000\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .y0                0.310    0.042    7.308    0.000\n   .y1                0.220    0.021   10.338    0.000\n   .y2                0.230    0.029    7.935    0.000\n   .y3                0.275    0.064    4.295    0.000\n    i                 0.973    0.076   12.854    0.000\n    s                 0.986    0.066   14.889    0.000\nConfrontiamo questi valori con quelli ottenuti dal modello misto.\n\nVarCorr(mix_mod) |&gt;\n    print()\n\n Groups   Name        Std.Dev. Corr \n subject  (Intercept) 1.01217       \n          time        1.01494  0.150\n Residual             0.49108       \n\n\nSi noti che il modello a crescita latente, per impostazione predefinita, assume una varianza eterogenea per ogni rilevazione temporale. I modelli misti, invece, per impostazione predefinita assumono la stessa varianza per ogni punto temporale. È però possibile specificare una stima separata della varianza nelle diverse rilevazioni temporali.\nSe vincoliamo le varianze ad essere uguali per ciascuna rilevazione temporale nel modello LGM, i due modelli producono delle stime identiche. La sintassi seguente viene utilizzata per forzare l’uguaglianza delle varianze in ciascuna rilevazione temporale.\n\nmodel &lt;- \"\n    # intercept and slope with fixed coefficients\n    i =~ 1*y0 + 1*y1 + 1*y2 + 1*y3\n    s =~ 0*y0 + 1*y1 + 2*y2 + 3*y3\n    y0 ~~ resvar*y0\n    y1 ~~ resvar*y1\n    y2 ~~ resvar*y2\n    y3 ~~ resvar*y3\n\"\n\nAdattiamo il nuovo modello ai dati.\n\ngrowth_curve_model &lt;- growth(model, data = dwide)\n\nEsaminiamo i risultati.\n\nsummary(growth_curve_model) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 27 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n  Number of equality constraints                     3\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                                 6.180\n  Degrees of freedom                                 8\n  P-value (Chi-square)                           0.627\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  i =~                                                \n    y0                1.000                           \n    y1                1.000                           \n    y2                1.000                           \n    y3                1.000                           \n  s =~                                                \n    y0                0.000                           \n    y1                1.000                           \n    y2                2.000                           \n    y3                3.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  i ~~                                                \n    s                 0.154    0.051    3.034    0.002\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    i                 0.502    0.049   10.278    0.000\n    s                 0.252    0.046    5.423    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .y0      (rsvr)    0.241    0.011   22.361    0.000\n   .y1      (rsvr)    0.241    0.011   22.361    0.000\n   .y2      (rsvr)    0.241    0.011   22.361    0.000\n   .y3      (rsvr)    0.241    0.011   22.361    0.000\n    i                 1.022    0.076   13.502    0.000\n    s                 1.028    0.068   15.095    0.000\n\n\n\nPer lme4 abbiamo:\n\nprint(VarCorr(mix_mod), comp = \"Var\")\n\n Groups   Name        Variance Cov  \n subject  (Intercept) 1.02448       \n          time        1.03011  0.154\n Residual             0.24116       \n\n\nIn entrambi i casi, la varianza residua è uguale a 0.241 e la correlazione tra intercette e pendenze casuali è uguale a 0.154.\nInoltre, le stime dei coefficienti casuali del modello misto sono identiche a quelle delle variabili latenti.\n\ncoef(mix_mod)[[1]] |&gt; \n    head()\n\n\nA data.frame: 6 x 2\n\n\n\n(Intercept)\ntime\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n-0.3967486\n-0.9050565\n\n\n2\n-1.5328622\n-1.5945199\n\n\n3\n0.5429873\n0.8759706\n\n\n4\n0.3095727\n-0.7887484\n\n\n5\n2.0327226\n3.5319068\n\n\n6\n2.0645454\n-1.1411935\n\n\n\n\n\n\nlavPredict(growth_curve_model) |&gt;\n    head()\n\n\nA matrix: 6 x 2 of type dbl\n\n\ni\ns\n\n\n\n\n-0.3966515\n-0.9050631\n\n\n-1.5324914\n-1.5946260\n\n\n0.5430942\n0.8759036\n\n\n0.3094388\n-0.7886563\n\n\n2.0328124\n3.5317637\n\n\n2.0637121\n-1.1407804",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>LGM e modelli misti</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/06_lgm_mixed.html#riflessioni-conclusive",
    "href": "chapters/lgm/06_lgm_mixed.html#riflessioni-conclusive",
    "title": "72  LGM e modelli misti",
    "section": "72.6 Riflessioni Conclusive",
    "text": "72.6 Riflessioni Conclusive\nIn conclusione, abbiamo visto che, nel caso più semplice in cui viene assunta la stessa varianza per ogni punto temporale, i modelli LGM producono risultati identici ai modelli misti. Tuttavia, la concettualizzazione del cambiamento nei termini di un modello a crescita latente offre molti vantaggi rispetto alla descrizione dei dati nei termini dei modelli misti in quanto i modelli LGM sono più flessibili e consentono la verifica di ipotesi statistiche che non possono essere esaminate nel contesto dei modelli misti.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>LGM e modelli misti</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/03_time_effects.html",
    "href": "chapters/lgm/03_time_effects.html",
    "title": "73  Dati longitudinali",
    "section": "",
    "text": "73.1 Introduzione\nL’obiettivo di questo capitolo è esaminare come è possibile estendere i modelli SEM per adattarli alle particolarità dei dati longitudinali. Per semplificare, cominciamo concentrandoci su due misurazioni temporali consecutive.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Dati longitudinali</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/03_time_effects.html#misurare-il-cambiamento",
    "href": "chapters/lgm/03_time_effects.html#misurare-il-cambiamento",
    "title": "73  Dati longitudinali",
    "section": "73.2 Misurare il Cambiamento",
    "text": "73.2 Misurare il Cambiamento\nIl modo più semplice per valutare il cambiamento individuale tra due momenti temporali consiste nel calcolare la differenza tra i punteggi ottenuti nelle due occasioni. Tuttavia, questa strategia ha un limite significativo: non disponiamo del punteggio “vero” dell’individuo in ciascun momento, ma solo di una misura influenzata dall’errore di misurazione. L’errore di misurazione può ridurre notevolmente la precisione delle stime sulle differenze individuali, compromettendo l’interpretazione del cambiamento.\nPer superare questo problema negli studi longitudinali, vengono impiegati i modelli di crescita latente (Latent Growth Models, LGM), appartenenti alla famiglia dei modelli a equazioni strutturali (SEM). Questi modelli permettono di stimare traiettorie di cambiamento per ciascun individuo, separando le componenti latenti dal rumore delle misurazioni.\n\n73.2.1 Componenti del Modello di Crescita Latente\nNei LGM, si assume che ogni individuo segua una propria traiettoria di cambiamento nel tempo. I dati osservati possono essere scomposti in tre componenti principali:\n\nPunteggi latenti: rappresentano il livello individuale del costrutto in un dato momento.\nPunteggi di cambiamento latenti: indicano il cambiamento individuale nel tempo.\nCaratteristiche uniche non osservate: includono gli errori di misurazione specifici per ogni momento.\n\nL’equazione generale del modello SEM è espressa come:\n\\[\n\\Sigma = \\Lambda \\Psi \\Lambda' + \\Theta,\n\\]\ndove:\n\n$ $ rappresenta la matrice delle varianze e covarianze teoriche.\n$ $ è la matrice dei carichi fattoriali, che descrive le relazioni tra indicatori e costrutti latenti.\n$ $ indica le varianze e covarianze tra i fattori latenti.\n$ $ rappresenta le varianze residue e covarianze tra gli errori di misura.\n\n\n\n73.2.2 Struttura del Modello di Misurazione Longitudinale\nIn un modello longitudinale, si definiscono tre fattori latenti principali:\n\nUn fattore che rappresenta il livello di base del costrutto in un dato momento.\nUn fattore che rappresenta il cambiamento nel costrutto tra momenti temporali.\nUn fattore che rappresenta l’errore di misurazione specifico per ciascun momento.\n\nPer illustrare come funziona questo modello, consideriamo un costrutto misurato in due occasioni. Il punteggio osservato di un individuo in un tempo specifico può essere descritto dalla formula:\n\\[\nx_{it} = \\tau_i + (1)\\xi_1 + (t)\\xi_2 + \\delta_{it},\n\\]\ndove:\n\n\\(\\tau_i\\) è il livello iniziale dell’individuo \\(i\\),\n\\(\\xi_1\\) rappresenta il livello latente al tempo \\(t_1\\),\n\\(\\xi_2\\) rappresenta il cambiamento latente tra i due momenti,\n\\(\\delta_{it}\\) è l’errore di misurazione specifico per l’individuo \\(i\\) al tempo \\(t\\).\n\n\n\n73.2.3 Modello per Più Occasioni di Misurazione\nQuando vengono utilizzati più indicatori in ciascun momento, la struttura del modello può essere rappresentata come segue:\n\\[\n\\begin{align}\nx_{1} &= 0 + (1)\\xi_{1} + (0)\\xi_{2} + \\delta_{1} \\notag\\\\\nx_{2} &= 0 + (1)\\xi_{1} + (1)\\xi_{2} + \\delta_{2} \\notag\\\\\nx_{3} &= 0 + (1)\\xi_{1} + (2)\\xi_{2} + \\delta_{3} \\notag\\\\\nx_{4} &= 0 + (1)\\xi_{1} + (4)\\xi_{2} + \\delta_{4} \\notag\\\\\nx_{5} &= 0 + (1)\\xi_{1} + (5)\\xi_{2} + \\delta_{5} \\notag\n\\end{align}\n\\]\nInoltre, nel modello si ipotizza una correlazione tra \\(\\xi_1\\) e \\(\\xi_2\\), rappresentata dalla matrice di intercorrelazione dei fattori:\n\\[\n\\mathbf{\\Phi} =\n\\begin{bmatrix}\n\\phi_{11} & \\\\\n\\phi_{21} & \\phi_{22}\n\\end{bmatrix}\n\\]\ndove:\n\n\\(\\phi_{11}\\) è la varianza dell’intercetta latente (livello di base),\n\\(\\phi_{22}\\) è la varianza della pendenza latente (cambiamento nel tempo),\n\\(\\phi_{21}\\) è la covarianza tra intercetta e pendenza, utile per comprendere come il livello iniziale sia associato alla velocità di cambiamento.\n\n\n\n73.2.4 Correlazioni tra Varianze Residue\nUn aspetto distintivo dei modelli longitudinali è la possibilità di correlare le varianze residue degli stessi indicatori misurati in momenti diversi (ad esempio, la correlazione tra X1 al Tempo 1 e X1 al Tempo 2). Questo consente di distinguere tra le informazioni stabili del costrutto nel tempo e le variazioni specifiche di ciascun indicatore a ogni misurazione.\n\n\n73.2.5 Interpretazione e Utilità\nQuesto approccio permette di esaminare in modo approfondito lo sviluppo o il cambiamento di un costrutto latente e dei suoi indicatori nel tempo, fornendo un quadro dettagliato delle dinamiche individuali e collettive di cambiamento.\nIl modello di crescita latente definito da queste equazioni produce previsioni sulla struttura delle medie e delle covarianze dei dati osservati. Queste previsioni sono utilizzate nel contesto della modellizzazione delle equazioni strutturali per stimare i parametri e valutare l’adattamento del modello ai dati. La struttura delle covarianze prevista dal modello è:\n\\[\n\\boldsymbol{\\Sigma} = \\boldsymbol{\\Lambda} \\boldsymbol{\\Phi} \\boldsymbol{\\Lambda}' + \\boldsymbol{\\Theta}.\n\\]\nLa figura Figura 73.1 rappresenta graficamente il percorso del modello di crescita latente (LGM) che stiamo analizzando.\n\n\n\n\n\n\nFigura 73.1: Modello di crescita latente.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Dati longitudinali</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/03_time_effects.html#la-variazione-temporale-di-positive-affect",
    "href": "chapters/lgm/03_time_effects.html#la-variazione-temporale-di-positive-affect",
    "title": "73  Dati longitudinali",
    "section": "73.3 La Variazione Temporale di Positive Affect",
    "text": "73.3 La Variazione Temporale di Positive Affect\nApplichiamo questo modello al caso in cui tre indicatori di Positive Affect (Glad, Cheerful, Happy) vengono misurati in due momenti del tempo (si veda Little (2023)).\nImportiamo i dati.\n\ndat &lt;- read.table(\n    file = \"../../data/grade7and8.long.823.dat\",\n    col.names = c(\n        \"PAT1P1\", \"PAT1P2\", \"PAT1P3\", \"NAT1P1\", \"NAT1P2\", \"NAT1P3\",\n        \"PAT2P1\", \"PAT2P2\", \"PAT2P3\", \"NAT2P1\", \"NAT2P2\", \"NAT2P3\",\n        \"PAT3P1\", \"PAT3P2\", \"PAT3P3\", \"NAT3P1\", \"NAT3P2\", \"NAT3P3\",\n        \"grade\", \"female\", \"black\", \"hispanic\", \"other\"\n    )\n)\nglimpse(dat)\n\nRows: 823\nColumns: 23\n$ PAT1P1   &lt;dbl&gt; 1.50000, 2.98116, 3.50000, 3.00000, 3.00000, 3.00000, 3.0~\n$ PAT1P2   &lt;dbl&gt; 1.50000, 2.98284, 4.00000, 3.50000, 2.50000, 2.50000, 2.5~\n$ PAT1P3   &lt;dbl&gt; 2.00000, 2.98883, 4.00000, 2.50000, 3.00000, 3.00000, 4.0~\n$ NAT1P1   &lt;dbl&gt; 2.50000, 1.56218, 1.50000, 1.50000, 1.00000, 1.50000, 1.0~\n$ NAT1P2   &lt;dbl&gt; 3.50000, 1.45688, 1.00000, 2.00000, 1.00000, 2.50000, 1.0~\n$ NAT1P3   &lt;dbl&gt; 3.00000, 1.65477, 1.00000, 1.50000, 1.00000, 2.50000, 1.0~\n$ PAT2P1   &lt;dbl&gt; 4.00000, 4.00000, 4.00000, 2.95942, 3.17170, 2.00000, 3.0~\n$ PAT2P2   &lt;dbl&gt; 4.00000, 4.00000, 2.50000, 2.99083, 2.87806, 2.00000, 3.0~\n$ PAT2P3   &lt;dbl&gt; 4.00000, 4.00000, 4.00000, 3.06670, 3.11031, 3.00000, 4.0~\n$ NAT2P1   &lt;dbl&gt; 2.00000, 1.00000, 1.00000, 1.65159, 1.65777, 2.00000, 1.0~\n$ NAT2P2   &lt;dbl&gt; 1.00000, 1.00000, 1.00000, 1.42599, 1.44804, 2.00000, 1.0~\n$ NAT2P3   &lt;dbl&gt; 2.00000, 1.00000, 1.00000, 1.67184, 1.56296, 2.00000, 1.0~\n$ PAT3P1   &lt;dbl&gt; 4.00000, 4.00000, 4.00000, 4.00000, 2.67109, 3.00000, 2.5~\n$ PAT3P2   &lt;dbl&gt; 4.00000, 4.00000, 4.00000, 3.50000, 2.85851, 2.00000, 2.0~\n$ PAT3P3   &lt;dbl&gt; 4.00000, 4.00000, 3.48114, 3.50000, 3.28099, 2.50000, 3.5~\n$ NAT3P1   &lt;dbl&gt; 1.00000, 1.00000, 1.18056, 1.00000, 1.19869, 2.00000, 1.0~\n$ NAT3P2   &lt;dbl&gt; 1.00000, 1.00000, 1.00000, 1.50000, 1.00000, 2.00000, 1.0~\n$ NAT3P3   &lt;dbl&gt; 2.50000, 1.00000, 1.62051, 1.00000, 1.00000, 3.00000, 1.0~\n$ grade    &lt;int&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ~\n$ female   &lt;int&gt; 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, ~\n$ black    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ~\n$ hispanic &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ~\n$ other    &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ~\n\n\nLa specificazione del modello SEM longitudinale per questi dati in lavaan può essere formulata in modo simile a un modello CFA per un singolo momento del tempo. In questo caso, ci sono due fattori comuni, che chiameremo Fattore_T1 e Fattore_T2, che vengono identificati dagli indicatori misurati nei due momenti del tempo. Questi due fattori comuni sono correlati tra loro.\nTuttavia, la differenza chiave rispetto ai casi precedenti è che i fattori specifici di ciascun indicatore nei due momenti del tempo sono anche correlati tra loro. Questo significa che, oltre alla correlazione tra i fattori comuni Fattore_T1 e Fattore_T2, dobbiamo anche specificare la correlazione tra i fattori specifici dei singoli indicatori nei due momenti del tempo.\n\nmod_1 &lt;- \"\n  # Definizione dei fattori latenti al tempo T1\n  Fattore_T1 =~ NA*PAT1P1 + PAT1P2 + PAT1P3\n\n  # Definizione dei fattori latenti al tempo T2\n  Fattore_T2 =~ NA*PAT2P1 + PAT2P2 + PAT2P3\n\n  # Varianza dei fattori latenti\n  Fattore_T1 ~~ 1*Fattore_T1\n  Fattore_T2 ~~ 1*Fattore_T2\n\n  # Covarianza tra i fattori latenti\n  Fattore_T1 ~~ Fattore_T2\n\n  # Definizione degli errori di misurazione per gli indicatori al tempo T1\n  PAT1P1 ~~ PAT1P1\n  PAT1P2 ~~ PAT1P2\n  PAT1P3 ~~ PAT1P3\n\n  # Definizione degli errori di misurazione per gli indicatori al tempo T2\n  PAT2P1 ~~ PAT2P1\n  PAT2P2 ~~ PAT2P2\n  PAT2P3 ~~ PAT2P3\n\n  # Covarianze tra i residui degli item tra T1 e T2\n  PAT1P1 ~~ PAT2P1\n  PAT1P2 ~~ PAT2P2\n  PAT1P3 ~~ PAT2P3\n\n  # Opzionale: Specifica delle medie degli indicatori (intercette)\n  PAT1P1 ~ 1\n  PAT1P2 ~ 1\n  PAT1P3 ~ 1\n  PAT2P1 ~ 1\n  PAT2P2 ~ 1\n  PAT2P3 ~ 1\n\"\n\nLe covarianze tra gli errori degli indicatori corrispondenti tra T1 e T2 sono stimate, indicando potenziali correlazioni tra gli errori degli stessi indicatori nei due momenti temporali.\nIn questo modello, i carichi fattoriali e le intercettazioni non sono ancora eguagliati nel tempo, il che significa che ogni set di indicatori è libero di avere relazioni uniche con il proprio fattore latente in ciascun momento temporale.\nQuesto modello è definito “configural-invariant” perché mantiene la stessa struttura fattoriale (o configurazione) nel tempo, ma non impone ancora l’equivalenza dei parametri tra i due momenti temporali.\nIl modello configural-invariant è spesso il punto di partenza per testare l’invarianza longitudinale in SEM, poiché stabilisce una base di confronto prima di imporre vincoli più stringenti come l’invarianza dei carichi fattoriali o delle intercette nei modelli successivi.\nAdattiamo il modello ai dati.\n\nfit_1 &lt;- lavaan::sem(mod_1, data = dat, meanstructure = TRUE)\n\n\nparameterEstimates(fit_1) |&gt; print()\n\n          lhs op        rhs   est    se       z pvalue ci.lower ci.upper\n1  Fattore_T1 =~     PAT1P1 0.670 0.022  30.862  0.000    0.628    0.713\n2  Fattore_T1 =~     PAT1P2 0.661 0.021  31.241  0.000    0.619    0.702\n3  Fattore_T1 =~     PAT1P3 0.643 0.021  29.979  0.000    0.601    0.685\n4  Fattore_T2 =~     PAT2P1 0.689 0.021  32.994  0.000    0.648    0.730\n5  Fattore_T2 =~     PAT2P2 0.680 0.021  33.049  0.000    0.639    0.720\n6  Fattore_T2 =~     PAT2P3 0.639 0.021  31.155  0.000    0.598    0.679\n7  Fattore_T1 ~~ Fattore_T1 1.000 0.000      NA     NA    1.000    1.000\n8  Fattore_T2 ~~ Fattore_T2 1.000 0.000      NA     NA    1.000    1.000\n9  Fattore_T1 ~~ Fattore_T2 0.552 0.027  20.141  0.000    0.498    0.606\n10     PAT1P1 ~~     PAT1P1 0.135 0.010  12.919  0.000    0.114    0.155\n11     PAT1P2 ~~     PAT1P2 0.121 0.010  12.308  0.000    0.102    0.141\n12     PAT1P3 ~~     PAT1P3 0.145 0.010  14.046  0.000    0.125    0.165\n13     PAT2P1 ~~     PAT2P1 0.102 0.008  12.160  0.000    0.086    0.119\n14     PAT2P2 ~~     PAT2P2 0.098 0.008  11.997  0.000    0.082    0.114\n15     PAT2P3 ~~     PAT2P3 0.125 0.009  14.711  0.000    0.108    0.142\n16     PAT1P1 ~~     PAT2P1 0.012 0.006   1.946  0.052    0.000    0.025\n17     PAT1P2 ~~     PAT2P2 0.005 0.006   0.884  0.377   -0.006    0.017\n18     PAT1P3 ~~     PAT2P3 0.011 0.006   1.781  0.075   -0.001    0.024\n19     PAT1P1 ~1            2.992 0.027 112.316  0.000    2.940    3.044\n20     PAT1P2 ~1            2.896 0.026 111.210  0.000    2.845    2.947\n21     PAT1P3 ~1            3.112 0.026 119.527  0.000    3.061    3.163\n22     PAT2P1 ~1            3.002 0.026 113.400  0.000    2.950    3.054\n23     PAT2P2 ~1            2.909 0.026 111.532  0.000    2.858    2.960\n24     PAT2P3 ~1            3.127 0.025 122.862  0.000    3.077    3.177\n25 Fattore_T1 ~1            0.000 0.000      NA     NA    0.000    0.000\n26 Fattore_T2 ~1            0.000 0.000      NA     NA    0.000    0.000\n\n\n\nsemPaths(fit_1,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 0.9,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\n\nfitMeasures(fit_1, c(\"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n\n  cfi   tli rmsea  srmr \n1.000 0.999 0.016 0.010 \n\n\nPotremmo pensare che modello di baseline (con cui possono essere confrontati i modelli che descrivono il cambiamento temporale) sia semplicemente il modello in cui non sono permesse covarianze.\n\nmod_2 &lt;- \"\n  # Definizione dei fattori latenti al tempo T1\n  Fattore_T1 =~ NA*PAT1P1 + PAT1P2 + PAT1P3\n\n  # Definizione dei fattori latenti al tempo T2\n  Fattore_T2 =~ NA*PAT2P1 + PAT2P2 + PAT2P3\n\n  # Varianza dei fattori latenti\n  Fattore_T1 ~~ 1*Fattore_T1\n  Fattore_T2 ~~ 1*Fattore_T2\n\n  # Covarianza tra i fattori latenti\n  Fattore_T1 ~~ 0*Fattore_T2\n\n  # Definizione degli errori di misurazione per gli indicatori al tempo T1\n  PAT1P1 ~~ PAT1P1\n  PAT1P2 ~~ PAT1P2\n  PAT1P3 ~~ PAT1P3\n\n  # Definizione degli errori di misurazione per gli indicatori al tempo T2\n  PAT2P1 ~~ PAT2P1\n  PAT2P2 ~~ PAT2P2\n  PAT2P3 ~~ PAT2P3\n\n  # Covarianze tra i residui degli item tra T1 e T2\n  # PAT1P1 ~~ PAT2P1\n  # PAT1P2 ~~ PAT2P2\n  # PAT1P3 ~~ PAT2P3\n\n  # Opzionale: Specifica delle medie degli indicatori (intercette)\n  PAT1P1 ~ 1\n  PAT1P2 ~ 1\n  PAT1P3 ~ 1\n  PAT2P1 ~ 1\n  PAT2P2 ~ 1\n  PAT2P3 ~ 1\n\"\n\n\nfit_2 &lt;- lavaan::sem(mod_2, data = dat, meanstructure = TRUE)\n\n\nsemPaths(fit_2,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 0.9,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\n\nparameterEstimates(fit_2) |&gt; print()\n\n          lhs op        rhs   est    se       z pvalue ci.lower ci.upper\n1  Fattore_T1 =~     PAT1P1 0.670 0.022  30.720      0    0.627    0.713\n2  Fattore_T1 =~     PAT1P2 0.658 0.021  30.943      0    0.616    0.700\n3  Fattore_T1 =~     PAT1P3 0.647 0.021  30.105      0    0.605    0.689\n4  Fattore_T2 =~     PAT2P1 0.689 0.021  32.917      0    0.648    0.730\n5  Fattore_T2 =~     PAT2P2 0.681 0.021  33.073      0    0.641    0.722\n6  Fattore_T2 =~     PAT2P3 0.636 0.021  30.961      0    0.596    0.676\n7  Fattore_T1 ~~ Fattore_T1 1.000 0.000      NA     NA    1.000    1.000\n8  Fattore_T2 ~~ Fattore_T2 1.000 0.000      NA     NA    1.000    1.000\n9  Fattore_T1 ~~ Fattore_T2 0.000 0.000      NA     NA    0.000    0.000\n10     PAT1P1 ~~     PAT1P1 0.135 0.011  12.641      0    0.114    0.156\n11     PAT1P2 ~~     PAT1P2 0.125 0.010  12.310      0    0.105    0.145\n12     PAT1P3 ~~     PAT1P3 0.141 0.010  13.503      0    0.120    0.161\n13     PAT2P1 ~~     PAT2P1 0.102 0.009  11.872      0    0.085    0.119\n14     PAT2P2 ~~     PAT2P2 0.097 0.008  11.609      0    0.080    0.113\n15     PAT2P3 ~~     PAT2P3 0.127 0.009  14.716      0    0.110    0.144\n16     PAT1P1 ~1            2.992 0.027 112.344      0    2.940    3.044\n17     PAT1P2 ~1            2.896 0.026 111.246      0    2.845    2.947\n18     PAT1P3 ~1            3.112 0.026 119.412      0    3.061    3.163\n19     PAT2P1 ~1            3.002 0.026 113.387      0    2.950    3.054\n20     PAT2P2 ~1            2.909 0.026 111.468      0    2.858    2.960\n21     PAT2P3 ~1            3.127 0.025 123.031      0    3.077    3.177\n22 Fattore_T1 ~1            0.000 0.000      NA     NA    0.000    0.000\n23 Fattore_T2 ~1            0.000 0.000      NA     NA    0.000    0.000\n\n\n\nfitMeasures(fit_2, c(\"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n\n  cfi   tli rmsea  srmr \n0.931 0.886 0.187 0.253 \n\n\nTuttavia, Little (2023) fa notare che, nel contesto dei disegni longitudinali, il modello di base adeguato prevede che vengano aggiunte al modello nullo delle aspettative aggiuntive, specificatamente che le medie e le varianze rimangano invariate nel tempo. Questa specificazione ampliata del modello nullo fornisce il confronto appropriato per analizzare e interpretare i dati longitudinali.\n\nmod_3 &lt;- \"\n  # Definizione dei fattori latenti al tempo T1\n  Fattore_T1 =~ NA*PAT1P1 + b1*PAT1P2 + b2*PAT1P3\n\n  # Definizione dei fattori latenti al tempo T2\n  Fattore_T2 =~ NA*PAT2P1 + b1*PAT2P2 + b2*PAT2P3\n\n  # Varianza dei fattori latenti\n  Fattore_T1 ~~ c1*Fattore_T1\n  Fattore_T2 ~~ c1*Fattore_T2\n\n  # Covarianza tra i fattori latenti (assumendo che sia 0)\n  Fattore_T1 ~~ 0*Fattore_T2\n\n  # Definizione degli errori di misurazione per gli indicatori al tempo T1\n  PAT1P1 ~~ a1*PAT1P1\n  PAT1P2 ~~ a2*PAT1P2\n  PAT1P3 ~~ a3*PAT1P3\n\n  # Definizione degli errori di misurazione per gli indicatori al tempo T2\n  PAT2P1 ~~ a1*PAT2P1\n  PAT2P2 ~~ a2*PAT2P2\n  PAT2P3 ~~ a3*PAT2P3\n\n  # Specifica delle medie degli indicatori (intercettazioni) uguali tra i due tempi\n  # PAT1P1 ~ m1\n  # PAT1P2 ~ m2\n  # PAT1P3 ~ m3\n  # PAT2P1 ~ m1\n  # PAT2P2 ~ m2\n  # PAT2P3 ~ m3\n\"\n\n\nfit_3 &lt;- lavaan::sem(mod_3, data = dat, meanstructure = TRUE)\n\n\nsemPaths(fit_3,\n    whatLabels = \"std\",\n    sizeMan = 10,\n    edge.label.cex = 0.9,\n    style = \"mx\",\n    nCharNodes = 0, nCharEdges = 0\n)\n\n\n\n\n\n\n\n\n\nparameterEstimates(fit_3) |&gt; print()\n\n          lhs op        rhs label   est    se       z pvalue ci.lower\n1  Fattore_T1 =~     PAT1P1       0.863 0.018  47.730      0    0.828\n2  Fattore_T1 =~     PAT1P2    b1 0.854 0.012  71.968      0    0.831\n3  Fattore_T1 =~     PAT1P3    b2 0.818 0.012  66.282      0    0.794\n4  Fattore_T2 =~     PAT2P1       0.871 0.018  48.118      0    0.835\n5  Fattore_T2 =~     PAT2P2    b1 0.854 0.012  71.968      0    0.831\n6  Fattore_T2 =~     PAT2P3    b2 0.818 0.012  66.282      0    0.794\n7  Fattore_T1 ~~ Fattore_T1    c1 0.614 0.014  44.667      0    0.587\n8  Fattore_T2 ~~ Fattore_T2    c1 0.614 0.014  44.667      0    0.587\n9  Fattore_T1 ~~ Fattore_T2       0.000 0.000      NA     NA    0.000\n10     PAT1P1 ~~     PAT1P1    a1 0.119 0.007  17.380      0    0.105\n11     PAT1P2 ~~     PAT1P2    a2 0.111 0.007  16.933      0    0.098\n12     PAT1P3 ~~     PAT1P3    a3 0.134 0.007  19.941      0    0.121\n13     PAT2P1 ~~     PAT2P1    a1 0.119 0.007  17.380      0    0.105\n14     PAT2P2 ~~     PAT2P2    a2 0.111 0.007  16.933      0    0.098\n15     PAT2P3 ~~     PAT2P3    a3 0.134 0.007  19.941      0    0.121\n16     PAT1P1 ~1                  2.992 0.026 113.076      0    2.940\n17     PAT1P2 ~1                  2.896 0.026 111.098      0    2.844\n18     PAT1P3 ~1                  3.112 0.026 120.907      0    3.062\n19     PAT2P1 ~1                  3.002 0.027 112.656      0    2.949\n20     PAT2P2 ~1                  2.909 0.026 111.616      0    2.858\n21     PAT2P3 ~1                  3.127 0.026 121.471      0    3.076\n22 Fattore_T1 ~1                  0.000 0.000      NA     NA    0.000\n23 Fattore_T2 ~1                  0.000 0.000      NA     NA    0.000\n   ci.upper\n1     0.898\n2     0.878\n3     0.842\n4     0.906\n5     0.878\n6     0.842\n7     0.641\n8     0.641\n9     0.000\n10    0.132\n11    0.123\n12    0.147\n13    0.132\n14    0.123\n15    0.147\n16    3.044\n17    2.947\n18    3.163\n19    3.054\n20    2.960\n21    3.177\n22    0.000\n23    0.000\n\n\n\nfitMeasures(fit_3, c(\"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; print()\n\n  cfi   tli rmsea  srmr \n0.927 0.916 0.160 0.254 \n\n\nPossiamo ora fare il confronto tra il modello di cambiamento latente e l’appropriato modello di confronto.\n\nlavTestLRT(fit_1, fit_3) |&gt; print()\n\n\nChi-Squared Difference Test\n\n      Df    AIC    BIC    Chisq Chisq diff  RMSEA Df diff Pr(&gt;Chisq)    \nfit_1  5 7427.8 7531.4   6.0645                                         \nfit_3 13 7693.5 7759.5 287.8078     281.74 0.2039       8  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nÈ evidente che, nel contesto di questi dati, un modello che presuma l’assenza di qualsiasi cambiamento è completamente inadeguato.\n\n\n\n\nLittle, T. D. (2023). Longitudinal structural equation modeling. Guilford Press.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Dati longitudinali</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/05_intro_panel.html",
    "href": "chapters/lgm/05_intro_panel.html",
    "title": "74  Specificare e Interpretare un Modello Longitudinale",
    "section": "",
    "text": "74.1 Introduzione\nNel presente capitolo esploreremo i modelli panel longitudinali tradizionali, con un’attenzione particolare alla struttura simplex e alle tecniche per ottimizzare, interpretare e arricchire questi modelli con covariate e analisi degli effetti indiretti.\nI modelli panel e i modelli SEM longitudinali si concentrano sulle relazioni predittive tra variabili latenti (ad esempio, atteggiamenti o abilità) e sulle loro variazioni nel tempo. Sebbene spesso usati come sinonimi, i modelli panel differiscono dai modelli di Confirmatory Factor Analysis (CFA) longitudinali: mentre i CFA analizzano la stabilità dei livelli medi dei costrutti, i modelli panel esplorano le interazioni dinamiche tra variabili nel corso del tempo. Un’importante distinzione va fatta anche con i modelli di crescita latente (LGM), che hanno l’obiettivo di mappare l’evoluzione temporale dei livelli medi di un costrutto, come il monitoraggio dello sviluppo di una competenza specifica.\nInoltre, le relazioni di regressione nei modelli panel suggeriscono un’interpretazione causale, ma questa deve essere affrontata con cautela. La causalità in questi modelli è implicata quando si osservano effetti predittivi coerenti nel tempo, ma è essenziale che i dati siano raccolti con rigore per permettere inferenze causali. Un elemento fondamentale è il controllo delle variabili confondenti, che permette di ridurre il rischio di bias e di migliorare la robustezza delle inferenze.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>74</span>  <span class='chapter-title'>Specificare e Interpretare un Modello Longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/05_intro_panel.html#il-processo-di-cambiamento-simplex",
    "href": "chapters/lgm/05_intro_panel.html#il-processo-di-cambiamento-simplex",
    "title": "74  Specificare e Interpretare un Modello Longitudinale",
    "section": "74.2 Il Processo di Cambiamento Simplex",
    "text": "74.2 Il Processo di Cambiamento Simplex\nUn modello efficace per rappresentare il cambiamento continuo e graduale nel tempo è la struttura simplex. Questa struttura si basa sull’assunto che gli individui cambino a un ritmo stabile, con influenze esterne minime. Nel modello simplex, la correlazione tra punti temporali decresce in modo prevedibile, secondo una progressione graduale. La Tabella 1 illustra una struttura di correlazione simplex in cui la stabilità decresce col passare del tempo.\nTabella 1.  Esempi di Strutture di Correlazione Simplex.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nT1\nT2\nT3\nT4\nT5\nT6\nT7\nT8\n\n\n\n\nT1\n–\n.800\n.640\n.512\n.410\n.328\n.262\n.210\n\n\nT2\n.528\n–\n.800\n.640\n.512\n.410\n.328\n.262\n\n\nT3\n.279\n.528\n–\n.800\n.640\n.512\n.410\n.328\n\n\nT4\n.147\n.279\n.528\n–\n.800\n.640\n.512\n.410\n\n\nT5\n.078\n.147\n.279\n.528\n–\n.800\n.640\n.512\n\n\nT6\n.041\n.078\n.147\n.279\n.528\n–\n.800\n.640\n\n\nT7\n.022\n.041\n.078\n.147\n.279\n.528\n–\n.800\n\n\nT8\n.011\n.022\n.041\n.078\n.147\n.279\n.528\n–\n\n\n\nNota: Le correlazioni sopra la diagonale sono basate sull’analogia del mescolamento delle carte con una correlazione autoregressiva di .528. Le correlazioni sotto la diagonale si basano su una stabilità iniziale più elevata (.80), indicando una persistenza più forte nel tempo.\nNel modello simplex, il cambiamento graduale e stabile è rappresentato da un coefficiente di stabilità costante tra punti temporali consecutivi, simile alla correlazione tra l’ordine delle carte in un mazzo dopo diverse mescolate.\nL’esempio del mazzo di carte mescolato è una metafora utile per comprendere come funziona una struttura di correlazione simplex e il concetto di autoregressività nei modelli longitudinali. Immaginiamo di avere un mazzo di carte perfettamente ordinato, in cui ogni carta ha una posizione specifica. Ogni volta che mescoliamo il mazzo, l’ordine cambia, ma non in modo totalmente casuale: la disposizione iniziale ha ancora una certa influenza sull’ordine risultante dopo la mescolata.\n\n74.2.1 Mescolamento e Correlazione\nSe consideriamo l’ordine delle carte prima e dopo una singola mescolata, possiamo calcolare la correlazione tra la posizione delle carte iniziale e quella dopo il mescolamento. Una sola mescolata modifica la disposizione delle carte, ma mantiene una certa somiglianza con l’ordine iniziale: diciamo, per esempio, che la correlazione è di 0.528. Questa correlazione rappresenta la stabilità del cambiamento: dopo una singola mescolata, le carte non sono ancora completamente in un ordine casuale.\nOgni successiva mescolata riduce ulteriormente questa correlazione. Dopo una seconda mescolata, la correlazione tra l’ordine originale e il nuovo ordine sarà inferiore, ad esempio 0.279. Con il terzo mescolamento, la correlazione continua a decrescere, e così via. Dopo circa sette mescolate perfette, l’ordine diventa quasi del tutto casuale, con una correlazione vicino a 0 rispetto all’ordine iniziale.\n\n\n74.2.2 Cosa Rappresenta nel Contesto dei Modelli Longitudinali\nIn un modello longitudinale con struttura simplex, ogni “mescolata” rappresenta un passaggio temporale in cui un fenomeno cambia gradualmente ma in modo prevedibile. La correlazione tra i punti temporali successivi diminuisce man mano che ci si allontana dal punto di partenza, proprio come la correlazione dell’ordine delle carte diminuisce con ogni mescolata.\n\nCorrelazione tra punti temporali consecutivi: rappresenta la stabilità immediata del costrutto. Più è alta la correlazione tra misurazioni consecutive, maggiore è la stabilità del fenomeno nel tempo.\nCorrelazione tra punti temporali distanti: rappresenta quanto il fenomeno rimanga stabile su periodi più lunghi. Una diminuzione graduale della correlazione, come nell’esempio del mazzo di carte, è tipica di processi che cambiano in modo costante ma senza grandi sconvolgimenti improvvisi.\n\nIn conclusione, l’esempio del mazzo di carte ci aiuta a visualizzare come un modello simplex cattura il cambiamento graduale e prevedibile in un processo. Ogni passaggio temporale influenza il successivo, ma con il tempo questa influenza diminuisce, portando a una correlazione minore tra i punti temporali distanti.\nQuesta struttura è utile nei modelli panel longitudinali perché descrive una dinamica di cambiamento continua e coerente, tipica di molti fenomeni psicologici e sociali che evolvono in modo graduale e prevedibile nel tempo.\n\n\n\n\n\n\nFigura 74.1: Stime dei parametri standardizzati dal modello simplex di mazzi di carte mescolati consecutivamente. Nota. Queste stime dei parametri provengono da un modello adattato ai dati nella tabella precedente. Questo modello ha 21 gradi di libertà e un adattamento perfetto del modello. Le correlazioni tra i mazzi separati da più di una mescolata sono riprodotte tracciando i percorsi di regressione tra ciascun mazzo consecutivo. Le linee tratteggiate mostrano le correlazioni riprodotte con l’ordine iniziale del mazzo. (Figura tratta da Little, 2023)\n\n\n\n\n\n74.2.3 Modelli Simplex e Modelli Autoregressivi (AR1 e AR2)\nIn termini formali, possiamo dire che la struttura simplex può essere vista come un’istanza di un modello autoregressivo di primo ordine (AR1), in cui ogni punto temporale è correlato solo con il precedente. Nei modelli AR1, l’effetto di ogni variabile dipende unicamente dalla sua osservazione immediatamente precedente. Per processi che mostrano maggiore persistenza nel tempo, si può invece adottare un modello autoregressivo di secondo ordine (AR2), dove ogni punto è influenzato non solo dal precedente, ma anche dal punto ancora precedente.\nIl modello AR2 suggerisce che l’influenza persiste per due passaggi temporali, implicando una stabilità più duratura rispetto al modello AR1. Questo approccio è utile per rappresentare processi in cui l’effetto di un evento non si dissipa immediatamente, ma ha un’influenza estesa nel tempo.\n\n\n74.2.4 Applicazioni della Struttura Simplex nella Ricerca Psicologica\nIn psicologia e scienze sociali, il modello simplex è frequentemente usato per studiare processi di cambiamento in campioni longitudinali. La semplicità di questa struttura la rende una scelta ideale per rappresentare fenomeni evolutivi graduali, come lo sviluppo di competenze o il cambiamento di atteggiamenti. La struttura simplex può anche essere estesa con l’inclusione di variabili contestuali, il che ne aumenta la flessibilità senza compromettere la chiarezza.\nPer comprendere la natura di un processo di cambiamento, è essenziale che il modello predittivo catturi correttamente il ritmo del cambiamento stesso. Una frequenza di misurazione adeguata permette di rilevare con precisione la velocità e la consistenza delle variazioni, aumentando la validità delle inferenze che si possono trarre.\nIn sintesi, i modelli panel longitudinali e la struttura simplex offrono potenti strumenti per analizzare il cambiamento e le relazioni temporali nei dati longitudinali. La struttura simplex, in particolare, è una rappresentazione versatile ed efficace dei processi di cambiamento graduale, utile per studiare fenomeni psicologici ed evolutivi in modo teoricamente informato e statisticamente robusto.\n\n\n74.2.5 Modello Simplex per il Mescolamento di Carte\nEsaminiamo qui di seguto l’implementazione del modello Simplex proposta da Little (2023) per i dati artificiali relativi all’esempio del mazzo di carte discusso in precedenza.\n\ntri_corr &lt;- c(\n    1, rep(0, 7),\n    0.523, 1, rep(0, 6),\n    0.279, 0.523, 1, rep(0, 5),\n    0.147, 0.279, 0.523, 1, rep(0, 4),\n    0.078, 0.147, 0.279, 0.523, 1, rep(0, 3),\n    0.041, 0.078, 0.147, 0.279, 0.523, 1, rep(0, 2),\n    0.022, 0.041, 0.078, 0.147, 0.279, 0.523, 1, 0,\n    0.011, 0.022, 0.041, 0.078, 0.147, 0.279, 0.523, 1\n)\nupper &lt;- matrix(tri_corr, 8, byrow = FALSE)\nlower &lt;- matrix(tri_corr, 8, byrow = TRUE)\nmycorr &lt;- upper + lower - diag(8)\n\nrownames(mycorr) &lt;- c(\"Time1\", \"Time2\", \"Time3\", \"Time4\", \"Time5\", \"Time6\", \"Time7\", \"Time8\")\ncolnames(mycorr) &lt;- c(\"Time1\", \"Time2\", \"Time3\", \"Time4\", \"Time5\", \"Time6\", \"Time7\", \"Time8\")\nmynob &lt;- 166\n\nmycorr |&gt; print()\n\n      Time1 Time2 Time3 Time4 Time5 Time6 Time7 Time8\nTime1 1.000 0.523 0.279 0.147 0.078 0.041 0.022 0.011\nTime2 0.523 1.000 0.523 0.279 0.147 0.078 0.041 0.022\nTime3 0.279 0.523 1.000 0.523 0.279 0.147 0.078 0.041\nTime4 0.147 0.279 0.523 1.000 0.523 0.279 0.147 0.078\nTime5 0.078 0.147 0.279 0.523 1.000 0.523 0.279 0.147\nTime6 0.041 0.078 0.147 0.279 0.523 1.000 0.523 0.279\nTime7 0.022 0.041 0.078 0.147 0.279 0.523 1.000 0.523\nTime8 0.011 0.022 0.041 0.078 0.147 0.279 0.523 1.000\n\n\n\nmod6_2 &lt;- \"\n    Time2 ~ Time1\n    Time3 ~ Time2\n    Time4 ~ Time3\n    Time5 ~ Time4\n    Time6 ~ Time5\n    Time7 ~ Time6\n    Time8 ~ Time7\n\n    Time1 ~~ 1*Time1\n    Time2 ~~ Time2\n    Time3 ~~ Time3\n    Time4 ~~ Time4\n    Time5 ~~ Time5\n    Time6 ~~ Time6\n    Time7 ~~ Time7\n    Time8 ~~ Time8\n\"\n\n\nfit6_2 &lt;- lavaan(mod6_2, sample.cov = mycorr, sample.nobs = mynob, fixed.x = FALSE)\n\n\nparameterEstimates(fit6_2) |&gt; print()\n\n     lhs op   rhs   est    se     z pvalue ci.lower ci.upper\n1  Time2  ~ Time1 0.523 0.066 7.930      0    0.394    0.652\n2  Time3  ~ Time2 0.523 0.066 7.912      0    0.393    0.653\n3  Time4  ~ Time3 0.523 0.066 7.908      0    0.393    0.653\n4  Time5  ~ Time4 0.523 0.066 7.906      0    0.393    0.653\n5  Time6  ~ Time5 0.523 0.066 7.906      0    0.393    0.653\n6  Time7  ~ Time6 0.523 0.066 7.906      0    0.393    0.653\n7  Time8  ~ Time7 0.523 0.066 7.906      0    0.393    0.653\n8  Time1 ~~ Time1 1.000 0.000    NA     NA    1.000    1.000\n9  Time2 ~~ Time2 0.722 0.079 9.110      0    0.567    0.877\n10 Time3 ~~ Time3 0.722 0.079 9.110      0    0.567    0.877\n11 Time4 ~~ Time4 0.722 0.079 9.110      0    0.567    0.877\n12 Time5 ~~ Time5 0.722 0.079 9.110      0    0.567    0.877\n13 Time6 ~~ Time6 0.722 0.079 9.110      0    0.567    0.877\n14 Time7 ~~ Time7 0.722 0.079 9.110      0    0.567    0.877\n15 Time8 ~~ Time8 0.722 0.079 9.110      0    0.567    0.877\n\n\nNel commentare il modello Simplex specificato, si può osservare che, per i dati artificiali in questione, la stima della correlazione tra costrutti latenti in momenti successivi risulta costante, con un valore di 0.523. Questo dato è in linea con i risultati ottenuti da Little (2023), che riporta una correlazione di 0.528. È importante notare la consistenza in queste stime, indicativa di una relazione stabile nel tempo tra i costrutti.\nInoltre, il modello mostra che la varianza delle variabili latenti rimane relativamente costante nel tempo, con un valore di 0.722. Questo suggerisce che, nonostante il passare del tempo e i possibili cambiamenti nei costrutti, la quantità di varianza che essi spiegano rimane simile. Un’eccezione a questo schema si trova nella varianza al Tempo 1, che è stata fissata a 1. Questa scelta metodologica è comune in molti modelli di serie temporali per stabilire un punto di riferimento o una scala di misurazione per le varianze nei tempi successivi.\nRifocalizziamoci sulle correlazioni nella parte inferiore della diagonale della Tabella 1, dove possiamo osservare una stabilità piuttosto elevata tra punti temporali successivi, con una correlazione di 0.80 tra ciascun punto temporale e il successivo. Tuttavia, all’aumentare dell’intervallo tra le misurazioni, la correlazione tra punti temporali distanti diminuisce in modo graduale e prevedibile. Ad esempio, nella tabella, la correlazione tra Tempo 1 e Tempo 3 è di 0.64, lo stesso valore che troviamo tra Tempo 2 e Tempo 4, tra Tempo 5 e Tempo 7, e così via per ogni coppia di punti temporali separati da uno spazio temporale intermedio. Alla massima distanza, la correlazione tra Tempo 1 e Tempo 8 è ancora leggermente positiva, pari a 0.210; con un numero crescente di punti temporali, questa correlazione si avvicinerebbe gradualmente a zero.\nQuesto schema di correlazioni evidenzia che, mentre la stabilità a breve termine (tra punti temporali adiacenti) è elevata, essa diminuisce all’aumentare della distanza tra le misurazioni. Questo riflette una riduzione dell’influenza o della connessione tra i costrutti latenti misurati a intervalli temporali più lunghi.\nNel triangolo superiore della tabella, la stabilità tra punti temporali adiacenti è più bassa, con una correlazione di 0.528. Qui, i punti temporali bi-contigui (separati da un intervallo intermedio) si correlano a 0.279 e la correlazione tra i punti più distanti, da Tempo 1 a Tempo 8, scende a 0.011, praticamente nulla. Entrambi questi schemi riflettono un tasso costante di cambiamento e sono ben rappresentati da un modello autoregressivo simplex.\nUn modello autoregressivo simplex è in grado di riprodurre tutte queste correlazioni attraverso effetti indiretti: nel modello, l’influenza del Tempo 1 sul Tempo 8 viene trasmessa indirettamente attraverso una sequenza di influenze dirette da un punto temporale al successivo (es., da Tempo 1 a Tempo 2, da Tempo 2 a Tempo 3, e così via fino a Tempo 8). Questo passaggio continuo di influenze permette di riprodurre lo schema di correlazioni osservato nella Tabella 1. Utilizzando le regole di tracciamento dei percorsi in un modello autoregressivo, possiamo osservare come questo schema di correlazioni diminuisca progressivamente, evidenziando il modo in cui il modello rappresenta il declino della connessione tra i punti temporali man mano che aumenta la distanza tra essi.\n\n0.523^{2:7} |&gt;\n    round(3) |&gt;\n    print()\n\n[1] 0.274 0.143 0.075 0.039 0.020 0.011\n\n\nIl modello presentato nella (little-fig-simplex?) è un modello Simplex univariato perché include un solo costrutto, rappresentato in più punti temporali. È importante notare che, in un modello come questo, si verifica e si garantisce la forte invarianza fattoriale del modello di misurazione. Inoltre, i residui corrispondenti presentano unicità correlate nel tempo per ogni occorrenza dello stesso indicatore. I coefficienti di percorso in questo modello riproducono perfettamente le correlazioni nella parte superiore della Tabella 1.\nLo stesso modello, quando applicato all’altro insieme di correlazioni nella Tabella 1, riprodurrebbe altrettanto perfettamente il pattern di correlazione. Gli effetti diretti in ciascun punto temporale adiacente sarebbero di .8, mentre l’effetto indiretto sarebbe il prodotto multiplo dei coefficienti di percorso diretti. Questo approccio mette in evidenza come le correlazioni tra punti temporali più lontani siano il risultato di una serie di influenze dirette che si susseguono nel tempo.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>74</span>  <span class='chapter-title'>Specificare e Interpretare un Modello Longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/05_intro_panel.html#basi-di-un-modello-panel",
    "href": "chapters/lgm/05_intro_panel.html#basi-di-un-modello-panel",
    "title": "74  Specificare e Interpretare un Modello Longitudinale",
    "section": "74.3 Basi di un Modello Panel",
    "text": "74.3 Basi di un Modello Panel\nConsideriamo ora il modello panel. Nella costruzione di un modello panel, le associazioni tra variabili osservate in momenti diversi sono spesso trasformate in percorsi di regressione direzionali. Le covarianze tra variabili al Tempo 1 sono generalmente viste come associazioni “a ordine zero,” ovvero associazioni non controllate per altre variabili. Nei momenti successivi, le covarianze tra variabili rappresentano invece varianze residue, o “fattori di disturbo,” ossia componenti di varianza non spiegate dagli effetti temporali precedenti.\nUn concetto chiave nei modelli panel è il percorso autoregressivo, che rappresenta la relazione predittiva tra lo stesso costrutto in momenti successivi. In aggiunta, i modelli panel possono includere effetti incrociati ritardati (o cross-lagged), dove una variabile predice un’altra variabile in un momento futuro. Entrambi questi tipi di percorso permettono di osservare le dinamiche temporali e la persistenza di influenze tra variabili.\n\n\n\n\n\n\nFigura 74.2: Etichette dei parametri per tre punti temporali con affetto positivo e affetto negativo: Un’analisi di base del modello panel direzionale. Nota. Si consente l’associazione delle varianze residue tra gli indicatori corrispondenti nel tempo. (Figura adattata da Little, 2023)\n\n\n\n\n74.3.1 Modello Panel per Affetto Positivo e Negativo\nLa figura (little-fig-simplex?) approfondisce l’uso dei modelli longitudinali tramite un esempio di modello CFA per studiare l’affetto positivo e negativo negli adolescenti. A differenza di un’analisi limitata a soli due punti temporali, questo esempio illustra una configurazione più complessa, in cui i due costrutti (affetto positivo e negativo) vengono misurati in tre momenti distinti. Questa struttura a più punti temporali permette di esaminare come i livelli di affetto positivo e negativo cambiano nel tempo e di osservare le interazioni tra i costrutti lungo diverse fasi della misurazione.\nI dati sono i seguenti.\n\ndat &lt;- read.table(\n    file = \"../../data/grade7and8.long.823.dat\",\n    col.names = c(\n        \"PAT1P1\", \"PAT1P2\", \"PAT1P3\", \"NAT1P1\", \"NAT1P2\", \"NAT1P3\",\n        \"PAT2P1\", \"PAT2P2\", \"PAT2P3\", \"NAT2P1\", \"NAT2P2\", \"NAT2P3\",\n        \"PAT3P1\", \"PAT3P2\", \"PAT3P3\", \"NAT3P1\", \"NAT3P2\", \"NAT3P3\",\n        \"grade\", \"female\", \"black\", \"hispanic\", \"other\"\n    )\n)\n\n\npsych::describe(dat[, 1:18])\n\n\nA psych: 18 x 13\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nPAT1P1\n1\n823\n2.991885\n0.7644692\n3.00000\n3.042751\n0.7413000\n1\n4\n3\n-0.4591316\n-0.3486015\n0.02664772\n\n\nPAT1P2\n2\n823\n2.895543\n0.7471531\n3.00000\n2.919860\n0.7413000\n1\n4\n3\n-0.2986814\n-0.3732379\n0.02604412\n\n\nPAT1P3\n3\n823\n3.112289\n0.7481646\n3.00000\n3.175932\n0.7413000\n1\n4\n3\n-0.5761073\n-0.2344880\n0.02607938\n\n\nNAT1P1\n4\n823\n1.706311\n0.7057569\n1.50000\n1.596773\n0.7413000\n1\n4\n3\n1.2112142\n1.2584152\n0.02460114\n\n\nNAT1P2\n5\n823\n1.450148\n0.6576346\n1.00000\n1.312657\n0.0000000\n1\n4\n3\n1.7700996\n3.1319596\n0.02292370\n\n\nNAT1P3\n6\n823\n1.453063\n0.6678318\n1.00000\n1.311948\n0.0000000\n1\n4\n3\n1.8185647\n3.3558790\n0.02327915\n\n\nPAT2P1\n7\n823\n3.001628\n0.7599034\n3.00000\n3.044683\n0.7413000\n1\n4\n3\n-0.4055187\n-0.5083138\n0.02648857\n\n\nPAT2P2\n8\n823\n2.909043\n0.7491414\n3.00000\n2.936239\n0.7413000\n1\n4\n3\n-0.2667719\n-0.5077469\n0.02611343\n\n\nPAT2P3\n9\n823\n3.126799\n0.7295410\n3.09211\n3.189561\n0.8778623\n1\n4\n3\n-0.6282730\n-0.1499079\n0.02543020\n\n\nNAT2P1\n10\n823\n1.695210\n0.6614440\n1.50000\n1.606124\n0.7413000\n1\n4\n3\n1.1219909\n1.2251472\n0.02305649\n\n\nNAT2P2\n11\n823\n1.537798\n0.6225102\n1.50000\n1.429653\n0.7413000\n1\n4\n3\n1.3883623\n1.9372133\n0.02169934\n\n\nNAT2P3\n12\n823\n1.580027\n0.6499109\n1.50000\n1.471439\n0.7413000\n1\n4\n3\n1.3401267\n1.8529497\n0.02265447\n\n\nPAT3P1\n13\n823\n2.886528\n0.7823545\n3.00000\n2.917427\n0.7413000\n1\n4\n3\n-0.2178390\n-0.7223282\n0.02727116\n\n\nPAT3P2\n14\n823\n2.849560\n0.7624570\n3.00000\n2.868214\n0.7413000\n1\n4\n3\n-0.1687695\n-0.6306593\n0.02657758\n\n\nPAT3P3\n15\n823\n3.056508\n0.7484883\n3.00000\n3.107101\n0.7413000\n1\n4\n3\n-0.4511290\n-0.5120333\n0.02609066\n\n\nNAT3P1\n16\n823\n1.723787\n0.6912895\n1.50000\n1.623666\n0.7413000\n1\n4\n3\n1.2328507\n1.5654947\n0.02409684\n\n\nNAT3P2\n17\n823\n1.575689\n0.6600865\n1.50000\n1.469134\n0.7413000\n1\n4\n3\n1.3962112\n2.0916581\n0.02300917\n\n\nNAT3P3\n18\n823\n1.641652\n0.6980201\n1.50000\n1.533856\n0.7413000\n1\n4\n3\n1.1666457\n1.0690468\n0.02433145\n\n\n\n\n\n\nplots_list &lt;- list()\n\n# Creazione della lista di grafici con i pannelli più grandi\nplots_list &lt;- list()\n\nfor (i in 1:16) {\n    col_name &lt;- names(dat)[i]\n    p &lt;- ggplot(dat, aes(x = !!sym(col_name))) +\n        geom_density(fill = \"blue\", color = \"black\", alpha = 0.5) +\n        ggtitle(col_name)\n    plots_list[[i]] &lt;- p\n}\n\n# Organizza e visualizza i grafici con pannelli più grandi\ndo.call(grid.arrange, c(plots_list, ncol = 4)) \n\n\n\n\n\n\n\n\nIniziamo a specificare il modello nullo.\n\nmod_null &lt;- \"\n    PAT1P1 ~~ V1*PAT1P1\n    PAT1P2 ~~ V2*PAT1P2\n    PAT1P3 ~~ V3*PAT1P3\n    NAT1P1 ~~ V4*NAT1P1\n    NAT1P2 ~~ V5*NAT1P2\n    NAT1P3 ~~ V6*NAT1P3\n\n    PAT2P1 ~~ V1*PAT2P1\n    PAT2P2 ~~ V2*PAT2P2\n    PAT2P3 ~~ V3*PAT2P3\n    NAT2P1 ~~ V4*NAT2P1\n    NAT2P2 ~~ V5*NAT2P2\n    NAT2P3 ~~ V6*NAT2P3\n\n    PAT3P1 ~~ V1*PAT3P1\n    PAT3P2 ~~ V2*PAT3P2\n    PAT3P3 ~~ V3*PAT3P3\n    NAT3P1 ~~ V4*NAT3P1\n    NAT3P2 ~~ V5*NAT3P2\n    NAT3P3 ~~ V6*NAT3P3\n\n    PAT1P1 ~ T1*1\n    PAT1P2 ~ T2*1\n    PAT1P3 ~ T3*1\n    NAT1P1 ~ T4*1\n    NAT1P2 ~ T5*1\n    NAT1P3 ~ T6*1\n\n    PAT2P1 ~ T1*1\n    PAT2P2 ~ T2*1\n    PAT2P3 ~ T3*1\n    NAT2P1 ~ T4*1\n    NAT2P2 ~ T5*1\n    NAT2P3 ~ T6*1\n\n    PAT3P1 ~ T1*1\n    PAT3P2 ~ T2*1\n    PAT3P3 ~ T3*1\n    NAT3P1 ~ T4*1\n    NAT3P2 ~ T5*1\n    NAT3P3 ~ T6*1\n\"\n\nIl modello nullo (baseline) è usato da Little (2023) come punto di partenza nell’analisi SEM e per i confronti con modelli più complessi. Il modello nullo specifica sei variabili osservate (PAT1P1, PAT1P2, PAT1P3, NAT1P1, NAT1P2, NAT1P3) misurate in tre punti temporali distinti. Questo implica che ci sono 18 variabili osservate in totale. Ogni variabile osservata ha la propria varianza unica che è stimata nel modello. Le medie delle 6 variabili misurate in ciascuno dei tre punti temporali sono assunte non variare in funzione del tempo. Nonostante il modello prenda in considerazione misurazioni ripetute nel tempo, non vi è alcuna specificazione di correlazioni o percorsi causali tra queste misure nel tempo, come sarebbe tipico per i modelli longitudinali. Essendo un modello nullo, non vengono specificate relazioni tra le variabili (varianze e medie) diverse dai loro effetti unici.\n\nfit_null &lt;- lavaan(mod_null, data = dat, orthogonal = TRUE)\n\n\nsummary(fit_null, standardized = T, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 32 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        36\n  Number of equality constraints                    24\n\n  Number of observations                           823\n\nModel Test User Model:\n                                                       \n  Test statistic                              11213.103\n  Degrees of freedom                                177\n  P-value (Chi-square)                            0.000\n\nModel Test Baseline Model:\n\n  Test statistic                             11131.067\n  Degrees of freedom                               153\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.000\n  Tucker-Lewis Index (TLI)                       0.131\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -15975.145\n  Loglikelihood unrestricted model (H1)     -10368.594\n                                                      \n  Akaike (AIC)                               31974.291\n  Bayesian (BIC)                             32030.846\n  Sample-size adjusted Bayesian (SABIC)      31992.739\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.275\n  90 Percent confidence interval - lower         0.271\n  90 Percent confidence interval - upper         0.280\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.328\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    PAT1P1    (T1)    2.960    0.016  190.946    0.000    2.960    3.843\n    PAT1P2    (T2)    2.885    0.015  190.376    0.000    2.885    3.831\n    PAT1P3    (T3)    3.099    0.015  207.418    0.000    3.099    4.174\n    NAT1P1    (T4)    1.708    0.014  123.730    0.000    1.708    2.490\n    NAT1P2    (T5)    1.521    0.013  116.519    0.000    1.521    2.345\n    NAT1P3    (T6)    1.558    0.014  114.474    0.000    1.558    2.304\n    PAT2P1    (T1)    2.960    0.016  190.946    0.000    2.960    3.843\n    PAT2P2    (T2)    2.885    0.015  190.376    0.000    2.885    3.831\n    PAT2P3    (T3)    3.099    0.015  207.418    0.000    3.099    4.174\n    NAT2P1    (T4)    1.708    0.014  123.730    0.000    1.708    2.490\n    NAT2P2    (T5)    1.521    0.013  116.519    0.000    1.521    2.345\n    NAT2P3    (T6)    1.558    0.014  114.474    0.000    1.558    2.304\n    PAT3P1    (T1)    2.960    0.016  190.946    0.000    2.960    3.843\n    PAT3P2    (T2)    2.885    0.015  190.376    0.000    2.885    3.831\n    PAT3P3    (T3)    3.099    0.015  207.418    0.000    3.099    4.174\n    NAT3P1    (T4)    1.708    0.014  123.730    0.000    1.708    2.490\n    NAT3P2    (T5)    1.521    0.013  116.519    0.000    1.521    2.345\n    NAT3P3    (T6)    1.558    0.014  114.474    0.000    1.558    2.304\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    PAT1P1    (V1)    0.593    0.017   35.135    0.000    0.593    1.000\n    PAT1P2    (V2)    0.567    0.016   35.135    0.000    0.567    1.000\n    PAT1P3    (V3)    0.551    0.016   35.135    0.000    0.551    1.000\n    NAT1P1    (V4)    0.471    0.013   35.135    0.000    0.471    1.000\n    NAT1P2    (V5)    0.421    0.012   35.135    0.000    0.421    1.000\n    NAT1P3    (V6)    0.457    0.013   35.135    0.000    0.457    1.000\n    PAT2P1    (V1)    0.593    0.017   35.135    0.000    0.593    1.000\n    PAT2P2    (V2)    0.567    0.016   35.135    0.000    0.567    1.000\n    PAT2P3    (V3)    0.551    0.016   35.135    0.000    0.551    1.000\n    NAT2P1    (V4)    0.471    0.013   35.135    0.000    0.471    1.000\n    NAT2P2    (V5)    0.421    0.012   35.135    0.000    0.421    1.000\n    NAT2P3    (V6)    0.457    0.013   35.135    0.000    0.457    1.000\n    PAT3P1    (V1)    0.593    0.017   35.135    0.000    0.593    1.000\n    PAT3P2    (V2)    0.567    0.016   35.135    0.000    0.567    1.000\n    PAT3P3    (V3)    0.551    0.016   35.135    0.000    0.551    1.000\n    NAT3P1    (V4)    0.471    0.013   35.135    0.000    0.471    1.000\n    NAT3P2    (V5)    0.421    0.012   35.135    0.000    0.421    1.000\n    NAT3P3    (V6)    0.457    0.013   35.135    0.000    0.457    1.000\n\n\n\n\n\n74.3.2 Modello SEM Iniziale\nNel modello SEM iniziale, Little (2023) definisce sei variabili latenti (Pos1, Pos2, Pos3, Neg1, Neg2, Neg3) che rappresentano costrutti psicologici positivi e negativi misurati in tre diversi momenti temporali. Ogni variabile latente è identificato da tre indicatori (per esempio, Pos1 è identificato da PAT1P1, PAT1P2, PAT1P3), con saturazioni fattoriali L1, L2, L3 che quantificano la relazione tra la variabile latenti e i suoi indicatori. Il modello stima la varianza di ciascuna variabile latente e la covarianza tra variabili latenti diverse. Le medie delle variabili latenti sono impostate a 1, indicando che sono considerate fisse. Il modello include stime per la varianza e la covarianza degli indicatori attraverso il tempo, suggerendo l’esistenza di correlazioni temporali tra gli stessi indicatori misurati in momenti diversi. Ci sono percorsi di regressione che collegano le variabili latenti nel tempo (ad esempio, Pos2 è influenzata da Pos1). Il modello impone alcuni vincoli sulle saturazioni fattoriali e sulle intercette degli indicatori.\nQuesto modello mira a esplorare le relazioni dinamiche e temporali tra variabili latenti, diversamente da un modello di invarianza configurale, che è più orientato alla valutazione della costanza della struttura fattoriale.\n\nSEMmod &lt;- \"\n    Pos1 =~ L1*PAT1P1 + L2*PAT1P2 + L3*PAT1P3\n    Neg1 =~ L4*NAT1P1 + L5*NAT1P2 + L6*NAT1P3\n    Pos2 =~ L1*PAT2P1 + L2*PAT2P2 + L3*PAT2P3\n    Neg2 =~ L4*NAT2P1 + L5*NAT2P2 + L6*NAT2P3\n    Pos3 =~ L1*PAT3P1 + L2*PAT3P2 + L3*PAT3P3\n    Neg3 =~ L4*NAT3P1 + L5*NAT3P2 + L6*NAT3P3\n\n    ## Latent factor variance and covariance (Psi matrix)\n    Pos1 ~~ Pos1 ##Psi 1,1\n    Pos1 ~~ Neg1 ##Psi 1,2\n    Neg1 ~~ Neg1 ##Psi 2,2\n\n    Pos2 ~~ Pos2 ##Psi 3,3\n    Pos2 ~~ Neg2 ##Psi 3,4\n    Neg2 ~~ Neg2 ##Psi 4,4\n\n    Pos3 ~~ Pos3  ##Psi 5,5\n    Pos3 ~~ Neg3  ##Psi 5,6\n    Neg3 ~~ Neg3  ##Psi 6,6\n\n    ## Latent means (Alpha matrix)\n    Pos1 ~ 1\n    Neg1 ~ 1\n    Pos2 ~ 1\n    Neg2 ~ 1\n    Pos3 ~ 1\n    Neg3 ~ 1\n\n    ## indicator resduals var-covar (Theta-Epsilon matrix)\n    ## Time1\n    PAT1P1 ~~ PAT1P1\n    PAT1P1 ~~ PAT2P1\n    PAT1P1 ~~ PAT3P1\n\n    PAT1P2 ~~ PAT1P2\n    PAT1P2 ~~ PAT2P2\n    PAT1P2 ~~ PAT3P2\n\n    PAT1P3 ~~ PAT1P3\n    PAT1P3 ~~ PAT2P3\n    PAT1P3 ~~ PAT3P3\n\n    NAT1P1 ~~ NAT1P1\n    NAT1P1 ~~ NAT2P1\n    NAT1P1 ~~ NAT3P1\n\n    NAT1P2 ~~ NAT1P2\n    NAT1P2 ~~ NAT2P2\n    NAT1P2 ~~ NAT3P2\n\n    NAT1P3 ~~ NAT1P3\n    NAT1P3 ~~ NAT2P3\n    NAT1P3 ~~ NAT3P3\n\n    #Time2\n    PAT2P1 ~~ PAT2P1\n    PAT2P1 ~~ PAT3P1\n\n    PAT2P2 ~~ PAT2P2\n    PAT2P2 ~~ PAT3P2\n\n    PAT2P3 ~~ PAT2P3\n    PAT2P3 ~~ PAT3P3\n\n    NAT2P1 ~~ NAT2P1\n    NAT2P1 ~~ NAT3P1\n\n    NAT2P2 ~~ NAT2P2\n    NAT2P2 ~~ NAT3P2\n\n    NAT2P3 ~~ NAT2P3\n    NAT2P3 ~~ NAT3P3\n\n    ## Time3\n    PAT3P1  ~~ PAT3P1\n    PAT3P2 ~~ PAT3P2\n    PAT3P3 ~~ PAT3P3\n    NAT3P1 ~~ NAT3P1\n    NAT3P2 ~~ NAT3P2\n    NAT3P3 ~~ NAT3P3\n\n    ##Indicator means/intercepts (Tau vector)\n    PAT1P1 ~ t1*1\n    PAT1P2 ~ t2*1\n    PAT1P3 ~ t3*1\n    NAT1P1 ~ t4*1\n    NAT1P2 ~ t5*1\n    NAT1P3 ~ t6*1\n\n    PAT2P1 ~ t1*1\n    PAT2P2 ~ t2*1\n    PAT2P3 ~ t3*1\n    NAT2P1 ~ t4*1\n    NAT2P2 ~ t5*1\n    NAT2P3 ~ t6*1\n\n    PAT3P1 ~ t1*1\n    PAT3P2 ~ t2*1\n    PAT3P3 ~ t3*1\n    NAT3P1 ~ t4*1\n    NAT3P2 ~ t5*1\n    NAT3P3 ~ t6*1\n\n    ##Regression paths here\n    Pos2 ~ Pos1\n    Pos3 ~ Pos1 + Pos2\n    Neg2 ~ Neg1\n    Neg3 ~ Neg1 + Neg2\n\n    ## Constraints\n    L1 == 3 - L2 - L3\n    L4 == 3 - L5 - L6\n    t1 == 0 - t2 - t3\n    t4 == 0 - t5 - t6\n\"\n\n\nfitSEM &lt;- lavaan(SEMmod, data = dat, meanstructure = TRUE)\n\n\nsummary(fitSEM, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 129 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        93\n  Number of equality constraints                    28\n\n  Number of observations                           823\n\nModel Test User Model:\n                                                      \n  Test statistic                               441.520\n  Degrees of freedom                               124\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                             11131.067\n  Degrees of freedom                               153\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.971\n  Tucker-Lewis Index (TLI)                       0.964\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -10589.354\n  Loglikelihood unrestricted model (H1)     -10368.594\n                                                      \n  Akaike (AIC)                               21308.709\n  Bayesian (BIC)                             21615.051\n  Sample-size adjusted Bayesian (SABIC)      21408.635\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.056\n  90 Percent confidence interval - lower         0.050\n  90 Percent confidence interval - upper         0.061\n  P-value H_0: RMSEA &lt;= 0.050                    0.044\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.045\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Pos1 =~                                                               \n    PAT1P1    (L1)    1.040    0.010  103.232    0.000    0.684    0.884\n    PAT1P2    (L2)    0.997    0.010   98.450    0.000    0.655    0.881\n    PAT1P3    (L3)    0.963    0.010   93.311    0.000    0.633    0.856\n  Neg1 =~                                                               \n    NAT1P1    (L4)    0.957    0.012   78.547    0.000    0.553    0.757\n    NAT1P2    (L5)    0.999    0.011   90.776    0.000    0.578    0.891\n    NAT1P3    (L6)    1.044    0.011   94.041    0.000    0.604    0.906\n  Pos2 =~                                                               \n    PAT2P1    (L1)    1.040    0.010  103.232    0.000    0.692    0.910\n    PAT2P2    (L2)    0.997    0.010   98.450    0.000    0.663    0.900\n    PAT2P3    (L3)    0.963    0.010   93.311    0.000    0.641    0.877\n  Neg2 =~                                                               \n    NAT2P1    (L4)    0.957    0.012   78.547    0.000    0.525    0.816\n    NAT2P2    (L5)    0.999    0.011   90.776    0.000    0.548    0.873\n    NAT2P3    (L6)    1.044    0.011   94.041    0.000    0.573    0.890\n  Pos3 =~                                                               \n    PAT3P1    (L1)    1.040    0.010  103.232    0.000    0.699    0.900\n    PAT3P2    (L2)    0.997    0.010   98.450    0.000    0.671    0.864\n    PAT3P3    (L3)    0.963    0.010   93.311    0.000    0.648    0.856\n  Neg3 =~                                                               \n    NAT3P1    (L4)    0.957    0.012   78.547    0.000    0.566    0.837\n    NAT3P2    (L5)    0.999    0.011   90.776    0.000    0.591    0.888\n    NAT3P3    (L6)    1.044    0.011   94.041    0.000    0.617    0.869\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Pos2 ~                                                                \n    Pos1              0.550    0.033   16.667    0.000    0.544    0.544\n  Pos3 ~                                                                \n    Pos1              0.340    0.039    8.822    0.000    0.333    0.333\n    Pos2              0.372    0.038    9.848    0.000    0.368    0.368\n  Neg2 ~                                                                \n    Neg1              0.445    0.033   13.494    0.000    0.468    0.468\n  Neg3 ~                                                                \n    Neg1              0.285    0.038    7.470    0.000    0.279    0.279\n    Neg2              0.408    0.040   10.113    0.000    0.379    0.379\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Pos1 ~~                                                               \n    Neg1             -0.063    0.015   -4.293    0.000   -0.166   -0.166\n .Pos2 ~~                                                               \n   .Neg2             -0.050    0.011   -4.503    0.000   -0.183   -0.183\n .Pos3 ~~                                                               \n   .Neg3             -0.074    0.011   -6.758    0.000   -0.288   -0.288\n .PAT1P1 ~~                                                             \n   .PAT2P1            0.010    0.006    1.630    0.103    0.010    0.089\n   .PAT3P1            0.007    0.007    1.039    0.299    0.007    0.058\n .PAT1P2 ~~                                                             \n   .PAT2P2            0.007    0.006    1.236    0.217    0.007    0.065\n   .PAT3P2            0.013    0.007    1.922    0.055    0.013    0.097\n .PAT1P3 ~~                                                             \n   .PAT2P3            0.012    0.006    1.866    0.062    0.012    0.088\n   .PAT3P3            0.012    0.007    1.723    0.085    0.012    0.081\n .NAT1P1 ~~                                                             \n   .NAT2P1            0.027    0.007    3.603    0.000    0.027    0.151\n   .NAT3P1            0.009    0.007    1.230    0.219    0.009    0.052\n .NAT1P2 ~~                                                             \n   .NAT2P2            0.001    0.005    0.105    0.916    0.001    0.006\n   .NAT3P2            0.006    0.005    1.169    0.242    0.006    0.066\n .NAT1P3 ~~                                                             \n   .NAT2P3           -0.001    0.005   -0.178    0.859   -0.001   -0.011\n   .NAT3P3           -0.008    0.006   -1.410    0.159   -0.008   -0.081\n .PAT2P1 ~~                                                             \n   .PAT3P1           -0.000    0.006   -0.068    0.946   -0.000   -0.004\n .PAT2P2 ~~                                                             \n   .PAT3P2            0.005    0.006    0.776    0.438    0.005    0.039\n .PAT2P3 ~~                                                             \n   .PAT3P3            0.010    0.006    1.538    0.124    0.010    0.072\n .NAT2P1 ~~                                                             \n   .NAT3P1            0.011    0.006    1.810    0.070    0.011    0.081\n .NAT2P2 ~~                                                             \n   .NAT3P2            0.008    0.005    1.621    0.105    0.008    0.088\n .NAT2P3 ~~                                                             \n   .NAT3P3           -0.004    0.006   -0.674    0.500   -0.004   -0.037\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    Pos1              3.000    0.024  124.676    0.000    4.564    4.564\n    Neg1              1.520    0.021   71.493    0.000    2.629    2.629\n   .Pos2              1.361    0.101   13.447    0.000    2.047    2.047\n   .Neg2              0.929    0.053   17.429    0.000    1.693    1.693\n   .Pos3              0.787    0.107    7.373    0.000    1.171    1.171\n   .Neg3              0.558    0.064    8.746    0.000    0.944    0.944\n   .PAT1P1    (t1)   -0.140    0.031   -4.557    0.000   -0.140   -0.181\n   .PAT1P2    (t2)   -0.088    0.031   -2.855    0.004   -0.088   -0.118\n   .PAT1P3    (t3)    0.228    0.031    7.238    0.000    0.228    0.308\n   .NAT1P1    (t4)    0.174    0.021    8.467    0.000    0.174    0.239\n   .NAT1P2    (t5)   -0.068    0.018   -3.706    0.000   -0.068   -0.105\n   .NAT1P3    (t6)   -0.106    0.018   -5.760    0.000   -0.106   -0.159\n   .PAT2P1    (t1)   -0.140    0.031   -4.557    0.000   -0.140   -0.184\n   .PAT2P2    (t2)   -0.088    0.031   -2.855    0.004   -0.088   -0.120\n   .PAT2P3    (t3)    0.228    0.031    7.238    0.000    0.228    0.312\n   .NAT2P1    (t4)    0.174    0.021    8.467    0.000    0.174    0.271\n   .NAT2P2    (t5)   -0.068    0.018   -3.706    0.000   -0.068   -0.108\n   .NAT2P3    (t6)   -0.106    0.018   -5.760    0.000   -0.106   -0.165\n   .PAT3P1    (t1)   -0.140    0.031   -4.557    0.000   -0.140   -0.180\n   .PAT3P2    (t2)   -0.088    0.031   -2.855    0.004   -0.088   -0.114\n   .PAT3P3    (t3)    0.228    0.031    7.238    0.000    0.228    0.301\n   .NAT3P1    (t4)    0.174    0.021    8.467    0.000    0.174    0.258\n   .NAT3P2    (t5)   -0.068    0.018   -3.706    0.000   -0.068   -0.102\n   .NAT3P3    (t6)   -0.106    0.018   -5.760    0.000   -0.106   -0.150\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    Pos1              0.432    0.024   18.356    0.000    1.000    1.000\n    Neg1              0.334    0.018   18.207    0.000    1.000    1.000\n   .Pos2              0.311    0.018   17.618    0.000    0.704    0.704\n   .Neg2              0.235    0.014   17.253    0.000    0.781    0.781\n   .Pos3              0.281    0.017   16.941    0.000    0.621    0.621\n   .Neg3              0.237    0.014   16.908    0.000    0.679    0.679\n   .PAT1P1            0.131    0.010   13.048    0.000    0.131    0.219\n   .PAT1P2            0.124    0.009   13.290    0.000    0.124    0.224\n   .PAT1P3            0.147    0.010   14.824    0.000    0.147    0.268\n   .NAT1P1            0.228    0.013   17.665    0.000    0.228    0.427\n   .NAT1P2            0.086    0.007   11.609    0.000    0.086    0.206\n   .NAT1P3            0.080    0.008   10.338    0.000    0.080    0.180\n   .PAT2P1            0.099    0.008   12.300    0.000    0.099    0.171\n   .PAT2P2            0.104    0.008   13.261    0.000    0.104    0.191\n   .PAT2P3            0.124    0.008   14.911    0.000    0.124    0.231\n   .NAT2P1            0.139    0.009   16.035    0.000    0.139    0.335\n   .NAT2P2            0.094    0.007   13.123    0.000    0.094    0.238\n   .NAT2P3            0.086    0.007   11.805    0.000    0.086    0.208\n   .PAT3P1            0.115    0.010   11.900    0.000    0.115    0.190\n   .PAT3P2            0.153    0.011   14.459    0.000    0.153    0.253\n   .PAT3P3            0.153    0.010   14.904    0.000    0.153    0.267\n   .NAT3P1            0.136    0.009   15.347    0.000    0.136    0.299\n   .NAT3P2            0.094    0.008   12.323    0.000    0.094    0.212\n   .NAT3P3            0.123    0.009   13.525    0.000    0.123    0.244\n\nConstraints:\n                                               |Slack|\n    L1 - (3-L2-L3)                               0.000\n    L4 - (3-L5-L6)                               0.000\n    t1 - (0-t2-t3)                               0.000\n    t4 - (0-t5-t6)                               0.000",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>74</span>  <span class='chapter-title'>Specificare e Interpretare un Modello Longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/05_intro_panel.html#invarianza",
    "href": "chapters/lgm/05_intro_panel.html#invarianza",
    "title": "74  Specificare e Interpretare un Modello Longitudinale",
    "section": "74.4 Invarianza",
    "text": "74.4 Invarianza\nL’invarianza nei modelli SEM panel è fondamentale per garantire che le misurazioni di un costrutto siano comparabili nel tempo. Nei modelli longitudinali, l’invarianza implica che il significato e la struttura di un costrutto rimangano stabili attraverso diverse occasioni di misurazione. Senza l’invarianza, qualsiasi cambiamento osservato potrebbe riflettere variazioni nella misurazione stessa, piuttosto che un cambiamento reale nel costrutto. Testare l’invarianza nei modelli SEM panel consente quindi di distinguere tra cambiamenti reali e artefatti della misurazione, supportando un’interpretazione valida delle traiettorie di sviluppo.\n\n74.4.1 Modello di Invarianza Configurale\nIn un modello di invarianza configurale, ci si aspetta che la struttura fattoriale, cioè il numero di fattori e il pattern di carichi fattoriali, sia la stessa in tutti i gruppi o momenti temporali considerati.\nOgni variabile latente (Pos1, Pos2, Pos3, Neg1, Neg2, Neg3) è misurata da un set specifico di indicatori in ciascuno dei tre momenti temporali. Ad esempio, Pos1 è misurata da PAT1P1, PAT1P2, e PAT1P3. I carichi fattoriali (L1, L2, L3, ecc.) sono specificati separatamente per ogni momento temporale. I vincoli imposti (ad esempio, L1 == 3 - L2 - L3) indicano che ci sono alcune restrizioni nella relazione tra i carichi fattoriali. Questi vincoli sono utilizzati per testare l’uguaglianza dei carichi attraverso i diversi tempi.\nIl modello stima separatamente la varianza di ciascun indicatore e di ciascuna variabile latente in ogni momento temporale. Il modello include covarianze sia tra gli indicatori in diversi momenti temporali sia tra le variabili latenti, suggerendo che esiste una correlazione tra le misurazioni nel tempo. Le medie delle variabili latenti e degli indicatori sono specificate imponendo alcuni vincoli (ad esempio, t1 == 0 - t2 - t3).\nI vincoli imposti sui carichi fattoriali e sulle medie degli indicatori permettono di testare se la struttura fattoriale è consistente nel tempo, che è l’essenza dell’invarianza configurale.\n\nmod_config &lt;- \"\n    Pos1 =~ L1*PAT1P1 + L2*PAT1P2 + L3*PAT1P3\n    Neg1 =~ L4*NAT1P1 + L5*NAT1P2 + L6*NAT1P3\n    Pos2 =~ L7*PAT2P1 + L8*PAT2P2 + L9*PAT2P3\n    Neg2 =~ L10*NAT2P1 + L11*NAT2P2 + L12*NAT2P3\n    Pos3 =~ L13*PAT3P1 + L14*PAT3P2 + L15*PAT3P3\n    Neg3 =~ L16*NAT3P1 + L17*NAT3P2 + L18*NAT3P3\n    L1 == 3 - L2 - L3\n    L4 == 3 - L5 - L6\n    L7 == 3 - L8 - L9\n    L10== 3 - L11- L12\n    L13== 3 - L14- L15\n    L16== 3 - L17- L18\n\n    PAT1P1~~PAT1P1\n    PAT1P2~~PAT1P2\n    PAT1P3~~PAT1P3\n    NAT1P1~~NAT1P1\n    NAT1P2~~NAT1P2\n    NAT1P3~~NAT1P3\n    PAT2P1~~PAT2P1\n    PAT2P2~~PAT2P2\n    PAT2P3~~PAT2P3\n    NAT2P1~~NAT2P1\n    NAT2P2~~NAT2P2\n    NAT2P3~~NAT2P3\n    PAT3P1~~PAT3P1\n    PAT3P2~~PAT3P2\n    PAT3P3~~PAT3P3\n    NAT3P1~~NAT3P1\n    NAT3P2~~NAT3P2\n    NAT3P3~~NAT3P3\n\n    Pos1~~Pos1\n    Neg1~~Neg1\n    Pos2~~Pos2\n    Neg2~~Neg2\n    Pos3~~Pos3\n    Neg3~~Neg3\n\n    PAT1P1~~PAT2P1 + PAT3P1\n    PAT2P1~~PAT3P1\n    PAT1P2~~PAT2P2 + PAT3P2\n    PAT2P2~~PAT3P2\n    PAT1P3~~PAT2P3 + PAT3P3\n    PAT2P3~~PAT3P3\n    NAT1P1~~NAT2P1 + NAT3P1\n    NAT2P1~~NAT3P1\n    NAT1P2~~NAT2P2 + NAT3P2\n    NAT2P2~~NAT3P2\n    NAT1P3~~NAT2P3 + NAT3P3\n    NAT2P3~~NAT3P3\n\n    Pos1~~Pos2 + Pos3 + Neg1 + Neg2 + Neg3\n    Pos2~~Pos3 + Neg1 + Neg2 + Neg3\n    Pos3~~Neg1 + Neg2 + Neg3\n    Neg1~~Neg2 + Neg3\n    Neg2~~Neg3\n\n    Pos1~NA*1\n    Neg1~NA*1\n    Pos2~NA*1\n    Neg2~NA*1\n    Pos3~NA*1\n    Neg3~NA*1\n\n    PAT1P1~t1*1\n    PAT1P2~t2*1\n    PAT1P3~t3*1\n    NAT1P1~t4*1\n    NAT1P2~t5*1\n    NAT1P3~t6*1\n    PAT2P1~t7*1\n    PAT2P2~t8*1\n    PAT2P3~t9*1\n    NAT2P1~t10*1\n    NAT2P2~t11*1\n    NAT2P3~t12*1\n    PAT3P1~t13*1\n    PAT3P2~t14*1\n    PAT3P3~t15*1\n    NAT3P1~t16*1\n    NAT3P2~t17*1\n    NAT3P3~t18*1\n\n    t1 == 0 - t2 - t3\n    t4 == 0 - t5 - t6\n    t7 == 0 - t8 - t9\n    t10== 0 - t11- t12\n    t13== 0 - t14- t15\n    t16== 0 - t17- t18\n\"\n\n\nfit_config &lt;- lavaan(mod_config, data = dat, meanstructure = TRUE)\n\n\nsummary(fit_config, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 160 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        99\n  Number of equality constraints                    12\n\n  Number of observations                           823\n\nModel Test User Model:\n                                                      \n  Test statistic                               352.232\n  Degrees of freedom                               102\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                             11131.067\n  Degrees of freedom                               153\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.977\n  Tucker-Lewis Index (TLI)                       0.966\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -10544.710\n  Loglikelihood unrestricted model (H1)     -10368.594\n                                                      \n  Akaike (AIC)                               21263.420\n  Bayesian (BIC)                             21673.447\n  Sample-size adjusted Bayesian (SABIC)      21397.168\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.055\n  90 Percent confidence interval - lower         0.048\n  90 Percent confidence interval - upper         0.061\n  P-value H_0: RMSEA &lt;= 0.050                    0.108\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.035\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Pos1 =~                                                               \n    PAT1P1    (L1)    1.021    0.018   57.311    0.000    0.672    0.878\n    PAT1P2    (L2)    0.999    0.018   57.022    0.000    0.658    0.882\n    PAT1P3    (L3)    0.980    0.018   54.726    0.000    0.644    0.862\n  Neg1 =~                                                               \n    NAT1P1    (L4)    0.911    0.023   40.011    0.000    0.521    0.740\n    NAT1P2    (L5)    1.032    0.019   53.312    0.000    0.591    0.899\n    NAT1P3    (L6)    1.057    0.020   54.058    0.000    0.605    0.907\n  Pos2 =~                                                               \n    PAT2P1    (L7)    1.031    0.015   67.338    0.000    0.690    0.909\n    PAT2P2    (L8)    1.010    0.015   66.147    0.000    0.676    0.905\n    PAT2P3    (L9)    0.958    0.016   60.988    0.000    0.641    0.877\n  Neg2 =~                                                               \n    NAT2P1   (L10)    0.973    0.020   49.201    0.000    0.537    0.824\n    NAT2P2   (L11)    0.976    0.019   52.312    0.000    0.538    0.866\n    NAT2P3   (L12)    1.052    0.019   55.900    0.000    0.581    0.894\n  Pos3 =~                                                               \n    PAT3P1   (L13)    1.065    0.018   60.101    0.000    0.709    0.907\n    PAT3P2   (L14)    0.981    0.018   53.971    0.000    0.653    0.857\n    PAT3P3   (L15)    0.954    0.018   52.524    0.000    0.636    0.849\n  Neg3 =~                                                               \n    NAT3P1   (L16)    0.994    0.019   52.539    0.000    0.586    0.852\n    NAT3P2   (L17)    0.989    0.018   54.815    0.000    0.583    0.884\n    NAT3P3   (L18)    1.017    0.019   53.509    0.000    0.600    0.861\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .PAT1P1 ~~                                                             \n   .PAT2P1            0.011    0.006    1.701    0.089    0.011    0.091\n   .PAT3P1            0.008    0.007    1.133    0.257    0.008    0.064\n .PAT2P1 ~~                                                             \n   .PAT3P1           -0.000    0.006   -0.039    0.969   -0.000   -0.002\n .PAT1P2 ~~                                                             \n   .PAT2P2            0.007    0.006    1.251    0.211    0.007    0.066\n   .PAT3P2            0.012    0.007    1.782    0.075    0.012    0.089\n .PAT2P2 ~~                                                             \n   .PAT3P2            0.005    0.006    0.732    0.464    0.005    0.037\n .PAT1P3 ~~                                                             \n   .PAT2P3            0.011    0.006    1.740    0.082    0.011    0.083\n   .PAT3P3            0.013    0.007    1.851    0.064    0.013    0.087\n .PAT2P3 ~~                                                             \n   .PAT3P3            0.010    0.006    1.524    0.127    0.010    0.071\n .NAT1P1 ~~                                                             \n   .NAT2P1            0.029    0.007    3.893    0.000    0.029    0.163\n   .NAT3P1            0.010    0.007    1.412    0.158    0.010    0.061\n .NAT2P1 ~~                                                             \n   .NAT3P1            0.009    0.006    1.506    0.132    0.009    0.069\n .NAT1P2 ~~                                                             \n   .NAT2P2            0.000    0.005    0.006    0.996    0.000    0.000\n   .NAT3P2            0.005    0.005    1.009    0.313    0.005    0.058\n .NAT2P2 ~~                                                             \n   .NAT3P2            0.008    0.005    1.660    0.097    0.008    0.087\n .NAT1P3 ~~                                                             \n   .NAT2P3            0.000    0.005    0.002    0.998    0.000    0.000\n   .NAT3P3           -0.006    0.006   -1.016    0.310   -0.006   -0.057\n .NAT2P3 ~~                                                             \n   .NAT3P3           -0.004    0.006   -0.729    0.466   -0.004   -0.039\n  Pos1 ~~                                                               \n    Pos2              0.243    0.019   12.892    0.000    0.552    0.552\n    Pos3              0.230    0.019   12.278    0.000    0.525    0.525\n    Neg1             -0.062    0.015   -4.217    0.000   -0.164   -0.164\n    Neg2             -0.059    0.014   -4.149    0.000   -0.162   -0.162\n    Neg3             -0.014    0.015   -0.913    0.361   -0.035   -0.035\n  Pos2 ~~                                                               \n    Pos3              0.241    0.019   12.725    0.000    0.542    0.542\n  Neg1 ~~                                                               \n    Pos2             -0.058    0.015   -3.963    0.000   -0.152   -0.152\n  Pos2 ~~                                                               \n    Neg2             -0.090    0.014   -6.254    0.000   -0.244   -0.244\n    Neg3             -0.028    0.015   -1.851    0.064   -0.071   -0.071\n  Neg1 ~~                                                               \n    Pos3             -0.010    0.015   -0.717    0.473   -0.027   -0.027\n  Neg2 ~~                                                               \n    Pos3             -0.047    0.014   -3.296    0.001   -0.128   -0.128\n  Pos3 ~~                                                               \n    Neg3             -0.078    0.015   -5.087    0.000   -0.199   -0.199\n  Neg1 ~~                                                               \n    Neg2              0.149    0.013   11.184    0.000    0.472    0.472\n    Neg3              0.149    0.014   10.586    0.000    0.441    0.441\n  Neg2 ~~                                                               \n    Neg3              0.167    0.014   11.998    0.000    0.514    0.514\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    Pos1              3.000    0.024  124.563    0.000    4.560    4.560\n    Neg1              1.537    0.021   72.416    0.000    2.684    2.684\n    Pos2              3.012    0.024  124.258    0.000    4.503    4.503\n    Neg2              1.604    0.020   78.945    0.000    2.907    2.907\n    Pos3              2.931    0.024  120.101    0.000    4.401    4.401\n    Neg3              1.647    0.022   75.997    0.000    2.793    2.793\n   .PAT1P1    (t1)   -0.071    0.054   -1.303    0.192   -0.071   -0.093\n   .PAT1P2    (t2)   -0.103    0.054   -1.916    0.055   -0.103   -0.138\n   .PAT1P3    (t3)    0.174    0.055    3.171    0.002    0.174    0.232\n   .NAT1P1    (t4)    0.307    0.037    8.291    0.000    0.307    0.436\n   .NAT1P2    (t5)   -0.136    0.031   -4.349    0.000   -0.136   -0.206\n   .NAT1P3    (t6)   -0.172    0.031   -5.455    0.000   -0.172   -0.257\n   .PAT2P1    (t7)   -0.106    0.047   -2.243    0.025   -0.106   -0.139\n   .PAT2P2    (t8)   -0.134    0.047   -2.860    0.004   -0.134   -0.180\n   .PAT2P3    (t9)    0.240    0.048    4.960    0.000    0.240    0.328\n   .NAT2P1   (t10)    0.135    0.033    4.058    0.000    0.135    0.207\n   .NAT2P2   (t11)   -0.027    0.031   -0.874    0.382   -0.027   -0.044\n   .NAT2P3   (t12)   -0.108    0.031   -3.419    0.001   -0.108   -0.166\n   .PAT3P1   (t13)   -0.234    0.053   -4.418    0.000   -0.234   -0.299\n   .PAT3P2   (t14)   -0.026    0.054   -0.481    0.631   -0.026   -0.034\n   .PAT3P3   (t15)    0.260    0.054    4.779    0.000    0.260    0.347\n   .NAT3P1   (t16)    0.087    0.033    2.663    0.008    0.087    0.127\n   .NAT3P2   (t17)   -0.054    0.031   -1.724    0.085   -0.054   -0.081\n   .NAT3P3   (t18)   -0.033    0.033   -1.019    0.308   -0.033   -0.048\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .PAT1P1            0.133    0.010   12.941    0.000    0.133    0.228\n   .PAT1P2            0.123    0.010   12.691    0.000    0.123    0.222\n   .PAT1P3            0.144    0.010   14.093    0.000    0.144    0.257\n   .NAT1P1            0.224    0.013   17.831    0.000    0.224    0.452\n   .NAT1P2            0.083    0.008   10.067    0.000    0.083    0.193\n   .NAT1P3            0.078    0.008    9.243    0.000    0.078    0.176\n   .PAT2P1            0.100    0.008   12.156    0.000    0.100    0.174\n   .PAT2P2            0.102    0.008   12.582    0.000    0.102    0.182\n   .PAT2P3            0.124    0.008   14.727    0.000    0.124    0.231\n   .NAT2P1            0.137    0.009   15.465    0.000    0.137    0.322\n   .NAT2P2            0.097    0.007   13.132    0.000    0.097    0.251\n   .NAT2P3            0.084    0.008   10.906    0.000    0.084    0.200\n   .PAT3P1            0.109    0.010   10.536    0.000    0.109    0.178\n   .PAT3P2            0.154    0.011   14.284    0.000    0.154    0.265\n   .PAT3P3            0.156    0.011   14.776    0.000    0.156    0.279\n   .NAT3P1            0.129    0.009   14.221    0.000    0.129    0.274\n   .NAT3P2            0.095    0.008   12.052    0.000    0.095    0.218\n   .NAT3P3            0.125    0.009   13.536    0.000    0.125    0.258\n    Pos1              0.433    0.024   18.354    0.000    1.000    1.000\n    Neg1              0.328    0.018   17.933    0.000    1.000    1.000\n    Pos2              0.448    0.024   18.745    0.000    1.000    1.000\n    Neg2              0.305    0.017   18.153    0.000    1.000    1.000\n    Pos3              0.444    0.024   18.323    0.000    1.000    1.000\n    Neg3              0.348    0.019   18.211    0.000    1.000    1.000\n\nConstraints:\n                                               |Slack|\n    L1 - (3-L2-L3)                               0.000\n    L4 - (3-L5-L6)                               0.000\n    L7 - (3-L8-L9)                               0.000\n    L10 - (3-L11-L12)                            0.000\n    L13 - (3-L14-L15)                            0.000\n    L16 - (3-L17-L18)                            0.000\n    t1 - (0-t2-t3)                               0.000\n    t4 - (0-t5-t6)                               0.000\n    t7 - (0-t8-t9)                               0.000\n    t10 - (0-t11-t12)                            0.000\n    t13 - (0-t14-t15)                            0.000\n    t16 - (0-t17-t18)                            0.000\n\n\n\n\n\n74.4.2 Modello di Invarianza Debole\nIl modello di invarianza debole, o invarianza metrica, è un passo oltre l’invarianza configurale nella SEM per testare l’uguaglianza di costrutti psicologici nel tempo. Mentre l’invarianza configurale si concentra sulla struttura fattoriale (cioè, la presenza e il pattern dei carichi fattoriali), l’invarianza debole agginge il vincolo dell’uguaglianza dei carichi fattoriali nei diversi momenti temporali.\nNel modello successivo, i carichi fattoriali per gli indicatori corrispondenti sono mantenuti costanti nelle tre rilevazioni temporali. Ad esempio, lo stesso valore per L1 è utilizzato per PAT1P1, PAT2P1 e PAT3P1 in tutti e tre i momenti temporali. Questo significa che questo modello verifica se la relazione tra le variabili latenti (Pos e Neg) e i loro indicatori (PAT e NAT) è la stessa nel tempo.\nIl modello stima separatamente la varianza di ciascun indicatore e di ciascuna variabile latente in ogni momento temporale. Questo è simile all’invarianza configurale.\nIl modello include covarianze sia tra gli indicatori in diversi momenti temporali sia tra le variabili latenti, permettendo di catturare la correlazione tra le misurazioni nel tempo.\nViene fatta un’ipotesi sulle medie degli indicatori, come mostrato nelle equazioni PAT1P1 ~ t1*1, ecc. I vincoli sulle medie degli indicatori (ad esempio, t1 == 0 - t2 - t3) suggeriscono che ci sono alcune restrizioni matematiche imposte sulle medie degli indicatori. Prendendo l’equazione t1 == 0 - t2 - t3 come esempio, questa impone una relazione diretta tra tre medie degli indicatori. In pratica, afferma che la media di un indicatore (rappresentata da t1) è definita come l’opposto della somma delle medie di altri due indicatori (t2 e t3). Questo tipo di vincolo può essere interpretato come un meccanismo di bilanciamento. Se t2 e t3 aumentano, allora t1 diminuisce di conseguenza, mantenendo una relazione bilanciata tra queste tre medie.\nMentre l’invarianza configurale richiede solo che la stessa struttura fattoriale sia presente attraverso i gruppi o nel tempo (ad esempio, gli stessi fattori con gli stessi indicatori), l’invarianza debole richiede anche che i carichi fattoriali siano gli stessi. Questo è un test più rigoroso dell’invarianza poiché non solo assume che le stesse variabili latenti siano misurate, ma anche che la forza della relazione tra le variabili latenti e i loro indicatori sia costante.\n\nmod_weak &lt;- \"\n    Pos1 =~ L1*PAT1P1 + L2*PAT1P2 + L3*PAT1P3\n    Neg1 =~ L4*NAT1P1 + L5*NAT1P2 + L6*NAT1P3\n    Pos2 =~ L1*PAT2P1 + L2*PAT2P2 + L3*PAT2P3\n    Neg2 =~ L4*NAT2P1 + L5*NAT2P2 + L6*NAT2P3\n    Pos3 =~ L1*PAT3P1 + L2*PAT3P2 + L3*PAT3P3\n    Neg3 =~ L4*NAT3P1 + L5*NAT3P2 + L6*NAT3P3\n\n    ## indicator residual variances (Theta-Epsilon matrix)\n    PAT1P1 ~~ PAT1P1\n    PAT1P2 ~~ PAT1P2\n    PAT1P3 ~~ PAT1P3\n    NAT1P1 ~~ NAT1P1\n    NAT1P2 ~~ NAT1P2\n    NAT1P3 ~~ NAT1P3\n    PAT2P1 ~~ PAT2P1\n    PAT2P2 ~~ PAT2P2\n    PAT2P3 ~~ PAT2P3\n    NAT2P1 ~~ NAT2P1\n    NAT2P2 ~~ NAT2P2\n    NAT2P3 ~~ NAT2P3\n    PAT3P1 ~~ PAT3P1\n    PAT3P2 ~~ PAT3P2\n    PAT3P3 ~~ PAT3P3\n    NAT3P1 ~~ NAT3P1\n    NAT3P2 ~~ NAT3P2\n    NAT3P3 ~~ NAT3P3\n\n    ### cross-time correlated residuals\n    ## correlate residuals of indicators with themselves across time\n    PAT1P1 ~~ PAT2P1 + PAT3P1\n    PAT2P1 ~~ PAT3P1\n    PAT1P2 ~~ PAT2P2 + PAT3P2\n    PAT2P2 ~~ PAT3P2\n    PAT1P3 ~~ PAT2P3 + PAT3P3\n    PAT2P3 ~~ PAT3P3\n    NAT1P1 ~~ NAT2P1 + NAT3P1\n    NAT2P1 ~~ NAT3P1\n    NAT1P2 ~~ NAT2P2 + NAT3P2\n    NAT2P2 ~~ NAT3P2\n    NAT1P3 ~~ NAT2P3 + NAT3P3\n    NAT2P3 ~~ NAT3P3\n\n    ## indicator intercepts (Tau vector), include labels for model constraints\n    PAT1P1 ~ t1*1\n    PAT1P2 ~ t2*1\n    PAT1P3 ~ t3*1\n    NAT1P1 ~ t4*1\n    NAT1P2 ~ t5*1\n    NAT1P3 ~ t6*1\n\n    PAT2P1 ~ t7*1\n    PAT2P2 ~ t8*1\n    PAT2P3 ~ t9*1\n    NAT2P1 ~ t10*1\n    NAT2P2 ~ t11*1\n    NAT2P3 ~ t12*1\n\n    PAT3P1 ~ t13*1\n    PAT3P2 ~ t14*1\n    PAT3P3 ~ t15*1\n    NAT3P1 ~ t16*1\n    NAT3P2 ~ t17*1\n    NAT3P3 ~ t18*1\n\n    ### latent factor variance (Psi matrix)\n    Pos1 ~~ Pos1\n    Neg1 ~~ Neg1\n    Pos2 ~~ Pos2\n    Neg2 ~~ Neg2\n    Pos3 ~~ Pos3\n    Neg3 ~~ Neg3\n\n    ### factor covariance\n    Pos1 ~~ Pos2 + Pos3 + Neg1 + Neg2 + Neg3\n    Pos2 ~~ Pos3 + Neg1 + Neg2 + Neg3\n    Pos3 ~~ Neg1 + Neg2 + Neg3\n    Neg1 ~~ Neg2 + Neg3\n    Neg2 ~~ Neg3\n\n    ## latent means (Alpha matrix)\n    Pos1 ~ 1\n    Neg1 ~ 1\n    Pos2 ~ 1\n    Neg2 ~ 1\n    Pos3 ~ 1\n    Neg3 ~ 1\n\n    ## model constraints\n    L1 == 3 - L2 - L3\n    L4 == 3 - L5 - L6\n    t1 == 0 - t2 - t3\n    t4 == 0 - t5 - t6\n    t7 == 0 - t8 - t9\n    t10 == 0 - t11 - t12\n    t13 == 0 - t14 - t15\n    t16 == 0 - t17 - t18\n\"\n\n\nfit_wk &lt;- lavaan(mod_weak, data = dat, meanstructure = TRUE)\n#### Did not converge on first run, used final estimates on starting values for next run\nfit_weak &lt;- lavaan(mod_weak, data = dat, meanstructure = TRUE, start = fit_wk)\n\n\nsummary(fit_weak, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 4 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        99\n  Number of equality constraints                    20\n\n  Number of observations                           823\n\nModel Test User Model:\n                                                      \n  Test statistic                               366.562\n  Degrees of freedom                               110\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                             11131.067\n  Degrees of freedom                               153\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.977\n  Tucker-Lewis Index (TLI)                       0.967\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -10551.875\n  Loglikelihood unrestricted model (H1)     -10368.594\n                                                      \n  Akaike (AIC)                               21261.750\n  Bayesian (BIC)                             21634.074\n  Sample-size adjusted Bayesian (SABIC)      21383.200\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.053\n  90 Percent confidence interval - lower         0.047\n  90 Percent confidence interval - upper         0.059\n  P-value H_0: RMSEA &lt;= 0.050                    0.182\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.035\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Pos1 =~                                                               \n    PAT1P1    (L1)    1.038    0.010  102.905    0.000    0.683    0.884\n    PAT1P2    (L2)    0.998    0.010   98.574    0.000    0.656    0.881\n    PAT1P3    (L3)    0.963    0.010   93.200    0.000    0.633    0.856\n  Neg1 =~                                                               \n    NAT1P1    (L4)    0.964    0.012   79.273    0.000    0.558    0.766\n    NAT1P2    (L5)    0.997    0.011   90.664    0.000    0.578    0.891\n    NAT1P3    (L6)    1.039    0.011   93.757    0.000    0.602    0.906\n  Pos2 =~                                                               \n    PAT2P1    (L1)    1.038    0.010  102.905    0.000    0.695    0.911\n    PAT2P2    (L2)    0.998    0.010   98.574    0.000    0.668    0.901\n    PAT2P3    (L3)    0.963    0.010   93.200    0.000    0.645    0.878\n  Neg2 =~                                                               \n    NAT2P1    (L4)    0.964    0.012   79.273    0.000    0.532    0.820\n    NAT2P2    (L5)    0.997    0.011   90.664    0.000    0.550    0.873\n    NAT2P3    (L6)    1.039    0.011   93.757    0.000    0.574    0.889\n  Pos3 =~                                                               \n    PAT3P1    (L1)    1.038    0.010  102.905    0.000    0.693    0.898\n    PAT3P2    (L2)    0.998    0.010   98.574    0.000    0.666    0.864\n    PAT3P3    (L3)    0.963    0.010   93.200    0.000    0.643    0.853\n  Neg3 =~                                                               \n    NAT3P1    (L4)    0.964    0.012   79.273    0.000    0.567    0.840\n    NAT3P2    (L5)    0.997    0.011   90.664    0.000    0.587    0.886\n    NAT3P3    (L6)    1.039    0.011   93.757    0.000    0.612    0.868\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .PAT1P1 ~~                                                             \n   .PAT2P1            0.010    0.006    1.643    0.100    0.010    0.090\n   .PAT3P1            0.008    0.007    1.171    0.242    0.008    0.065\n .PAT2P1 ~~                                                             \n   .PAT3P1            0.000    0.006    0.046    0.963    0.000    0.003\n .PAT1P2 ~~                                                             \n   .PAT2P2            0.008    0.006    1.304    0.192    0.008    0.068\n   .PAT3P2            0.012    0.007    1.761    0.078    0.012    0.089\n .PAT2P2 ~~                                                             \n   .PAT3P2            0.005    0.006    0.796    0.426    0.005    0.040\n .PAT1P3 ~~                                                             \n   .PAT2P3            0.011    0.006    1.784    0.074    0.011    0.084\n   .PAT3P3            0.013    0.007    1.780    0.075    0.013    0.084\n .PAT2P3 ~~                                                             \n   .PAT3P3            0.010    0.006    1.482    0.138    0.010    0.069\n .NAT1P1 ~~                                                             \n   .NAT2P1            0.029    0.007    3.931    0.000    0.029    0.166\n   .NAT3P1            0.011    0.007    1.474    0.141    0.011    0.063\n .NAT2P1 ~~                                                             \n   .NAT3P1            0.010    0.006    1.625    0.104    0.010    0.073\n .NAT1P2 ~~                                                             \n   .NAT2P2           -0.000    0.005   -0.038    0.970   -0.000   -0.002\n   .NAT3P2            0.005    0.005    1.052    0.293    0.005    0.059\n .NAT2P2 ~~                                                             \n   .NAT3P2            0.008    0.005    1.654    0.098    0.008    0.089\n .NAT1P3 ~~                                                             \n   .NAT2P3           -0.000    0.005   -0.079    0.937   -0.000   -0.005\n   .NAT3P3           -0.006    0.006   -1.103    0.270   -0.006   -0.063\n .NAT2P3 ~~                                                             \n   .NAT3P3           -0.004    0.006   -0.780    0.435   -0.004   -0.042\n  Pos1 ~~                                                               \n    Pos2              0.243    0.019   12.903    0.000    0.552    0.552\n    Pos3              0.231    0.019   12.304    0.000    0.526    0.526\n    Neg1             -0.062    0.015   -4.210    0.000   -0.163   -0.163\n    Neg2             -0.059    0.014   -4.163    0.000   -0.162   -0.162\n    Neg3             -0.014    0.015   -0.921    0.357   -0.036   -0.036\n  Pos2 ~~                                                               \n    Pos3              0.243    0.019   12.757    0.000    0.544    0.544\n  Neg1 ~~                                                               \n    Pos2             -0.059    0.015   -3.979    0.000   -0.153   -0.153\n  Pos2 ~~                                                               \n    Neg2             -0.091    0.014   -6.280    0.000   -0.246   -0.246\n    Neg3             -0.028    0.015   -1.875    0.061   -0.072   -0.072\n  Neg1 ~~                                                               \n    Pos3             -0.011    0.015   -0.731    0.465   -0.028   -0.028\n  Neg2 ~~                                                               \n    Pos3             -0.047    0.014   -3.305    0.001   -0.128   -0.128\n  Pos3 ~~                                                               \n    Neg3             -0.079    0.015   -5.118    0.000   -0.200   -0.200\n  Neg1 ~~                                                               \n    Neg2              0.152    0.014   11.288    0.000    0.477    0.477\n    Neg3              0.151    0.014   10.634    0.000    0.443    0.443\n  Neg2 ~~                                                               \n    Neg3              0.167    0.014   12.005    0.000    0.514    0.514\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .PAT1P1    (t1)   -0.123    0.032   -3.855    0.000   -0.123   -0.160\n   .PAT1P2    (t2)   -0.100    0.032   -3.107    0.002   -0.100   -0.134\n   .PAT1P3    (t3)    0.223    0.033    6.799    0.000    0.223    0.301\n   .NAT1P1    (t4)    0.225    0.022   10.149    0.000    0.225    0.309\n   .NAT1P2    (t5)   -0.081    0.019   -4.220    0.000   -0.081   -0.126\n   .NAT1P3    (t6)   -0.144    0.019   -7.423    0.000   -0.144   -0.217\n   .PAT2P1    (t7)   -0.127    0.032   -3.989    0.000   -0.127   -0.166\n   .PAT2P2    (t8)   -0.099    0.032   -3.094    0.002   -0.099   -0.133\n   .PAT2P3    (t9)    0.225    0.033    6.910    0.000    0.225    0.307\n   .NAT2P1   (t10)    0.149    0.022    6.793    0.000    0.149    0.230\n   .NAT2P2   (t11)   -0.061    0.020   -3.101    0.002   -0.061   -0.098\n   .NAT2P3   (t12)   -0.087    0.020   -4.399    0.000   -0.087   -0.136\n   .PAT3P1   (t13)   -0.157    0.031   -5.018    0.000   -0.157   -0.203\n   .PAT3P2   (t14)   -0.077    0.032   -2.426    0.015   -0.077   -0.099\n   .PAT3P3   (t15)    0.234    0.032    7.256    0.000    0.234    0.310\n   .NAT3P1   (t16)    0.136    0.022    6.076    0.000    0.136    0.202\n   .NAT3P2   (t17)   -0.066    0.020   -3.251    0.001   -0.066   -0.100\n   .NAT3P3   (t18)   -0.070    0.021   -3.384    0.001   -0.070   -0.100\n    Pos1              3.000    0.024  124.639    0.000    4.563    4.563\n    Neg1              1.537    0.021   71.637    0.000    2.652    2.652\n    Pos2              3.012    0.024  124.219    0.000    4.502    4.502\n    Neg2              1.604    0.020   78.947    0.000    2.908    2.908\n    Pos3              2.931    0.024  119.853    0.000    4.391    4.391\n    Neg3              1.647    0.022   76.092    0.000    2.797    2.797\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .PAT1P1            0.130    0.010   13.072    0.000    0.130    0.219\n   .PAT1P2            0.124    0.009   13.267    0.000    0.124    0.223\n   .PAT1P3            0.147    0.010   14.837    0.000    0.147    0.268\n   .NAT1P1            0.220    0.013   17.540    0.000    0.220    0.414\n   .NAT1P2            0.087    0.007   11.767    0.000    0.087    0.207\n   .NAT1P3            0.079    0.008   10.385    0.000    0.079    0.179\n   .PAT2P1            0.099    0.008   12.350    0.000    0.099    0.170\n   .PAT2P2            0.104    0.008   13.257    0.000    0.104    0.188\n   .PAT2P3            0.123    0.008   14.898    0.000    0.123    0.229\n   .NAT2P1            0.138    0.009   15.933    0.000    0.138    0.327\n   .NAT2P2            0.094    0.007   13.203    0.000    0.094    0.237\n   .NAT2P3            0.087    0.007   11.961    0.000    0.087    0.209\n   .PAT3P1            0.115    0.010   11.939    0.000    0.115    0.193\n   .PAT3P2            0.150    0.010   14.356    0.000    0.150    0.253\n   .PAT3P3            0.154    0.010   14.973    0.000    0.154    0.272\n   .NAT3P1            0.134    0.009   15.181    0.000    0.134    0.294\n   .NAT3P2            0.094    0.008   12.427    0.000    0.094    0.215\n   .NAT3P3            0.123    0.009   13.604    0.000    0.123    0.247\n    Pos1              0.432    0.024   18.358    0.000    1.000    1.000\n    Neg1              0.336    0.018   18.230    0.000    1.000    1.000\n    Pos2              0.448    0.024   18.754    0.000    1.000    1.000\n    Neg2              0.304    0.017   18.188    0.000    1.000    1.000\n    Pos3              0.446    0.024   18.349    0.000    1.000    1.000\n    Neg3              0.347    0.019   18.230    0.000    1.000    1.000\n\nConstraints:\n                                               |Slack|\n    L1 - (3-L2-L3)                               0.000\n    L4 - (3-L5-L6)                               0.000\n    t1 - (0-t2-t3)                               0.000\n    t4 - (0-t5-t6)                               0.000\n    t7 - (0-t8-t9)                               0.000\n    t10 - (0-t11-t12)                            0.000\n    t13 - (0-t14-t15)                            0.000\n    t16 - (0-t17-t18)                            0.000\n\n\n\n\n\n74.4.3 Modello di Invarianza Forte\nIl modello di invarianza forte è un passaggio ulteriore nell’analisi dell’invarianza in un contesto di modellazione SEM longitudinale. Mentre l’invarianza configurale si concentra sulla struttura fattoriale e l’invarianza debole aggiunge l’uguaglianza dei carichi fattoriali, l’invarianza forte va oltre per includere anche l’uguaglianza delle medie degli indicatori.\nCome nei modelli di invarianza debole, i carichi fattoriali (L1, L2, L3, L4, L5, L6) sono mantenuti uguali attraverso i diversi momenti temporali, indicando che la forza della relazione tra le variabili latenti e i loro indicatori è costante.\nIl modello impone che le medie degli indicatori siano uguali attraverso i diversi momenti temporali. Questo è indicato dalle equazioni come PAT1P1 ~ t1*1, PAT2P1 ~ t1*1, e PAT3P1 ~ t1*1, dove t1 è lo stesso in tutti e tre i momenti temporali.\nIl modello continua a stimare separatamente la varianza degli indicatori e la covarianza sia tra gli indicatori in diversi momenti temporali sia tra le variabili latenti.\nSono imposti alcuni vincoli, come t1 == 0 - t2 - t3, che servono a identificare il modello e riflettono ipotesi teoriche sulle relazioni tra gli indicatori.\nL’invarianza forte è fondamentale per garantire che le misure di un costrutto siano completamente comparabili nel tempo o tra i gruppi. Se un modello dimostra invarianza forte, significa che non solo la relazione tra le variabili latenti e i loro indicatori è costante, ma anche che il livello di base di ciascun indicatore è lo stesso. Questo è cruciale per confronti delle medie latenti o per esaminare i cambiamenti nel tempo.\n\nmod_strong &lt;- \"\n    ### loadings\n    Pos1 =~ L1*PAT1P1 + L2*PAT1P2 + L3*PAT1P3\n    Neg1 =~ L4*NAT1P1 + L5*NAT1P2 + L6*NAT1P3\n    Pos2 =~ L1*PAT2P1 + L2*PAT2P2 + L3*PAT2P3\n    Neg2 =~ L4*NAT2P1 + L5*NAT2P2 + L6*NAT2P3\n    Pos3 =~ L1*PAT3P1 + L2*PAT3P2 + L3*PAT3P3\n    Neg3 =~ L4*NAT3P1 + L5*NAT3P2 + L6*NAT3P3\n    L1 == 3 - L2 - L3\n    L4 == 3 - L5 - L6\n\n    ### factor variance\n    Pos1 ~~ Pos1\n    Neg1 ~~ Neg1\n    Pos2 ~~ Pos2\n    Neg2 ~~ Neg2\n    Pos3 ~~ Pos3\n    Neg3 ~~ Neg3\n\n    ### factor covariance\n    Pos1 ~~ Pos2 + Pos3 + Neg1 + Neg2 + Neg3\n    Pos2 ~~ Pos3 + Neg1 + Neg2 + Neg3\n    Pos3 ~~ Neg1 + Neg2 + Neg3\n    Neg1 ~~ Neg2 + Neg3\n    Neg2 ~~ Neg3\n\n    ### residual variance\n    PAT1P1 ~~ PAT1P1\n    PAT1P2 ~~ PAT1P2\n    PAT1P3 ~~ PAT1P3\n    NAT1P1 ~~ NAT1P1\n    NAT1P2 ~~ NAT1P2\n    NAT1P3 ~~ NAT1P3\n    PAT2P1 ~~ PAT2P1\n    PAT2P2 ~~ PAT2P2\n    PAT2P3 ~~ PAT2P3\n    NAT2P1 ~~ NAT2P1\n    NAT2P2 ~~ NAT2P2\n    NAT2P3 ~~ NAT2P3\n    PAT3P1 ~~ PAT3P1\n    PAT3P2 ~~ PAT3P2\n    PAT3P3 ~~ PAT3P3\n    NAT3P1 ~~ NAT3P1\n    NAT3P2 ~~ NAT3P2\n    NAT3P3 ~~ NAT3P3\n\n    ### cross-time correlated residuals\n    PAT1P1 ~~ PAT2P1 + PAT3P1\n    PAT2P1 ~~ PAT3P1\n    PAT1P2 ~~ PAT2P2 + PAT3P2\n    PAT2P2 ~~ PAT3P2\n    PAT1P3 ~~ PAT2P3 + PAT3P3\n    PAT2P3 ~~ PAT3P3\n    NAT1P1 ~~ NAT2P1 + NAT3P1\n    NAT2P1 ~~ NAT3P1\n    NAT1P2 ~~ NAT2P2 + NAT3P2\n    NAT2P2 ~~ NAT3P2\n    NAT1P3 ~~ NAT2P3 + NAT3P3\n    NAT2P3 ~~ NAT3P3\n\n    ## latent mean\n    Pos1 ~ 1\n    Neg1 ~ 1\n    Pos2 ~ 1\n    Neg2 ~ 1\n    Pos3 ~ 1\n    Neg3 ~ 1\n\n    ## intercept\n    PAT1P1 ~ t1*1\n    PAT1P2 ~ t2*1\n    PAT1P3 ~ t3*1\n    NAT1P1 ~ t4*1\n    NAT1P2 ~ t5*1\n    NAT1P3 ~ t6*1\n    PAT2P1 ~ t1*1\n    PAT2P2 ~ t2*1\n    PAT2P3 ~ t3*1\n    NAT2P1 ~ t4*1\n    NAT2P2 ~ t5*1\n    NAT2P3 ~ t6*1\n    PAT3P1 ~ t1*1\n    PAT3P2 ~ t2*1\n    PAT3P3 ~ t3*1\n    NAT3P1 ~ t4*1\n    NAT3P2 ~ t5*1\n    NAT3P3 ~ t6*1\n\n    t1 == 0 - t2 - t3\n    t4 == 0 - t5 - t6\n    \"\n\n\nfit_strong &lt;- lavaan(mod_strong, data = dat, meanstructure = TRUE)\n\n\nsummary(fit_strong, standardized = TRUE, fit.measures = TRUE) |&gt; print()\n\nlavaan 0.6-19 ended normally after 148 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        99\n  Number of equality constraints                    28\n\n  Number of observations                           823\n\nModel Test User Model:\n                                                      \n  Test statistic                               418.737\n  Degrees of freedom                               118\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                             11131.067\n  Degrees of freedom                               153\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.973\n  Tucker-Lewis Index (TLI)                       0.964\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -10577.963\n  Loglikelihood unrestricted model (H1)     -10368.594\n                                                      \n  Akaike (AIC)                               21297.925\n  Bayesian (BIC)                             21632.545\n  Sample-size adjusted Bayesian (SABIC)      21407.076\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.056\n  90 Percent confidence interval - lower         0.050\n  90 Percent confidence interval - upper         0.061\n  P-value H_0: RMSEA &lt;= 0.050                    0.052\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.037\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Pos1 =~                                                               \n    PAT1P1    (L1)    1.040    0.010  103.200    0.000    0.684    0.884\n    PAT1P2    (L2)    0.997    0.010   98.587    0.000    0.656    0.881\n    PAT1P3    (L3)    0.963    0.010   93.301    0.000    0.633    0.856\n  Neg1 =~                                                               \n    NAT1P1    (L4)    0.957    0.012   78.488    0.000    0.553    0.757\n    NAT1P2    (L5)    0.999    0.011   90.830    0.000    0.578    0.891\n    NAT1P3    (L6)    1.044    0.011   94.108    0.000    0.604    0.906\n  Pos2 =~                                                               \n    PAT2P1    (L1)    1.040    0.010  103.200    0.000    0.696    0.911\n    PAT2P2    (L2)    0.997    0.010   98.587    0.000    0.667    0.900\n    PAT2P3    (L3)    0.963    0.010   93.301    0.000    0.644    0.878\n  Neg2 =~                                                               \n    NAT2P1    (L4)    0.957    0.012   78.488    0.000    0.527    0.816\n    NAT2P2    (L5)    0.999    0.011   90.830    0.000    0.550    0.873\n    NAT2P3    (L6)    1.044    0.011   94.108    0.000    0.576    0.891\n  Pos3 =~                                                               \n    PAT3P1    (L1)    1.040    0.010  103.200    0.000    0.694    0.898\n    PAT3P2    (L2)    0.997    0.010   98.587    0.000    0.665    0.864\n    PAT3P3    (L3)    0.963    0.010   93.301    0.000    0.642    0.853\n  Neg3 =~                                                               \n    NAT3P1    (L4)    0.957    0.012   78.488    0.000    0.563    0.836\n    NAT3P2    (L5)    0.999    0.011   90.830    0.000    0.588    0.887\n    NAT3P3    (L6)    1.044    0.011   94.108    0.000    0.614    0.868\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Pos1 ~~                                                               \n    Pos2              0.243    0.019   12.901    0.000    0.552    0.552\n    Pos3              0.231    0.019   12.306    0.000    0.527    0.527\n    Neg1             -0.062    0.015   -4.219    0.000   -0.164   -0.164\n    Neg2             -0.059    0.014   -4.163    0.000   -0.162   -0.162\n    Neg3             -0.014    0.015   -0.919    0.358   -0.035   -0.035\n  Pos2 ~~                                                               \n    Pos3              0.243    0.019   12.758    0.000    0.544    0.544\n  Neg1 ~~                                                               \n    Pos2             -0.059    0.015   -3.971    0.000   -0.152   -0.152\n  Pos2 ~~                                                               \n    Neg2             -0.091    0.014   -6.291    0.000   -0.246   -0.246\n    Neg3             -0.028    0.015   -1.882    0.060   -0.072   -0.072\n  Neg1 ~~                                                               \n    Pos3             -0.011    0.015   -0.722    0.470   -0.028   -0.028\n  Neg2 ~~                                                               \n    Pos3             -0.047    0.014   -3.309    0.001   -0.128   -0.128\n  Pos3 ~~                                                               \n    Neg3             -0.079    0.015   -5.140    0.000   -0.201   -0.201\n  Neg1 ~~                                                               \n    Neg2              0.152    0.013   11.258    0.000    0.475    0.475\n    Neg3              0.150    0.014   10.606    0.000    0.442    0.442\n  Neg2 ~~                                                               \n    Neg3              0.166    0.014   11.985    0.000    0.513    0.513\n .PAT1P1 ~~                                                             \n   .PAT2P1            0.010    0.006    1.662    0.096    0.010    0.091\n   .PAT3P1            0.007    0.007    1.089    0.276    0.007    0.061\n .PAT2P1 ~~                                                             \n   .PAT3P1           -0.000    0.006   -0.017    0.986   -0.000   -0.001\n .PAT1P2 ~~                                                             \n   .PAT2P2            0.008    0.006    1.325    0.185    0.008    0.069\n   .PAT3P2            0.012    0.007    1.749    0.080    0.012    0.088\n .PAT2P2 ~~                                                             \n   .PAT3P2            0.005    0.006    0.780    0.435    0.005    0.039\n .PAT1P3 ~~                                                             \n   .PAT2P3            0.011    0.006    1.785    0.074    0.011    0.084\n   .PAT3P3            0.013    0.007    1.782    0.075    0.013    0.084\n .PAT2P3 ~~                                                             \n   .PAT3P3            0.010    0.006    1.486    0.137    0.010    0.070\n .NAT1P1 ~~                                                             \n   .NAT2P1            0.028    0.007    3.711    0.000    0.028    0.156\n   .NAT3P1            0.009    0.007    1.149    0.251    0.009    0.049\n .NAT2P1 ~~                                                             \n   .NAT3P1            0.011    0.006    1.798    0.072    0.011    0.080\n .NAT1P2 ~~                                                             \n   .NAT2P2            0.000    0.005    0.038    0.970    0.000    0.002\n   .NAT3P2            0.006    0.005    1.169    0.243    0.006    0.066\n .NAT2P2 ~~                                                             \n   .NAT3P2            0.008    0.005    1.642    0.101    0.008    0.089\n .NAT1P3 ~~                                                             \n   .NAT2P3           -0.001    0.005   -0.176    0.861   -0.001   -0.011\n   .NAT3P3           -0.008    0.006   -1.350    0.177   -0.008   -0.077\n .NAT2P3 ~~                                                             \n   .NAT3P3           -0.004    0.006   -0.713    0.476   -0.004   -0.039\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    Pos1              3.000    0.024  124.671    0.000    4.563    4.563\n    Neg1              1.520    0.021   71.482    0.000    2.629    2.629\n    Pos2              3.012    0.024  124.262    0.000    4.502    4.502\n    Neg2              1.605    0.020   79.147    0.000    2.912    2.912\n    Pos3              2.929    0.024  119.856    0.000    4.389    4.389\n    Neg3              1.647    0.022   76.221    0.000    2.800    2.800\n   .PAT1P1    (t1)   -0.140    0.031   -4.557    0.000   -0.140   -0.181\n   .PAT1P2    (t2)   -0.089    0.031   -2.892    0.004   -0.089   -0.120\n   .PAT1P3    (t3)    0.229    0.031    7.275    0.000    0.229    0.309\n   .NAT1P1    (t4)    0.174    0.021    8.461    0.000    0.174    0.238\n   .NAT1P2    (t5)   -0.068    0.018   -3.691    0.000   -0.068   -0.104\n   .NAT1P3    (t6)   -0.107    0.018   -5.781    0.000   -0.107   -0.160\n   .PAT2P1    (t1)   -0.140    0.031   -4.557    0.000   -0.140   -0.183\n   .PAT2P2    (t2)   -0.089    0.031   -2.892    0.004   -0.089   -0.120\n   .PAT2P3    (t3)    0.229    0.031    7.275    0.000    0.229    0.312\n   .NAT2P1    (t4)    0.174    0.021    8.461    0.000    0.174    0.270\n   .NAT2P2    (t5)   -0.068    0.018   -3.691    0.000   -0.068   -0.107\n   .NAT2P3    (t6)   -0.107    0.018   -5.781    0.000   -0.107   -0.165\n   .PAT3P1    (t1)   -0.140    0.031   -4.557    0.000   -0.140   -0.181\n   .PAT3P2    (t2)   -0.089    0.031   -2.892    0.004   -0.089   -0.116\n   .PAT3P3    (t3)    0.229    0.031    7.275    0.000    0.229    0.304\n   .NAT3P1    (t4)    0.174    0.021    8.461    0.000    0.174    0.259\n   .NAT3P2    (t5)   -0.068    0.018   -3.691    0.000   -0.068   -0.102\n   .NAT3P3    (t6)   -0.107    0.018   -5.781    0.000   -0.107   -0.151\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    Pos1              0.432    0.024   18.357    0.000    1.000    1.000\n    Neg1              0.334    0.018   18.210    0.000    1.000    1.000\n    Pos2              0.448    0.024   18.753    0.000    1.000    1.000\n    Neg2              0.304    0.017   18.183    0.000    1.000    1.000\n    Pos3              0.445    0.024   18.343    0.000    1.000    1.000\n    Neg3              0.346    0.019   18.219    0.000    1.000    1.000\n   .PAT1P1            0.130    0.010   13.046    0.000    0.130    0.218\n   .PAT1P2            0.124    0.009   13.297    0.000    0.124    0.224\n   .PAT1P3            0.147    0.010   14.841    0.000    0.147    0.268\n   .NAT1P1            0.228    0.013   17.679    0.000    0.228    0.427\n   .NAT1P2            0.086    0.007   11.639    0.000    0.086    0.206\n   .NAT1P3            0.080    0.008   10.321    0.000    0.080    0.179\n   .PAT2P1            0.099    0.008   12.317    0.000    0.099    0.170\n   .PAT2P2            0.104    0.008   13.290    0.000    0.104    0.189\n   .PAT2P3            0.123    0.008   14.903    0.000    0.123    0.229\n   .NAT2P1            0.139    0.009   16.054    0.000    0.139    0.333\n   .NAT2P2            0.094    0.007   13.168    0.000    0.094    0.237\n   .NAT2P3            0.086    0.007   11.823    0.000    0.086    0.206\n   .PAT3P1            0.115    0.010   11.933    0.000    0.115    0.193\n   .PAT3P2            0.151    0.010   14.387    0.000    0.151    0.254\n   .PAT3P3            0.154    0.010   14.968    0.000    0.154    0.272\n   .NAT3P1            0.136    0.009   15.342    0.000    0.136    0.301\n   .NAT3P2            0.094    0.008   12.327    0.000    0.094    0.213\n   .NAT3P3            0.123    0.009   13.539    0.000    0.123    0.246\n\nConstraints:\n                                               |Slack|\n    L1 - (3-L2-L3)                               0.000\n    L4 - (3-L5-L6)                               0.000\n    t1 - (0-t2-t3)                               0.000\n    t4 - (0-t5-t6)                               0.000\n\n\n\n\n\n74.4.4 Confronto tra Modelli\nUn confronto tra i modelli precedenti può essere eseguito mediante il test del rapporto tra verosimiglianze.\n\nout &lt;- compareFit(fit_null, fitSEM, fit_config, fit_weak, fit_strong)\nsummary(out) |&gt; \n    print()\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n            Df   AIC   BIC    Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)\nfit_config 102 21263 21673   352.23                                      \nfit_weak   110 21262 21634   366.56       14.3 0.03101       8  0.0735478\nfit_strong 118 21298 21633   418.74       52.2 0.08191       8  1.557e-08\nfitSEM     124 21309 21615   441.52       22.8 0.05830       6  0.0008723\nfit_null   177 31974 32031 11213.10    10771.6 0.49571      53  &lt; 2.2e-16\n              \nfit_config    \nfit_weak   .  \nfit_strong ***\nfitSEM     ***\nfit_null   ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n####################### Model Fit Indices ###########################\n                     chisq  df pvalue        rmsea          cfi\nfit_config 352.232&lt;U+2020&gt; 102   .000        .055  .977&lt;U+2020&gt;\nfit_weak          366.562  110   .000 .053&lt;U+2020&gt;        .977 \nfit_strong        418.737  118   .000        .056         .973 \nfitSEM            441.520  124   .000        .056         .971 \nfit_null        11213.103  177   .000        .275         .000 \n                    tli         srmr               aic               bic\nfit_config        .966         .035         21263.420         21673.447 \nfit_weak   .967&lt;U+2020&gt; .035&lt;U+2020&gt; 21261.750&lt;U+2020&gt;        21634.074 \nfit_strong        .964         .037         21297.925         21632.545 \nfitSEM            .964         .045         21308.709  21615.051&lt;U+2020&gt;\nfit_null          .131         .328         31974.291         32030.846 \n\n################## Differences in Fit Indices #######################\n                      df  rmsea    cfi    tli  srmr       aic       bic\nfit_weak - fit_config  8 -0.001 -0.001  0.002 0.000    -1.670   -39.373\nfit_strong - fit_weak  8  0.002 -0.004 -0.003 0.002    36.175    -1.529\nfitSEM - fit_strong    6  0.000 -0.002  0.000 0.008    10.783   -17.494\nfit_null - fitSEM     53  0.219 -0.971 -0.833 0.282 10665.582 10415.795\n\nThe following lavaan models were compared:\n    fit_config\n    fit_weak\n    fit_strong\n    fitSEM\n    fit_null\nTo view results, assign the compareFit() output to an object and  use the summary() method; see the class?FitDiff help page.\n\n\nI valori di Chisq indicano il grado di adattamento dei modelli ai dati. Valori più bassi indicano un migliore adattamento. Dall’elenco, fit_config ha il valore di Chi-square più basso, suggerendo il miglior adattamento tra i modelli confrontati. Un valore di Chisq diff significativo (basso valore p) suggerisce che il modello più vincolato ha un adattamento significativamente peggiore.Chisq diff di 14.3 con un valore p di 0.0735 indica che non c’è una differenza statisticamente significativa nel fit tra i modelli configurale e debole. Questo suggerisce che l’aggiunta dell’invarianza debole (uguaglianza dei carichi fattoriali) non peggiora significativamente il fit. Chisq diff è 52.2 con un valore p molto basso (1.557e-08), indica che l’aggiunta dell’invarianza forte (uguaglianza delle medie) peggiora significativamente il fit rispetto al modello debole. Una differenza di 22.8 nel Chi-square e un valore p basso (0.0008723) suggeriscono che il modello forte ha un fit significativamente peggiore rispetto al modello SEM base. Il modello nullo ha un valore molto alto di Chi-square, indicando, come previsto, un adattamento molto scarso. Questo è normale per i modelli nulli e serve come riferimento estremo.\nIl RMSEA è un indice di bontà di adattamento che considera la complessità del modello. Valori inferiori a 0.05 indicano un buon adattamento, valori tra 0.05 e 0.08 indicano un adattamento accettabile, e valori superiori a 0.10 sono generalmente considerati inaccettabili. In questo caso, il RMSEA aumenta da fit_config a fit_strong, suggerendo un peggioramento dell’adattamento con l’aggiunta di vincoli più forti.\nIn conclusione, i risultati indicano che l’aggiunta di vincoli di invarianza debole non peggiora significativamente il fit, mentre l’aggiunta di vincoli di invarianza forte riduce in modo significativo la bontà di adattamento del modello. Questo suggerisce che, mentre i carichi fattoriali possono essere considerati invarianti tra i gruppi o nel tempo, le medie degli indicatori potrebbero non esserlo.\nLittle (2023) nota che, con un campione così grande, disponiamo di un livello di potere statistico sufficiente anche per rilevare differenze minuscole. Quindi, i risultati dei test statistici precedenti vanno presi con un grano di sale. In particolare, Little (2023) nota che il modello di invarianza forte fornisce evidenze di un adattamento soddisfacente e che il peggioramento dell’adattamento rispetto al modello di invarianza debole è, quantitativamente, estremamente piccolo se esaminato rispetto alle dimensioni di CFI, TLI, RMSEA, e SRMR. Per queste ragioni, Little (2023) conclude affermando che il modello di invarianza forte risulta giustificato da questi dati. I criteri per determinare una perdita eccessiva dell’adattamento, data la potenza della dimensione del campione, sono un valore p inferiore a .001, un cambiamento nel CFI superiore a .002, o una stima puntuale dell’RMSEA che cade al di fuori dell’intervallo di confidenza del modello di invarianza forte.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>74</span>  <span class='chapter-title'>Specificare e Interpretare un Modello Longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/05_intro_panel.html#riflessioni-conclusive",
    "href": "chapters/lgm/05_intro_panel.html#riflessioni-conclusive",
    "title": "74  Specificare e Interpretare un Modello Longitudinale",
    "section": "74.5 Riflessioni Conclusive",
    "text": "74.5 Riflessioni Conclusive\nIn questo capitolo, abbiamo affrontato i passaggi e molte delle problematiche associate all’adattamento di un modello panel standard ai dati longitudinali. Nei dati longitudinali, il continuum lungo il quale sono ordinati i costrutti è rappresentato dal tempo.\nI modelli panel di base che abbiamo esaminato qui sono solo alcuni tra i vari tipi di modelli che possono essere adattati ai dati panel. In questo contesto, l’analisi dei dati longitudinali implica un approccio sistematico per esaminare come determinati costrutti o variabili cambiano nel corso del tempo. Questo può includere l’analisi di tendenze, cicli o pattern nei dati raccolti in diversi momenti.\nAdattare un modello panel a dati longitudinali richiede una comprensione approfondita sia della natura dei dati sia delle tecniche statistiche utilizzate. Questo processo può comportare sfide specifiche, come la gestione di dati mancanti, l’accounting per la variabilità sia tra i soggetti che all’interno dello stesso soggetto nel tempo, e la scelta del modello statistico più appropriato in base alla struttura dei dati e agli obiettivi della ricerca.\nI modelli panel di base, come quelli discussi in questo capitolo, sono un punto di partenza fondamentale. Tuttavia, esistono molte altre varianti e approfondimenti di questi modelli che possono essere esplorati per adattarsi meglio a scenari complessi o per rispondere a specifiche domande di ricerca. Questi includono modelli panel più avanzati che possono tener conto di effetti casuali, effetti fissi, o che possono essere usati per analizzare le interazioni tra variabili nel tempo.\nL’obiettivo finale di questi modelli è di fornire una rappresentazione accurata di come i costrutti si evolvono nel tempo, permettendo ai ricercatori di trarre conclusioni affidabili dai loro dati.\n\n\n\n\nLittle, T. D. (2023). Longitudinal structural equation modeling. Guilford Press.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>74</span>  <span class='chapter-title'>Specificare e Interpretare un Modello Longitudinale</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/07_growth_1.html",
    "href": "chapters/lgm/07_growth_1.html",
    "title": "75  Curve di crescita latente",
    "section": "",
    "text": "75.1 Introduzione\nNel capitolo precedente, abbiamo esplorato come i modelli di Crescita Latente (LGM) possano essere correlati e confrontati con i modelli lineari ad effetti misti attraverso l’uso di dati simulati. In particolare, abbiamo osservato come sia possibile strutturare un modello LGM che incorpori un fattore latente per la variazione delle intercette individuali, il quale cattura le dinamiche del cambiamento delle medie nel tempo, e un secondo fattore latente che riflette le variazioni individuali nelle pendenze delle rette di regressione.\nAbbiamo esaminato in dettaglio il processo di definizione di questi fattori latenti, mettendo in atto una serie di vincoli sugli indicatori che identificano le variabili latenti. In particolare, abbiamo visto come l’applicazione dei vincoli 0, 1, 2, 3 alle saturazioni fattoriali per il fattore “pendenza” determini una relazione lineare tra la media del costrutto e il tempo. Questo approccio è particolarmente efficace quando le misurazioni del costrutto sono state effettuate a intervalli regolari.\nIn questo capitolo, ci dedicheremo all’approfondimento di questo argomento, spostando la nostra attenzione dall’ambito teorico e simulato all’analisi di un set di dati reali. Questo passaggio ci fornirà una visione più chiara e concreta di come questi modelli possano essere impiegati nell’analisi di dati longitudinali reali, con tutti le loro sfaccettature e sfide.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>75</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/07_growth_1.html#una-applicazione-concreta",
    "href": "chapters/lgm/07_growth_1.html#una-applicazione-concreta",
    "title": "75  Curve di crescita latente",
    "section": "75.2 Una applicazione concreta",
    "text": "75.2 Una applicazione concreta\nEsaminiamo l’adattamento di un modello LGM ad un campione di dati reali. In questo tutorial, considereremo il cambiamento nel rendimento in matematica dei bambini durante la scuola elementare e media utilizzando il set di dati NLSY-CYA {cite:p}grimm2016growth. Iniziamo a leggere i dati.\n\n#set filepath for data file\nfilepath &lt;- \"https://raw.githubusercontent.com/LRI-2/Data/main/GrowthModeling/nlsy_math_wide_R.dat\"\n\n# read in the text data file using the url() function\ndat &lt;- read.table(\n  file = url(filepath),\n  na.strings = \".\"\n) # indicates the missing data designator\n\n# copy data with new name\nnlsy_math_wide &lt;- dat\n\n# Give the variable names\nnames(nlsy_math_wide) &lt;- c(\n  \"id\", \"female\", \"lb_wght\", \"anti_k1\",\n  \"math2\", \"math3\", \"math4\", \"math5\", \"math6\", \"math7\", \"math8\",\n  \"age2\", \"age3\", \"age4\", \"age5\", \"age6\", \"age7\", \"age8\",\n  \"men2\", \"men3\", \"men4\", \"men5\", \"men6\", \"men7\", \"men8\",\n  \"spring2\", \"spring3\", \"spring4\", \"spring5\", \"spring6\", \"spring7\", \"spring8\",\n  \"anti2\", \"anti3\", \"anti4\", \"anti5\", \"anti6\", \"anti7\", \"anti8\"\n)\n\n# view the first few observations (and columns) in the data set\nhead(nlsy_math_wide[, 1:11], 10)\n\n\nA data.frame: 10 x 11\n\n\n\nid\nfemale\nlb_wght\nanti_k1\nmath2\nmath3\nmath4\nmath5\nmath6\nmath7\nmath8\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n201\n1\n0\n0\nNA\n38\nNA\n55\nNA\nNA\nNA\n\n\n2\n303\n1\n0\n1\n26\nNA\nNA\n33\nNA\nNA\nNA\n\n\n3\n2702\n0\n0\n0\n56\nNA\n58\nNA\nNA\nNA\n80\n\n\n4\n4303\n1\n0\n0\nNA\n41\n58\nNA\nNA\nNA\nNA\n\n\n5\n5002\n0\n0\n4\nNA\nNA\n46\nNA\n54\nNA\n66\n\n\n6\n5005\n1\n0\n0\n35\nNA\n50\nNA\n60\nNA\n59\n\n\n7\n5701\n0\n0\n2\nNA\n62\n61\nNA\nNA\nNA\nNA\n\n\n8\n6102\n0\n0\n0\nNA\nNA\n55\n67\nNA\n81\nNA\n\n\n9\n6801\n1\n0\n0\nNA\n54\nNA\n62\nNA\n66\nNA\n\n\n10\n6802\n0\n0\n0\nNA\n55\nNA\n66\nNA\n68\nNA\n\n\n\n\n\nIl nostro interesse specifico riguarda il cambiamento relativo alle misure ripetute di matematica, da math2 a math8. Selezioniamo dunque le variabili di interesse.\n\nnlsy_math_sub &lt;- nlsy_math_wide |&gt;\n    dplyr::select(\"id\", \"math2\", \"math3\", \"math4\", \"math5\", \"math6\", \"math7\", \"math8\")\n\nTrasformiamo i dati in formato long.\n\nnlsy_math_long &lt;- reshape(\n  data = nlsy_math_sub,\n  timevar = c(\"grade\"),\n  idvar = \"id\",\n  varying = c(\n    \"math2\", \"math3\", \"math4\",\n    \"math5\", \"math6\", \"math7\", \"math8\"\n  ),\n  direction = \"long\", sep = \"\"\n)\n\nOrdiniamo i dati in base alle variabili id e grade.\n\nnlsy_math_long &lt;- nlsy_math_long[order(nlsy_math_long$id, nlsy_math_long$grade), ]\n\nRimuoviamo gli NA dalla variabile math per potere generare il grafico con le traiettorie individuali di sviluppo.\n\nnlsy_math_long &lt;- nlsy_math_long[which(is.na(nlsy_math_long$math) == FALSE), ]\n\nEsaminiamo i dati grezzi.\n\nnlsy_math_long |&gt;\n  ggplot(aes(x = grade, y = math)) +\n  geom_point(\n    size = 1.2,\n    alpha = .8,\n    # to add some random noise for plotting purposes\n    position = \"jitter\"\n  ) +\n  labs(title = \"PAT Mathematics as a function of Grade\")\n\n\n\n\n\n\n\n\nAggiungiamo al grafico le retta dei minimi quadrati calcolata su tutti i dati (ignorando il ragruppamento dei dati in funzione dei partecipanti).\n\nnlsy_math_long |&gt;\n    ggplot(aes(x = grade, y = math)) +\n    geom_point(\n        size = 1.2,\n        alpha = .8,\n        # to add some random noise for plotting purposes\n        position = \"jitter\"\n    ) +\n    geom_smooth(\n        method = lm,\n        se = FALSE,\n        col = \"blue\",\n        linewidth = 1.5,\n        alpha = .8\n    ) + # to add regression line\n    labs(title = \"PAT Mathematics as a function of Grade\")\n\n\n\n\n\n\n\n\nEsaminiamo le traiettorie di cambiamento intra-individuale.\n\n# intraindividual change trajetories\nnlsy_math_long |&gt;\n  ggplot(\n    aes(x = grade, y = math, group = id)\n  ) + # setting variables\n  geom_point(size = .5) + # adding points to plot\n  geom_line(alpha = 0.3) + # adding lines to plot\n  # setting the x-axis with breaks and labels\n  scale_x_continuous(\n    limits = c(2, 8),\n    breaks = c(2, 3, 4, 5, 6, 7, 8),\n    name = \"Grade at Testing\"\n  ) +\n  # setting the y-axis with limits breaks and labels\n  scale_y_continuous(\n    limits = c(10, 90),\n    breaks = c(10, 30, 50, 70, 90),\n    name = \"PIAT Mathematics\"\n  )",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>75</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/07_growth_1.html#modello-di-assenza-di-crescita",
    "href": "chapters/lgm/07_growth_1.html#modello-di-assenza-di-crescita",
    "title": "75  Curve di crescita latente",
    "section": "75.3 Modello di assenza di crescita",
    "text": "75.3 Modello di assenza di crescita\nDall’analisi dei grafici precedenti, si osserva che i punteggi di matematica mostrano un incremento sistematico nel tempo. Per iniziare l’analisi, adotteremo un modello di assenza di crescita come benchmark di base per il confronto con modelli più complessi successivi.\nIn questo modello si assume che i punteggi di matematica degli studenti rimangano invariati nel corso del tempo. Esso mira a stimare, per ogni studente, il “valore vero” dei loro punteggi in matematica, senza prendere in considerazione eventuali variazioni nel tempo. Poiché non contempla la dinamica temporale dei punteggi, questo modello rappresenta una situazione di stallo o assenza di sviluppo, risultando spesso di limitato interesse e pertanto generalmente non viene adottato in analisi più approfondite.\nIl modello di assenza della crescita è caratterizzato dalla presenza di una variabile latente e di un’intercetta, la quale rappresenta un livello medio di performance che si mantiene costante nel tempo. Questa configurazione del modello permette di stabilire un punto di partenza per comprendere se e in che misura i punteggi di matematica variano effettivamente nel corso del tempo, quando confrontati con modelli che considerano la crescita o l’evoluzione dei punteggi.\nPer definire il modello di assenza di crescita, utilizziamo la seguente sintassi di lavaan.\n\nng_math_lavaan_model &lt;- ' \n  # latent variable definitions\n      #intercept\n      eta_1 =~ 1*math2\n      eta_1 =~ 1*math3\n      eta_1 =~ 1*math4\n      eta_1 =~ 1*math5\n      eta_1 =~ 1*math6\n      eta_1 =~ 1*math7\n      eta_1 =~ 1*math8\n\n  # factor variances\n      eta_1 ~~ eta_1\n\n  # covariances among factors \n      #none (only 1 factor)\n\n  # factor means \n      eta_1 ~ start(30)*1\n\n  # manifest variances (made equivalent by naming theta)\n      math2 ~~ theta*math2\n      math3 ~~ theta*math3\n      math4 ~~ theta*math4\n      math5 ~~ theta*math5\n      math6 ~~ theta*math6\n      math7 ~~ theta*math7\n      math8 ~~ theta*math8\n  # manifest means (fixed at zero)\n      math2 ~ 0*1\n      math3 ~ 0*1\n      math4 ~ 0*1\n      math5 ~ 0*1\n      math6 ~ 0*1\n      math7 ~ 0*1\n      math8 ~ 0*1\n' #end of model definition\n\nAdattiamo il modello ai dati.\n\nng_math_lavaan_fit &lt;- sem(ng_math_lavaan_model,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\"\n)\n\nNel codice fornito, l’opzione missing = \"fiml\" utilizzata nella funzione sem() specifica il metodo “Full Information Maximum Likelihood” (FIML) per gestire i dati mancanti nel dataset. FIML è un approccio sofisticato per la gestione dei dati mancanti in analisi statistiche complesse come i modelli SEM. A differenza di metodi più semplici come l’eliminazione lista per lista o l’imputazione media, FIML utilizza tutte le informazioni disponibili nel dataset, inclusi i pattern dei dati mancanti, per produrre stime dei parametri. Questo metodo è particolarmente utile quando si lavora con dataset longitudinali o complessi dove i dati mancanti sono comuni. FIML è considerato un approccio più accurato e meno distorto rispetto ad altri metodi, in quanto non si limita a utilizzare solo i casi completi, ma incorpora l’intero insieme di dati disponibili, comprese le osservazioni parziali.\nEsaminiamo la soluzione.\n\nsummary(ng_math_lavaan_fit, fit.measures = TRUE, standardized = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 18 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n  Number of equality constraints                     6\n\n                                                  Used       Total\n  Number of observations                           932         933\n  Number of missing patterns                        60            \n\nModel Test User Model:\n                                                      \n  Test statistic                              1759.002\n  Degrees of freedom                                32\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               862.334\n  Degrees of freedom                                21\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.000\n  Tucker-Lewis Index (TLI)                      -0.347\n                                                      \n  Robust Comparative Fit Index (CFI)             0.000\n  Robust Tucker-Lewis Index (TLI)                0.093\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -8745.952\n  Loglikelihood unrestricted model (H1)      -7866.451\n                                                      \n  Akaike (AIC)                               17497.903\n  Bayesian (BIC)                             17512.415\n  Sample-size adjusted Bayesian (SABIC)      17502.888\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.241\n  90 Percent confidence interval - lower         0.231\n  90 Percent confidence interval - upper         0.250\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n                                                      \n  Robust RMSEA                                   0.467\n  90 Percent confidence interval - lower         0.402\n  90 Percent confidence interval - upper         0.534\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080             1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.480\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  eta_1 =~                                                              \n    math2             1.000                               6.850    0.536\n    math3             1.000                               6.850    0.536\n    math4             1.000                               6.850    0.536\n    math5             1.000                               6.850    0.536\n    math6             1.000                               6.850    0.536\n    math7             1.000                               6.850    0.536\n    math8             1.000                               6.850    0.536\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    eta_1            45.915    0.324  141.721    0.000    6.703    6.703\n   .math2             0.000                               0.000    0.000\n   .math3             0.000                               0.000    0.000\n   .math4             0.000                               0.000    0.000\n   .math5             0.000                               0.000    0.000\n   .math6             0.000                               0.000    0.000\n   .math7             0.000                               0.000    0.000\n   .math8             0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    eta_1            46.917    4.832    9.709    0.000    1.000    1.000\n   .math2   (thet)  116.682    4.548   25.656    0.000  116.682    0.713\n   .math3   (thet)  116.682    4.548   25.656    0.000  116.682    0.713\n   .math4   (thet)  116.682    4.548   25.656    0.000  116.682    0.713\n   .math5   (thet)  116.682    4.548   25.656    0.000  116.682    0.713\n   .math6   (thet)  116.682    4.548   25.656    0.000  116.682    0.713\n   .math7   (thet)  116.682    4.548   25.656    0.000  116.682    0.713\n   .math8   (thet)  116.682    4.548   25.656    0.000  116.682    0.713\n\n\n\nGeneriamo il diagramma di percorso.\n\nsemPaths(ng_math_lavaan_fit, what = \"path\", whatLabels = \"par\")\n\n\n\n\n\n\n\n\nCalcoliamo le traiettorie predette.\n\n#obtaining predicted factor scores for individuals\nnlsy_math_predicted &lt;- as.data.frame(cbind(nlsy_math_wide$id,lavPredict(ng_math_lavaan_fit)))\n\n#naming columns\nnames(nlsy_math_predicted) &lt;- c(\"id\", \"eta_1\")\n\n#looking at data\nhead(nlsy_math_predicted) \n\n\nA data.frame: 6 x 2\n\n\n\nid\neta_1\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n201\n46.17558\n\n\n2\n303\n38.59816\n\n\n3\n2702\n56.16725\n\n\n4\n4303\n47.51278\n\n\n5\n5002\n51.06429\n\n\n6\n5005\n49.05038\n\n\n\n\n\n\n# calculating implied manifest scores\nnlsy_math_predicted$math2 &lt;- 1 * nlsy_math_predicted$eta_1\nnlsy_math_predicted$math3 &lt;- 1 * nlsy_math_predicted$eta_1\nnlsy_math_predicted$math4 &lt;- 1 * nlsy_math_predicted$eta_1\nnlsy_math_predicted$math5 &lt;- 1 * nlsy_math_predicted$eta_1\nnlsy_math_predicted$math6 &lt;- 1 * nlsy_math_predicted$eta_1\nnlsy_math_predicted$math7 &lt;- 1 * nlsy_math_predicted$eta_1\nnlsy_math_predicted$math8 &lt;- 1 * nlsy_math_predicted$eta_1\n\n# reshaping wide to long\nnlsy_math_predicted_long &lt;- reshape(\n  data = nlsy_math_predicted,\n  timevar = c(\"grade\"),\n  idvar = \"id\",\n  varying = c(\n    \"math2\", \"math3\", \"math4\",\n    \"math5\", \"math6\", \"math7\", \"math8\"\n  ),\n  direction = \"long\", sep = \"\"\n)\n# sorting for easy viewing\n# order by id and time\nnlsy_math_predicted_long &lt;- nlsy_math_predicted_long[order(nlsy_math_predicted_long$id, nlsy_math_predicted_long$grade), ]\n\n# intraindividual change trajetories\nggplot(\n  data = nlsy_math_predicted_long, # data set\n  aes(x = grade, y = math, group = id)\n) + # setting variables\n  # geom_point(size=.5) + #adding points to plot\n  geom_line(alpha = 0.1) + # adding lines to plot\n  # setting the x-axis with breaks and labels\n  scale_x_continuous(\n    limits = c(2, 8),\n    breaks = c(2, 3, 4, 5, 6, 7, 8),\n    name = \"Grade at Testing\"\n  ) +\n  # setting the y-axis with limits breaks and labels\n  scale_y_continuous(\n    limits = c(10, 90),\n    breaks = c(10, 30, 50, 70, 90),\n    name = \"Predicted PIAT Mathematics\"\n  )\n\n\n\n\n\n\n\n\nDal grafico risulta evidente che il modello impiegato genera una serie di linee orizzontali, ognuna rappresentante la traiettoria statica dell’abilità matematica per ogni individuo. In questo modello, l’intercetta associata a ciascuna di queste linee orizzontali corrisponde al “valore vero” dell’abilità matematica di ogni bambino. Conformemente alle ipotesi del modello, questo valore si mantiene invariato nel corso del tempo, suggerendo che, secondo il modello, l’abilità matematica di ciascun individuo non subisce variazioni o sviluppi significativi nel periodo osservato.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>75</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/07_growth_1.html#modello-di-crescita-lineare",
    "href": "chapters/lgm/07_growth_1.html#modello-di-crescita-lineare",
    "title": "75  Curve di crescita latente",
    "section": "75.4 Modello di crescita lineare",
    "text": "75.4 Modello di crescita lineare\nNella discussione dei modelli di crescita, il modello di assenza di crescita viene sempre seguito dall’esame del modello di crescita lineare. Infatti, i modelli di crescita lineare rappresentano spesso il punto di partenza quando si cerca di comprendere il cambiamento all’interno dell’individuo. Successivamente, possono essere considerati anche modelli di crescita non lineare. Procediamo dunque all’implementazione di un modello di crescita latente lineare.\n\nlg_math_lavaan_model &lt;- '\n  # latent variable definitions\n      #intercept (note intercept is a reserved term)\n      eta_1 =~ 1*math2\n      eta_1 =~ 1*math3\n      eta_1 =~ 1*math4\n      eta_1 =~ 1*math5\n      eta_1 =~ 1*math6\n      eta_1 =~ 1*math7\n      eta_1 =~ 1*math8\n\n      #linear slope \n      eta_2 =~ 0*math2\n      eta_2 =~ 1*math3\n      eta_2 =~ 2*math4\n      eta_2 =~ 3*math5\n      eta_2 =~ 4*math6\n      eta_2 =~ 5*math7\n      eta_2 =~ 6*math8\n\n  # factor variances\n      eta_1 ~~ eta_1\n      eta_2 ~~ eta_2\n\n  # covariances among factors \n      eta_1 ~~ eta_2\n\n  # factor means \n      eta_1 ~ 1\n      eta_2 ~ 1\n\n  # manifest variances (made equivalent by naming theta)\n      math2 ~~ theta*math2\n      math3 ~~ theta*math3\n      math4 ~~ theta*math4\n      math5 ~~ theta*math5\n      math6 ~~ theta*math6\n      math7 ~~ theta*math7\n      math8 ~~ theta*math8\n  # manifest means (fixed at zero)\n      math2 ~ 0*1\n      math3 ~ 0*1\n      math4 ~ 0*1\n      math5 ~ 0*1\n      math6 ~ 0*1\n      math7 ~ 0*1\n      math8 ~ 0*1\n' #end of model definition\n\nAdattiamo il modello ai dati.\n\nlg_math_lavaan_fit &lt;- sem(lg_math_lavaan_model,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\"\n)\n\nEsaminiamo il risultato ottenuto.\n\nsummary(lg_math_lavaan_fit, fit.measures = TRUE, standardized = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 38 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        12\n  Number of equality constraints                     6\n\n                                                  Used       Total\n  Number of observations                           932         933\n  Number of missing patterns                        60            \n\nModel Test User Model:\n                                                      \n  Test statistic                               204.484\n  Degrees of freedom                                29\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               862.334\n  Degrees of freedom                                21\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.791\n  Tucker-Lewis Index (TLI)                       0.849\n                                                      \n  Robust Comparative Fit Index (CFI)             0.896\n  Robust Tucker-Lewis Index (TLI)                0.925\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7968.693\n  Loglikelihood unrestricted model (H1)      -7866.451\n                                                      \n  Akaike (AIC)                               15949.386\n  Bayesian (BIC)                             15978.410\n  Sample-size adjusted Bayesian (SABIC)      15959.354\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.081\n  90 Percent confidence interval - lower         0.070\n  90 Percent confidence interval - upper         0.091\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.550\n                                                      \n  Robust RMSEA                                   0.134\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.233\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.136\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.792\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.121\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  eta_1 =~                                                              \n    math2             1.000                               8.035    0.800\n    math3             1.000                               8.035    0.799\n    math4             1.000                               8.035    0.792\n    math5             1.000                               8.035    0.779\n    math6             1.000                               8.035    0.762\n    math7             1.000                               8.035    0.742\n    math8             1.000                               8.035    0.719\n  eta_2 =~                                                              \n    math2             0.000                               0.000    0.000\n    math3             1.000                               0.856    0.085\n    math4             2.000                               1.712    0.169\n    math5             3.000                               2.568    0.249\n    math6             4.000                               3.424    0.325\n    math7             5.000                               4.279    0.395\n    math8             6.000                               5.135    0.459\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  eta_1 ~~                                                              \n    eta_2            -0.181    1.150   -0.158    0.875   -0.026   -0.026\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    eta_1            35.267    0.355   99.229    0.000    4.389    4.389\n    eta_2             4.339    0.088   49.136    0.000    5.070    5.070\n   .math2             0.000                               0.000    0.000\n   .math3             0.000                               0.000    0.000\n   .math4             0.000                               0.000    0.000\n   .math5             0.000                               0.000    0.000\n   .math6             0.000                               0.000    0.000\n   .math7             0.000                               0.000    0.000\n   .math8             0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    eta_1            64.562    5.659   11.408    0.000    1.000    1.000\n    eta_2             0.733    0.327    2.238    0.025    1.000    1.000\n   .math2   (thet)   36.230    1.867   19.410    0.000   36.230    0.359\n   .math3   (thet)   36.230    1.867   19.410    0.000   36.230    0.358\n   .math4   (thet)   36.230    1.867   19.410    0.000   36.230    0.352\n   .math5   (thet)   36.230    1.867   19.410    0.000   36.230    0.341\n   .math6   (thet)   36.230    1.867   19.410    0.000   36.230    0.326\n   .math7   (thet)   36.230    1.867   19.410    0.000   36.230    0.309\n   .math8   (thet)   36.230    1.867   19.410    0.000   36.230    0.290\n\n\n\n\nprint(fitMeasures(lg_math_lavaan_fit, c(\"chisq\", \"df\", \"pvalue\", \"cfi\", \"rmsea\")))\n\n  chisq      df  pvalue     cfi   rmsea \n204.484  29.000   0.000   0.791   0.081 \n\n\nGeneriamo un diagramma di percorso.\n\nsemPaths(lg_math_lavaan_fit, what = \"path\", whatLabels = \"par\")\n\n\n\n\n\n\n\n\nEsaminiamo le traiettorie di crescita.\n\nnlsy_math_predicted &lt;- as.data.frame(\n    cbind(nlsy_math_wide$id, lavPredict(lg_math_lavaan_fit))\n)\n\n#naming columns\nnames(nlsy_math_predicted) &lt;- c(\"id\", \"eta_1\", \"eta_2\")\n\nhead(nlsy_math_predicted)\n\n\nA data.frame: 6 x 3\n\n\n\nid\neta_1\neta_2\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n201\n36.94675\n4.534084\n\n\n2\n303\n26.03589\n4.050780\n\n\n3\n2702\n49.70187\n4.594149\n\n\n4\n4303\n41.04200\n4.548064\n\n\n5\n5002\n37.01240\n4.496746\n\n\n6\n5005\n37.68809\n4.324198\n\n\n\n\n\n\n#calculating implied manifest scores\nnlsy_math_predicted$math2 &lt;- 1 * nlsy_math_predicted$eta_1 + 0 * nlsy_math_predicted$eta_2\nnlsy_math_predicted$math3 &lt;- 1 * nlsy_math_predicted$eta_1 + 1 * nlsy_math_predicted$eta_2\nnlsy_math_predicted$math4 &lt;- 1 * nlsy_math_predicted$eta_1 + 2 * nlsy_math_predicted$eta_2\nnlsy_math_predicted$math5 &lt;- 1 * nlsy_math_predicted$eta_1 + 3 * nlsy_math_predicted$eta_2\nnlsy_math_predicted$math6 &lt;- 1 * nlsy_math_predicted$eta_1 + 4 * nlsy_math_predicted$eta_2\nnlsy_math_predicted$math7 &lt;- 1 * nlsy_math_predicted$eta_1 + 5 * nlsy_math_predicted$eta_2\nnlsy_math_predicted$math8 &lt;- 1 * nlsy_math_predicted$eta_1 + 6 * nlsy_math_predicted$eta_2\n\n\n# reshaping wide to long\nnlsy_math_predicted_long &lt;- reshape(\n  data = nlsy_math_predicted,\n  timevar = c(\"grade\"),\n  idvar = \"id\",\n  varying = c(\n    \"math2\", \"math3\", \"math4\", \"math5\", \"math6\", \"math7\", \"math8\"\n  ),\n  direction = \"long\", sep = \"\"\n)\n\n\nhead(nlsy_math_predicted_long)\n\n\nA data.frame: 6 x 5\n\n\n\nid\neta_1\neta_2\ngrade\nmath\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n201.2\n201\n36.94675\n4.534084\n2\n36.94675\n\n\n303.2\n303\n26.03589\n4.050780\n2\n26.03589\n\n\n2702.2\n2702\n49.70187\n4.594149\n2\n49.70187\n\n\n4303.2\n4303\n41.04200\n4.548064\n2\n41.04200\n\n\n5002.2\n5002\n37.01240\n4.496746\n2\n37.01240\n\n\n5005.2\n5005\n37.68809\n4.324198\n2\n37.68809\n\n\n\n\n\n\n# sorting for easy viewing\n# order by id and time\nnlsy_math_predicted_long &lt;-\n  nlsy_math_predicted_long[order(nlsy_math_predicted_long$id, nlsy_math_predicted_long$grade), ]\n\n\nhead(nlsy_math_predicted_long)\n\n\nA data.frame: 6 x 5\n\n\n\nid\neta_1\neta_2\ngrade\nmath\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n201.2\n201\n36.94675\n4.534084\n2\n36.94675\n\n\n201.3\n201\n36.94675\n4.534084\n3\n41.48083\n\n\n201.4\n201\n36.94675\n4.534084\n4\n46.01492\n\n\n201.5\n201\n36.94675\n4.534084\n5\n50.54900\n\n\n201.6\n201\n36.94675\n4.534084\n6\n55.08309\n\n\n201.7\n201\n36.94675\n4.534084\n7\n59.61717\n\n\n\n\n\n\nggplot(\n  data = nlsy_math_predicted_long, # data set\n  aes(x = grade, y = math, group = id)\n) + # setting variables\n  # geom_point(size=.5) + #adding points to plot\n  geom_line(alpha = 0.15) + # adding lines to plot\n  # setting the x-axis with breaks and labels\n  scale_x_continuous(\n    limits = c(2, 8),\n    breaks = c(2, 3, 4, 5, 6, 7, 8),\n    name = \"Grade at Testing\"\n  ) +\n  # setting the y-axis with limits breaks and labels\n  scale_y_continuous(\n    limits = c(10, 90),\n    breaks = c(10, 30, 50, 70, 90),\n    name = \"Predicted PIAT Mathematics\"\n  )\n\n\n\n\n\n\n\n\nIl modello di crescita latente lineare descrive la traiettoria di sviluppo di ogni bambino attraverso una linea retta, mettendo in luce le variazioni individuali nelle competenze matematiche nel corso del tempo. Il grafico illustra che, per ciascun bambino, si registra un incremento “reale” di circa 5 punti nell’abilità matematica per ogni anno scolastico. Questo modello, quindi, non solo traccia la progressione lineare delle competenze matematiche, ma rivela anche un pattern di crescita coerente e uniforme tra i bambini nel periodo considerato.\n\n75.4.1 Sintassi alternativa\nPer semplificare la scrittura del modello possiamo usare la funzione growth. Tuttavia, per il modello discusso in precedenza, è necessario specificare un parametro aggiuntivo rispetto ai default di growth: vogliamo che le varianze residue di math siano costanti nel tempo.\n\nm1 &lt;-   '\n  i =~ 1*math2 + 1*math3 + 1*math4 + 1*math5 + 1*math6 + 1*math7 + 1*math8  \n  s =~ 0 * math2 + 1 * math3 + 2 * math4 + 3 * math5 + 4 * math6 + 5 * math7 + 6 * math8\n  \n  # manifest variances (made equivalent by naming theta)\n  math2 ~~ theta*math2\n  math3 ~~ theta*math3\n  math4 ~~ theta*math4\n  math5 ~~ theta*math5\n  math6 ~~ theta*math6\n  math7 ~~ theta*math7\n  math8 ~~ theta*math8\n'\n\nAdattiamo il modello.\n\nfit_m1 &lt;- growth(\n  m1,\n  data = nlsy_math_wide,\n  estimator = \"ML\",\n  missing = \"fiml\"\n)\n\nOtteniamo in questo modo lo stesso risultato trovato con la precedente specificazione del modello.\n\nprint(fitMeasures(fit_m1, c(\"chisq\", \"df\", \"pvalue\", \"cfi\", \"rmsea\")))\n\n  chisq      df  pvalue     cfi   rmsea \n204.484  29.000   0.000   0.791   0.081 \n\n\n\n\n75.4.2 Interpretazione dei Parametri del Modello\nNell’output relativo alla sezione Intercepts, il parametro eta_1, con un valore di 35.267, rappresenta la previsione del punteggio in matematica al tempo iniziale $ t_0 $. Questo valore indica la media iniziale dei punteggi in matematica per gli studenti. Per quanto riguarda il parametro eta_2, il suo valore di 4.339 suggerisce che, ad ogni incremento unitario nell’arco temporale considerato, ci si aspetta un aumento medio di 4.339 punti nel punteggio predetto di matematica.\nPassando alla sezione Variances, il valore di eta_1 pari a 64.562 indica la varianza tra gli studenti nelle intercette, cioè la variabilità dei valori iniziali di matematica tra i diversi studenti. Il valore di eta_2, pari a 0.733, rappresenta invece la varianza tra gli studenti nelle pendenze, ossia la variabilità dei tassi di crescita dei punteggi in matematica tra gli studenti. Calcolando l’intervallo $ 35.267 $ e assumendo una distribuzione normale, otteniamo una stima dell’intervallo al 95% per i valori plausibili delle medie dei punteggi in matematica tra gli studenti. Questo intervallo non rappresenta un intervallo di fiducia frequentista, ma piuttosto un intervallo attorno alla stima del valore vero. Analogamente, l’intervallo $ 4.339 $ fornisce una stima dell’intervallo al 95% per i valori plausibili delle pendenze dei punteggi in matematica tra gli studenti.\nLa covarianza stimata di -0.181 (con SE = 1.150) suggerisce che non vi è una relazione significativa tra intercette e pendenze. Se la covarianza fosse stata positiva, avremmo potuto interpretarla come un’indicazione che studenti con un punteggio iniziale più alto in matematica tendono a mostrare un maggiore incremento dei punteggi nel tempo. Al contrario, una covarianza negativa tra intercetta e pendenza implicherebbe che studenti con punteggi iniziali più alti tendono a mostrare un aumento meno marcato dei punteggi nel tempo.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>75</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/07_growth_1.html#confronto-con-il-modello-a-effetti-misti",
    "href": "chapters/lgm/07_growth_1.html#confronto-con-il-modello-a-effetti-misti",
    "title": "75  Curve di crescita latente",
    "section": "75.5 Confronto con il Modello a Effetti Misti",
    "text": "75.5 Confronto con il Modello a Effetti Misti\nProcediamo ora all’analisi degli stessi dati impiegando un modello a effetti misti. Dobbiamo però tenere presente che, in questo contesto, non saremo in grado di replicare esattamente gli stessi risultati ottenuti con il modello di crescita latente (LGM), a causa della presenza di dati mancanti. Nel modello LGM, abbiamo adottato l’approccio della massima verosimiglianza (ML) per la stima dei parametri, gestendo i dati mancanti attraverso l’uso del metodo fiml (Full Information Maximum Likelihood) implementato nel software lavaan. Questo metodo non comporta l’imputazione dei dati mancanti, ma sfrutta le informazioni disponibili in ciascun caso per stimare i parametri secondo il criterio della massima verosimiglianza.\nTuttavia, quando si tratta di modelli a effetti misti, il metodo FIML non è generalmente una strategia applicabile. Di conseguenza, per procedere con l’analisi in questo contesto, adotteremo una soluzione alternativa, consistente nell’eliminazione dei casi che presentano dati mancanti. Questo approccio, sebbene meno sofisticato rispetto al FIML, ci permetterà di procedere con l’analisi del modello a effetti misti, pur con una certa limitazione dovuta alla riduzione del campione di dati disponibili.\nIn formato long, i dati sono i seguenti.\n\nnlsy_math_long |&gt;\n    head()\n\n\nA data.frame: 6 x 3\n\n\n\nid\ngrade\nmath\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n\n\n\n\n201.3\n201\n3\n38\n\n\n201.5\n201\n5\n55\n\n\n303.2\n303\n2\n26\n\n\n303.5\n303\n5\n33\n\n\n2702.2\n2702\n2\n56\n\n\n2702.4\n2702\n4\n58\n\n\n\n\n\nSottraiamo 2 dalla variabile grade in modo che il valore 0 corrisponda alla prima rilevazione temporale. In questo modo, l’intercetta rappresenterà il valore atteso del punteggio di matematica per la prima rilevazione temporale (quando grade è pari a 2).\n\nnlsy_math_long$grade_c2 &lt;- nlsy_math_long$grade - 2\n\nNel contesto del modello a effetti misti, utilizziamo la funzione lmer per adattare il modello. In questa configurazione ((1 | id)), adottiamo un modello con intercette casuali che prevede una pendenza uniforme per tutti gli individui, implicando un tasso di crescita costante per ciascuno. Questa scelta è coerente con le traiettorie di crescita illustrate nella figura precedente.\nL’utilizzo dell’opzione REML = FALSE nel modello specifica che stiamo applicando il metodo della massima verosimiglianza (ML) per la stima dei parametri, anziché l’approccio REML (Restricted Maximum Likelihood), che è il metodo predefinito nella funzione lmer.\nIn aggiunta, l’opzione na.action = na.exclude viene utilizzata per indicare che le osservazioni contenenti dati mancanti saranno escluse dall’analisi. Questo significa che tali osservazioni non contribuiranno alla stima dei parametri del modello, permettendoci di procedere con l’analisi nonostante la presenza di dati incompleti. Questo approccio, benché pratico, può avere implicazioni sulla rappresentatività e sulla generalizzabilità dei risultati, specialmente se la quantità di dati mancanti è sostanziale.\n\nfit_lmer &lt;- lmer(\n    math ~ 1 + grade_c2 + (1 | id),\n    data = nlsy_math_long,\n    REML = FALSE,\n    na.action = na.exclude\n)\n\n\nsummary(fit_lmer) |&gt;\n    print()\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: math ~ 1 + grade_c2 + (1 | id)\n   Data: nlsy_math_long\n\n     AIC      BIC   logLik deviance df.resid \n 15957.7  15980.5  -7974.8  15949.7     2217 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2082 -0.5265  0.0081  0.5456  2.5651 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n id       (Intercept) 67.30    8.204   \n Residual             39.31    6.270   \nNumber of obs: 2221, groups:  id, 932\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 35.33081    0.36264   97.43\ngrade_c2     4.29352    0.08266   51.94\n\nCorrelation of Fixed Effects:\n         (Intr)\ngrade_c2 -0.555\n\n\nDall’output vediamo che il punteggio di matematica in corrispondenza del secondo grado scolastico (codificato qui con 0) è uguale a 35.33 (0.36). Il tasso di crescita, ovvero l’aumento atteso dei punteggi di matematica per ciascun grado scolastico è uguale a 4.29 (0.08).\nUna rappresentazione grafica dei punteggi predetti dal modello misto può essere ottenuta nel modo seguente.\n\ngr &lt;- emmeans::ref_grid(fit_lmer, cov.keep= c('grade_c2'))\nemm &lt;- emmeans(gr, spec= c('grade_c2'), level= 0.95)\n\n\nnlsy_math_long |&gt;\n    ggplot(aes(x= grade_c2, y= math)) +\n        geom_ribbon(\n            data= data.frame(emm), \n            aes(ymin= lower.CL, ymax= upper.CL, y= NULL), fill= 'grey80'\n        ) +\n        geom_line(data= data.frame(emm), aes(y= emmean)) +\n        geom_point() \n\n\n\n\n\n\n\n\nQuesti risultati, ottenuti escludendo tutte le osservazioni con dati mancanti, sono comunque molto simili ai risultati ottenuti usando lavaan (si veda la figura con le traiettorie di crescita del modello LGM).",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>75</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/07_growth_1.html#modello-di-crescita-non-lineare",
    "href": "chapters/lgm/07_growth_1.html#modello-di-crescita-non-lineare",
    "title": "75  Curve di crescita latente",
    "section": "75.6 Modello di Crescita Non Lineare",
    "text": "75.6 Modello di Crescita Non Lineare\nIn alcuni casi, può essere utile esplorare la possibilità che il cambiamento osservato segua una traiettoria non lineare. Il metodo adottato per costruire un modello di crescita non lineare può essere paragonato all’utilizzo di variabili dummy in un modello di regressione lineare. Tuttavia, in questo contesto, apportiamo una modifica specifica ai carichi fattoriali associati alla variabile latente che rappresenta la pendenza.\nNel modello di crescita non lineare, fissiamo il primo carico fattoriale a 0 e l’ultimo a 1. Questa configurazione implica che il primo punto temporale rappresenta il punto di partenza, mentre l’ultimo indica la conclusione dell’intervallo temporale considerato. I carichi fattoriali intermedi, invece, non sono fissi e vengono stimati liberamente dal modello. Questa impostazione permette di interpretare la pendenza come l’entità complessiva del cambiamento che si verifica tra l’inizio e la fine dell’intervallo temporale considerato.\nI carichi fattoriali che vengono stimati rappresentano la proporzione del cambiamento complessivo che si è verificato fino a quel particolare punto temporale, rispetto al cambiamento totale osservato durante l’intero intervallo. In altre parole, questi carichi fattoriali intermedi offrono una misura di quanto il cambiamento si sia sviluppato a ogni punto temporale intermedio, in rapporto al cambiamento totale che si è verificato dall’inizio alla fine del periodo considerato.\nAttraverso questo approccio, il modello di crescita non lineare fornisce una comprensione più dettagliata e flessibile della dinamica del cambiamento, permettendo di catturare traiettorie che potrebbero non essere adeguatamente descritte da un modello lineare.\n\nmod_nl &lt;- \"\n    i =~ 1*math2 + 1*math3 + 1*math4 + 1*math5 + 1*math6 + 1*math7 + 1*math8\n    s = ~ 0 * math2 + math3 + math4 + math5 + math6 + math7 + 1*math8\n    math2 ~~ theta*math2\n    math3 ~~ theta*math3\n    math4 ~~ theta*math4\n    math5 ~~ theta*math5\n    math6 ~~ theta*math6\n    math7 ~~ theta*math7\n    math8 ~~ theta*math8\n\"\n\nAdattiamo il modello ai dati.\n\nfit_nl &lt;- growth(\n  mod_nl,\n  data = nlsy_math_wide, \n  estimator = \"ML\",\n  missing = \"fiml\"\n)\n\nEsaminiamo la soluzione.\n\nsummary(fit_nl, fit.measures = TRUE, standardized = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 128 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        17\n  Number of equality constraints                     6\n\n                                                  Used       Total\n  Number of observations                           932         933\n  Number of missing patterns                        60            \n\nModel Test User Model:\n                                                      \n  Test statistic                                52.947\n  Degrees of freedom                                24\n  P-value (Chi-square)                           0.001\n\nModel Test Baseline Model:\n\n  Test statistic                               862.334\n  Degrees of freedom                                21\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.966\n  Tucker-Lewis Index (TLI)                       0.970\n                                                      \n  Robust Comparative Fit Index (CFI)             1.000\n  Robust Tucker-Lewis Index (TLI)                1.023\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7892.924\n  Loglikelihood unrestricted model (H1)      -7866.451\n                                                      \n  Akaike (AIC)                               15807.848\n  Bayesian (BIC)                             15861.059\n  Sample-size adjusted Bayesian (SABIC)      15826.124\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.036\n  90 Percent confidence interval - lower         0.023\n  90 Percent confidence interval - upper         0.049\n  P-value H_0: RMSEA &lt;= 0.050                    0.961\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n                                                      \n  Robust RMSEA                                   0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.177\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.644\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.284\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.094\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i =~                                                                  \n    math2             1.000                               8.509    0.839\n    math3             1.000                               8.509    0.857\n    math4             1.000                               8.509    0.851\n    math5             1.000                               8.509    0.840\n    math6             1.000                               8.509    0.823\n    math7             1.000                               8.509    0.808\n    math8             1.000                               8.509    0.791\n  s =~                                                                  \n    math2             0.000                               0.000    0.000\n    math3             0.295    0.019   15.783    0.000    1.849    0.186\n    math4             0.533    0.019   28.588    0.000    3.346    0.335\n    math5             0.664    0.021   31.083    0.000    4.167    0.411\n    math6             0.799    0.022   36.470    0.000    5.016    0.485\n    math7             0.901    0.030   30.314    0.000    5.656    0.537\n    math8             1.000                               6.276    0.583\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i ~~                                                                  \n    s               -13.303    7.281   -1.827    0.068   -0.249   -0.249\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    i                32.400    0.474   68.399    0.000    3.808    3.808\n    s                25.539    0.731   34.916    0.000    4.070    4.070\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .math2   (thet)   30.502    1.678   18.182    0.000   30.502    0.296\n   .math3   (thet)   30.502    1.678   18.182    0.000   30.502    0.310\n   .math4   (thet)   30.502    1.678   18.182    0.000   30.502    0.305\n   .math5   (thet)   30.502    1.678   18.182    0.000   30.502    0.297\n   .math6   (thet)   30.502    1.678   18.182    0.000   30.502    0.286\n   .math7   (thet)   30.502    1.678   18.182    0.000   30.502    0.275\n   .math8   (thet)   30.502    1.678   18.182    0.000   30.502    0.264\n    i                72.408    6.590   10.988    0.000    1.000    1.000\n    s                39.385   11.371    3.464    0.001    1.000    1.000\n\n\n\nEffettuiamo il test del rapporto di verosimiglianze per confrontare il modello di crescita lineare con quello che assume una crescita non lineare.\n\nlavTestLRT(fit_m1, fit_nl) |&gt;\n    print()\n\n\nChi-Squared Difference Test\n\n       Df   AIC   BIC   Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nfit_nl 24 15808 15861  52.947                                          \nfit_m1 29 15949 15978 204.484     151.54 0.17733       5  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIl risultato indica che il modello che assume un cambiamento non lineare si adatta meglio ai dati. Possiamo visualizzare il cambiamento nel modo seguente.\n\n# extract just th eloadings of the slopes\nloadings &lt;- parameterestimates(fit_nl) %&gt;% # get estimates\n  filter(lhs == \"s\", op == \"=~\") %&gt;% # filter the rows we want\n  .[[\"est\"]] # extract \"est\" variable\n# print result\nprint(loadings)\n\n[1] 0.0000000 0.2946469 0.5331544 0.6640287 0.7992433 0.9012769 1.0000000\n\n\n\n# predict scores\npred_lgm3 &lt;- predict(fit_nl)\n# create long data for each individual\npred_lgm3_long &lt;- map(loadings, # loop over time\n                      function(x) pred_lgm3[, 1] + \n                        x * pred_lgm3[, 2]) %&gt;% \n  reduce(cbind) %&gt;% # bring together the wave predictions \n  as.data.frame()\n\n\n# predict scores\npred_lgm3 &lt;- predict(fit_nl)\n# create long data for each individual\npred_lgm3_long &lt;- map(loadings, # loop over time\n                      function(x) pred_lgm3[, 1] + \n                        x * pred_lgm3[, 2]) %&gt;% \n  reduce(cbind) %&gt;% # bring together the wave predictions \n  as.data.frame() %&gt;% # make data frame\n  setNames(str_c(\"Grade \", 1:7)) %&gt;% # give names to variables\n  mutate(id = row_number()) %&gt;% # make unique id\n  gather(-id, key = grade, value = pred) # make long format\npred_lgm3_long %&gt;% \n  ggplot(aes(grade, pred, group = id)) + # what variables to plot?\n  geom_line(alpha = 0.05) + # add a transparent line for each person\n  stat_summary( # add average line\n    aes(group = 1),\n    fun = mean,\n    geom = \"line\",\n    size = 1.5,\n    color = \"green\"\n  ) + \n  stat_summary(data = pred_lgm3_long, # add average from linear model\n               aes(group = 1),\n               fun = mean,\n               geom = \"line\",\n               size = 1.5,\n               color = \"red\",\n               alpha = 0.5\n  ) +\n  stat_summary(data = pred_lgm3_long, # add average from squared model\n               aes(group = 1),\n               fun = mean,\n               geom = \"line\",\n               size = 1.5,\n               color = \"blue\",\n               alpha = 0.5\n  ) +\n  labs(y = \"Predicted PIAT Mathematics\", # labels\n       x = \"Grade at Testing\")",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>75</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/07_growth_1.html#riflessioni-finali",
    "href": "chapters/lgm/07_growth_1.html#riflessioni-finali",
    "title": "75  Curve di crescita latente",
    "section": "75.7 Riflessioni Finali",
    "text": "75.7 Riflessioni Finali\nQuesto capitolo ha esplorato l’implementazione e l’adattamento dei modelli di crescita lineare all’interno del framework della modellizzazione delle equazioni strutturali (SEM), utilizzando il pacchetto lavaan in R. Abbiamo illustrato come calcolare e visualizzare graficamente le traiettorie di crescita predette da questi modelli.\nI modelli di crescita lineare rappresentano un punto di partenza essenziale per analizzare il cambiamento individuale nel tempo. Tuttavia, possono non essere sempre in grado di descrivere accuratamente il processo di cambiamento. Per questa ragione, è opportuno valutare anche altri modelli e, eventualmente, esaminare le variazioni tra diversi gruppi. L’impiego dei modelli di crescita all’interno dei framework SEM e dei modelli a effetti misti presenta sia vantaggi sia limitazioni. Ad esempio, i modelli SEM offrono indici di adattamento globale quali RMSEA, CFI e TLI, che non sono disponibili nell’approccio dei modelli a effetti misti, i quali si basano piuttosto su criteri come AIC e BIC e su strumenti diagnostici quali i grafici dei residui.\nUn aspetto cruciale nell’adattamento dei modelli di crescita lineare è la scelta della metrica temporale. Nel nostro esempio, abbiamo utilizzato il grado scolastico come indicatore temporale, ma esistono altre opzioni possibili. Ad esempio, l’età al momento del test potrebbe essere una metrica più appropriata, in quanto potrebbe riflettere più accuratamente gli intervalli tra le misurazioni. Si deve inoltre considerare che l’utilizzo del grado scolastico può avere delle limitazioni, ad esempio in casi di studenti che ripetono o saltano un anno.\nLa posizione dell’intercetta può essere scelta in qualsiasi punto del continuum temporale. Nel nostro esempio, abbiamo centrato l’intercetta sulla valutazione della seconda elementare, in quanto era il primo dato disponibile. Tuttavia, è importante selezionare un punto di origine che sia significativo per lo studio specifico. Per esempio, posizionare l’intercetta alla fine dell’ottava elementare potrebbe essere rilevante per studi che mirano a valutare la preparazione degli studenti per la scuola superiore.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>75</span>  <span class='chapter-title'>Curve di crescita latente</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/08_growth_cont.html",
    "href": "chapters/lgm/08_growth_cont.html",
    "title": "76  Il tempo su una metrica continua",
    "section": "",
    "text": "76.1 Introduzione\nQuando si affronta lo studio del cambiamento in un individuo, un aspetto cruciale è la selezione di una scala temporale adeguata per osservare questo cambiamento. Nel capitolo precedente, ad esempio, abbiamo adottato il grado scolastico come nostra scala temporale di riferimento, strutturando le osservazioni su questa base. Tuttavia, il grado scolastico non è l’unica scala temporale applicabile a tali dati. Altre scale significative potrebbero essere l’età o le specifiche occasioni in cui si effettuano le misurazioni.\nCi sono scale temporali che rappresentano intervalli discreti, come le occasioni di misurazione, dove i valori assunti sono specifici e comuni tra i partecipanti. In questo contesto, però, potrebbe non essere possibile valutare ogni partecipante ad ogni occasione di misurazione. D’altra parte, esistono scale temporali più fluide, come l’età, dove i valori sono unici per ciascun partecipante e non condivisi.\nInteressante è notare come la stessa scala temporale possa essere impiegata sia in un contesto discreto che continuo. Ad esempio, l’età può essere approssimata all’anno più vicino, mentre il grado scolastico può essere definito più precisamente, considerando l’anno scolastico e il numero di giorni trascorsi dall’inizio dell’anno scolastico.\nIn questo capitolo, ci concentreremo sulle tecniche per modellare la crescita individuale utilizzando una scala temporale continua, esplorando come questa possa fornire una comprensione più dettagliata e sfumata del cambiamento all’interno della persona.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>76</span>  <span class='chapter-title'>Il tempo su una metrica continua</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/08_growth_cont.html#lapplicazione-della-finestra-temporale",
    "href": "chapters/lgm/08_growth_cont.html#lapplicazione-della-finestra-temporale",
    "title": "76  Il tempo su una metrica continua",
    "section": "76.2 L’Applicazione della Finestra Temporale",
    "text": "76.2 L’Applicazione della Finestra Temporale\nProseguendo nell’analisi delle metriche del tempo, un approccio interessante è quello della finestra temporale, particolarmente utile per dati che presentano occasioni di misurazione variabili individualmente. Questa metodologia cerca di standardizzare la variabilità temporale individuale su una scala temporale discreta. Un esempio pratico di questo può essere visto nell’arrotondamento dell’età o del tempo al semestre o al quarto d’anno più vicino.\nQuesto metodo, benché utile, rappresenta ancora un’approssimazione della realtà temporale. Riducendo la dimensione delle finestre temporali si può aumentare la precisione, ma questo può comportare una maggiore dispersione dei dati, rendendo così più complessa l’accuratezza delle stime.\nNell’applicazione pratica di questo esempio, definiamo le finestre temporali in termini di semestri. Pertanto, lavoriamo con i dati in formato long, arrotondando l’età al semestre più vicino, e successivamente convertiamo questi dati in formato wide. Questo consente la loro integrazione nel framework SEM, facilitando l’analisi e l’interpretazione dei cambiamenti individuali nel tempo.\nPer questo esempio considereremo i dati di abilità matematica NLSY-CYA Long Data [si veda {cite:t}grimm2016growth]. Iniziamo a leggere i dati.\n\n#set filepath for data file\nfilepath &lt;- \"https://raw.githubusercontent.com/LRI-2/Data/main/GrowthModeling/nlsy_math_long_R.dat\"\n#read in the text data file using the url() function\ndat &lt;- read.table(file=url(filepath),\n                  na.strings = \".\")  #indicates the missing data designator\n#copy data with new name \nnlsy_math_long &lt;- dat  \n\n#Add names the columns of the data set\nnames(nlsy_math_long) = c('id'     , 'female', 'lb_wght', \n                          'anti_k1', 'math'  , 'grade'  ,\n                          'occ'    , 'age'   , 'men'    ,\n                          'spring' , 'anti')\n\n#subset to the variables of interest\nnlsy_math_long &lt;- nlsy_math_long[ ,c(\"id\", \"math\", \"grade\", \"age\")]\n#view the first few observations in the data set \nhead(nlsy_math_long, 10)\n\n\nA data.frame: 10 x 4\n\n\n\nid\nmath\ngrade\nage\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n201\n38\n3\n111\n\n\n2\n201\n55\n5\n135\n\n\n3\n303\n26\n2\n121\n\n\n4\n303\n33\n5\n145\n\n\n5\n2702\n56\n2\n100\n\n\n6\n2702\n58\n4\n125\n\n\n7\n2702\n80\n8\n173\n\n\n8\n4303\n41\n3\n115\n\n\n9\n4303\n58\n4\n135\n\n\n10\n5002\n46\n4\n117\n\n\n\n\n\n\n#intraindividual change trajetories\nggplot(data=nlsy_math_long,                    #data set\n       aes(x = age, y = math, group = id)) + #setting variables\n  geom_point(size=.5) + #adding points to plot\n  geom_line(alpha = 0.5) +  #adding lines to plot\n  #setting the x-axis with breaks and labels\n  scale_x_continuous(#limits=c(2,8),\n                     #breaks = c(2,3,4,5,6,7,8), \n                     name = \"Age at Testing\") +    \n  #setting the y-axis with limits breaks and labels\n  scale_y_continuous(limits=c(10,90), \n                     breaks = c(10,30,50,70,90), \n                     name = \"PIAT Mathematics\")\n\n\n\n\n\n\n\n\nImplementiamo il metodo della finestra temporale e ricodifichiamo i dati in formato wide.\n\n# creating new age variable scaled in years\nnlsy_math_long$ageyr &lt;- (nlsy_math_long$age / 12)\nhead(nlsy_math_long)\n\n\nA data.frame: 6 x 5\n\n\n\nid\nmath\ngrade\nage\nageyr\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n201\n38\n3\n111\n9.250000\n\n\n2\n201\n55\n5\n135\n11.250000\n\n\n3\n303\n26\n2\n121\n10.083333\n\n\n4\n303\n33\n5\n145\n12.083333\n\n\n5\n2702\n56\n2\n100\n8.333333\n\n\n6\n2702\n58\n4\n125\n10.416667\n\n\n\n\n\n\n# rounding to nearest half-year\n# multiplied by 10 to remove decimal for easy conversion to wide\nnlsy_math_long$agewindow &lt;- plyr::round_any(nlsy_math_long$ageyr * 10, 5)\nhead(nlsy_math_long)\n\n\nA data.frame: 6 x 6\n\n\n\nid\nmath\ngrade\nage\nageyr\nagewindow\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n201\n38\n3\n111\n9.250000\n90\n\n\n2\n201\n55\n5\n135\n11.250000\n110\n\n\n3\n303\n26\n2\n121\n10.083333\n100\n\n\n4\n303\n33\n5\n145\n12.083333\n120\n\n\n5\n2702\n56\n2\n100\n8.333333\n85\n\n\n6\n2702\n58\n4\n125\n10.416667\n105\n\n\n\n\n\n\n# reshaping long to wide (just variables of interest)\nnlsy_math_wide &lt;- reshape(\n  data = nlsy_math_long[, c(\"id\", \"math\", \"agewindow\")],\n  timevar = c(\"agewindow\"),\n  idvar = c(\"id\"),\n  v.names = c(\"math\"),\n  direction = \"wide\", sep = \"\"\n)\n\n# reordering columns for easy viewing\nnlsy_math_wide &lt;- nlsy_math_wide[, c(\n  \"id\", \"math70\", \"math75\", \"math80\", \"math85\", \"math90\", \"math95\", \"math100\", \"math105\", \"math110\", \"math115\", \"math120\", \"math125\", \"math130\", \"math135\", \"math140\", \"math145\"\n)]\n# looking at the data\nhead(nlsy_math_wide)\n\n\nA data.frame: 6 x 17\n\n\n\nid\nmath70\nmath75\nmath80\nmath85\nmath90\nmath95\nmath100\nmath105\nmath110\nmath115\nmath120\nmath125\nmath130\nmath135\nmath140\nmath145\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n201\nNA\nNA\nNA\nNA\n38\nNA\nNA\nNA\n55\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n3\n303\nNA\nNA\nNA\nNA\nNA\nNA\n26\nNA\nNA\nNA\n33\nNA\nNA\nNA\nNA\nNA\n\n\n5\n2702\nNA\nNA\nNA\n56\nNA\nNA\nNA\n58\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n80\n\n\n8\n4303\nNA\nNA\nNA\nNA\nNA\n41\nNA\nNA\n58\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n10\n5002\nNA\nNA\nNA\nNA\nNA\nNA\n46\nNA\nNA\nNA\n54\nNA\nNA\nNA\n66\nNA\n\n\n13\n5005\nNA\nNA\n35\nNA\nNA\n50\nNA\nNA\nNA\n60\nNA\nNA\nNA\n59\nNA\nNA\n\n\n\n\n\nSpecifichiamo il modello SEM.\n\nlg_math_age_lavaan_model &lt;- '\n  # latent variable definitions\n      #intercept (note intercept is a reserved term)\n      eta_1 =~  1*math70 +\n                1*math75 +\n                1*math80 +\n                1*math85 +\n                1*math90 +\n                1*math95 +\n                1*math100 +\n                1*math105 +\n                1*math110 +\n                1*math115 +\n                1*math120 +\n                1*math125 +\n                1*math130 +\n                1*math135 +\n                1*math140 +\n                1*math145 \n\n      #linear slope (note intercept is a reserved term)\n      eta_2 =~ -1*math70 +\n               -0.5*math75 +\n                0*math80 +\n                0.5*math85 +\n                1*math90 +\n                1.5*math95 +\n                2*math100 +\n                2.5*math105 +\n                3*math110 +\n                3.5*math115 +\n                4*math120 +\n                4.5*math125 +\n                5*math130 +\n                5.5*math135 +\n                6*math140 +\n                6.5*math145\n\n  # factor variances\n      eta_1 ~~ start(65)*eta_1\n      eta_2 ~~ start(.75)*eta_2\n\n  # covariances among factors \n      eta_1 ~~ start(1.2)*eta_2\n\n  # manifest variances (made equivalent by naming theta)\n      math70 ~~ start(35)*theta*math70\n      math75 ~~ theta*math75\n      math80 ~~ theta*math80\n      math85 ~~ theta*math85\n      math90 ~~ theta*math90\n      math95 ~~ theta*math95\n      math100 ~~ theta*math100\n      math105 ~~ theta*math105\n      math110 ~~ theta*math110\n      math115 ~~ theta*math115\n      math120 ~~ theta*math120\n      math125 ~~ theta*math125\n      math130 ~~ theta*math130\n      math135 ~~ theta*math135\n      math140 ~~ theta*math140\n      math145 ~~ theta*math145\n      \n  # manifest means (fixed at zero)\n      math70 ~ 0*1\n      math75 ~ 0*1\n      math80 ~ 0*1\n      math85 ~ 0*1\n      math90 ~ 0*1\n      math95 ~ 0*1\n      math100 ~ 0*1\n      math105 ~ 0*1\n      math110 ~ 0*1\n      math115 ~ 0*1\n      math120 ~ 0*1\n      math125 ~ 0*1\n      math130 ~ 0*1\n      math135 ~ 0*1\n      math140 ~ 0*1\n      math145 ~ 0*1\n\n  # factor means (estimated freely)\n      eta_1 ~ start(35)*1\n      eta_2 ~ start(4)*1\n' #end of model definition\n\nIn questo modello, si definiscono due variabili latenti: l’intercetta latente (eta_1) e la pendenza lineare (eta_2). La scelta dei coefficienti per eta_2 consente di modellare una traiettoria di crescita lineare nel tempo. Ogni coefficiente corrisponde al peso assegnato a ciascuna misura di matematica (math70, math75, …, math145) nell’espressione della pendenza lineare.\n\nI coefficienti vanno da -1 a 6.5, aumentando di 0.5 ad ogni passaggio. Questa progressione rappresenta l’aumento lineare nel tempo. Ad esempio, math70 ha un coefficiente di -1, math75 ha un coefficiente di -0.5, e così via fino a math145, che ha un coefficiente di 6.5.\nI coefficienti sono scelti per mantenere una distanza temporale costante tra ogni punto di misurazione. Ad esempio, la differenza di 0.5 tra i coefficienti di math70 e math75 implica che il lasso di tempo tra queste due misurazioni è costante rispetto alle altre misurazioni.\nÈ interessante notare che il coefficiente per math80 è 0. Questo implica che math80 è stato scelto come punto di riferimento o centro per la pendenza lineare. I valori negativi e positivi dei coefficienti rappresentano misurazioni prima e dopo questo punto di riferimento, rispettivamente.\n\nAdattiamo il modello ai dati.\n\n#estimating the model using sem() function\nlg_math_age_lavaan_fit &lt;- sem(lg_math_age_lavaan_model,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\"\n)\n\nEsaminiamo la soluzione.\n\nsummary(lg_math_age_lavaan_fit, fit.measures = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 40 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n  Number of equality constraints                    15\n\n  Number of observations                           932\n  Number of missing patterns                       139\n\nModel Test User Model:\n                                                      \n  Test statistic                               295.028\n  Degrees of freedom                               146\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              1053.342\n  Degrees of freedom                               120\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.840\n  Tucker-Lewis Index (TLI)                       0.869\n                                                      \n  Robust Comparative Fit Index (CFI)             0.003\n  Robust Tucker-Lewis Index (TLI)                0.181\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7928.559\n  Loglikelihood unrestricted model (H1)      -7781.045\n                                                      \n  Akaike (AIC)                               15869.117\n  Bayesian (BIC)                             15898.141\n  Sample-size adjusted Bayesian (SABIC)      15879.086\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.033\n  90 Percent confidence interval - lower         0.028\n  90 Percent confidence interval - upper         0.039\n  P-value H_0: RMSEA &lt;= 0.050                    1.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n                                                      \n  Robust RMSEA                                   4.193\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: Robust RMSEA &lt;= 0.050               NaN\n  P-value H_0: Robust RMSEA &gt;= 0.080               NaN\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.314\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math70            1.000                           \n    math75            1.000                           \n    math80            1.000                           \n    math85            1.000                           \n    math90            1.000                           \n    math95            1.000                           \n    math100           1.000                           \n    math105           1.000                           \n    math110           1.000                           \n    math115           1.000                           \n    math120           1.000                           \n    math125           1.000                           \n    math130           1.000                           \n    math135           1.000                           \n    math140           1.000                           \n    math145           1.000                           \n  eta_2 =~                                            \n    math70           -1.000                           \n    math75           -0.500                           \n    math80            0.000                           \n    math85            0.500                           \n    math90            1.000                           \n    math95            1.500                           \n    math100           2.000                           \n    math105           2.500                           \n    math110           3.000                           \n    math115           3.500                           \n    math120           4.000                           \n    math125           4.500                           \n    math130           5.000                           \n    math135           5.500                           \n    math140           6.000                           \n    math145           6.500                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2             1.157    1.010    1.146    0.252\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .math70            0.000                           \n   .math75            0.000                           \n   .math80            0.000                           \n   .math85            0.000                           \n   .math90            0.000                           \n   .math95            0.000                           \n   .math100           0.000                           \n   .math105           0.000                           \n   .math110           0.000                           \n   .math115           0.000                           \n   .math120           0.000                           \n   .math125           0.000                           \n   .math130           0.000                           \n   .math135           0.000                           \n   .math140           0.000                           \n   .math145           0.000                           \n    eta_1            35.236    0.347  101.512    0.000\n    eta_2             4.229    0.081   51.910    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            65.063    5.503   11.824    0.000\n    eta_2             0.725    0.277    2.616    0.009\n   .math70  (thet)   32.337    1.695   19.083    0.000\n   .math75  (thet)   32.337    1.695   19.083    0.000\n   .math80  (thet)   32.337    1.695   19.083    0.000\n   .math85  (thet)   32.337    1.695   19.083    0.000\n   .math90  (thet)   32.337    1.695   19.083    0.000\n   .math95  (thet)   32.337    1.695   19.083    0.000\n   .math100 (thet)   32.337    1.695   19.083    0.000\n   .math105 (thet)   32.337    1.695   19.083    0.000\n   .math110 (thet)   32.337    1.695   19.083    0.000\n   .math115 (thet)   32.337    1.695   19.083    0.000\n   .math120 (thet)   32.337    1.695   19.083    0.000\n   .math125 (thet)   32.337    1.695   19.083    0.000\n   .math130 (thet)   32.337    1.695   19.083    0.000\n   .math135 (thet)   32.337    1.695   19.083    0.000\n   .math140 (thet)   32.337    1.695   19.083    0.000\n   .math145 (thet)   32.337    1.695   19.083    0.000\n\n\n\n\nparameterEstimates(lg_math_age_lavaan_fit) |&gt;\n    print()\n\n       lhs op     rhs label    est    se       z pvalue ci.lower ci.upper\n1    eta_1 =~  math70        1.000 0.000      NA     NA    1.000    1.000\n2    eta_1 =~  math75        1.000 0.000      NA     NA    1.000    1.000\n3    eta_1 =~  math80        1.000 0.000      NA     NA    1.000    1.000\n4    eta_1 =~  math85        1.000 0.000      NA     NA    1.000    1.000\n5    eta_1 =~  math90        1.000 0.000      NA     NA    1.000    1.000\n6    eta_1 =~  math95        1.000 0.000      NA     NA    1.000    1.000\n7    eta_1 =~ math100        1.000 0.000      NA     NA    1.000    1.000\n8    eta_1 =~ math105        1.000 0.000      NA     NA    1.000    1.000\n9    eta_1 =~ math110        1.000 0.000      NA     NA    1.000    1.000\n10   eta_1 =~ math115        1.000 0.000      NA     NA    1.000    1.000\n11   eta_1 =~ math120        1.000 0.000      NA     NA    1.000    1.000\n12   eta_1 =~ math125        1.000 0.000      NA     NA    1.000    1.000\n13   eta_1 =~ math130        1.000 0.000      NA     NA    1.000    1.000\n14   eta_1 =~ math135        1.000 0.000      NA     NA    1.000    1.000\n15   eta_1 =~ math140        1.000 0.000      NA     NA    1.000    1.000\n16   eta_1 =~ math145        1.000 0.000      NA     NA    1.000    1.000\n17   eta_2 =~  math70       -1.000 0.000      NA     NA   -1.000   -1.000\n18   eta_2 =~  math75       -0.500 0.000      NA     NA   -0.500   -0.500\n19   eta_2 =~  math80        0.000 0.000      NA     NA    0.000    0.000\n20   eta_2 =~  math85        0.500 0.000      NA     NA    0.500    0.500\n21   eta_2 =~  math90        1.000 0.000      NA     NA    1.000    1.000\n22   eta_2 =~  math95        1.500 0.000      NA     NA    1.500    1.500\n23   eta_2 =~ math100        2.000 0.000      NA     NA    2.000    2.000\n24   eta_2 =~ math105        2.500 0.000      NA     NA    2.500    2.500\n25   eta_2 =~ math110        3.000 0.000      NA     NA    3.000    3.000\n26   eta_2 =~ math115        3.500 0.000      NA     NA    3.500    3.500\n27   eta_2 =~ math120        4.000 0.000      NA     NA    4.000    4.000\n28   eta_2 =~ math125        4.500 0.000      NA     NA    4.500    4.500\n29   eta_2 =~ math130        5.000 0.000      NA     NA    5.000    5.000\n30   eta_2 =~ math135        5.500 0.000      NA     NA    5.500    5.500\n31   eta_2 =~ math140        6.000 0.000      NA     NA    6.000    6.000\n32   eta_2 =~ math145        6.500 0.000      NA     NA    6.500    6.500\n33   eta_1 ~~   eta_1       65.063 5.503  11.824  0.000   54.278   75.849\n34   eta_2 ~~   eta_2        0.725 0.277   2.616  0.009    0.182    1.268\n35   eta_1 ~~   eta_2        1.157 1.010   1.146  0.252   -0.822    3.136\n36  math70 ~~  math70 theta 32.337 1.695  19.083  0.000   29.016   35.658\n37  math75 ~~  math75 theta 32.337 1.695  19.083  0.000   29.016   35.658\n38  math80 ~~  math80 theta 32.337 1.695  19.083  0.000   29.016   35.658\n39  math85 ~~  math85 theta 32.337 1.695  19.083  0.000   29.016   35.658\n40  math90 ~~  math90 theta 32.337 1.695  19.083  0.000   29.016   35.658\n41  math95 ~~  math95 theta 32.337 1.695  19.083  0.000   29.016   35.658\n42 math100 ~~ math100 theta 32.337 1.695  19.083  0.000   29.016   35.658\n43 math105 ~~ math105 theta 32.337 1.695  19.083  0.000   29.016   35.658\n44 math110 ~~ math110 theta 32.337 1.695  19.083  0.000   29.016   35.658\n45 math115 ~~ math115 theta 32.337 1.695  19.083  0.000   29.016   35.658\n46 math120 ~~ math120 theta 32.337 1.695  19.083  0.000   29.016   35.658\n47 math125 ~~ math125 theta 32.337 1.695  19.083  0.000   29.016   35.658\n48 math130 ~~ math130 theta 32.337 1.695  19.083  0.000   29.016   35.658\n49 math135 ~~ math135 theta 32.337 1.695  19.083  0.000   29.016   35.658\n50 math140 ~~ math140 theta 32.337 1.695  19.083  0.000   29.016   35.658\n51 math145 ~~ math145 theta 32.337 1.695  19.083  0.000   29.016   35.658\n52  math70 ~1                0.000 0.000      NA     NA    0.000    0.000\n53  math75 ~1                0.000 0.000      NA     NA    0.000    0.000\n54  math80 ~1                0.000 0.000      NA     NA    0.000    0.000\n55  math85 ~1                0.000 0.000      NA     NA    0.000    0.000\n56  math90 ~1                0.000 0.000      NA     NA    0.000    0.000\n57  math95 ~1                0.000 0.000      NA     NA    0.000    0.000\n58 math100 ~1                0.000 0.000      NA     NA    0.000    0.000\n59 math105 ~1                0.000 0.000      NA     NA    0.000    0.000\n60 math110 ~1                0.000 0.000      NA     NA    0.000    0.000\n61 math115 ~1                0.000 0.000      NA     NA    0.000    0.000\n62 math120 ~1                0.000 0.000      NA     NA    0.000    0.000\n63 math125 ~1                0.000 0.000      NA     NA    0.000    0.000\n64 math130 ~1                0.000 0.000      NA     NA    0.000    0.000\n65 math135 ~1                0.000 0.000      NA     NA    0.000    0.000\n66 math140 ~1                0.000 0.000      NA     NA    0.000    0.000\n67 math145 ~1                0.000 0.000      NA     NA    0.000    0.000\n68   eta_1 ~1               35.236 0.347 101.512  0.000   34.556   35.917\n69   eta_2 ~1                4.229 0.081  51.910  0.000    4.069    4.389\n\n\n\ninspect(lg_math_age_lavaan_fit, what=\"est\") |&gt;\n    print()\n\n$lambda\n        eta_1 eta_2\nmath70      1  -1.0\nmath75      1  -0.5\nmath80      1   0.0\nmath85      1   0.5\nmath90      1   1.0\nmath95      1   1.5\nmath100     1   2.0\nmath105     1   2.5\nmath110     1   3.0\nmath115     1   3.5\nmath120     1   4.0\nmath125     1   4.5\nmath130     1   5.0\nmath135     1   5.5\nmath140     1   6.0\nmath145     1   6.5\n\n$theta\n        math70 math75 math80 math85 math90 math95 mth100 mth105 mth110\nmath70  32.337                                                        \nmath75   0.000 32.337                                                 \nmath80   0.000  0.000 32.337                                          \nmath85   0.000  0.000  0.000 32.337                                   \nmath90   0.000  0.000  0.000  0.000 32.337                            \nmath95   0.000  0.000  0.000  0.000  0.000 32.337                     \nmath100  0.000  0.000  0.000  0.000  0.000  0.000 32.337              \nmath105  0.000  0.000  0.000  0.000  0.000  0.000  0.000 32.337       \nmath110  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 32.337\nmath115  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\nmath120  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\nmath125  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\nmath130  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\nmath135  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\nmath140  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\nmath145  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n        mth115 mth120 mth125 mth130 mth135 mth140 mth145\nmath70                                                  \nmath75                                                  \nmath80                                                  \nmath85                                                  \nmath90                                                  \nmath95                                                  \nmath100                                                 \nmath105                                                 \nmath110                                                 \nmath115 32.337                                          \nmath120  0.000 32.337                                   \nmath125  0.000  0.000 32.337                            \nmath130  0.000  0.000  0.000 32.337                     \nmath135  0.000  0.000  0.000  0.000 32.337              \nmath140  0.000  0.000  0.000  0.000  0.000 32.337       \nmath145  0.000  0.000  0.000  0.000  0.000  0.000 32.337\n\n$psi\n       eta_1  eta_2\neta_1 65.063       \neta_2  1.157  0.725\n\n$nu\n        intrcp\nmath70       0\nmath75       0\nmath80       0\nmath85       0\nmath90       0\nmath95       0\nmath100      0\nmath105      0\nmath110      0\nmath115      0\nmath120      0\nmath125      0\nmath130      0\nmath135      0\nmath140      0\nmath145      0\n\n$alpha\n      intrcp\neta_1 35.236\neta_2  4.229\n\n\n\nCreiamo un diagramma di percorso.\n\nlg_math_age_lavaan_fit |&gt;\n    semPaths(\n        style = \"ram\",\n        whatLabels = \"par\", edge.label.cex = .6,\n        label.prop = 0.9, edge.label.color = \"black\", rotation = 4,\n        equalizeManifests = FALSE, optimizeLatRes = TRUE, node.width = 1.5,\n        edge.width = 0.5, shapeMan = \"rectangle\", shapeLat = \"ellipse\",\n        shapeInt = \"triangle\", sizeMan = 4, sizeInt = 2, sizeLat = 4,\n        curve = 2, unCol = \"#070b8c\"\n    )",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>76</span>  <span class='chapter-title'>Il tempo su una metrica continua</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/09_time_inv_cov.html",
    "href": "chapters/lgm/09_time_inv_cov.html",
    "title": "77  Covariate indipendenti dal tempo",
    "section": "",
    "text": "77.1 Introduzione\nNell’ambito degli studi sui modelli di crescita lineare, una questione rilevante è capire in che modo le differenze individuali nelle traiettorie di cambiamento siano influenzate da altre variabili. Il presente capitolo si dedica all’integrazione di covariate invarianti nel tempo in questi modelli di crescita.\nLe covariate invarianti nel tempo sono quelle variabili che rimangono costanti per ogni individuo durante il periodo di studio. Esempi tipici includono il genere, le condizioni sperimentali, lo stato socio-economico, e altri attributi che non subiscono modifiche nel tempo. In termini di modellazione, queste variabili vengono trattate come fattori indipendenti in un modello di regressione multipla, dove l’intercetta e la pendenza del modello di crescita lineare fungono da variabili dipendenti. Le covariate possono assumere vari formati: possono essere continue (es. età), ordinali (es. livelli di istruzione) o categoriche (es. genere).\nIncludere covariate invarianti nel tempo permette di esplorare come le differenze individuali nella traiettoria di crescita (sia in termini di intercetta che di pendenza) siano associate a queste variabili. Questo approccio offre la possibilità di indagare le ragioni sottostanti le diverse modalità di cambiamento tra gli individui.\nIn sintesi, l’integrazione di covariate invarianti nel tempo nei modelli di crescita lineare fornisce una visione più dettagliata delle dinamiche individuali e del modo in cui vari fattori possono influenzare le traiettorie di crescita. Questo approccio arricchisce la comprensione dei fenomeni studiati, pur richiedendo un’interpretazione cauta e informata dei risultati ottenuti.\nPer questo esempio considereremo i dati di prestazione matematica dal data set NLSY-CYA Long Data [si veda {cite:t}grimm2016growth]. Iniziamo a leggere i dati.\n# set filepath for data file\nfilepath &lt;- \"https://raw.githubusercontent.com/LRI-2/Data/main/GrowthModeling/nlsy_math_long_R.dat\"\n# read in the text data file using the url() function\ndat &lt;- read.table(\n  file = url(filepath),\n  na.strings = \".\"\n) # indicates the missing data designator\n# copy data with new name\nnlsy_math_long &lt;- dat\n\n# Add names the columns of the data set\nnames(nlsy_math_long) &lt;- c(\n  \"id\", \"female\", \"lb_wght\",\n  \"anti_k1\", \"math\", \"grade\",\n  \"occ\", \"age\", \"men\",\n  \"spring\", \"anti\"\n)\n\n# view the first few observations in the data set\nhead(nlsy_math_long, 10)\n\n\nA data.frame: 10 x 11\n\n\n\nid\nfemale\nlb_wght\nanti_k1\nmath\ngrade\nocc\nage\nmen\nspring\nanti\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n201\n1\n0\n0\n38\n3\n2\n111\n0\n1\n0\n\n\n2\n201\n1\n0\n0\n55\n5\n3\n135\n1\n1\n0\n\n\n3\n303\n1\n0\n1\n26\n2\n2\n121\n0\n1\n2\n\n\n4\n303\n1\n0\n1\n33\n5\n3\n145\n0\n1\n2\n\n\n5\n2702\n0\n0\n0\n56\n2\n2\n100\nNA\n1\n0\n\n\n6\n2702\n0\n0\n0\n58\n4\n3\n125\nNA\n1\n2\n\n\n7\n2702\n0\n0\n0\n80\n8\n4\n173\nNA\n1\n2\n\n\n8\n4303\n1\n0\n0\n41\n3\n2\n115\n0\n0\n1\n\n\n9\n4303\n1\n0\n0\n58\n4\n3\n135\n0\n1\n2\n\n\n10\n5002\n0\n0\n4\n46\n4\n2\n117\nNA\n1\n4\nnlsy_math_long |&gt;\n  ggplot(\n    aes(grade, math, group = id)\n  ) +\n  geom_line(alpha = 0.3) + # add individual line with transparency\n  stat_summary( # add average line\n    aes(group = 1),\n    fun = mean,\n    geom = \"line\",\n    linewidth = 1.5,\n    color = \"blue\"\n  ) +\n  labs(x = \"Grade at testing\", y = \"PAT Mathematics\")\nPer ottenere una visione più dettagliata dei cambiamenti a livello individuale, possiamo selezionare casualmente un campione di 20 individui e registrare, per ciascuno di essi, l’evoluzione dei loro punteggi in matematica nel tempo.\n# sample 20 ids\npeople &lt;- unique(nlsy_math_long$id) %&gt;% sample(20)\n# do separate graph for each individual\nnlsy_math_long %&gt;% \n  filter(id %in% people) %&gt;%  # filter only sampled cases\n  ggplot(aes(grade, math, group = 1)) +\n  geom_line() +\n  facet_wrap(~id) + # a graph for each individual\n  labs(x = \"Grade at testing\", y = \"PAT Mathematics\")\nPer semplicità, leggiamo gli stessi dati in formato wide da un file.\n# set filepath for data file\nfilepath &lt;- \"https://raw.githubusercontent.com/LRI-2/Data/main/GrowthModeling/nlsy_math_wide_R.dat\"\n\n# read in the text data file using the url() function\ndat &lt;- read.table(\n  file = url(filepath),\n  na.strings = \".\"\n) # indicates the missing data designator\n\n# copy data with new name\nnlsy_math_wide &lt;- dat\n\n# Give the variable names\nnames(nlsy_math_wide) &lt;- c(\n  \"id\", \"female\", \"lb_wght\", \"anti_k1\",\n  \"math2\", \"math3\", \"math4\", \"math5\", \"math6\", \"math7\", \"math8\",\n  \"age2\", \"age3\", \"age4\", \"age5\", \"age6\", \"age7\", \"age8\",\n  \"men2\", \"men3\", \"men4\", \"men5\", \"men6\", \"men7\", \"men8\",\n  \"spring2\", \"spring3\", \"spring4\", \"spring5\", \"spring6\", \"spring7\", \"spring8\",\n  \"anti2\", \"anti3\", \"anti4\", \"anti5\", \"anti6\", \"anti7\", \"anti8\"\n)\n\n\n# view the first few observations (and columns) in the data set\nhead(nlsy_math_wide[, 1:11], 10)\n\n\nA data.frame: 10 x 11\n\n\n\nid\nfemale\nlb_wght\nanti_k1\nmath2\nmath3\nmath4\nmath5\nmath6\nmath7\nmath8\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n201\n1\n0\n0\nNA\n38\nNA\n55\nNA\nNA\nNA\n\n\n2\n303\n1\n0\n1\n26\nNA\nNA\n33\nNA\nNA\nNA\n\n\n3\n2702\n0\n0\n0\n56\nNA\n58\nNA\nNA\nNA\n80\n\n\n4\n4303\n1\n0\n0\nNA\n41\n58\nNA\nNA\nNA\nNA\n\n\n5\n5002\n0\n0\n4\nNA\nNA\n46\nNA\n54\nNA\n66\n\n\n6\n5005\n1\n0\n0\n35\nNA\n50\nNA\n60\nNA\n59\n\n\n7\n5701\n0\n0\n2\nNA\n62\n61\nNA\nNA\nNA\nNA\n\n\n8\n6102\n0\n0\n0\nNA\nNA\n55\n67\nNA\n81\nNA\n\n\n9\n6801\n1\n0\n0\nNA\n54\nNA\n62\nNA\n66\nNA\n\n\n10\n6802\n0\n0\n0\nNA\n55\nNA\n66\nNA\n68\nNA\nSpecifichiamo il modello SEM (si noti che, anche in questo caso, la scrittura del modello può essere semplificata usando la funzione growth).\nI covarianti invarianti nel tempo valutati qui includono lb_wght, una variabile dicotomica codificata come dummy che indica se il bambino aveva un peso alla nascita normale (codificato 0) o basso (codificato 1), e anti_k1, una variabile continua con valori che variano da 0 a 8 indicando il grado in cui il bambino manifestava comportamenti antisociali all’asilo o in prima elementare (punteggi più alti indicano un comportamento più antisociale).\n#writing out linear growth model with tic in full SEM way \nlg_math_tic_lavaan_model &lt;- '\n    #latent variable definitions\n            #intercept\n              eta1 =~ 1*math2+\n                      1*math3+\n                      1*math4+\n                      1*math5+\n                      1*math6+\n                      1*math7+\n                      1*math8\n            #linear slope\n              eta2 =~ 0*math2+\n                      1*math3+\n                      2*math4+\n                      3*math5+\n                      4*math6+\n                      5*math7+\n                      6*math8\n\n          #factor variances\n            eta1 ~~ eta1\n            eta2 ~~ eta2\n\n          #factor covariance\n            eta1 ~~ eta2\n\n          #manifest variances (set equal by naming theta)\n            math2 ~~ theta*math2\n            math3 ~~ theta*math3\n            math4 ~~ theta*math4\n            math5 ~~ theta*math5\n            math6 ~~ theta*math6\n            math7 ~~ theta*math7\n            math8 ~~ theta*math8\n\n          #latent means (freely estimated)\n            eta1 ~ 1\n            eta2 ~ 1\n\n          #manifest means (fixed to zero)\n            math2 ~ 0*1\n            math3 ~ 0*1\n            math4 ~ 0*1\n            math5 ~ 0*1\n            math6 ~ 0*1\n            math7 ~ 0*1\n            math8 ~ 0*1\n\n        #Time invariant covaraite\n        #regression of time-invariant covariate on intercept and slope factors\n            eta1 ~ lb_wght + anti_k1\n            eta2 ~ lb_wght + anti_k1\n\n        #variance of TIV covariates\n            lb_wght ~~ lb_wght\n            anti_k1 ~~ anti_k1\n\n        #covariance of TIV covariates\n            lb_wght ~~ anti_k1\n\n        #means of TIV covariates (freely estimated)\n            lb_wght ~ 1\n            anti_k1 ~ 1\n' #end of model definition\nAdattiamo il modello ai dati.\nlg_math_tic_lavaan_fit &lt;- sem(lg_math_tic_lavaan_model,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\"\n)\nEsaminiamo la soluzione ottenuta.\nsummary(lg_math_tic_lavaan_fit, fit.measures = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 110 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n  Number of equality constraints                     6\n\n  Number of observations                           933\n  Number of missing patterns                        61\n\nModel Test User Model:\n                                                      \n  Test statistic                               220.221\n  Degrees of freedom                                39\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               892.616\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.788\n  Tucker-Lewis Index (TLI)                       0.805\n                                                      \n  Robust Comparative Fit Index (CFI)             0.920\n  Robust Tucker-Lewis Index (TLI)                0.926\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -9785.085\n  Loglikelihood unrestricted model (H1)      -9674.975\n                                                      \n  Akaike (AIC)                               19600.171\n  Bayesian (BIC)                             19672.747\n  Sample-size adjusted Bayesian (SABIC)      19625.108\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.071\n  90 Percent confidence interval - lower         0.062\n  90 Percent confidence interval - upper         0.080\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.046\n                                                      \n  Robust RMSEA                                   0.100\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.183\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.218\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.654\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.097\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta1 =~                                             \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta2 =~                                             \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta1 ~                                              \n    lb_wght          -2.716    1.294   -2.099    0.036\n    anti_k1          -0.551    0.232   -2.369    0.018\n  eta2 ~                                              \n    lb_wght           0.625    0.333    1.873    0.061\n    anti_k1          -0.019    0.059   -0.327    0.743\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .eta1 ~~                                             \n   .eta2             -0.078    1.145   -0.068    0.945\n  lb_wght ~~                                          \n    anti_k1           0.007    0.014    0.548    0.584\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .eta1             36.290    0.497   73.052    0.000\n   .eta2              4.315    0.122   35.420    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n    lb_wght           0.080    0.009    9.031    0.000\n    anti_k1           1.454    0.050   29.216    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .eta1             63.064    5.609   11.242    0.000\n   .eta2              0.713    0.326    2.185    0.029\n   .math2   (thet)   36.257    1.868   19.406    0.000\n   .math3   (thet)   36.257    1.868   19.406    0.000\n   .math4   (thet)   36.257    1.868   19.406    0.000\n   .math5   (thet)   36.257    1.868   19.406    0.000\n   .math6   (thet)   36.257    1.868   19.406    0.000\n   .math7   (thet)   36.257    1.868   19.406    0.000\n   .math8   (thet)   36.257    1.868   19.406    0.000\n    lb_wght           0.074    0.003   21.599    0.000\n    anti_k1           2.312    0.107   21.599    0.000\nInterpretazione dei risultati per eta1 (Intercetta).\nInterpretazione dei risultati per eta2 (Pendenza).\nIn sintesi, il peso alla nascita basso e i comportamenti antisociali sembrano influenzare negativamente il valore iniziale (intercetta) del costrutto misurato. Il peso alla nascita e i comportamenti antisociali non sembrano avere un impatto significativo sulla pendenza.\nCreiamo un diagramma di percorso.\nlg_math_tic_lavaan_fit |&gt;\n    semPaths(\n        style = \"lisrel\",\n        whatLabels = \"std\", edge.label.cex = .6,\n        label.prop = 0.9, edge.label.color = \"black\", rotation = 4,\n        equalizeManifests = FALSE, optimizeLatRes = TRUE, node.width = 1.5,\n        edge.width = 0.5, shapeMan = \"rectangle\", shapeLat = \"ellipse\",\n        shapeInt = \"triangle\", sizeMan = 4, sizeInt = 2, sizeLat = 4,\n        curve = 2, unCol = \"#070b8c\"\n    )",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>77</span>  <span class='chapter-title'>Covariate indipendenti dal tempo</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/09_time_inv_cov.html#introduzione",
    "href": "chapters/lgm/09_time_inv_cov.html#introduzione",
    "title": "77  Covariate indipendenti dal tempo",
    "section": "",
    "text": "È importante sottolineare che, pur fornendo insights significativi, i risultati ottenuti da questi modelli non implicano relazioni causali. Questi modelli hanno limitazioni simili a quelle dei modelli di regressione standard in termini di inferenza causale.\nÈ cruciale considerare il contesto nel quale le covariate invarianti nel tempo sono state raccolte. Se queste provengono da un contesto sperimentale con assegnazione casuale, le inferenze potrebbero essere più robuste. Tuttavia, nel caso di dati osservazionali, è necessario un’attenta considerazione per evitare interpretazioni errate o eccessivamente assertive riguardo la causalità.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlb_wght: L’effetto stimato di lb_wght sull’intercetta (eta1) è di -2.716 con uno standard error di 1.294. Questo valore di -2.716 indica che avere un peso alla nascita basso (codificato come 1) è associato ad una riduzione media di 2.716 unità nel valore iniziale di eta1, rispetto a un peso alla nascita normale (codificato come 0). Questo risultato è statisticamente significativo, come indicato dal valore di P (0.036), che è inferiore a 0.05.\nanti_k1: Per anti_k1, l’effetto stimato sull’intercetta è di -0.551 con uno standard error di 0.232. Questo suggerisce che per ogni unità di aumento nel punteggio di comportamento antisociale (anti_k1), il valore iniziale di eta1 diminuisce in media di 0.551 unità. Anche questo risultato è statisticamente significativo, con un valore di P di 0.018.\n\n\n\nlb_wght: Per la pendenza (eta2), l’effetto stimato di lb_wght è di 0.625 con uno standard error di 0.333. Ciò indica che avere un peso alla nascita basso è associato ad un aumento medio di 0.625 unità nella pendenza di eta2, rispetto a un peso normale alla nascita. Tuttavia, questo risultato non è statisticamente significativo al livello del 5%, dato che il valore di P è 0.061, che è leggermente superiore a 0.05.\nanti_k1: L’effetto stimato di anti_k1 sulla pendenza è molto piccolo (-0.019) e non è statisticamente significativo (valore di P = 0.743), suggerendo che non c’è una relazione chiara tra il comportamento antisociale in età precoce e il cambiamento nel tempo di eta2.\n\n\n\n\n\n77.1.1 Valutare il contributo delle covariate\nUna domanda comune in questo approccio per comprendere le associazioni tra covarianti invarianti nel tempo e traiettorie individuali è se l’aggiunta dei covarianti invarianti nel tempo sia stata utile. Nel framework di modellazione multilivello, i -2LL ottenuti quando si adattano modelli con e senza i covarianti invarianti nel tempo possono essere confrontati direttamente (se nessun partecipante è stato escluso dall’analisi a causa di dati incompleti sui covarianti invarianti nel tempo), fornendo un modo per valutare l’adattamento relativo del modello. Specificamente, possiamo esaminare la differenza tra i -2LL rispetto alla differenza nel numero di parametri stimati (o differenza nei gradi di libertà).\n\n#writing out linear growth model with tic in full SEM way \nlg_math_ticZERO_lavaan_model &lt;- '\n    #latent variable definitions\n            #intercept\n              eta1 =~ 1*math2+\n                      1*math3+\n                      1*math4+\n                      1*math5+\n                      1*math6+\n                      1*math7+\n                      1*math8\n            #linear slope\n              eta2 =~ 0*math2+\n                      1*math3+\n                      2*math4+\n                      3*math5+\n                      4*math6+\n                      5*math7+\n                      6*math8\n\n          #factor variances\n            eta1 ~~ eta1\n            eta2 ~~ eta2\n\n          #factor covariance\n            eta1 ~~ eta2\n\n          #manifest variances (set equal by naming theta)\n            math2 ~~ theta*math2\n            math3 ~~ theta*math3\n            math4 ~~ theta*math4\n            math5 ~~ theta*math5\n            math6 ~~ theta*math6\n            math7 ~~ theta*math7\n            math8 ~~ theta*math8\n\n          #latent means (freely estimated)\n            eta1 ~ 1\n            eta2 ~ 1\n\n          #manifest means (fixed to zero)\n            math2 ~ 0*1\n            math3 ~ 0*1\n            math4 ~ 0*1\n            math5 ~ 0*1\n            math6 ~ 0*1\n            math7 ~ 0*1\n            math8 ~ 0*1\n\n        #Time invariant covaraite\n          #regression of time-invariant covariate on intercept and slope factors\n          #FIXED to 0\n            eta1 ~ 0*lb_wght + 0*anti_k1\n            eta2 ~ 0*lb_wght + 0*anti_k1\n\n        #variance of TIV covariates\n            lb_wght ~~ lb_wght\n            anti_k1 ~~ anti_k1\n\n        #covariance of TIV covaraites\n            lb_wght ~~ anti_k1\n\n        #means of TIV covariates (freely estimated)\n            lb_wght ~ 1\n            anti_k1 ~ 1\n' #end of model definition\n\nAdattiamo il modello ai dati.\n\nlg_math_ticZERO_lavaan_fit &lt;- sem(lg_math_ticZERO_lavaan_model,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\"\n)\n\nEsaminiamo il risultato ottenuto.\n\nsummary(lg_math_ticZERO_lavaan_fit, fit.measures = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 85 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        17\n  Number of equality constraints                     6\n\n  Number of observations                           933\n  Number of missing patterns                        61\n\nModel Test User Model:\n                                                      \n  Test statistic                               234.467\n  Degrees of freedom                                43\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               892.616\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.776\n  Tucker-Lewis Index (TLI)                       0.813\n                                                      \n  Robust Comparative Fit Index (CFI)             0.917\n  Robust Tucker-Lewis Index (TLI)                0.931\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -9792.208\n  Loglikelihood unrestricted model (H1)      -9674.975\n                                                      \n  Akaike (AIC)                               19606.416\n  Bayesian (BIC)                             19659.638\n  Sample-size adjusted Bayesian (SABIC)      19624.703\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.069\n  90 Percent confidence interval - lower         0.061\n  90 Percent confidence interval - upper         0.078\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.020\n                                                      \n  Robust RMSEA                                   0.097\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.173\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.208\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.646\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.103\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta1 =~                                             \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta2 =~                                             \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta1 ~                                              \n    lb_wght           0.000                           \n    anti_k1           0.000                           \n  eta2 ~                                              \n    lb_wght           0.000                           \n    anti_k1           0.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .eta1 ~~                                             \n   .eta2             -0.181    1.150   -0.158    0.875\n  lb_wght ~~                                          \n    anti_k1           0.007    0.014    0.548    0.584\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .eta1             35.267    0.355   99.229    0.000\n   .eta2              4.339    0.088   49.136    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n    lb_wght           0.080    0.009    9.031    0.000\n    anti_k1           1.454    0.050   29.216    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .eta1             64.562    5.659   11.408    0.000\n   .eta2              0.733    0.327    2.238    0.025\n   .math2   (thet)   36.230    1.867   19.410    0.000\n   .math3   (thet)   36.230    1.867   19.410    0.000\n   .math4   (thet)   36.230    1.867   19.410    0.000\n   .math5   (thet)   36.230    1.867   19.410    0.000\n   .math6   (thet)   36.230    1.867   19.410    0.000\n   .math7   (thet)   36.230    1.867   19.410    0.000\n   .math8   (thet)   36.230    1.867   19.410    0.000\n    lb_wght           0.074    0.003   21.599    0.000\n    anti_k1           2.312    0.107   21.599    0.000\n\n\n\nGeneriamo il diagramma di percorso.\n\nlg_math_ticZERO_lavaan_fit |&gt;\n    semPaths(\n        style = \"lisrel\",\n        whatLabels = \"std\", edge.label.cex = .6,\n        label.prop = 0.9, edge.label.color = \"black\", rotation = 4,\n        equalizeManifests = FALSE, optimizeLatRes = TRUE, node.width = 1.5,\n        edge.width = 0.5, shapeMan = \"rectangle\", shapeLat = \"ellipse\",\n        shapeInt = \"triangle\", sizeMan = 4, sizeInt = 2, sizeLat = 4,\n        curve = 2, unCol = \"#070b8c\"\n    )\n\n\n\n\n\n\n\n\nEseguiamo il confronto tra i due modelli mediante il test del rapporto tra verosimiglianze.\n\nlavTestLRT(lg_math_tic_lavaan_fit, lg_math_ticZERO_lavaan_fit) |&gt;\n    print()\n\n\nChi-Squared Difference Test\n\n                           Df   AIC   BIC  Chisq Chisq diff    RMSEA\nlg_math_tic_lavaan_fit     39 19600 19673 220.22                    \nlg_math_ticZERO_lavaan_fit 43 19606 19660 234.47     14.245 0.052395\n                           Df diff Pr(&gt;Chisq)   \nlg_math_tic_lavaan_fit                          \nlg_math_ticZERO_lavaan_fit       4   0.006552 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNel nostro esempio, il -2LL per il modello di crescita lineare era 15.937 e il -2LL per il modello di crescita lineare con due predittori invarianti nel tempo (basso peso alla nascita e comportamenti antisociali) dell’intercetta e della pendenza era 15.923, una differenza di 14. La differenza nel numero di parametri stimati era (19 – 15) = 4. Quindi, il miglioramento dell’adattamento era significativo (χ2(4) = 14, p &lt; .01), indicando che basso peso alla nascita e comportamenti antisociali erano predittori utili. Parallelamente alle differenze nei -2LL, le differenze in AIC e BIC hanno anche indicato un miglioramento nell’adattamento del modello (criteri di informazione più bassi indicano un migliore adattamento) quando i covarianti invarianti nel tempo erano inclusi nel modello.\nNel framework di modellazione delle equazioni strutturali, è tipico valutare l’adattamento globale (ad esempio, RMSEA, CFI, TLI); tuttavia, raramente l’aggiunta di covarianti invarianti nel tempo cambia significativamente questi indici. L’adattamento relativo dei modelli può essere informativo come nel framework di modellazione multilivello. Come sopra, le differenze nei -2LL (o χ2) possono essere calcolate per testare se l’inclusione dei covarianti invarianti nel tempo ha migliorato significativamente l’adattamento del modello. Si noti che, nel framework di modellazione delle equazioni strutturali, il modello di confronto (baseline) non è semplicemente un modello senza i covarianti invarianti nel tempo. Piuttosto, è un modello che include i covarianti invarianti nel tempo ma vincola i loro effetti sull’intercetta e sulla pendenza a 0.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>77</span>  <span class='chapter-title'>Covariate indipendenti dal tempo</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/09_time_inv_cov.html#confronto-con-il-modello-misto",
    "href": "chapters/lgm/09_time_inv_cov.html#confronto-con-il-modello-misto",
    "title": "77  Covariate indipendenti dal tempo",
    "section": "77.2 Confronto con il modello misto",
    "text": "77.2 Confronto con il modello misto\nEseguiamo ora l’analisi statistica utilizzando un modello misto con intercetta e pendenza casuale. Confronteremo un modello ridotto, che include solo l’effetto del tempo, con un modello completo che include le covariate esaminate in precedenza. Il modello completo include gli effetti principali delle covariate e l’interazione tra le covariate e il tempo.\nAdattiamo il modello “completo”.\n\nnlsy_math_long$grade_c2 &lt;- nlsy_math_long$grade-2\n\nfit2_lmer &lt;- lmer(\n    math ~ 1 + grade_c2 + lb_wght + anti_k1 + I(grade_c2 * lb_wght) + I(grade_c2 * anti_k1) +\n        (1 + grade_c2 | id),\n    data = nlsy_math_long,\n    REML = FALSE,\n    na.action = na.exclude\n)\n\nEsaminiamo i risultati ottenuti.\n\nsummary(fit2_lmer)\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: math ~ 1 + grade_c2 + lb_wght + anti_k1 + I(grade_c2 * lb_wght) +  \n    I(grade_c2 * anti_k1) + (1 + grade_c2 | id)\n   Data: nlsy_math_long\n\n     AIC      BIC   logLik deviance df.resid \n 15943.1  16000.2  -7961.6  15923.1     2211 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.07986 -0.52517 -0.00867  0.53079  2.53455 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n id       (Intercept) 63.0653  7.941         \n          grade_c2     0.7141  0.845    -0.01\n Residual             36.2543  6.021         \nNumber of obs: 2221, groups:  id, 932\n\nFixed effects:\n                      Estimate Std. Error t value\n(Intercept)           36.28983    0.49630  73.120\ngrade_c2               4.31521    0.12060  35.782\nlb_wght               -2.71621    1.29359  -2.100\nanti_k1               -0.55087    0.23246  -2.370\nI(grade_c2 * lb_wght)  0.62463    0.33314   1.875\nI(grade_c2 * anti_k1) -0.01930    0.05886  -0.328\n\nCorrelation of Fixed Effects:\n            (Intr) grd_c2 lb_wgh ant_k1 I(_2*l\ngrade_c2    -0.529                            \nlb_wght     -0.194  0.096                     \nanti_k1     -0.671  0.358 -0.025              \nI(grd_2*l_)  0.091 -0.168 -0.532  0.026       \nI(gr_2*a_1)  0.343 -0.660  0.026 -0.529 -0.055\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00577269 (tol = 0.002, component 1)\n\n\nAdattiamo un modello misto vincolato senza covariate, utilizzando un modello con intercetta e pendenza casuale.\n\nfit3_lmer &lt;- lmer(\n    math ~ 1 + grade_c2 + (1 + grade_c2 | id),\n    data = nlsy_math_long,\n    REML = FALSE,\n    na.action = na.exclude\n)\n\nConfrontiamo i due modelli utilizzando il test del rapporto di verosimiglianza.\n\nanova(fit2_lmer, fit3_lmer) |&gt; \n    print()\n\nData: nlsy_math_long\nModels:\nfit3_lmer: math ~ 1 + grade_c2 + (1 + grade_c2 | id)\nfit2_lmer: math ~ 1 + grade_c2 + lb_wght + anti_k1 + I(grade_c2 * lb_wght) + I(grade_c2 * anti_k1) + (1 + grade_c2 | id)\n          npar   AIC   BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)   \nfit3_lmer    6 15949 15984 -7968.7    15937                        \nfit2_lmer   10 15943 16000 -7961.6    15923 14.245  4   0.006552 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLa differenza nei -2LL tra questi due modelli era 14, con una differenza di 4 gradi di libertà, χ2(4) = 14, p &lt; .01. Questa differenza è identica a quella ottenuta confrontando i modelli nel framework di modellazione LGM. Giungiamo alla stessa conclusione riguardo l’importanza del basso peso alla nascita e dei comportamenti antisociali nel framework di modellazione multilivello e quando esaminiamo le differenze nelle traiettorie matematiche dei bambini.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>77</span>  <span class='chapter-title'>Covariate indipendenti dal tempo</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/09_time_inv_cov.html#considerazioni-importanti",
    "href": "chapters/lgm/09_time_inv_cov.html#considerazioni-importanti",
    "title": "77  Covariate indipendenti dal tempo",
    "section": "77.3 Considerazioni Importanti",
    "text": "77.3 Considerazioni Importanti\nLa maggior parte delle ricerche nel modello di crescita cerca di comprendere come le caratteristiche interpersonali (ovvero, i covarianti invarianti nel tempo) siano associate a differenze interpersonali nei cambiamenti intrapersonali catturati dai dati longitudinali. Nel nostro esempio illustrativo, ci siamo limitati a due covarianti invarianti nel tempo per semplicità. Tuttavia, è possibile includere contemporaneamente nel modello diversi covarianti invarianti nel tempo e le interazioni tra di essi. Come in tutte le analisi di regressione, è essenziale un’adeguata scala e centratura dei covarianti invarianti nel tempo per ottenere stime di parametri sostanzialmente significative. Tutte le pratiche comuni nella regressione, come l’esame delle interazioni (moderazione) tra i covarianti invarianti nel tempo, relazioni non lineari e test di mediazione, sono possibili e implementate in modi tipici. Ad esempio, è possibile calcolare variabili prodotto, includerle nel dataset e inserirle come predittori aggiuntivi per esaminare effetti interattivi. Inoltre, gli effetti dei covarianti invarianti nel tempo possono essere aggiunti al modello in modo gerarchico per isolare se il loro inserimento ha migliorato significativamente l’adattamento del modello (simile all’esame del cambiamento significativo in R^2).\n\n77.3.1 Varianza Spiegata\nOltre a valutare l’importanza dei covarianti invarianti nel tempo, i ricercatori vogliono anche sapere quanto della varianza nell’intercetta e nella pendenza sia stata spiegata dai covarianti invarianti nel tempo. In altre parole, quale proporzione delle differenze interpersonali nell’intercetta e nelle pendenze è stata spiegata dai covarianti invarianti nel tempo. Nei framework di modellazione multilivello e di equazioni strutturali, è possibile confrontare le stime di varianza dell’intercetta e della pendenza ottenute in modelli con e senza covarianti invarianti nel tempo. Nel nostro esempio, ad esempio, la stima della varianza dell’intercetta era 64.562 per il modello di crescita lineare senza covarianti invarianti nel tempo e 63.064 quando il basso peso alla nascita e i comportamenti antisociali erano inclusi come covarianti invarianti nel tempo. La differenza tra le varianze stimate era di 1.498. Convertendo questa differenza in una proporzione della varianza originale, troviamo che i covarianti invarianti nel tempo hanno spiegato lo 0.023 (1.498/64.562) ovvero il 2.3% delle differenze interpersonali nell’intercetta. Calcoli simili per la pendenza hanno prodotto una varianza spiegata dello 0.027 (2.7%).\n\n\n77.3.2 Coefficienti Standardizzati\nNell’ambito della ricerca, oltre alla valutazione della varianza spiegata, può essere utile calcolare i coefficienti standardizzati. Questi aiutano a determinare l’importanza di ciascun predittore e funzionano come una misura della grandezza dell’effetto. I coefficienti di regressione di secondo livello (percorsi), che partono dai covarianti invarianti nel tempo verso l’intercetta e la pendenza, sono inizialmente non standardizzati. Per ottenere i coefficienti standardizzati, è necessario moltiplicare il coefficiente non standardizzato per il rapporto tra la deviazione standard del predittore (cioè, il covariante invariante nel tempo) e quella del risultato (cioè, l’intercetta o la pendenza).\nLa formula generale per il calcolo di un coefficiente standardizzato è:\n\\[ \\beta^* = \\frac{b \\times \\sigma_{\\text{predittore}}}{\\sigma_{\\text{risultato}}}, \\]\ndove:\n\n\\(\\beta^*\\) rappresenta il coefficiente standardizzato.\n\\(b\\) è il coefficiente non standardizzato.\n\\(\\sigma_{\\text{predittore}}\\) è la deviazione standard del predittore, come i comportamenti antisociali nel nostro caso.\n\\(\\sigma_{\\text{risultato}}\\) è la deviazione standard del risultato, come l’intercetta nel nostro caso.\n\nApplicando questa formula, il coefficiente standardizzato per l’effetto dei comportamenti antisociali sull’intercetta è dato da:\n\\[\n\\beta^* = \\frac{-0.551 \\times \\sigma_{\\text{anti\\_k1}}}{\\sqrt{63.065 + (0.080 \\times \\sigma_{\\text{X1}} \\times 0.080) + (-0.551 \\times \\sigma_{\\text{anti\\_k1}} \\times -0.551) + 2 \\times (0.080 \\times \\sigma_{\\text{X1,anti\\_k1}} \\times -0.551)}}\n\\]\nQui, \\(\\sigma_{\\text{anti\\_k1}}\\) rappresenta la deviazione standard dei comportamenti antisociali, \\(\\sigma_{\\text{X1}}\\) è la deviazione standard di un altro predittore, se presente, e \\(\\sigma_{\\text{X1,anti\\_k1}}\\) è la covarianza tra i due predittori.\nIn conclusione, il calcolo produce:\n\\[\n-0.551 \\times 2.312 / \\sqrt{63.065 + (0.080 \\times 0.074 \\times 0.080) + (-0.551 \\times 2.312 \\times -0.551) + 2 \\times (0.080 \\times 0.007 \\times -0.551)} = -0.105\n\\]\nQuindi, l’effetto dei comportamenti antisociali sui punteggi di matematica di seconda elementare è risultato essere di piccola entità.\n\n\n77.3.3 Riflessioni Conclusive\nIn questo capitolo, abbiamo esaminato il modello di crescita lineare con covarianti invarianti nel tempo, un modello spesso utilizzato per esaminare le differenze individuali nella crescita e nel cambiamento. L’uso di questo modello implica una serie di assunzioni. Innanzitutto, il modello presume l’invarianza della struttura del cambiamento per tutte le persone. Ciò significa che si assume che tutti i bambini, indipendentemente dai loro punteggi sui covarianti invarianti nel tempo, seguano una traiettoria di crescita lineare. Inoltre, abbiamo ipotizzato che la grandezza della varianza residua nell’intercetta e nella pendenza, così come la covarianza residua tra l’intercetta e la pendenza, siano le stesse per i bambini con valori diversi sui covarianti invarianti nel tempo.\nAssumiamo anche che la varianza residua dei punteggi osservati sia equivalente per tutti i bambini. In altre parole, indipendentemente dai valori dei covarianti invarianti nel tempo, l’inadeguatezza del modello lineare è identica. Nel nostro esempio, abbiamo ipotizzato che la grandezza delle fluttuazioni annuali nelle prestazioni matematiche dei bambini con livelli inferiori o superiori di comportamento antisociale fosse equivalente. Dato che queste assunzioni potrebbero essere vere o meno, esse dovrebbero essere attentamente considerate prima di intraprendere tali analisi.\nNel prossimo capitolo, discuteremo i modelli di crescita per gruppi multipli che facilitano un esame approfondito di queste assunzioni per determinati tipi di covarianti invarianti nel tempo, in particolare quelli che sono variabili categoriche, ordinali o variabili continue che sono state categorizzate (ad esempio, tramite uno split mediano). Questo approccio permette una verifica più accurata e specifica delle ipotesi del modello in contesti diversi e con differenti tipologie di dati.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>77</span>  <span class='chapter-title'>Covariate indipendenti dal tempo</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/10_growth_groups.html",
    "href": "chapters/lgm/10_growth_groups.html",
    "title": "78  Modelli di crescita latenti a gruppi multipli",
    "section": "",
    "text": "78.1 Introduzione\nNel capitolo precedente, abbiamo approfondito i modelli di crescita con covarianti invarianti nel tempo. In questo capitolo, esploreremo un approccio alternativo per studiare le differenze individuali nel cambiamento: l’analisi comparativa tra gruppi (McArdle, 1989; McArdle & Hamagami, 1996). Sebbene i modelli di crescita con covarianti invarianti nel tempo siano efficaci nell’analizzare le differenze nelle traiettorie medie di crescita, questi modelli presentano limitazioni nell’indagare altri aspetti dei cambiamenti intrapersonali e delle differenze interpersonali in tali processi.\nSenza adeguati ampliamenti, i modelli basati esclusivamente su covarianti invarianti nel tempo non forniscono informazioni sulle variazioni nelle varianze e covarianze tra i fattori di crescita, né sulla variabilità residua e sulla dinamica dei cambiamenti intrapersonali. Nel presente capitolo, dimostreremo come l’approccio di confronto tra gruppi possa essere impiegato per esaminare le differenze in qualsiasi aspetto del modello di crescita. Questa flessibilità metodologica ci permette di acquisire una comprensione più profonda su come e perché gli individui mostrino percorsi di sviluppo diversificati.\nPer i nostri esempi, utilizziamo i punteggi di rendimento in matematica dai dati NLSY-CYA [si veda {cite:t}grimm2016growth]. Importiamo i dati.\n# set filepath for data file\nfilepath &lt;- \"https://raw.githubusercontent.com/LRI-2/Data/main/GrowthModeling/nlsy_math_long_R.dat\"\n\n# read in the text data file using the url() function\ndat &lt;- read.table(\n  file = url(filepath),\n  na.strings = \".\"\n) # indicates the missing data designator\n\n# copy data with new name\nnlsy_math_long &lt;- dat\n\n# Add names the columns of the data set\nnames(nlsy_math_long) &lt;- c(\n  \"id\", \"female\", \"lb_wght\",\n  \"anti_k1\", \"math\", \"grade\",\n  \"occ\", \"age\", \"men\",\n  \"spring\", \"anti\"\n)\n\n# reducing to variables of interest\nnlsy_math_long &lt;- nlsy_math_long[, c(\"id\", \"grade\", \"math\", \"lb_wght\")]\n\n# adding another dummy code variable for normal birth weight that coded the opposite of the low brithweight variable.\nnlsy_math_long$nb_wght &lt;- 1 - nlsy_math_long$lb_wght\n\n# view the first few observations in the data set\nhead(nlsy_math_long, 10) |&gt;\n  print()\n\n     id grade math lb_wght nb_wght\n1   201     3   38       0       1\n2   201     5   55       0       1\n3   303     2   26       0       1\n4   303     5   33       0       1\n5  2702     2   56       0       1\n6  2702     4   58       0       1\n7  2702     8   80       0       1\n8  4303     3   41       0       1\n9  4303     4   58       0       1\n10 5002     4   46       0       1\nEsaminiamo le curve di crescita nei due gruppi.\n# intraindividual change trajetories\nggplot(\n  data = nlsy_math_long, # data set\n  aes(x = grade, y = math, group = id)\n) + # setting variables\n  geom_point(size = .5) + # adding points to plot\n  geom_line(alpha=0.3) + # adding lines to plot\n  # setting the x-axis with breaks and labels\n  scale_x_continuous(\n    limits = c(2, 8),\n    breaks = c(2, 3, 4, 5, 6, 7, 8),\n    name = \"Grade at Testing\"\n  ) +\n  # setting the y-axis with limits breaks and labels\n  scale_y_continuous(\n    limits = c(10, 90),\n    breaks = c(10, 30, 50, 70, 90),\n    name = \"PIAT Mathematics\"\n  ) +\n  facet_wrap(~lb_wght)\nPer semplicità, carichiamo di nuovo i dati già trasformati in formato wide.\n# set filepath for data file\nfilepath &lt;- \"https://raw.githubusercontent.com/LRI-2/Data/main/GrowthModeling/nlsy_math_wide_R.dat\"\n# read in the text data file using the url() function\ndat &lt;- read.table(\n  file = url(filepath),\n  na.strings = \".\"\n) # indicates the missing data designator\n# copy data with new name\nnlsy_math_wide &lt;- dat\n\n# Give the variable names\nnames(nlsy_math_wide) &lt;- c(\n  \"id\", \"female\", \"lb_wght\", \"anti_k1\",\n  \"math2\", \"math3\", \"math4\", \"math5\", \"math6\", \"math7\", \"math8\",\n  \"age2\", \"age3\", \"age4\", \"age5\", \"age6\", \"age7\", \"age8\",\n  \"men2\", \"men3\", \"men4\", \"men5\", \"men6\", \"men7\", \"men8\",\n  \"spring2\", \"spring3\", \"spring4\", \"spring5\", \"spring6\", \"spring7\", \"spring8\",\n  \"anti2\", \"anti3\", \"anti4\", \"anti5\", \"anti6\", \"anti7\", \"anti8\"\n)\n\n# view the first few observations (and columns) in the data set\nhead(nlsy_math_wide[, 1:11], 10)\n\n\nA data.frame: 10 x 11\n\n\n\nid\nfemale\nlb_wght\nanti_k1\nmath2\nmath3\nmath4\nmath5\nmath6\nmath7\nmath8\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n201\n1\n0\n0\nNA\n38\nNA\n55\nNA\nNA\nNA\n\n\n2\n303\n1\n0\n1\n26\nNA\nNA\n33\nNA\nNA\nNA\n\n\n3\n2702\n0\n0\n0\n56\nNA\n58\nNA\nNA\nNA\n80\n\n\n4\n4303\n1\n0\n0\nNA\n41\n58\nNA\nNA\nNA\nNA\n\n\n5\n5002\n0\n0\n4\nNA\nNA\n46\nNA\n54\nNA\n66\n\n\n6\n5005\n1\n0\n0\n35\nNA\n50\nNA\n60\nNA\n59\n\n\n7\n5701\n0\n0\n2\nNA\n62\n61\nNA\nNA\nNA\nNA\n\n\n8\n6102\n0\n0\n0\nNA\nNA\n55\n67\nNA\n81\nNA\n\n\n9\n6801\n1\n0\n0\nNA\n54\nNA\n62\nNA\n66\nNA\n\n\n10\n6802\n0\n0\n0\nNA\n55\nNA\n66\nNA\n68\nNA",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>78</span>  <span class='chapter-title'>Modelli di crescita latenti a gruppi multipli</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/10_growth_groups.html#invarianza-tra-gruppi",
    "href": "chapters/lgm/10_growth_groups.html#invarianza-tra-gruppi",
    "title": "78  Modelli di crescita latenti a gruppi multipli",
    "section": "78.2 Invarianza tra gruppi",
    "text": "78.2 Invarianza tra gruppi\nDefiniamo il modello di crescita latente per i due gruppi.\n\n# writing out linear growth model in full SEM way\nmg_math_lavaan_model &lt;- \"\n  # latent variable definitions\n      #intercept (note intercept is a reserved term)\n      eta_1 =~ 1*math2\n      eta_1 =~ 1*math3\n      eta_1 =~ 1*math4\n      eta_1 =~ 1*math5\n      eta_1 =~ 1*math6\n      eta_1 =~ 1*math7\n      eta_1 =~ 1*math8\n\n      #linear slope\n      eta_2 =~ 0*math2\n      eta_2 =~ 1*math3\n      eta_2 =~ 2*math4\n      eta_2 =~ 3*math5\n      eta_2 =~ 4*math6\n      eta_2 =~ 5*math7\n      eta_2 =~ 6*math8\n\n  # factor variances\n      eta_1 ~~ eta_1\n      eta_2 ~~ eta_2\n\n  # covariances among factors\n      eta_1 ~~ eta_2\n\n  # factor means\n      eta_1 ~ start(35)*1\n      eta_2 ~ start(4)*1\n\n  # manifest variances (made equivalent by naming theta)\n      math2 ~~ theta*math2\n      math3 ~~ theta*math3\n      math4 ~~ theta*math4\n      math5 ~~ theta*math5\n      math6 ~~ theta*math6\n      math7 ~~ theta*math7\n      math8 ~~ theta*math8\n  # manifest means (fixed at zero)\n      math2 ~ 0*1\n      math3 ~ 0*1\n      math4 ~ 0*1\n      math5 ~ 0*1\n      math6 ~ 0*1\n      math7 ~ 0*1\n      math8 ~ 0*1\n\" # end of model definition\n\nAdattiamo il modello ai dati specificando la separazione delle osservazioni in due gruppi e introducendo i vincoli di eguaglianza tra gruppi sulle saturazioni fattoriali, le medie, le varianze, le covarianze, e i residui. In questo modello, sostanzialmente, non c’è alcune differenza tra gruppi.\n\nmg_math_lavaan_fitM1 &lt;- sem(mg_math_lavaan_model,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\",\n  group = \"lb_wght\", # to separate groups\n  group.equal = c(\n    \"loadings\", # for constraints\n    \"means\",\n    \"lv.variances\",\n    \"lv.covariances\",\n    \"residuals\"\n  )\n)\n\nEsaminiamo i risultati.\n\nsummary(mg_math_lavaan_fitM1, fit.measures = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 24 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        24\n  Number of equality constraints                    18\n\n  Number of observations per group:               Used       Total\n    0                                              857         858\n    1                                               75          75\n  Number of missing patterns per group:                           \n    0                                               60            \n    1                                               25            \n\nModel Test User Model:\n                                                      \n  Test statistic                               249.111\n  Degrees of freedom                                64\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    0                                          191.954\n    1                                           57.156\n\nModel Test Baseline Model:\n\n  Test statistic                               887.887\n  Degrees of freedom                                42\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.781\n  Tucker-Lewis Index (TLI)                       0.856\n                                                      \n  Robust Comparative Fit Index (CFI)             1.000\n  Robust Tucker-Lewis Index (TLI)                0.346\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7968.693\n  Loglikelihood unrestricted model (H1)      -7844.138\n                                                      \n  Akaike (AIC)                               15949.386\n  Bayesian (BIC)                             15978.410\n  Sample-size adjusted Bayesian (SABIC)      15959.354\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.079\n  90 Percent confidence interval - lower         0.069\n  90 Percent confidence interval - upper         0.089\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.436\n                                                      \n  Robust RMSEA                                   0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: Robust RMSEA &lt;= 0.050             1.000\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.128\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nGroup 1 [0]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta_2 =~                                            \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2   (.17.)   -0.181    1.150   -0.158    0.875\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1   (.18.)   35.267    0.355   99.229    0.000\n    eta_2   (.19.)    4.339    0.088   49.136    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1   (.15.)   64.562    5.659   11.408    0.000\n    eta_2   (.16.)    0.733    0.327    2.238    0.025\n   .math2   (thet)   36.230    1.867   19.410    0.000\n   .math3   (thet)   36.230    1.867   19.410    0.000\n   .math4   (thet)   36.230    1.867   19.410    0.000\n   .math5   (thet)   36.230    1.867   19.410    0.000\n   .math6   (thet)   36.230    1.867   19.410    0.000\n   .math7   (thet)   36.230    1.867   19.410    0.000\n   .math8   (thet)   36.230    1.867   19.410    0.000\n\n\nGroup 2 [1]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta_2 =~                                            \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2   (.17.)   -0.181    1.150   -0.158    0.875\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1   (.18.)   35.267    0.355   99.229    0.000\n    eta_2   (.19.)    4.339    0.088   49.136    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1   (.15.)   64.562    5.659   11.408    0.000\n    eta_2   (.16.)    0.733    0.327    2.238    0.025\n   .math2   (thet)   36.230    1.867   19.410    0.000\n   .math3   (thet)   36.230    1.867   19.410    0.000\n   .math4   (thet)   36.230    1.867   19.410    0.000\n   .math5   (thet)   36.230    1.867   19.410    0.000\n   .math6   (thet)   36.230    1.867   19.410    0.000\n   .math7   (thet)   36.230    1.867   19.410    0.000\n   .math8   (thet)   36.230    1.867   19.410    0.000\n\n\n\nCreiamo il diagramma di percorso.\n\nsemPaths(mg_math_lavaan_fitM1, what = \"path\", whatLabels = \"par\")",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>78</span>  <span class='chapter-title'>Modelli di crescita latenti a gruppi multipli</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/10_growth_groups.html#vincoli-sulle-medie",
    "href": "chapters/lgm/10_growth_groups.html#vincoli-sulle-medie",
    "title": "78  Modelli di crescita latenti a gruppi multipli",
    "section": "78.3 Vincoli sulle medie",
    "text": "78.3 Vincoli sulle medie\nTrasformiamo ora il modello restrittivo specificato in precedenza allentando via via i vincoli che abbiamo introdotto. In questo modello rendiamo possibile la differenza tra le medie nei due gruppi.\n\nmg_math_lavaan_fitM2 &lt;- sem(mg_math_lavaan_model,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\",\n  group = \"lb_wght\", # to separate groups\n  group.equal = c(\n    \"loadings\", # for constraints\n    # \"means\", commented out so can differ\n    \"lv.variances\",\n    \"lv.covariances\",\n    \"residuals\"\n  )\n)\n\nEsaminiamo i risultati.\n\nsummary(mg_math_lavaan_fitM2, fit.measures = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 31 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        24\n  Number of equality constraints                    16\n\n  Number of observations per group:               Used       Total\n    0                                              857         858\n    1                                               75          75\n  Number of missing patterns per group:                           \n    0                                               60            \n    1                                               25            \n\nModel Test User Model:\n                                                      \n  Test statistic                               243.910\n  Degrees of freedom                                62\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    0                                          191.440\n    1                                           52.470\n\nModel Test Baseline Model:\n\n  Test statistic                               887.887\n  Degrees of freedom                                42\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.785\n  Tucker-Lewis Index (TLI)                       0.854\n                                                      \n  Robust Comparative Fit Index (CFI)             1.000\n  Robust Tucker-Lewis Index (TLI)                0.326\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7966.093\n  Loglikelihood unrestricted model (H1)      -7844.138\n                                                      \n  Akaike (AIC)                               15948.185\n  Bayesian (BIC)                             15986.884\n  Sample-size adjusted Bayesian (SABIC)      15961.477\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.079\n  90 Percent confidence interval - lower         0.069\n  90 Percent confidence interval - upper         0.090\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.472\n                                                      \n  Robust RMSEA                                   0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: Robust RMSEA &lt;= 0.050             1.000\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.127\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nGroup 1 [0]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta_2 =~                                            \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2   (.17.)   -0.035    1.144   -0.031    0.975\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            35.488    0.369   96.080    0.000\n    eta_2             4.292    0.092   46.898    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1   (.15.)   63.704    5.637   11.301    0.000\n    eta_2   (.16.)    0.699    0.325    2.147    0.032\n   .math2   (thet)   36.321    1.871   19.413    0.000\n   .math3   (thet)   36.321    1.871   19.413    0.000\n   .math4   (thet)   36.321    1.871   19.413    0.000\n   .math5   (thet)   36.321    1.871   19.413    0.000\n   .math6   (thet)   36.321    1.871   19.413    0.000\n   .math7   (thet)   36.321    1.871   19.413    0.000\n   .math8   (thet)   36.321    1.871   19.413    0.000\n\n\nGroup 2 [1]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta_2 =~                                            \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2   (.17.)   -0.035    1.144   -0.031    0.975\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            32.733    1.244   26.314    0.000\n    eta_2             4.905    0.320   15.320    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1   (.15.)   63.704    5.637   11.301    0.000\n    eta_2   (.16.)    0.699    0.325    2.147    0.032\n   .math2   (thet)   36.321    1.871   19.413    0.000\n   .math3   (thet)   36.321    1.871   19.413    0.000\n   .math4   (thet)   36.321    1.871   19.413    0.000\n   .math5   (thet)   36.321    1.871   19.413    0.000\n   .math6   (thet)   36.321    1.871   19.413    0.000\n   .math7   (thet)   36.321    1.871   19.413    0.000\n   .math8   (thet)   36.321    1.871   19.413    0.000\n\n\n\nEseguiamo il confronto tra i due modelli.\n\nlavTestLRT(mg_math_lavaan_fitM1, mg_math_lavaan_fitM2) |&gt;\n    print()\n\n\nChi-Squared Difference Test\n\n                     Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff\nmg_math_lavaan_fitM2 62 15948 15987 243.91                            \nmg_math_lavaan_fitM1 64 15949 15978 249.11     5.2005 0.058601       2\n                     Pr(&gt;Chisq)  \nmg_math_lavaan_fitM2             \nmg_math_lavaan_fitM1    0.07425 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNon vi è evidenza che consentire una differenza tra medie tra gruppi migliori l’adattamento del modello.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>78</span>  <span class='chapter-title'>Modelli di crescita latenti a gruppi multipli</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/10_growth_groups.html#vincoli-sulle-varianzecovarianze",
    "href": "chapters/lgm/10_growth_groups.html#vincoli-sulle-varianzecovarianze",
    "title": "78  Modelli di crescita latenti a gruppi multipli",
    "section": "78.4 Vincoli sulle varianze/covarianze",
    "text": "78.4 Vincoli sulle varianze/covarianze\nNel modello M3 consentiamo che anche le varianza e le covarianza differiscano tra gruppi, oltre alle medie.\n\nmg_math_lavaan_fitM3 &lt;- sem(mg_math_lavaan_model,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\",\n  group = \"lb_wght\", # to separate groups\n  group.equal = c(\n    \"loadings\", # for constraints\n    # \"means\", commented out so can differ\n    # \"lv.variances\",\n    # \"lv.covariances\",\n    \"residuals\"\n  )\n)\n\nEsaminiamo i risultati.\n\nsummary(mg_math_lavaan_fitM3, fit.measures = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 57 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        24\n  Number of equality constraints                    13\n\n  Number of observations per group:               Used       Total\n    0                                              857         858\n    1                                               75          75\n  Number of missing patterns per group:                           \n    0                                               60            \n    1                                               25            \n\nModel Test User Model:\n                                                      \n  Test statistic                               241.182\n  Degrees of freedom                                59\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    0                                          191.157\n    1                                           50.024\n\nModel Test Baseline Model:\n\n  Test statistic                               887.887\n  Degrees of freedom                                42\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.785\n  Tucker-Lewis Index (TLI)                       0.847\n                                                      \n  Robust Comparative Fit Index (CFI)             1.000\n  Robust Tucker-Lewis Index (TLI)                0.320\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7964.728\n  Loglikelihood unrestricted model (H1)      -7844.138\n                                                      \n  Akaike (AIC)                               15951.457\n  Bayesian (BIC)                             16004.668\n  Sample-size adjusted Bayesian (SABIC)      15969.732\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.081\n  90 Percent confidence interval - lower         0.071\n  90 Percent confidence interval - upper         0.092\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.598\n                                                      \n  Robust RMSEA                                   0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: Robust RMSEA &lt;= 0.050             1.000\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.124\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nGroup 1 [0]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta_2 =~                                            \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2             0.243    1.147    0.212    0.832\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            35.485    0.365   97.271    0.000\n    eta_2             4.293    0.091   47.089    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            61.062    5.692   10.727    0.000\n    eta_2             0.663    0.326    2.033    0.042\n   .math2   (thet)   36.276    1.870   19.402    0.000\n   .math3   (thet)   36.276    1.870   19.402    0.000\n   .math4   (thet)   36.276    1.870   19.402    0.000\n   .math5   (thet)   36.276    1.870   19.402    0.000\n   .math6   (thet)   36.276    1.870   19.402    0.000\n   .math7   (thet)   36.276    1.870   19.402    0.000\n   .math8   (thet)   36.276    1.870   19.402    0.000\n\n\nGroup 2 [1]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta_2 =~                                            \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2            -3.801    4.912   -0.774    0.439\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            32.850    1.413   23.241    0.000\n    eta_2             4.881    0.341   14.332    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            95.283   24.221    3.934    0.000\n    eta_2             1.297    1.315    0.986    0.324\n   .math2   (thet)   36.276    1.870   19.402    0.000\n   .math3   (thet)   36.276    1.870   19.402    0.000\n   .math4   (thet)   36.276    1.870   19.402    0.000\n   .math5   (thet)   36.276    1.870   19.402    0.000\n   .math6   (thet)   36.276    1.870   19.402    0.000\n   .math7   (thet)   36.276    1.870   19.402    0.000\n   .math8   (thet)   36.276    1.870   19.402    0.000\n\n\n\nConfrontiamo il modello M2 con il modello M3.\n\nlavTestLRT(mg_math_lavaan_fitM2, mg_math_lavaan_fitM3) |&gt;\n    print()\n\n\nChi-Squared Difference Test\n\n                     Df   AIC   BIC  Chisq Chisq diff RMSEA Df diff\nmg_math_lavaan_fitM3 59 15952 16005 241.18                         \nmg_math_lavaan_fitM2 62 15948 15987 243.91     2.7283     0       3\n                     Pr(&gt;Chisq)\nmg_math_lavaan_fitM3           \nmg_math_lavaan_fitM2     0.4354\n\n\nNon ci sono evidenze che una differenza nelle varianze e nelle covarianze tra gruppi migliori la bontà dell’adattamento del modello ai dati.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>78</span>  <span class='chapter-title'>Modelli di crescita latenti a gruppi multipli</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/10_growth_groups.html#vincoli-sui-residui",
    "href": "chapters/lgm/10_growth_groups.html#vincoli-sui-residui",
    "title": "78  Modelli di crescita latenti a gruppi multipli",
    "section": "78.5 Vincoli sui residui",
    "text": "78.5 Vincoli sui residui\nEsaminiamo ora il vincolo sulle covarianze residue. Iniziamo a specificare il modello in una nuova forma.\n\nmg_math_lavaan_model4 &lt;- \"\n  # latent variable definitions\n      #intercept (note intercept is a reserved term)\n      eta_1 =~ 1*math2\n      eta_1 =~ 1*math3\n      eta_1 =~ 1*math4\n      eta_1 =~ 1*math5\n      eta_1 =~ 1*math6\n      eta_1 =~ 1*math7\n      eta_1 =~ 1*math8\n\n      #linear slope\n      eta_2 =~ 0*math2\n      eta_2 =~ 1*math3\n      eta_2 =~ 2*math4\n      eta_2 =~ 3*math5\n      eta_2 =~ 4*math6\n      eta_2 =~ 5*math7\n      eta_2 =~ 6*math8\n\n  # factor variances\n      eta_1 ~~ start(60)*eta_1\n      eta_2 ~~ start(.75)*eta_2\n\n  # covariances among factors\n      eta_1 ~~ eta_2\n\n  # factor means\n      eta_1 ~ start(35)*1\n      eta_2 ~ start(4)*1\n\n  # manifest variances (made equivalent by naming theta)\n      math2 ~~ c(theta1,theta2)*math2\n      math3 ~~ c(theta1,theta2)*math3\n      math4 ~~ c(theta1,theta2)*math4\n      math5 ~~ c(theta1,theta2)*math5\n      math6 ~~ c(theta1,theta2)*math6\n      math7 ~~ c(theta1,theta2)*math7\n      math8 ~~ c(theta1,theta2)*math8\n  # manifest means (fixed at zero)\n      math2 ~ 0*1\n      math3 ~ 0*1\n      math4 ~ 0*1\n      math5 ~ 0*1\n      math6 ~ 0*1\n      math7 ~ 0*1\n      math8 ~ 0*1\n\" # end of model definition\n\nAdattiamo il modello ai dati.\n\nmg_math_lavaan_fitM4 &lt;- sem(mg_math_lavaan_model4,\n  data = nlsy_math_wide,\n  meanstructure = TRUE,\n  estimator = \"ML\",\n  missing = \"fiml\",\n  group = \"lb_wght\", # to separate groups\n  group.equal = c(\"loadings\")\n) # for constraints\n\nEsaminiamo i risulati.\n\nsummary(mg_math_lavaan_fitM4, fit.measures = TRUE) |&gt;\n    print()\n\nlavaan 0.6-19 ended normally after 62 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        24\n  Number of equality constraints                    12\n\n  Number of observations per group:               Used       Total\n    0                                              857         858\n    1                                               75          75\n  Number of missing patterns per group:                           \n    0                                               60            \n    1                                               25            \n\nModel Test User Model:\n                                                      \n  Test statistic                               237.836\n  Degrees of freedom                                58\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    0                                          190.833\n    1                                           47.004\n\nModel Test Baseline Model:\n\n  Test statistic                               887.887\n  Degrees of freedom                                42\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.787\n  Tucker-Lewis Index (TLI)                       0.846\n                                                      \n  Robust Comparative Fit Index (CFI)             1.000\n  Robust Tucker-Lewis Index (TLI)                0.294\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7963.056\n  Loglikelihood unrestricted model (H1)      -7844.138\n                                                      \n  Akaike (AIC)                               15950.111\n  Bayesian (BIC)                             16008.159\n  Sample-size adjusted Bayesian (SABIC)      15970.048\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.082\n  90 Percent confidence interval - lower         0.071\n  90 Percent confidence interval - upper         0.092\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.607\n                                                      \n  Robust RMSEA                                   0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: Robust RMSEA &lt;= 0.050             1.000\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.124\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\n\nGroup 1 [0]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta_2 =~                                            \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2            -0.063    1.161   -0.054    0.957\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            35.481    0.365   97.257    0.000\n    eta_2             4.297    0.091   47.145    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            62.287    5.728   10.873    0.000\n    eta_2             0.774    0.334    2.314    0.021\n   .math2   (tht1)   35.172    1.904   18.473    0.000\n   .math3   (tht1)   35.172    1.904   18.473    0.000\n   .math4   (tht1)   35.172    1.904   18.473    0.000\n   .math5   (tht1)   35.172    1.904   18.473    0.000\n   .math6   (tht1)   35.172    1.904   18.473    0.000\n   .math7   (tht1)   35.172    1.904   18.473    0.000\n   .math8   (tht1)   35.172    1.904   18.473    0.000\n\n\nGroup 2 [1]:\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 =~                                            \n    math2             1.000                           \n    math3             1.000                           \n    math4             1.000                           \n    math5             1.000                           \n    math6             1.000                           \n    math7             1.000                           \n    math8             1.000                           \n  eta_2 =~                                            \n    math2             0.000                           \n    math3             1.000                           \n    math4             2.000                           \n    math5             3.000                           \n    math6             4.000                           \n    math7             5.000                           \n    math8             6.000                           \n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  eta_1 ~~                                            \n    eta_2             0.745    5.522    0.135    0.893\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            32.800    1.407   23.314    0.000\n    eta_2             4.873    0.341   14.298    0.000\n   .math2             0.000                           \n   .math3             0.000                           \n   .math4             0.000                           \n   .math5             0.000                           \n   .math6             0.000                           \n   .math7             0.000                           \n   .math8             0.000                           \n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    eta_1            79.627   25.629    3.107    0.002\n    eta_2            -0.157    1.477   -0.106    0.915\n   .math2   (tht2)   48.686    8.444    5.766    0.000\n   .math3   (tht2)   48.686    8.444    5.766    0.000\n   .math4   (tht2)   48.686    8.444    5.766    0.000\n   .math5   (tht2)   48.686    8.444    5.766    0.000\n   .math6   (tht2)   48.686    8.444    5.766    0.000\n   .math7   (tht2)   48.686    8.444    5.766    0.000\n   .math8   (tht2)   48.686    8.444    5.766    0.000\n\n\n\nFacciamo un confronto tra la bontà di adattamento del modello M3 e del modello M4.\n\nlavTestLRT(mg_math_lavaan_fitM3, mg_math_lavaan_fitM4) |&gt;\n    print()\n\n\nChi-Squared Difference Test\n\n                     Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff\nmg_math_lavaan_fitM4 58 15950 16008 237.84                            \nmg_math_lavaan_fitM3 59 15952 16005 241.18     3.3457 0.070948       1\n                     Pr(&gt;Chisq)  \nmg_math_lavaan_fitM4             \nmg_math_lavaan_fitM3    0.06738 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnche in questo caso non otteniamo un risultato che fornisce evidenza di differenze tra i due gruppi.\nIn sintesi, possiamo dire che le evidenze presenti suggeriscono che i modelli di crescita latente per di due gruppi hanno parametri uguali per ciò che concerne le saturazioni fattoriali, le medie, le varianze, le covarianze e i residui.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>78</span>  <span class='chapter-title'>Modelli di crescita latenti a gruppi multipli</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/10_growth_groups.html#riflessioni-conclusive",
    "href": "chapters/lgm/10_growth_groups.html#riflessioni-conclusive",
    "title": "78  Modelli di crescita latenti a gruppi multipli",
    "section": "78.6 Riflessioni Conclusive",
    "text": "78.6 Riflessioni Conclusive\nL’approccio a gruppi multipli, utilizzato per studiare le differenze interpersonali nei cambiamenti intrapersonali, è estremamente efficace. Qui ci siamo concentrati sui modelli di crescita lineare per descrivere i cambiamenti intrapersonali e le differenze interpersonali in questi cambiamenti, ma è anche possibile considerare modelli non lineari più complessi. Per esempio, in alcune situazioni, un gruppo potrebbe seguire una traiettoria di crescita lineare (ad esempio, un gruppo di controllo), mentre un altro potrebbe seguire una crescita esponenziale (ad esempio, un gruppo di intervento).\nQuando consideriamo l’ipotesi che gruppi diversi di individui possano seguire traiettorie di cambiamento intrapersonale differenti, l’utilità del framework a gruppi multipli diventa ancora più evidente. Abbiamo presentato il framework a gruppi multipli come alternativa all’approccio con covarianti invarianti nel tempo, tuttavia i due approcci possono essere integrati. Come descritto nel capitolo precedente, i covarianti invarianti nel tempo sono utilizzati per spiegare le differenze interpersonali nell’intercetta e nella pendenza. Includendo i covarianti invarianti nel tempo nel framework a gruppi multipli, possiamo spiegare la variabilità nell’intercetta e nella pendenza all’interno di ciascun gruppo e testare se le relazioni tra i covarianti invarianti nel tempo e l’intercetta e la pendenza differiscono tra i gruppi.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>78</span>  <span class='chapter-title'>Modelli di crescita latenti a gruppi multipli</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html",
    "href": "chapters/prediction/01_prediction.html",
    "title": "80  Predizione",
    "section": "",
    "text": "80.1 Introduzione\nLe predizioni possono essere di vario tipo. Alcune riguardano dati categoriali, mentre altre si basano su dati continui. Per i dati categoriali, possiamo valutare le predizioni utilizzando una tabella 2x2, nota come matrice di confusione, o con modelli di regressione logistica. Invece, per i dati continui, possiamo utilizzare la regressione multipla o varianti come il modello a equazioni strutturali o i modelli misti.\nConsideriamo un esempio pratico di predizione della probabilità di contrarre l’HIV, utilizzando le seguenti informazioni (Petersen, 2024):",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#introduzione",
    "href": "chapters/prediction/01_prediction.html#introduzione",
    "title": "80  Predizione",
    "section": "",
    "text": "Tasso di base dell’HIV (P(HIV)): 0.3% (0.003). Questa è la probabilità che una persona nella popolazione generale abbia l’HIV.\nSensibilità del test (P(Test+ HIV)): 95% (0.95). Questa è la probabilità che il test risulti positivo se la persona ha effettivamente l’HIV.\nSpecificità del test (P(Test- ¬HIV)): 99.28% (0.9928). Questa è la probabilità che il test risulti negativo se la persona non ha l’HIV.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#calcolo-della-probabilità-di-hiv-dato-un-test-positivo",
    "href": "chapters/prediction/01_prediction.html#calcolo-della-probabilità-di-hiv-dato-un-test-positivo",
    "title": "80  Predizione",
    "section": "80.2 Calcolo della probabilità di HIV dato un test positivo",
    "text": "80.2 Calcolo della probabilità di HIV dato un test positivo\nPer calcolare la probabilità di avere l’HIV dato un test positivo (P(HIV Test+)), utilizziamo il teorema di Bayes:\n\\[\nP(HIV \\mid Test+) = \\frac{P(Test+ \\mid HIV) \\times P(HIV)}{P(Test+)}.\n\\]\nAbbiamo bisogno di calcolare il denominatore, ovvero la probabilità complessiva di ottenere un test positivo (P(Test+)). Questo valore include sia i veri positivi che i falsi positivi:\n\\[\nP(Test+) = P(Test+ \\mid HIV) \\times P(HIV) + P(Test+ \\mid \\neg HIV) \\times P(\\neg HIV),\n\\]\ndove:\n\n\\(P(Test+ \\mid \\neg HIV) = 1 - P(Test- \\mid \\neg HIV) = 1 - 0.9928 = 0.0072\\) (tasso di falsi positivi),\n\\(P(\\neg HIV) = 1 - P(HIV) = 1 - 0.003 = 0.997\\).\n\nCalcoliamo \\(P(Test+)\\):\n\\[\nP(Test+) = (0.95 \\times 0.003) + (0.0072 \\times 0.997) \\approx 0.010027.\n\\]\nOra possiamo calcolare \\(P(HIV \\mid Test+)\\):\n\\[\nP(HIV \\mid Test+) = \\frac{0.95 \\times 0.003}{0.010027} \\approx 0.2844 \\text{ o 28.44\\%}.\n\\]\nQuindi, se il test risulta positivo, la probabilità di avere l’HIV è circa il 28.44%.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#calcolo-della-probabilità-di-un-secondo-test-positivo",
    "href": "chapters/prediction/01_prediction.html#calcolo-della-probabilità-di-un-secondo-test-positivo",
    "title": "80  Predizione",
    "section": "80.3 Calcolo della probabilità di un secondo test positivo",
    "text": "80.3 Calcolo della probabilità di un secondo test positivo\nDopo un primo test positivo, la probabilità di avere l’HIV è aumentata al 28.44%. Ora calcoleremo la probabilità che un secondo test risulti positivo e la conseguente probabilità di avere l’HIV dopo due test positivi consecutivi.\nPer calcolare \\(P(\\text{Secondo Test+})\\), consideriamo due scenari:\n\nLa persona ha effettivamente l’HIV:\n\nProbabilità: \\(P(HIV \\mid Test+) = 0.2844\\).\nProbabilità di un test positivo: \\(P(\\text{Test+} \\mid HIV) = 0.95\\) (sensibilità del test).\n\nLa persona non ha l’HIV:\n\nProbabilità: \\(P(\\neg HIV \\mid Test+) = 1 - P(HIV \\mid Test+) = 0.7156\\).\nProbabilità di un test positivo: \\(P(\\text{Test+} \\mid \\neg HIV) = 0.0072\\) (tasso di falsi positivi).\n\n\nUtilizziamo la formula della probabilità totale:\n\\[\n\\begin{aligned}\nP(\\text{Secondo Test+}) &= P(\\text{Test+} \\mid HIV) \\times P(HIV \\mid Test+) + \\\\\n&\\quad P(\\text{Test+} \\mid \\neg HIV) \\times P(\\neg HIV \\mid Test+).\n\\end{aligned}\n\\]\nSostituendo i valori:\n\\[\nP(\\text{Secondo Test+}) = (0.95 \\times 0.2844) + (0.0072 \\times 0.7156) \\approx 0.2753.\n\\]\nApplichiamo nuovamente il teorema di Bayes per calcolare la probabilità di avere l’HIV dopo un secondo test positivo:\n\\[\nP(HIV \\mid \\text{Secondo Test+}) = \\frac{P(\\text{Secondo Test+} \\mid HIV) \\times P(HIV \\mid Test+)}{P(\\text{Secondo Test+})}.\n\\]\nSostituendo i valori:\n\\[\nP(HIV \\mid \\text{Secondo Test+}) = \\frac{0.95 \\times 0.2844}{0.2753} \\approx 0.981.\n\\]\nIn conclusione, dopo un secondo test positivo, la probabilità di avere l’HIV aumenta notevolmente, passando dal 28.44% al 98.1%. Questo drastico aumento dimostra l’importanza di:\n\nconsiderare il tasso di base (prevalenza) nella popolazione;\naggiornare progressivamente le probabilità con nuove evidenze;\ninterpretare i risultati di test diagnostici multipli in modo bayesiano.\n\nL’analisi evidenzia come l’accumulo di evidenze attraverso test ripetuti, in linea con i principi del teorema di Bayes, possa portare a una stima molto più accurata della probabilità di avere una condizione medica, riducendo sostanzialmente l’incertezza iniziale.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#accuratezza-delle-predizioni",
    "href": "chapters/prediction/01_prediction.html#accuratezza-delle-predizioni",
    "title": "80  Predizione",
    "section": "80.4 Accuratezza delle Predizioni",
    "text": "80.4 Accuratezza delle Predizioni\nPer valutare l’accuratezza delle predizioni, Petersen (2024) considera un esempio adattato da Meehl & Rosen (1955). L’esercito americano esegue un test sui candidati per escludere quelli che hanno basse probabilità di superare l’addestramento di base. Per analizzare l’accuratezza di queste predizioni, possiamo utilizzare una matrice di confusione.\n\n\n\n\n\n\n\n\n\n\nDecision (Prediction)\nActual Adjustment (Poor)\nActual Adjustment (Good)\nTotal Predicted\nSelection Ratio\n\n\n\n\nReject\nTP = 86 (.043)\nFP = 422 (.211)\n508\nSR = 0.254\n\n\nRetain\nFN = 14 (.007)\nTN = 1,478 (.739)\n1,492\n1 - SR = 0.746\n\n\nTotal Actual\n100\n1,900\nN = 2,000\n\n\n\nBase Rate\nBR = 0.05\n1 - BR = 0.95\n\n\n\n\n\nUna matrice di confusione è una tabella che mette a confronto le predizioni fatte da un modello con gli esiti reali. Quando si tratta di una predizione dicotomica (ad esempio, sì/no o positivo/negativo), la matrice di confusione è organizzata in una tabella 2x2 che descrive le seguenti combinazioni.\n\nVero positivo (TP): La predizione indica che la persona possiede la caratteristica, e ciò risulta corretto perché la persona effettivamente la possiede.\nVero negativo (TN): La predizione indica che la persona non possiede la caratteristica, e ciò risulta corretto perché la persona effettivamente non la possiede.\nFalso positivo (FP): La predizione indica che la persona possiede la caratteristica, ma in realtà la persona non la possiede.\nFalso negativo (FN): La predizione indica che la persona non possiede la caratteristica, ma in realtà la persona la possiede.\n\nQuesti quattro risultati sono alla base dell’analisi dell’accuratezza di un modello predittivo. Il termine “vero” indica una predizione corretta, mentre “falso” rappresenta un errore. “Positivo” e “negativo” si riferiscono rispettivamente al fatto che la predizione indichi la presenza o l’assenza di una determinata caratteristica.\n\n80.4.1 Calcolo dei Tassi Marginali\nCon le informazioni presenti nella matrice di confusione, possiamo calcolare i tassi marginali, ovvero le probabilità globali che una persona presenti una certa caratteristica o sia classificata in un determinato modo.\n\nTasso di base (BR): Questo rappresenta la probabilità marginale che una persona abbia la caratteristica di interesse. Ad esempio, se 100 persone su 2.000 mostrano un cattivo adattamento, il tasso di base è:\n\\[\nBR = \\frac{FN + TP}{N} = \\frac{100}{2000} = 0.05\n\\]\nCiò significa che il 5% dei candidati ha un cattivo adattamento.\nRapporto di selezione (SR): Questo indica la probabilità marginale che una persona venga esclusa dal test. Se 508 persone vengono escluse, il rapporto di selezione è:\n\\[\nSR = \\frac{TP + FP}{N} = \\frac{508}{2000} = 0.254\n\\]\nIl 25.4% dei candidati è stato escluso.\n\nIl rapporto di selezione può dipendere dal punteggio di cutoff del test o da fattori esterni, come il numero di candidati che possono essere trattati o inclusi nel programma.\n\n\n80.4.2 Percentuale di Accuratezza\nLa percentuale di accuratezza è un indicatore generale di quante predizioni sono corrette. Si calcola dividendo il numero di predizioni corrette (TP + TN) per il numero totale di predizioni (N), e moltiplicando per 100:\n\\[\n\\text{Percentuale di accuratezza} = 100 \\times \\frac{TP + TN}{N}.\n\\]\nNel nostro esempio, il 78% delle predizioni è corretto, il che indica che il modello ha una buona accuratezza complessiva.\n\n\n80.4.3 Accuratezza per Caso\nSebbene il 78% di accuratezza possa sembrare un buon risultato, è essenziale confrontare questo valore con quello che si otterrebbe semplicemente per puro caso. Questo confronto ci aiuta a capire se il modello apporta un reale valore aggiunto rispetto a una previsione casuale.\nAd esempio, consideriamo un tasso di base (BR) del 5% e un rapporto di selezione (SR) del 25,4%. La probabilità di ottenere un vero positivo per caso è data da:\n\\[\nP(TP) = BR \\times SR = 0.05 \\times 0.254 = 0.0127.\n\\]\nQui, il tasso di base BR = 0,05 rappresenta la probabilità che un individuo appartenga al gruppo con la caratteristica di interesse, mentre il rapporto di selezione SR = 0,254 indica la probabilità che l’individuo venga classificato come positivo. Moltiplicando queste due probabilità marginali, otteniamo la probabilità congiunta di ottenere un vero positivo per caso, pari a 0,0127.\nAnalogamente, la probabilità di ottenere un vero negativo per caso è data da:\n\\[\nP(TN) = (1 - BR) \\times (1 - SR) = 0.95 \\times 0.746 = 0.7087.\n\\]\nPertanto, la percentuale di accuratezza per caso, ossia l’accuratezza attesa basandosi esclusivamente sulle probabilità marginali (BR e SR) senza informazioni specifiche del modello, è:\n\\[\nP(\\text{Accuratezza per caso}) = P(TP) + P(TN) = 0.0127 + 0.7087 = 0.7214.\n\\]\nIn questo esempio, il 72,14% di accuratezza potrebbe essere raggiunto anche senza l’uso del modello, semplicemente per caso. Dato che il nostro modello raggiunge un’accuratezza del 78%, il reale incremento di accuratezza rispetto al caso è solo del 6%. Questo evidenzia l’importanza di confrontare l’accuratezza del modello con quella ottenibile per puro caso per valutare il suo valore aggiunto.\nConfrontando le aspettative casuali con l’accuratezza effettiva del modello, possiamo quindi misurare il reale beneficio del modello. Se l’accuratezza effettiva supera di poco quella ottenibile per caso, significa che il modello offre un miglioramento limitato rispetto a una semplice scelta casuale.\n\n\n80.4.4 Predire dal Tasso di Base\nChiediamoci ora cosa accadrebbe se facessimo una previsione basata solo sulla probabilità generale degli esiti (tasso di base), senza considerare alcun modello predittivo.\nCon un tasso di base basso (BR = 0.05) di un insufficiente adattamento alla vita militare, possiamo massimizzare l’accuratezza complessiva scegliendo di non “escludere” nessuno. Questo equivale a un rapporto di selezione (SR) pari a zero, indicando che non classifichiamo alcun caso come “Reject”. In questo scenario, tutti i casi sarebbero previsti come “Retain”, aumentando l’accuratezza totale ma rinunciando completamente alla possibilità di identificare casi positivi.\nSe applichiamo questa logica alla matrice di confusione dei dati:\n\n\n\n\n\n\n\n\n\nDecision (Prediction)\nActual Adjustment (Poor)\nActual Adjustment (Good)\nTotal Predicted\n\n\n\n\nReject\nTP = 0\nFP = 0\n0\n\n\nRetain\nFN = 100\nTN = 1,900\n2,000\n\n\nTotal Actual\n100\n1,900\nN = 2,000\n\n\n\n\nCon SR = 0 (nessun caso viene classificato come “Reject”), otterremo:\n\nTN (Vero Negativo) = 1,900 (tutti i casi corretti di “Retain”)\nFN (Falso Negativo) = 100 (tutti i casi di “Poor” classificati erroneamente come “Retain”).\n\n\nL’accuratezza complessiva in questo caso sarà quindi:\n\\[\nP(\\text{Accuratezza}) = \\frac{\\text{TP} + \\text{TN}}{N} = \\frac{0 + 1,900}{2,000} = 0.95\n\\]\nIn sintesi:\n\n“Predire dal Tasso di Base” implica usare l’esito prevalente per ogni predizione senza cercare di individuare i casi positivi;\nnei dati forniti, impostando SR = 0 otteniamo un’accuratezza del 95%, che è superiore all’accuratezza del 78% del modello originale;\nquesto approccio aumenta l’accuratezza complessiva ma non offre alcuna informazione discriminativa.\n\nIn conclusione, optare per l’esito più probabile in ogni predizione (cioè, predire sempre in base al tasso di base) può portare a un’elevata accuratezza, come osservato da Meehl e Rosen (1955), soprattutto quando il tasso di base è molto basso o molto alto. Questo effetto ci mostra quanto sia importante confrontare l’accuratezza del nostro modello con l’accuratezza che potremmo ottenere (1) per puro caso e (2) utilizzando solo il tasso di base. Questo confronto è cruciale, poiché l’accuratezza di un modello può cambiare notevolmente a seconda del contesto in cui viene applicato. Infatti, in alcuni contesti, dove il tasso di base si discosta molto dal 50%, l’uso del modello può addirittura ridurre la sua capacità di predizione accurata.\nInoltre, è importante considerare che la percentuale di accuratezza tratta tutti i tipi di errore allo stesso modo, senza fare distinzioni. Tuttavia, nella pratica, non tutti gli errori hanno lo stesso peso o importanza. Il costo di un falso positivo può essere molto diverso da quello di un falso negativo, e questa differenza può variare a seconda del contesto specifico, come verrà discusso nel prossimo paragrafo.\n\n\n80.4.5 Diversi Tipi di Errori e i loro Costi\nIn un processo di classificazione, non tutti gli errori hanno lo stesso costo. Esistono due tipi principali di errori: i falsi positivi e i falsi negativi, ciascuno con implicazioni diverse che dipendono dal contesto della predizione.\nSpesso, l’accuratezza complessiva può essere aumentata affidandosi semplicemente al tasso di base, ma in molte situazioni può essere preferibile utilizzare uno strumento di screening, anche a costo di una minore accuratezza complessiva, se ciò consente di minimizzare errori specifici che hanno costi elevati. Ad esempio:\n\nScreening medico: Consideriamo uno strumento di screening per l’HIV. I falsi positivi (classificare erroneamente una persona come a rischio) comportano costi come la necessità di test di conferma e, talvolta, ansia temporanea per l’individuo. Tuttavia, un falso negativo (non identificare una persona effettivamente a rischio) ha costi molto più alti, poiché potrebbe portare a un mancato intervento precoce, con conseguenze gravi per la salute. In questo caso, i costi associati ai falsi negativi superano di gran lunga quelli dei falsi positivi, rendendo lo screening preferibile nonostante una diminuzione dell’accuratezza complessiva.\nSelezione del personale in situazioni di rischio: La CIA, ad esempio, ha utilizzato strumenti di selezione per identificare potenziali spie durante periodi di guerra. Un falso positivo in questo contesto (considerare erroneamente una persona come una spia) potrebbe risultare nell’esclusione di un candidato innocente. Un falso negativo (assumere una persona che è effettivamente una spia) comporta rischi molto più gravi, rendendo cruciale l’identificazione corretta delle spie, anche a costo di più falsi positivi.\n\nIl modo in cui i costi degli errori vengono valutati dipende fortemente dal contesto. Alcuni potenziali costi dei falsi positivi includono trattamenti medici non necessari o il rischio di incarcerare una persona innocente. Al contrario, i falsi negativi possono portare al rilascio di una persona pericolosa, alla mancata individuazione di una malattia grave, o al mancato riconoscimento di un rischio imminente.\n\n\n80.4.6 Importanza del Rapporto di Selezione e del Tasso di Base\nIl costo degli errori può variare a seconda di come si imposta il rapporto di selezione (cioè, quanto rigorosamente si applica il criterio per accettare o escludere un individuo). La scelta di un rapporto di selezione meno restrittivo o più restrittivo influisce sulla probabilità di incorrere in falsi positivi e falsi negativi e può dipendere dal contesto e dai costi associati agli errori.\n\nCriterio meno rigido: Se escludere candidati è costoso, ad esempio quando si ha la necessità di assumere molte persone, potrebbe essere più utile un criterio di selezione permissivo, che accetta anche persone con un rischio potenziale.\nCriterio più rigido: In contesti in cui non è necessario accettare molti individui, si può adottare un criterio di selezione più rigido per ridurre i rischi, scartando un numero maggiore di candidati sospetti.\n\nQuando il rapporto di selezione differisce dal tasso di base degli esiti negativi effettivi, inevitabilmente si generano errori:\n\nSe, ad esempio, il rapporto di selezione prevede di escludere il 25% dei candidati, ma solo il 5% risulta effettivamente “non idoneo,” il risultato sarà un numero elevato di falsi positivi.\nD’altro canto, se si esclude solo l’1% dei candidati mentre il tasso di non idoneità è del 5%, si finirà per includere molti falsi negativi.\n\n\n\n80.4.7 Predizioni e Affidabilità in Condizioni di Basso Tasso di Base\nFare predizioni accurate diventa particolarmente complesso quando il tasso di base è basso, come nel caso di eventi rari (ad esempio, il suicidio). In questi casi, il numero di casi positivi reali è molto ridotto, rendendo difficile identificare correttamente i pochi eventi positivi senza generare numerosi falsi positivi o falsi negativi.\nQuesta difficoltà può essere compresa in relazione alla teoria classica dei test, che definisce l’affidabilità come il rapporto tra la varianza del punteggio vero e la varianza del punteggio osservato. Con un tasso di base molto basso, la varianza del punteggio vero è ridotta, il che abbassa l’affidabilità della misura e rende più complessa una predizione accurata.\n\n\n80.4.8 Sensibilità, Specificità, PPV e NPV\nCome abbiamo visto, la percentuale di accuratezza da sola non è sufficiente per valutare l’efficacia di un modello, poiché è molto influenzata dai tassi di base. Ad esempio, se il tasso di base è basso, potremmo ottenere un’alta percentuale di accuratezza semplicemente affermando che nessuno ha la condizione; se è alto, affermando che tutti ce l’hanno. Perciò, è essenziale considerare altre metriche di accuratezza, come sensibilità (SN), specificità (SP), valore predittivo positivo (PPV) e valore predittivo negativo (NPV).\nQueste metriche, che si possono calcolare dalla matrice di confusione, ci aiutano a valutare se il modello è efficace nel rilevare la condizione senza includere erroneamente i casi negativi. Analizziamole in dettaglio:\n\nSensibilità (SN): indica la capacità del test di identificare correttamente i veri positivi, cioè le persone con la condizione. Si calcola come la proporzione di veri positivi (\\(\\text{TP}\\)) rispetto al totale di persone con la condizione (\\(\\text{TP} + \\text{FN}\\)):\n\\[\n\\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{86}{86 + 14} = 0.86\n\\]\nSpecificità (SP): misura la capacità del test di identificare correttamente i veri negativi, ossia le persone senza la condizione. Si calcola come la proporzione di veri negativi (\\(\\text{TN}\\)) rispetto al totale di persone senza la condizione (\\(\\text{TN} + \\text{FP}\\)):\n\\[\n\\frac{\\text{TN}}{\\text{TN} + \\text{FP}} = \\frac{1,478}{1,478 + 422} = 0.78\n\\]\nValore Predittivo Positivo (PPV): indica la probabilità che una persona classificata come positiva abbia effettivamente la condizione. Si calcola come la proporzione di veri positivi (\\(\\text{TP}\\)) sul totale dei positivi stimati (\\(\\text{TP} + \\text{FP}\\)):\n\\[\n\\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{86}{86 + 422} = 0.17\n\\]\nValore Predittivo Negativo (NPV): rappresenta la probabilità che una persona classificata come negativa non abbia effettivamente la condizione. Si calcola come la proporzione di veri negativi (\\(\\text{TN}\\)) sul totale dei negativi stimati (\\(\\text{TN} + \\text{FN}\\)):\n\\[\n\\frac{\\text{TN}}{\\text{TN} + \\text{FN}} = \\frac{1,478}{1,478 + 14} = 0.99\n\\]\n\nOgni misura è espressa come una proporzione, variando da 0 a 1, dove valori più alti indicano una maggiore accuratezza per ciascun aspetto specifico. Usando queste metriche otteniamo un quadro dettagliato dell’efficacia dello strumento a un determinato cutoff.\n\n\n80.4.9 Interpretazione delle Metriche\nIn questo caso, il nostro strumento mostra: - Alta sensibilità (0,86): è efficace nel rilevare chi ha la condizione. - Bassa specificità (0,78): classifica erroneamente come positivi molti casi che non hanno la condizione. - Basso PPV (0,17): la maggior parte dei casi classificati come positivi sono in realtà negativi, indicando una frequenza elevata di falsi positivi. - Alto NPV (0,99): quasi tutti i casi classificati come negativi non hanno la condizione.\nQuindi, pur avendo una buona capacità di rilevare i positivi (alta sensibilità), il modello è meno efficace nel limitare i falsi positivi (basso PPV). Questo potrebbe essere accettabile se l’obiettivo è identificare tutti i potenziali casi positivi, anche a costo di includere molti falsi positivi, ma potrebbe non essere ideale se il costo degli errori di falsa positività è elevato.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#alcune-stime-di-accuratezza-dipendono-dal-cutoff",
    "href": "chapters/prediction/01_prediction.html#alcune-stime-di-accuratezza-dipendono-dal-cutoff",
    "title": "80  Predizione",
    "section": "80.5 Alcune Stime di Accuratezza Dipendono dal Cutoff",
    "text": "80.5 Alcune Stime di Accuratezza Dipendono dal Cutoff\nSensibilità, specificità, PPV e NPV variano in base al cutoff (ovvero, la soglia) per la classificazione. Consideriamo il seguente esempio. Degli alieni visitano la Terra e sviluppano un test per determinare se una bacca è commestibile o non commestibile.\n\nsampleSize &lt;- 1000\n\nedibleScores &lt;- rnorm(sampleSize, 50, 15)\ninedibleScores &lt;- rnorm(sampleSize, 100, 15)\n\nedibleData &lt;- data.frame(score = c(edibleScores, inedibleScores), type = c(rep(\"edible\", sampleSize), rep(\"inedible\", sampleSize)))\n\ncutoff &lt;- 75\n\nhist_edible &lt;- density(edibleScores, from = 0, to = 150) %$%\n    data.frame(x = x, y = y) %&gt;%\n    mutate(area = x &gt;= cutoff)\n\nhist_edible$type[hist_edible$area == TRUE] &lt;- \"edible_FP\"\nhist_edible$type[hist_edible$area == FALSE] &lt;- \"edible_TN\"\n\nhist_inedible &lt;- density(inedibleScores, from = 0, to = 150) %$%\n    data.frame(x = x, y = y) %&gt;%\n    mutate(area = x &lt; cutoff)\n\nhist_inedible$type[hist_inedible$area == TRUE] &lt;- \"inedible_FN\"\nhist_inedible$type[hist_inedible$area == FALSE] &lt;- \"inedible_TP\"\n\ndensity_data &lt;- bind_rows(hist_edible, hist_inedible)\n\ndensity_data$type &lt;- factor(density_data$type, levels = c(\"edible_TN\", \"inedible_TP\", \"edible_FP\", \"inedible_FN\"))\n\nLa Figura successiva mostra le distribuzioni dei punteggi in base al tipo di bacca. Si può notare come ci sono due distribuzioni distinte, ma con una certa sovrapposizione. Pertanto, qualsiasi cutoff selezionato comporterà almeno alcune classificazioni errate. L’entità della sovrapposizione delle distribuzioni riflette la quantità di errore di misurazione dello strumento rispetto alla caratteristica di interesse.\n\nggplot(data = edibleData, aes(x = score, ymin = 0, fill = type)) +\n    geom_density(alpha = .5) +\n    scale_fill_manual(name = \"Tipo di Bacca\", values = c(viridis(2)[1], viridis(2)[2])) +\n    scale_y_continuous(name = \"Frequenza\") \n\n\n\n\n\n\n\n\nLa Figura successiva mostra le distribuzioni dei punteggi in base al tipo di bacca con un cutoff. La linea rossa indica il cutoff: il livello al di sopra del quale le bacche vengono classificate come non commestibili. Ci sono errori su entrambi i lati del cutoff. Sotto il cutoff, ci sono dei falsi negativi (blu): bacche non commestibili erroneamente classificate come commestibili. Sopra il cutoff, ci sono dei falsi positivi (verde): bacche commestibili erroneamente classificate come non commestibili. I costi dei falsi negativi potrebbero includere malattia o morte derivanti dal consumo di bacche non commestibili, mentre i costi dei falsi positivi potrebbero includere maggiore tempo per trovare cibo, insufficienza di cibo e fame.\n\nggplot(data = density_data, aes(x = x, ymin = 0, ymax = y, fill = type)) +\n    geom_ribbon(alpha = 1) +\n    scale_fill_manual(\n        name = \"Tipo di Bacca\",\n        values = c(viridis(4)[4], viridis(4)[1], viridis(4)[3], viridis(4)[2]),\n        breaks = c(\"edible_TN\", \"inedible_TP\", \"edible_FP\", \"inedible_FN\"),\n        labels = c(\"Commestibile: TN\", \"Non Commestibile: TP\", \"Commestibile: FP\", \"Non Commestibile: FN\")\n    ) +\n    geom_line(aes(y = y)) +\n    geom_vline(xintercept = cutoff, color = \"red\", linewidth = 2) +\n    scale_x_continuous(name = \"Punteggio\") +\n    scale_y_continuous(name = \"Frequenza\") \n\n\n\n\n\n\n\n\nA seconda dei nostri obiettivi di valutazione, potremmo voler usare un diverso rapporto di selezione modificando il cutoff. La Figura mostra le distribuzioni dei punteggi quando si aumenta il cutoff. Ora ci sono più falsi negativi (blu) e meno falsi positivi (verde). Se alziamo il cutoff per essere più conservativi, il numero di falsi negativi aumenta, mentre il numero di falsi positivi diminuisce. Di conseguenza, aumentando il cutoff, la sensibilità e il valore predittivo negativo (NPV) diminuiscono, mentre la specificità e il valore predittivo positivo (PPV) aumentano. Un cutoff più alto potrebbe essere ottimale se i costi dei falsi positivi sono considerati superiori a quelli dei falsi negativi. Ad esempio, se gli alieni non possono rischiare di mangiare bacche non commestibili perché sono fatali, e ci sono abbastanza bacche commestibili per nutrire la colonia aliena.\n\n# Raise the cutoff\ncutoff &lt;- 85\n\nggplot(data = density_data, aes(x = x, ymin = 0, ymax = y, fill = type)) +\n    geom_ribbon(alpha = 1) +\n    scale_fill_manual(\n        name = \"Tipo di Bacca\",\n        values = c(viridis(4)[4], viridis(4)[1], viridis(4)[3], viridis(4)[2]),\n        breaks = c(\"edible_TN\", \"inedible_TP\", \"edible_FP\", \"inedible_FN\"),\n        labels = c(\"Commestibile: TN\", \"Non Commestibile: TP\", \"Commestibile: FP\", \"Non Commestibile: FN\")\n    ) +\n    geom_line(aes(y = y)) +\n    geom_vline(xintercept = cutoff, color = \"red\", linewidth = 2) +\n    scale_x_continuous(name = \"Punteggio\") +\n    scale_y_continuous(name = \"Frequenza\") +\n    theme(\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank()\n    )\n\n\n\n\n\n\n\n\nIn alternativa, possiamo abbassare il cutoff per essere più liberali. La Figura seguente mostra le distribuzioni dei punteggi quando abbassiamo il cutoff. Ora ci sono meno falsi negativi (blu) e più falsi positivi (verde). Abbassando il cutoff, la sensibilità e il NPV aumentano, mentre la specificità e il PPV diminuiscono. Un cutoff più basso potrebbe essere ottimale se i costi dei falsi negativi sono considerati superiori a quelli dei falsi positivi. Ad esempio, se gli alieni non possono rischiare di perdere bacche commestibili perché sono scarse, e mangiare bacche non commestibili comporta solo disagi temporanei.\n\n# Lower the cutoff\ncutoff &lt;- 65\n\nggplot(data = density_data, aes(x = x, ymin = 0, ymax = y, fill = type)) +\n    geom_ribbon(alpha = 1) +\n    scale_fill_manual(\n        name = \"Tipo di Bacca\",\n        values = c(viridis(4)[4], viridis(4)[1], viridis(4)[3], viridis(4)[2]),\n        breaks = c(\"edible_TN\", \"inedible_TP\", \"edible_FP\", \"inedible_FN\"),\n        labels = c(\"Commestibile: TN\", \"Non Commestibile: TP\", \"Commestibile: FP\", \"Non Commestibile: FN\")\n    ) +\n    geom_line(aes(y = y)) +\n    geom_vline(xintercept = cutoff, color = \"red\", linewidth = 2) +\n    scale_x_continuous(name = \"Punteggio\") +\n    scale_y_continuous(name = \"Frequenza\") +\n    theme(\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank()\n    )\n\n\n\n\n\n\n\n\nIn sintesi, sensibilità e specificità variano in base al cutoff utilizzato per la classificazione. Se aumentiamo il cutoff, la specificità e il PPV aumentano, mentre la sensibilità e il NPV diminuiscono. Se abbassiamo il cutoff, la sensibilità e il NPV aumentano, mentre la specificità e il PPV diminuiscono. Pertanto, il cutoff ottimale dipende dai costi associati ai falsi negativi e ai falsi positivi. Se i falsi negativi sono più costosi, dovremmo impostare un cutoff basso; se i falsi positivi sono più costosi, dovremmo impostare un cutoff alto.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#teoria-della-detezione-del-segnale",
    "href": "chapters/prediction/01_prediction.html#teoria-della-detezione-del-segnale",
    "title": "80  Predizione",
    "section": "80.6 Teoria della Detezione del Segnale",
    "text": "80.6 Teoria della Detezione del Segnale\nLa teoria della detezione del segnale (Signal Detection Theory, SDT) è una teoria probabilistica utilizzata per il rilevamento di uno stimolo (segnale) all’interno di un insieme di stimoli che include anche stimoli non target (rumore). La SDT è nata durante lo sviluppo del radar (RAdio Detection And Ranging) e del sonar (SOund Navigation And Ranging) durante la Seconda Guerra Mondiale, basandosi su ricerche in ambito sensoriale-percettivo. Il settore militare desiderava determinare quali oggetti rilevati da radar/sonar fossero effettivamente aerei o sottomarini nemici, e quali fossero solo rumore (ad esempio, oggetti diversi nell’ambiente).\nLa SDT ha permesso di valutare il numero di errori commessi dagli operatori (cioè, quanto fossero precisi) e di scomporre tali errori in diverse categorie. La teoria distingue tra sensibilità e bias. Nella SDT, la sensibilità (o discriminabilità) è la capacità di un test di distinguere tra uno stimolo target e stimoli non target, ossia quanto bene il test riesca a rilevare il segnale tra i rumori. Il bias rappresenta invece la tendenza del test a sovrastimare o sottostimare la probabilità di un evento target rispetto al tasso reale di occorrenza di tale evento.\nAlcuni operatori radar/sonar non erano molto sensibili alla differenza tra segnale e rumore, a causa di fattori come l’età o la capacità di distinguere sottili variazioni di segnale. Gli individui con bassa sensibilità, che quindi non riuscivano a distinguere efficacemente tra segnale e rumore, venivano esclusi, poiché la sensibilità era considerata una competenza che difficilmente si può insegnare. Altri operatori, pur avendo una buona sensibilità, mostravano bias sistematici o scarsa calibrazione, cioè commettevano errori sistematici nel giudicare i segnali, ad esempio sovra-rifiutando o sotto-rifiutando il target.\nSovra-rifiutare significa produrre molti falsi negativi (cioè, giudicare un segnale sicuro quando in realtà non lo è), mentre sotto-rifiutare genera molti falsi positivi (cioè, giudicare un segnale come pericoloso quando in realtà non lo è). Un operatore con buona sensibilità ma bias sistematico veniva considerato più facile da addestrare rispetto a chi aveva una bassa sensibilità. Gli operatori radar e sonar venivano quindi selezionati in base alla loro sensibilità nel distinguere tra segnale e rumore, e poi addestrati per migliorare la calibrazione e ridurre il bias sistematico, evitando così di sovra- o sotto-rifiutare gli stimoli.\nAnche se la SDT è stata sviluppata inizialmente durante la Seconda Guerra Mondiale, oggi ha un ruolo importante in molti ambiti della scienza e della medicina. Un esempio in medicina è il rilevamento di tumori nella radiologia. La SDT è fondamentale anche in psicologia, specialmente nella psicologia cognitiva. Ad esempio, ricerche sulla percezione sociale hanno dimostrato che gli uomini tendono a mostrare una scarsa sensibilità nel distinguere le manifestazioni di interesse sessuale nelle donne, confondendo la cordialità con l’interesse sessuale. Inoltre, gli uomini tendono ad avere un bias sistematico, sovrastimando l’interesse sessuale delle donne nei loro confronti, mostrando così una soglia troppo bassa nel giudicare tali segnali.\nLe metriche SDT di sensibilità includono \\(d'\\) (d-prime), \\(A\\) (o \\(A'\\)), e l’area sotto la curva ROC (Receiver Operating Characteristic). Le metriche di bias includono \\(\\beta\\), \\(c\\) e \\(b\\).\n\n80.6.1 Curva ROC (Receiver Operating Characteristic)\nL’asse delle x della curva ROC rappresenta il tasso di falsi allarmi o tasso di falsi positivi (\\(1 -\\) specificità). L’asse delle y rappresenta il tasso di successi o tasso di veri positivi (sensibilità). La curva ROC traccia la combinazione tra sensibilità e specificità per ogni possibile valore di cutoff.\nIniziamo con un cutoff pari a zero (in alto a destra sulla curva ROC). In questo caso, la sensibilità è pari a 1.0 e la specificità è 0, e il punto corrispondente viene tracciato sulla curva. Con un cutoff di zero, il test decide di agire su ogni stimolo (quindi il test è estremamente liberale). Aumentiamo progressivamente il cutoff e tracciamo la sensibilità e la specificità a ogni valore di cutoff. All’aumentare del cutoff, la sensibilità diminuisce e la specificità aumenta.\nTerminiamo con il valore di cutoff più alto possibile, dove la sensibilità è pari a 0 e la specificità è 1.0 (in altre parole, il test non agisce mai; quindi è il massimo della conservatività). Ogni punto sulla curva ROC corrisponde a una coppia di tasso di successi (sensibilità) e tasso di falsi allarmi (falsi positivi) risultante da uno specifico valore di cutoff. Dopodiché, possiamo collegare i punti con delle linee o curve per ottenere la curva ROC.\nLa Figura seguente mostra un esempio empirico di curva ROC, dove le linee connettono i tassi di successi e di falsi allarmi.\n\nplot(roc(aSAH$outcome, aSAH$s100b), legacy.axes = TRUE, print.auc = TRUE)\n\n\n\n\n\n\n\n\nCreiamo ora una curva ROC lisciata, dove viene tracciata una curva continua e adattata per connettere i tassi di successi e di falsi allarmi.\n\nplot(roc(aSAH$outcome, aSAH$s100b, smooth = TRUE), legacy.axes = TRUE, print.auc = TRUE)\n\n\n\n\n\n\n\n\n\n\n80.6.2 Area Sotto la Curva ROC (AUC)\nLe metodologie ROC possono essere utilizzate per confrontare e calcolare il potere discriminativo degli strumenti di misurazione, senza essere influenzati da fattori come il selection ratio, il base rate e i costi e benefici associati. L’analisi ROC fornisce un indice quantitativo di quanto bene uno strumento possa prevedere un segnale di interesse o discriminare tra segnali diversi. Questo approccio ci aiuta a capire con quale frequenza la nostra valutazione è corretta.\nSe scegliamo casualmente due osservazioni, e una è corretta mentre l’altra è sbagliata, la precisione sarebbe del 50%, ma questo rifletterebbe una risposta casuale, quindi inutile. L’area geometrica sotto la curva ROC riflette l’accuratezza discriminativa della misura. Questo indice è noto come AUC (Area Under the Curve) della curva ROC. AUC quantifica il potere discriminativo di un test. Più precisamente, AUC rappresenta la probabilità che, selezionando casualmente un target e un non-target, il test classifichi correttamente il target come tale. I valori dell’AUC variano da 0.0 a 1.0, dove 0.5 rappresenta la precisione casuale, come indicato dalla linea diagonale nella curva ROC. Un test è utile nella misura in cui la sua curva ROC si trova sopra la linea diagonale, indicando che la sua accuratezza discriminativa è superiore al caso.\n\nplot(roc(aSAH$outcome, aSAH$s100b, smooth = TRUE), legacy.axes = TRUE, print.auc = TRUE, auc.polygon = TRUE)\n\n\n\n\n\n\n\n\n\n# Simulazione dei dati per AUC\nsimulateDataFromAUC &lt;- function(auc, n) {\n    t &lt;- sqrt(log(1 / (1 - auc)**2))\n    z &lt;- t - ((2.515517 + 0.802853 * t + 0.0103328 * t**2) / (1 + 1.432788 * t + 0.189269 * t**2 + 0.001308 * t**3))\n    d &lt;- z * sqrt(2)\n\n    x &lt;- c(rnorm(n / 2, mean = 0), rnorm(n / 2, mean = d))\n    y &lt;- c(rep(0, n / 2), rep(1, n / 2))\n\n    data &lt;- data.frame(x = x, y = y)\n\n    return(data)\n}\n\nset.seed(52242)\n\nauc60 &lt;- simulateDataFromAUC(.60, 50000)\nauc70 &lt;- simulateDataFromAUC(.70, 50000)\nauc80 &lt;- simulateDataFromAUC(.80, 50000)\nauc90 &lt;- simulateDataFromAUC(.90, 50000)\nauc95 &lt;- simulateDataFromAUC(.95, 50000)\nauc99 &lt;- simulateDataFromAUC(.99, 50000)\n\nplot(roc(y ~ x, auc60, smooth = TRUE), legacy.axes = TRUE, print.auc = TRUE, print.auc.x = .52, print.auc.y = .61, print.auc.pattern = \"%.2f\")\nplot(roc(y ~ x, auc70, smooth = TRUE), legacy.axes = TRUE, print.auc = TRUE, print.auc.x = .6, print.auc.y = .67, print.auc.pattern = \"%.2f\", add = TRUE)\nplot(roc(y ~ x, auc80, smooth = TRUE), legacy.axes = TRUE, print.auc = TRUE, print.auc.x = .695, print.auc.y = .735, print.auc.pattern = \"%.2f\", add = TRUE)\nplot(roc(y ~ x, auc90, smooth = TRUE), legacy.axes = TRUE, print.auc = TRUE, print.auc.x = .805, print.auc.y = .815, print.auc.pattern = \"%.2f\", add = TRUE)\nplot(roc(y ~ x, auc95, smooth = TRUE), legacy.axes = TRUE, print.auc = TRUE, print.auc.x = .875, print.auc.y = .865, print.auc.pattern = \"%.2f\", add = TRUE)\nplot(roc(y ~ x, auc99, smooth = TRUE), legacy.axes = TRUE, print.auc = TRUE, print.auc.x = .94, print.auc.y = .94, print.auc.pattern = \"%.2f\", add = TRUE)\n\n\n\n\n\n\n\n\nAd esempio, se l’AUC è pari a 0.75, ciò significa che il punteggio complessivo di un individuo che possiede la caratteristica in questione sarà più alto nel 75% dei casi rispetto a quello di un individuo che non la possiede. In termini più semplici, l’AUC fornisce la probabilità che lo strumento classifichi correttamente, se scegliamo casualmente un esito positivo e uno negativo.\nL’AUC è un indice più robusto rispetto alla percentuale di accuratezza perché la percentuale di accuratezza può essere influenzata da fattori come il base rate. L’AUC misura quanto un test è migliore del caso nel discriminare tra esiti diversi. Inoltre, è utile come indicatore generale di accuratezza discriminativa, poiché mostra quanto un test sia accurato su tutti i possibili cutoff.\nAnche se conoscere l’accuratezza del test a ogni cutoff può essere utile per selezionare il cutoff ottimale, nella realtà non siamo interessati a tutti i possibili cutoff, poiché non tutti gli errori hanno lo stesso costo.\nIniziare {#gettingStarted-prediction}\nCaricare le Librerie {#loadLibraries-prediction}\nPreparare i Dati {#prepareData-prediction} Caricamento dei Dati {#loadData-prediction}\nIl dataset aSAH del pacchetto pROC contiene i punteggi dei test (s100b) e gli esiti clinici (outcome) di pazienti.\n\ndata(aSAH)\nmydataSDT &lt;- aSAH\n\nPer garantire la riproducibilità, imposto il seed qui sotto. L’utilizzo dello stesso seed garantirà gli stessi risultati ogni volta. Non c’è nulla di speciale in questo seed specifico.\n\nset.seed(52242)\n\nmydataSDT$testScore &lt;- mydataSDT$s100b\nmydataSDT &lt;- mydataSDT %&gt;%\n    mutate(testScoreSimple = ntile(testScore, 10))\n\nmydataSDT$predictedProbability &lt;-\n    (mydataSDT$s100b - min(mydataSDT$s100b, na.rm = TRUE)) /\n        (max(mydataSDT$s100b, na.rm = TRUE) - min(mydataSDT$s100b, na.rm = TRUE))\nmydataSDT$continuousOutcome &lt;- mydataSDT$testScore +\n    rnorm(nrow(mydataSDT), mean = 0.20, sd = 0.20)\nmydataSDT$disorder &lt;- NA\nmydataSDT$disorder[mydataSDT$outcome == \"Good\"] &lt;- 0\nmydataSDT$disorder[mydataSDT$outcome == \"Poor\"] &lt;- 1\n\nLa curva ROC (Receiver Operating Characteristic) mostra la combinazione tra il tasso di successo (o sensibilità) e il tasso di falsi allarmi (\\(1 - \\text{specificità}\\)) per ogni possibile soglia di cutoff. La curva dimostra come, all’aumentare della soglia (diventando più conservativa), la sensibilità diminuisce e la specificità aumenta, e viceversa.\nLe curve ROC possono essere generate utilizzando il pacchetto pROC, e gli esempi mostrano che la misura ha un’accuratezza moderata—è più accurata del caso, ma c’è margine di miglioramento.\nCurva ROC Empirica {#empiricalROC} Il codice per generare una curva ROC empirica è mostrato qui sotto, e il grafico è visibile in Figura (ref?)(fig\n\nrocCurve &lt;- roc(data = mydataSDT, response = disorder, predictor = testScore, smooth = FALSE)\n\n\nplot(rocCurve, legacy.axes = TRUE, print.auc = TRUE)\n\n\n\n\n\n\n\n\nUna curva ROC empirica con i cutoff sovrapposti è mostrata\n\npred &lt;- prediction(\n    na.omit(mydataSDT[, c(\"testScoreSimple\", \"disorder\")])$testScoreSimple,\n    na.omit(mydataSDT[, c(\"testScoreSimple\", \"disorder\")])$disorder\n)\nperf &lt;- performance(pred, \"tpr\", \"fpr\")\nplot(perf, print.cutoffs.at = 1:11, text.adj = c(1, -1), ylim = c(0, 1.05))\nabline(coef = c(0, 1))\n\n\n\n\n\n\n\n\nCurva ROC Liscia si ottiene nel modo seguente.\n\nrocCurveSmooth &lt;- roc(data = mydataSDT, response = disorder, predictor = testScore, smooth = TRUE)\nplot(rocCurveSmooth, legacy.axes = TRUE, print.auc = TRUE)\n\n\n\n\n\n\n\n\nStatistica di Youden J {#youdenJ} La soglia della statistica di Youden J è il punto in cui il test ha la massima combinazione (somma) di sensibilità e specificità: \\(\\text{max}(\\text{sensitivity} + \\text{specificity} - 1)\\).\n\nyoudenJ &lt;- coords(rocCurve, x = \"best\", best.method = \"youden\")\nyoudenJthreshold &lt;- youdenJ$threshold\nyoudenJspecificity &lt;- youdenJ$specificity\nyoudenJsensitivity &lt;- youdenJ$sensitivity\n\nyoudenJ\n\n\nA data.frame: 1 x 3\n\n\nthreshold\nspecificity\nsensitivity\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0.205\n0.8055556\n0.6341463\n\n\n\n\n\nPer questo test, la soglia ottimale secondo la statistica di Youden J è \\(r youdenJthreshold\\), con una sensibilità di \\(r youdenJsensitivity\\) e una specificità di \\(r youdenJspecificity\\).\nPunto più vicino alla parte in alto a sinistra della curva ROC {#topLeftROC} Il punto più vicino alla parte superiore sinistra della curva ROC, dove sensibilità e specificità sono perfette, è dato da: \\(\\text{min}[(1 - \\text{sensitivity})^2 + (1 - \\text{specificity})^2]\\).\n\nclosestTopLeft &lt;- coords(rocCurve, x = \"best\", best.method = \"closest.topleft\")\nclosestTopLeftthreshold &lt;- closestTopLeft$threshold\nclosestTopLeftspecificity &lt;- closestTopLeft$specificity\nclosestTopLeftsensitivity &lt;- closestTopLeft$sensitivity\n\nclosestTopLeft\n\n\nA data.frame: 1 x 3\n\n\nthreshold\nspecificity\nsensitivity\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0.205\n0.8055556\n0.6341463\n\n\n\n\n\nPer questo test, la combinazione di sensibilità e specificità è ottimale alla soglia di \\(r closestTopLeftthreshold\\), con una sensibilità di \\(r closestTopLeftsensitivity\\) e una specificità di \\(r closestTopLeftspecificity\\).",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#predictionAccuracy",
    "href": "chapters/prediction/01_prediction.html#predictionAccuracy",
    "title": "80  Predizione",
    "section": "80.7 Accuratezza della Predizione attraverso i Cutoff",
    "text": "80.7 Accuratezza della Predizione attraverso i Cutoff\nEsistono due dimensioni principali dell’accuratezza: (1) la discriminazione (ad esempio, sensibilità, specificità, area sotto la curva ROC) e (2) la calibrazione. Alcuni indici generali di accuratezza combinano la discriminazione e la calibrazione.\nIl pacchetto petersenlab include la funzione accuracyOverall(), che stima l’accuratezza della predizione su tutti i cutoff.\nEcco un esempio di codice che utilizza questa funzione:\n\naccuracyOverall(\n    predicted = mydataSDT$testScore,\n    actual = mydataSDT$disorder\n) %&gt;%\n    t() %&gt;%\n    round(., 2)\n\naccuracyOverall(\n    predicted = mydataSDT$testScore,\n    actual = mydataSDT$disorder,\n    dropUndefined = TRUE\n) %&gt;%\n    t() %&gt;%\n    round(., 2)\n\n\nA matrix: 12 x 1 of type dbl\n\n\nME\n-0.12\n\n\nMAE\n0.34\n\n\nMSE\n0.21\n\n\nRMSE\n0.46\n\n\nMPE\n-Inf\n\n\nMAPE\nInf\n\n\nsMAPE\n82.72\n\n\nMASE\n0.74\n\n\nRMSLE\n0.30\n\n\nrsquared\n0.17\n\n\nrsquaredAdj\n0.17\n\n\nrsquaredPredictive\n0.12\n\n\n\n\n\n\nA matrix: 12 x 1 of type dbl\n\n\nME\n-0.12\n\n\nMAE\n0.34\n\n\nMSE\n0.21\n\n\nRMSE\n0.46\n\n\nMPE\n60.29\n\n\nMAPE\n65.51\n\n\nsMAPE\n82.72\n\n\nMASE\n0.74\n\n\nRMSLE\n0.30\n\n\nrsquared\n0.17\n\n\nrsquaredAdj\n0.17\n\n\nrsquaredPredictive\n0.12\n\n\n\n\n\nIn questo esempio, la funzione accuracyOverall() calcola l’accuratezza complessiva della predizione su tutta la gamma di cutoff disponibili, fornendo una sintesi del grado di accuratezza globale del modello.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#calibration",
    "href": "chapters/prediction/01_prediction.html#calibration",
    "title": "80  Predizione",
    "section": "80.8 Calibrazione",
    "text": "80.8 Calibrazione\nQuando si tratta di un risultato categorico, la calibrazione è il grado in cui una stima probabilistica di un evento riflette la reale probabilità sottostante di quell’evento.\nQuando si tratta di un risultato continuo, la calibrazione indica quanto i valori previsti siano vicini ai valori effettivi osservati.\nL’importanza di esaminare la calibrazione, oltre alla discriminazione, è descritta da Lindhiem (2020).\nLa calibrazione è diventata centrale nella valutazione dell’accuratezza delle previsioni meteorologiche.\nAd esempio, nei giorni in cui un meteorologo prevede il 60% di possibilità di pioggia, dovrebbe effettivamente piovere circa il 60% delle volte.\nGrazie ai progressi nella comprensione scientifica dei sistemi meteorologici, le previsioni della pioggia sono diventate più accurate.\nLe previsioni della National Weather Service, per esempio, sono ben calibrate.\nTuttavia, le previsioni di pioggia fatte da meteorologi televisivi locali possono essere esagerate per aumentare l’audience (Silver, 2012).\nCuriosamente, alcune previsioni di pioggia di The Weather Channel risultano miscalibrate in certe condizioni (Bickel, 2008).\nAd esempio, nei giorni in cui viene prevista una probabilità di pioggia del 20%, la probabilità reale è in realtà intorno al 5%.\nQuesta miscalibrazione è deliberata, poiché le persone tendono a essere più arrabbiate se viene detto loro che non pioverà e invece piove (falsi negativi) rispetto al contrario (falsi positivi).\nCome osserva Silver (2012), “Se piove quando non dovrebbe, le persone si arrabbiano con il meteorologo, mentre una giornata inaspettatamente soleggiata è vista come un bonus fortuito.”\nLa calibrazione non è importante solo per le previsioni meteorologiche, ma anche per la valutazione psicologica.\nEsistono diversi metodi per esaminare la calibrazione, come i Brier Scores, il test di Hosmer-Lemeshow, lo z di Spiegelhalter e la differenza media tra i valori previsti e osservati a diversi intervalli di soglie, rappresentata graficamente tramite un calibration plot.\n\n80.8.1 Calibration Plot\nUn calibration plot può aiutare a individuare la miscalibrazione.\nQuesto grafico rappresenta la probabilità prevista di un evento sull’asse x e la probabilità effettiva osservata sull’asse y.\nLe previsioni sono suddivise in gruppi (spesso 10).\nLa linea diagonale rappresenta previsioni perfettamente calibrate.\nQuando le previsioni si discostano da questa linea, indicano miscalibrazione.\nEsistono quattro pattern generali di miscalibrazione: overextremity, underextremity, overprediction, e underprediction.\n\nOverextremity si verifica quando le probabilità previste sono troppo vicine agli estremi (zero o uno).\n\nUnderextremity accade quando le probabilità previste sono troppo lontane dagli estremi.\n\nOverprediction si ha quando le probabilità previste sono costantemente maggiori di quelle osservate.\n\nUnderprediction si ha quando le probabilità previste sono costantemente minori di quelle osservate.\n\nPer generare un calibration plot, possiamo utilizzare il pacchetto PredictABEL. Di seguito è riportato un esempio di codice in R per creare questo grafico:\n\ndata(aSAH)\nmydataSDT &lt;- aSAH\n\nset.seed(52242)\n\nmydataSDT$testScore &lt;- mydataSDT$s100b\nmydataSDT &lt;- mydataSDT %&gt;%\n    mutate(testScoreSimple = ntile(testScore, 10))\n\nmydataSDT$predictedProbability &lt;-\n    (mydataSDT$s100b - min(mydataSDT$s100b, na.rm = TRUE)) /\n        (max(mydataSDT$s100b, na.rm = TRUE) - min(mydataSDT$s100b, na.rm = TRUE))\nmydataSDT$continuousOutcome &lt;- mydataSDT$testScore +\n    rnorm(nrow(mydataSDT), mean = 0.20, sd = 0.20)\nmydataSDT$disorder &lt;- NA\nmydataSDT$disorder[mydataSDT$outcome == \"Good\"] &lt;- 0\nmydataSDT$disorder[mydataSDT$outcome == \"Poor\"] &lt;- 1\n\ncolNumberOutcome &lt;- which(names(mydataSDT) == \"disorder\")\nmyDataNoMissing &lt;- na.omit(mydataSDT)\n\n\nplotCalibration(\n    data = na.omit(myDataNoMissing),\n    cOutcome = colNumberOutcome,\n    predRisk = myDataNoMissing$predictedProbability,\n    groups = 10\n)\n\n\n    $Table_HLtest\n        \n\nA matrix: 10 x 5 of type dbl\n\n\n\ntotal\nmeanpred\nmeanobs\npredicted\nobserved\n\n\n\n\n[0.0000,0.0245)\n20\n0.013\n0.200\n0.26\n4\n\n\n0.0245\n7\n0.025\n0.143\n0.17\n1\n\n\n0.0294\n8\n0.029\n0.250\n0.24\n2\n\n\n[0.0343,0.0441)\n14\n0.036\n0.214\n0.50\n3\n\n\n[0.0441,0.0588)\n11\n0.051\n0.364\n0.56\n4\n\n\n[0.0588,0.0686)\n8\n0.061\n0.125\n0.49\n1\n\n\n[0.0686,0.1324)\n12\n0.094\n0.417\n1.12\n5\n\n\n[0.1324,0.2059)\n12\n0.165\n0.583\n1.98\n7\n\n\n[0.2059,0.2598)\n10\n0.222\n0.300\n2.22\n3\n\n\n[0.2598,1.0000]\n11\n0.408\n1.000\n4.49\n11\n\n\n\n\n\n    $Chi_square\n        154.23\n    $df\n        8\n    $p_value\n        0\n\n\n\n\n\n\n\n\n\n\nQuesto calibration plot mostra che le probabilità previste erano consistentemente inferiori a quelle effettive osservate, suggerendo un problema di underprediction. Per correggere questa miscalibrazione, le probabilità previste dovrebbero essere aumentate affinché risultino coerenti con quelle osservate.\n\n80.8.1.1 Brier Scores\n\n\n80.8.1.2 Brier Scores\nI punteggi di Brier (Brier scores) offrono una misura di accuratezza per le previsioni probabilistiche, valutando quanto le probabilità previste si avvicinino ai risultati reali. Questo punteggio è calcolato come la media dei quadrati delle differenze tra le probabilità previste e i risultati osservati, permettendo di valutare la qualità delle previsioni considerando sia errori di sovrastima che di sottostima.\nUn punteggio di Brier basso indica una migliore calibrazione delle previsioni, con un valore di 0 che rappresenta una previsione perfettamente calibrata (cioè, le probabilità previste coincidono esattamente con i risultati osservati). Valori più vicini a 1 indicano invece una calibrazione peggiore, suggerendo che le previsioni si discostano ampiamente dai risultati effettivi.\nIl pacchetto rms di R può essere utilizzato per calcolare i punteggi di Brier, come mostrato nell’esempio seguente:\n\nval.prob(\n    mydataSDT$predictedProbability,\n    mydataSDT$disorder,\n    pl = FALSE\n)[\"Brier\"]\n\nBrier: 0.261212926954575\n\n\nIn sintesi, il punteggio di Brier rappresenta una misura sintetica della precisione e della calibrazione di un modello predittivo: quanto più è basso, tanto maggiore è la capacità del modello di riflettere accuratamente le probabilità reali.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#matrice-di-confusione",
    "href": "chapters/prediction/01_prediction.html#matrice-di-confusione",
    "title": "80  Predizione",
    "section": "80.9 Matrice di Confusione",
    "text": "80.9 Matrice di Confusione\nUna matrice di confusione (detta anche tabella 2x2 di accuratezza, tabella di cross-tabulation o tabella di contingenza) è una matrice utilizzata per i dati categoriali che mostra l’esito predetto su una dimensione e l’esito reale (la verità) sull’altra. Se le predizioni e gli esiti sono dicotomici, la matrice di confusione è una matrice 2x2 con due righe e due colonne che rappresentano le quattro possibili combinazioni previste-reali (i cosiddetti outcome decisionali). In questo caso, la matrice di confusione fornisce un conteggio tabellare dei casi corretti (true positives e true negatives) rispetto agli errori (false positives e false negatives).\nConteggio Assoluto {#confusionMatrix-number}\nÈ possibile visualizzare i dati di una matrice di confusione utilizzando il seguente codice:\n\ncutoff &lt;- 0.205\n\nmydataSDT$diagnosis &lt;- NA\nmydataSDT$diagnosis[mydataSDT$testScore &lt; cutoff] &lt;- 0\nmydataSDT$diagnosis[mydataSDT$testScore &gt;= cutoff] &lt;- 1\n\nmydataSDT$diagnosisFactor &lt;- factor(\n    mydataSDT$diagnosis,\n    levels = c(1, 0),\n    labels = c(\"Decision: Diagnosis\", \"Decision: No Diagnosis\")\n)\n\nmydataSDT$disorderFactor &lt;- factor(\n    mydataSDT$disorder,\n    levels = c(1, 0),\n    labels = c(\"Truth: Disorder\", \"Truth: No Disorder\")\n)\n\ntable(mydataSDT$diagnosisFactor, mydataSDT$disorderFactor)\n\n                        \n                         Truth: Disorder Truth: No Disorder\n  Decision: Diagnosis                 26                 14\n  Decision: No Diagnosis              15                 58\n\n\nConteggio con Margini Aggiunti {#confusionMatrix-numberMargins}\nPer aggiungere i margini ai conteggi assoluti:\n\naddmargins(table(mydataSDT$diagnosisFactor, mydataSDT$disorderFactor))\n\n\nA table: 3 x 3 of type dbl\n\n\n\nTruth: Disorder\nTruth: No Disorder\nSum\n\n\n\n\nDecision: Diagnosis\n26\n14\n40\n\n\nDecision: No Diagnosis\n15\n58\n73\n\n\nSum\n41\n72\n113\n\n\n\n\n\nPer calcolare le proporzioni:\n\nprop.table(table(mydataSDT$diagnosisFactor, mydataSDT$disorderFactor))\n\n                        \n                         Truth: Disorder Truth: No Disorder\n  Decision: Diagnosis          0.2300885          0.1238938\n  Decision: No Diagnosis       0.1327434          0.5132743\n\n\nAggiungi i margini alle proporzioni:\n\naddmargins(prop.table(table(mydataSDT$diagnosisFactor, mydataSDT$disorderFactor)))\n\n\nA table: 3 x 3 of type dbl\n\n\n\nTruth: Disorder\nTruth: No Disorder\nSum\n\n\n\n\nDecision: Diagnosis\n0.2300885\n0.1238938\n0.3539823\n\n\nDecision: No Diagnosis\n0.1327434\n0.5132743\n0.6460177\n\n\nSum\n0.3628319\n0.6371681\n1.0000000\n\n\n\n\n\n\n80.9.1 True Positives (TP)\nI true positives (TP) sono i casi in cui una classificazione positiva (ad esempio, la presenza di un disturbo) è corretta, ovvero il test indica che la classificazione è presente, e lo è davvero. Più alto è il numero di true positives rispetto alla dimensione del campione, maggiore è l’accuratezza. La formula per calcolare i true positives è:\n\\[\nTP = BR \\times SR \\times N\n\\]\nEcco come calcolare i true positives in R:\n\nTPvalue &lt;- length(which(\n    mydataSDT$diagnosis == 1 & mydataSDT$disorder == 1\n))\n\nTPvalue\n\n26\n\n\n\n\n80.9.2 True Negatives (TN)\nI true negatives (TN) sono i casi in cui una classificazione negativa (assenza di disturbo) è corretta, ovvero il test indica che la classificazione non è presente e questa effettivamente non è presente. La formula per calcolare i true negatives è:\n\\[\nTN = (1 - BR) \\times (1 - SR) \\times N\n\\]\nPer calcolare i true negatives in R:\n\nTNvalue &lt;- length(which(\n    mydataSDT$diagnosis == 0 & mydataSDT$disorder == 0\n))\n\nTNvalue\n\n58\n\n\n\n\n80.9.3 False Positives (FP)\nI false positives (FP) sono i casi in cui una classificazione positiva è errata, ovvero il test indica la presenza di un disturbo che in realtà non è presente. Valori più bassi di false positives riflettono una maggiore accuratezza. La formula per calcolare i false positives è:\n\\[\nFP = (1 - BR) \\times SR \\times N\n\\]\nEcco come calcolare i false positives in R:\n\nFPvalue &lt;- length(which(\n    mydataSDT$diagnosis == 1 & mydataSDT$disorder == 0\n))\n\nFPvalue\n\n14\n\n\n\n\n80.9.4 False Negatives (FN)\nI false negatives (FN) sono i casi in cui una classificazione negativa è errata, ovvero il test indica l’assenza di un disturbo che in realtà è presente. Valori più bassi di false negatives riflettono una maggiore accuratezza. La formula per calcolare i false negatives è:\n\\[\nFN = BR \\times (1 - SR) \\times N\n\\]\nPer calcolare i false negatives in R:\n\nFNvalue &lt;- length(which(\n    mydataSDT$diagnosis == 0 & mydataSDT$disorder == 1\n))\n\nFNvalue\n\n15\n\n\n\n\n80.9.5 Dimensione del Campione (N)\nLa dimensione del campione può essere calcolata sommando il numero di TP, TN, FP e FN:\n\nsampleSize &lt;- function(TP, TN, FP, FN) {\n    value &lt;- TP + TN + FP + FN\n\n    return(value)\n}\n\nsampleSize(\n    TP = TPvalue,\n    TN = TNvalue,\n    FP = FPvalue,\n    FN = FNvalue\n)\n\n113\n\n\n\n\n80.9.6 Selection Ratio (SR)\nThe selection ratio (SR) is the marginal probability of selection, independent of other things: \\(P(R_i)\\). In clinical psychology, the selection ratio is the proportion of people who test positive for the disorder, as in Equation @ref(eq:selectionRatio):\n\\[\n\\begin{aligned}\n  \\text{SR} &= P(R_i) \\\\\n  &= \\frac{\\text{TP} + \\text{FP}}{N}\n\\end{aligned}\n(\\#eq:selectionRatio)\n\\]\n\nselectionRatio &lt;- function(TP, TN, FP, FN){\n  N &lt;- TP + TN + FP + FN\n  value &lt;- (TP + FP)/N\n  \n  return(value)\n}\n\nselectionRatio(\n    TP = TPvalue,\n    TN = TNvalue,\n    FP = FPvalue,\n    FN = FNvalue\n)\n\nselectionRatioValue &lt;- selectionRatio(TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue)\n\n0.353982300884956\n\n\n\n\n80.9.7 Base Rate (BR)\nThe base rate (BR) of a classification is its marginal probability, independent of other things: \\(P(C_i)\\). In clinical psychology, the base rate of a disorder is its prevalence in the population, as in Equation @ref(eq:baseRate). Without additional information, the base rate is used as the initial pretest probability.\n\\[\n\\begin{aligned}\n  \\text{BR} &= P(C_i) \\\\\n  &= \\frac{\\text{TP} + \\text{FN}}{N}\n\\end{aligned}\n(\\#eq:baseRate)\n\\]\n\nbaseRate &lt;- function(TP, TN, FP, FN){\n  N &lt;- TP + TN + FP + FN\n  value &lt;- (TP + FN)/N\n  \n  return(value)\n}\n\nbaseRate(\n  TP = TPvalue,\n  TN = TNvalue,\n  FP = FPvalue,\n  FN = FNvalue)\n\nbaseRateValue &lt;- baseRate(TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue)\n\n0.36283185840708\n\n\n\n\n80.9.8 Pretest Odds\nThe pretest odds of a classification can be estimated using the pretest probability (i.e., base rate). To convert a probability to odds, divide the probability by one minus that probability, as in Equation @ref(eq:pretestOdds).\n\\[\n\\begin{aligned}\n  \\text{pretest odds} &= \\frac{\\text{pretest probability}}{1 - \\text{pretest probability}} \\\\\n\\end{aligned}\n(\\#eq:pretestOdds)\n\\]\n\npretestOdds &lt;- function(TP, TN, FP, FN, pretestProb = NULL){\n  if(!is.null(pretestProb)){\n    pretestProbability &lt;- pretestProb\n  } else {\n    N &lt;- TP + TN + FP + FN\n    pretestProbability &lt;- (TP + FN)/N\n  }\n  \n  value &lt;- pretestProbability / (1 - pretestProbability)\n  \n  return(value)\n}\n\npretestOdds(\n  TP = TPvalue,\n  TN = TNvalue,\n  FP = FPvalue,\n  FN = FNvalue)\n\npretestOdds(pretestProb = baseRate(\n  TP = TPvalue,\n  TN = TNvalue,\n  FP = FPvalue,\n  FN = FNvalue))\n\npretestOddsValue &lt;- pretestOdds(TP = TPvalue,\n                                TN = TNvalue,\n                                FP = FPvalue,\n                                FN = FNvalue)\n\n0.569444444444444\n\n\n0.569444444444444\n\n\n\n\n80.9.9 Percent Accuracy\nPercent Accuracy is also called overall accuracy. Higher values reflect greater accuracy. The formula for percent accuracy is in Equation @ref(eq:percentAccuracy). Percent accuracy has several problems. First, it treats all errors (FP and FN) as equally important. However, in practice, it is rarely the case that false positives and false negatives are equally important. Second, percent accuracy can be misleading because it is highly influenced by base rates. You can have a high percent accuracy by predicting from the base rate and saying that no one has the characteristic (if the base rate is low) or that everyone has the characteristic (if the base rate is high). Thus, it is also important to consider other aspects of accuracy.\n\\[\\begin{equation}\n\\text{Percent Accuracy} = 100\\% \\times \\frac{\\text{TP} + \\text{TN}}{N}\n(\\#eq:percentAccuracy)\n\\end{equation}\\]\n\npercentAccuracy &lt;- function(TP, TN, FP, FN){\n  N &lt;- TP + TN + FP + FN\n  value &lt;- 100 * ((TP + TN)/N)\n  \n  return(value)\n}\n\npercentAccuracy(\n  TP = TPvalue,\n  TN = TNvalue,\n  FP = FPvalue,\n  FN = FNvalue)\n\npercentAccuracyValue &lt;- percentAccuracy(TP = TPvalue,\n                                        TN = TNvalue,\n                                        FP = FPvalue,\n                                        FN = FNvalue)\n\n74.3362831858407\n\n\n\n\n80.9.10 Percent Accuracy by Chance\nThe formula for calculating percent accuracy by chance is in Equation @ref(eq:PercentAccuracyByChance).\n\\[\n\\begin{aligned}\n  \\text{Percent Accuracy by Chance} &= 100\\% \\times [P(\\text{TP}) + P(\\text{TN})] \\\\\n  &= 100\\% \\times \\{(\\text{BR} \\times {\\text{SR}}) + [(1 - \\text{BR}) \\times (1 - \\text{SR})]\\}\n\\end{aligned}\n(\\#eq:PercentAccuracyByChance)\n\\]\n```{r, class.source = “fold-hide”} percentAccuracyByChance &lt;- function(TP, TN, FP, FN){ N &lt;- TP + TN + FP + FN BR &lt;- (TP + FN)/N SR &lt;- (TP + FP)/N value &lt;- 100 * ((BR * SR) + ((1 - BR) * (1 - SR)))\nreturn(value) }\n\n```{r}\npercentAccuracyByChance(\n  TP = TPvalue,\n  TN = TNvalue,\n  FP = FPvalue,\n  FN = FNvalue)\n{r, include = FALSE} percentAccuracyByChanceValue &lt;- percentAccuracyByChance(TP = TPvalue,                                                         TN = TNvalue,                                                         FP = FPvalue,                                                         FN = FNvalue)\n\n\n80.9.11 Percent Accuracy Predicting from the Base Rate\nPredicting from the base rate is going with the most likely outcome in every prediction. It is also called “betting from the base rate”. If the base rate is less than .50, it would involve predicting that the condition is absent for every case. If the base rate is .50 or above, it would involve predicting that the condition is present for every case. Predicting from the base rate is a special case of percent accuracy by chance when the selection ratio is set to either one (if the base rate \\(\\geq\\) .5) or zero (if the base rate &lt; .5).\n```{r, class.source = “fold-hide”} percentAccuracyPredictingFromBaseRate &lt;- function(TP, TN, FP, FN){ N &lt;- TP + TN + FP + FN BR &lt;- (TP + FN)/N\nifelse(BR &gt;= .5, SR &lt;- 1, NA) ifelse(BR &lt; .5, SR &lt;- 0, NA)\nvalue &lt;- 100 * ((BR * SR) + ((1 - BR) * (1 - SR)))\nreturn(value) }\n\n```{r}\npercentAccuracyPredictingFromBaseRate(\n  TP = TPvalue,\n  TN = TNvalue,\n  FP = FPvalue,\n  FN = FNvalue)\n{r, include = FALSE} percentAccuracyPredictingFromBaseRateValue &lt;- percentAccuracyPredictingFromBaseRate(TP = TPvalue,                                                                                     TN = TNvalue,                                                                                     FP = FPvalue,                                                                                     FN = FNvalue)\n\n\n80.9.12 Relative Improvement Over Chance (RIOC)\nRelative improvement over chance (RIOC) is a prediction’s improvement over chance as a proportion of the maximum possible improvement over chance, as described by (Farrington1989?). Higher values reflect greater accuracy. The formula for calculating RIOC is in Equation @ref(eq:relativeImprovementOverChance).\n\\[\n\\begin{aligned}\n  \\text{relative improvement over chance (RIOC)} &= \\frac{\\text{total correct} - \\text{chance correct}}{\\text{maximum correct} - \\text{chance correct}} \\\\\n\\end{aligned}\n(\\#eq:relativeImprovementOverChance)\n\\]\n```{r, class.source = “fold-hide”} relativeImprovementOverChance &lt;- function(TP, TN, FP, FN){ N &lt;- TP + TN + FP + FN actualYes &lt;- TP + FN predictedYes &lt;- TP + FP value &lt;- ((N * (TP + TN)) - (actualYes * predictedYes + (N - predictedYes) * (N - actualYes))) / ((N * (actualYes + N - predictedYes)) - (actualYes * predictedYes + (N - predictedYes) * (N - actualYes)))\nreturn(value) }\n\n```{r}\nrelativeImprovementOverChance(\n  TP = TPvalue,\n  TN = TNvalue,\n  FP = FPvalue,\n  FN = FNvalue)\n{r, include = FALSE} relativeImprovementOverChanceValue &lt;- relativeImprovementOverChance(TP = TPvalue,                                                                     TN = TNvalue,                                                                     FP = FPvalue,                                                                     FN = FNvalue)\n\n\n80.9.13 Relative Improvement Over Predicting from the Base Rate\nRelative improvement over predicting from the base rate is a prediction’s improvement over predicting from the base rate as a proportion of the maximum possible improvement over predicting from the base rate. Higher values reflect greater accuracy. The formula for calculating relative improvement over predicting from the base rate is in Equation @ref(eq:relativeImprovementOverPredictingFromBaseRate).\n\\[\n\\scriptsize\n\\begin{aligned}\n  \\text{relative improvement over predicting from base rate} &= \\frac{\\text{total correct} - \\text{correct by predicting from base rate}}{\\text{maximum correct} - \\text{correct by predicting from base rate}} \\\\\n\\end{aligned}\n(\\#eq:relativeImprovementOverPredictingFromBaseRate)\n\\]\n```{r, class.source = “fold-hide”} relativeImprovementOverPredictingFromBaseRate &lt;- function(TP, TN, FP, FN){ N &lt;- TP + TN + FP + FN BR &lt;- (TP + FN)/N\nifelse(BR &gt;= .5, SR &lt;- 1, NA) ifelse(BR &lt; .5, SR &lt;- 0, NA)\nactualYes &lt;- TP + FN predictedYes &lt;- SR * N value &lt;- ((N * (TP + TN)) - (actualYes * predictedYes + (N - predictedYes) * (N - actualYes))) / ((N * (actualYes + N - predictedYes)) - (actualYes * predictedYes + (N - predictedYes) * (N - actualYes)))\nreturn(value) }\n\n```{r}\nrelativeImprovementOverPredictingFromBaseRate(\n  TP = TPvalue,\n  TN = TNvalue,\n  FP = FPvalue,\n  FN = FNvalue)\n{r, include = FALSE} relativeImprovementOverPredictingFromBaseRateValue &lt;- relativeImprovementOverPredictingFromBaseRate(   TP = TPvalue,   TN = TNvalue,   FP = FPvalue,   FN = FNvalue)\n\n\n80.9.14 Sensitivity (SN)\nSensitivity (SN) is also called true positive rate (TPR), hit rate (HR), or recall. Sensitivity is the conditional probability of a positive test given that the person has the condition: \\(P(R \\mid C)\\). Higher values reflect greater accuracy. The formula for calculating sensitivity is in Equation @ref(eq:sensitivity). As described in Section @ref(accuracyCutoff) and as depicted in Figure @ref(fig:sensitivitySpecificity), as the cutoff increases (becomes more conservative), sensitivity decreases. As the cutoff decreases, sensitivity increases.\n\\[\n\\begin{aligned}\n  \\text{sensitivity (SN)} &= P(R \\mid C) \\\\\n  &= \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{\\text{TP}}{N \\times \\text{BR}} = 1 - \\text{FNR}\n\\end{aligned}\n(\\#eq:sensitivity)\n\\]\n```{r, class.source = “fold-hide”} sensitivity &lt;- function(TP, TN, FP, FN){ value &lt;- TP/(TP + FN)\nreturn(value) }\n\n```{r}\nsensitivity(\n  TP = TPvalue,\n  FN = FNvalue)\n{r, include = FALSE} sensitivityValue &lt;- sensitivity(TP = TPvalue, FN = FNvalue)\nBelow I compute sensitivity and specificity at every possible cutoff.\n```{r, class.source = “fold-hide”} possibleCutoffs &lt;- unique(na.omit(mydataSDT$testScore)) possibleCutoffs &lt;- possibleCutoffs[order(possibleCutoffs)] possibleCutoffs &lt;- c( possibleCutoffs, max(possibleCutoffs, na.rm = TRUE) + 0.01)\nspecificity &lt;- function(TP, TN, FP, FN){ value &lt;- TN/(TN + FP)\nreturn(value) }\naccuracyVariables &lt;- c(“cutoff”, “TP”, “TN”, “FP”, “FN”)\naccuracyStats &lt;- data.frame(matrix( nrow = length(possibleCutoffs), ncol = length(accuracyVariables)))\nnames(accuracyStats) &lt;- accuracyVariables\nfor(i in 1:length(possibleCutoffs)){ newCutoff &lt;- possibleCutoffs[i]\nmydataSDT\\(diagnosis &lt;- NA\n  mydataSDT\\)diagnosis[mydataSDT\\(testScore &lt; newCutoff] &lt;- 0\n  mydataSDT\\)diagnosis[mydataSDT$testScore &gt;= newCutoff] &lt;- 1\naccuracyStats[i, “cutoff”] &lt;- newCutoff accuracyStats[i, “TP”] &lt;- length(which( mydataSDT\\(diagnosis == 1 & mydataSDT\\)disorder == 1)) accuracyStats[i, “TN”] &lt;- length(which( mydataSDT\\(diagnosis == 0 & mydataSDT\\)disorder == 0)) accuracyStats[i, “FP”] &lt;- length(which( mydataSDT\\(diagnosis == 1 & mydataSDT\\)disorder == 0)) accuracyStats[i, “FN”] &lt;- length(which( mydataSDT\\(diagnosis == 0 & mydataSDT\\)disorder == 1)) }\naccuracyStats\\(sensitivity &lt;- accuracyStats\\)TPrate &lt;- sensitivity( TP = accuracyStats\\(TP,\n  TN = accuracyStats\\)TN, FP = accuracyStats\\(FP,\n  FN = accuracyStats\\)FN)\naccuracyStats\\(specificity &lt;- accuracyStats\\)TNrate &lt;- specificity( TP = accuracyStats\\(TP,\n  TN = accuracyStats\\)TN, FP = accuracyStats\\(FP,\n  FN = accuracyStats\\)FN)\nsensitivitySpecificityData &lt;- pivot_longer( accuracyStats, cols = all_of(c(“sensitivity”,“specificity”)))\n\n```{r sensitivitySpecificity, echo = FALSE, results = \"hide\", out.width = \"100%\", fig.align = \"center\", fig.cap = \"Sensitivity and Specificity as a Function of the Cutoff.\"}\nggplot(\n  sensitivitySpecificityData,\n  aes(\n    x = cutoff,\n    y = value,\n    color = name)) +\n  geom_line(linewidth = 2) +\n  scale_x_continuous(name = \"cutoff (liberal to conservative)\") +\n  scale_color_viridis_d(name = \"\") +\n  theme_bw()\n\n\n80.9.15 Specificity (SP)\nSpecificity (SP) is also called true negative rate (TNR) or selectivity. Specificity is the conditional probability of a negative test given that the person does not have the condition: \\(P(\\text{not } R \\mid \\text{not } C)\\). Higher values reflect greater accuracy. The formula for calculating specificity is in Equation @ref(eq:specificity). As described in Section @ref(accuracyCutoff) and as depicted in Figure @ref(fig:sensitivitySpecificity), as the cutoff increases (becomes more conservative), specificity increases. As the cutoff decreases, specificity decreases.\n\\[\n\\begin{aligned}\n  \\text{specificity (SP)} &= P(\\text{not } R \\mid \\text{not } C) \\\\\n  &= \\frac{\\text{TN}}{\\text{TN} + \\text{FP}} = \\frac{\\text{TN}}{N (1 - \\text{BR})} = 1 - \\text{FPR}\n\\end{aligned}\n(\\#eq:specificity)\n\\]\n```{r, class.source = “fold-hide”} specificity &lt;- function(TP, TN, FP, FN){ value &lt;- TN/(TN + FP)\nreturn(value) }\n\n```{r}\nspecificity(TN = TNvalue, FP = FPvalue)\n{r, include = FALSE} specificityValue &lt;- specificity(   TN = TNvalue,   FP = FPvalue)\n\n\n80.9.16 False Negative Rate (FNR)\nThe false negative rate (FNR) is also called the miss rate. The false negative rate is the conditional probability of a negative test given that the person has the condition: \\(P(\\text{not } R \\mid C)\\). Lower values reflect greater accuracy. The formula for calculating false negative rate is in Equation @ref(eq:falseNegativeRate).\n\\[\n\\begin{aligned}\n  \\text{false negative rate (FNR)} &= P(\\text{not } R \\mid C) \\\\\n  &= \\frac{\\text{FN}}{\\text{FN} + \\text{TP}} = \\frac{\\text{FN}}{N \\times \\text{BR}} = 1 - \\text{TPR}\n\\end{aligned}\n(\\#eq:falseNegativeRate)\n\\]\n```{r, class.source = “fold-hide”} falseNegativeRate &lt;- function(TP, TN, FP, FN){ value &lt;- FN/(FN + TP)\nreturn(value) }\n\n```{r}\nfalseNegativeRate(\n  TP = TPvalue,\n  FN = FNvalue)\n{r, include = FALSE} falseNegativeRate(TP = TPvalue, FN = FNvalue)\n\n\n80.9.17 False Positive Rate (FPR)\nThe false positive rate (FPR) is also called the false alarm rate (FAR) or fall-out. The false positive rate is the conditional probability of a positive test given that the person does not have the condition: \\(P(R \\mid \\text{not } C)\\). Lower values reflect greater accuracy. The formula for calculating false positive rate is in Equation @ref(eq:falsePositiveRate).\n\\[\n\\begin{aligned}\n  \\text{false positive rate (FPR)} &= P(R \\mid \\text{not } C) \\\\\n  &= \\frac{\\text{FP}}{\\text{FP} + \\text{TN}} = \\frac{\\text{FP}}{N (1 - \\text{BR})} = 1 - \\text{TNR}\n\\end{aligned}\n(\\#eq:falsePositiveRate)\n\\]\n```{r, class.source = “fold-hide”} falsePositiveRate &lt;- function(TP, TN, FP, FN){ value &lt;- FP/(FP + TN)\nreturn(value) }\n\n```{r}\nfalsePositiveRate(\n  TN = TNvalue,\n  FP = FPvalue)\n{r, include = FALSE} falsePositiveRateValue &lt;- falsePositiveRate(TN = TNvalue, FP = FPvalue)\n\n\n80.9.18 Positive Predictive Value (PPV)\nThe positive predictive value (PPV) is also called the positive predictive power (PPP) or precision. Many people confuse sensitivity (\\(P(R \\mid C)\\)) with its inverse conditional probability, PPV (\\(P(C \\mid R)\\)). PPV is the conditional probability of having the condition given a positive test: \\(P(C \\mid R)\\). Higher values reflect greater accuracy. The formula for calculating positive predictive value is in Equation @ref(eq:positivePredictiveValue).\nPPV can be low even when sensitivity is high because it depends not only on sensitivity, but also on specificity and the base rate. Because PPV depends on the base rate, PPV is not an intrinsic property of a measure. The same measure will have a different PPV in different contexts with different base rates (Treat2023?). As described in Section @ref(accuracyCutoff) and as depicted in Figure @ref(fig:ppvNPVbaseRate), as the base rate increases, PPV increases. As the base rate decreases, PPV decreases. PPV also differs as a function of the cutoff. As described in Section @ref(accuracyCutoff) and as depicted in Figure @ref(fig:ppvNPVcutoff), as the cutoff increases (becomes more conservative), PPV increases. As the cutoff decreases (becomes more liberal), PPV decreases.\n\\[\n\\small\n\\begin{aligned}\n  \\text{positive predictive value (PPV)} &= P(C \\mid R) \\\\\n  &= \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{\\text{TP}}{N \\times \\text{SR}}\\\\\n  &= \\frac{\\text{sensitivity} \\times {\\text{BR}}}{\\text{sensitivity} \\times {\\text{BR}} + [(1 - \\text{specificity}) \\times (1 - \\text{BR})]}\n\\end{aligned}\n(\\#eq:positivePredictiveValue)\n\\]\n```{r, class.source = “fold-hide”} positivePredictiveValue &lt;- function(TP, TN, FP, FN, BR = NULL, SN, SP){ if(is.null(BR)){ value &lt;- TP/(TP + FP) } else{ value &lt;- (SN * BR)/(SN * BR + (1 - SP) * (1 - BR)) }\nreturn(value) }\n\n```{r}\npositivePredictiveValue(\n  TP = TPvalue,\n  FP = FPvalue)\n\npositivePredictiveValue(\n  BR = baseRate(\n    TP = TPvalue,\n    TN = TNvalue,\n    FP = FPvalue,\n    FN = FNvalue),\n  SN = sensitivity(\n    TP = TPvalue,\n    FN = FNvalue),\n  SP = specificity(\n    TN = TNvalue,\n    FP = FPvalue))\n{r, include = FALSE} positivePredictivevalueValue &lt;- positivePredictiveValue(TP = TPvalue, FP = FPvalue)\nBelow I compute PPV and NPV at every possible base rate given the sensitivity and specificity at the current cutoff.\n```{r, class.source = “fold-hide”} negativePredictiveValue &lt;- function(TP, TN, FP, FN, BR = NULL, SN, SP){ if(is.null(BR)){ value &lt;- TN/(TN + FN) } else{ value &lt;- (SP * (1 - BR))/(SP * (1 - BR) + (1 - SN) * BR) }\nreturn(value) }\nppvNPVbaseRateData &lt;- data.frame( BR = seq(from = 0, to = 1, by = .01), SN = sensitivity( TP = TPvalue, FN = FNvalue), SP = specificity( TN = TNvalue, FP = FPvalue))\nppvNPVbaseRateData\\(positivePredictiveValue &lt;- positivePredictiveValue(\n  BR = ppvNPVbaseRateData\\)BR, SN = ppvNPVbaseRateData\\(SN,\n  SP = ppvNPVbaseRateData\\)SP)\nppvNPVbaseRateData\\(negativePredictiveValue &lt;- negativePredictiveValue(\n  BR = ppvNPVbaseRateData\\)BR, SN = ppvNPVbaseRateData\\(SN,\n  SP = ppvNPVbaseRateData\\)SP)\nppvNPVbaseRateData_long &lt;- pivot_longer( ppvNPVbaseRateData, cols = all_of(c( “positivePredictiveValue”,“negativePredictiveValue”)))\n\n```{r ppvNPVbaseRate, echo = FALSE, results = \"hide\", out.width = \"100%\", fig.align = \"center\", fig.cap = \"Positive Predictive Value and Negative Predictive Value as a Function of the Base Rate.\"}\nggplot(ppvNPVbaseRateData_long, aes(x = BR, y = value, color = name)) +\n  geom_line(linewidth = 2) +\n  scale_x_continuous(name = \"base rate\") +\n  scale_y_continuous(name = \"predictive value\") +\n  scale_color_viridis_d(name = \"\",\n                        breaks = c(\"negativePredictiveValue\",\"positivePredictiveValue\"),\n                        labels = c(\"Negative Predictive Value\",\"Positive Predictive Value\")) +\n  theme_bw()\nBelow I compute PPV and NPV at every possible cutoff.\n```{r, class.source = “fold-hide”} accuracyStats\\(positivePredictiveValue &lt;- positivePredictiveValue(\n  TP = accuracyStats\\)TP, TN = accuracyStats\\(TN,\n  FP = accuracyStats\\)FP, FN = accuracyStats$FN)\naccuracyStats\\(negativePredictiveValue &lt;- negativePredictiveValue(\n  TP = accuracyStats\\)TP, TN = accuracyStats\\(TN,\n  FP = accuracyStats\\)FP, FN = accuracyStats$FN)\nppvNPVcutoffData &lt;- pivot_longer( accuracyStats, cols = all_of(c( “positivePredictiveValue”,“negativePredictiveValue”)))\n\n```{r ppvNPVcutoff, echo = FALSE, results = \"hide\", out.width = \"100%\", fig.align = \"center\", fig.cap = \"Positive Predictive Value and Negative Predictive Value as a Function of the Cutoff.\"}\nggplot(ppvNPVcutoffData, aes(x = cutoff, y = value, color = name)) +\n  geom_line(linewidth = 2) +\n  scale_x_continuous(name = \"cutoff (liberal to conservative)\", limits = c(0.05,2.09)) +\n  scale_y_continuous(name = \"predictive value\") +\n  scale_color_viridis_d(name = \"\", breaks = c(\"negativePredictiveValue\",\"positivePredictiveValue\"), labels = c(\"Negative Predictive Value\",\"Positive Predictive Value\")) +\n  theme_bw()\n\n\n80.9.19 Negative Predictive Value (NPV)\nThe negative predictive value (NPV) is also called the negative predictive power (NPP). Many people confuse specificity (\\(P(\\text{not } R \\mid \\text{not } C)\\)) with its inverse conditional probability, NPV (\\(P(\\text{not } C \\mid  \\text{not } R)\\)). NPV is the conditional probability of not having the condition given a negative test: \\(P(\\text{not } C \\mid  \\text{not } R)\\). Higher values reflect greater accuracy. The formula for calculating negative predictive value is in Equation @ref(eq:negativePredictiveValue).\nNPV can be low even when specificity is high because it depends not only on specificity, but also on sensitivity and the base rate. Because NPV depends on the base rate, NPV is not an intrinsic property of a measure. The same measure will have a different NPV in different contexts with different base rates (Treat2023?). As described in Section @ref(accuracyCutoff) and as depicted in Figure @ref(fig:ppvNPVbaseRate), as the base rate increases, NPV decreases. As the base rate decreases, NPV increases. NPV also differs as a function of the cutoff. As described in Section @ref(accuracyCutoff) and as depicted in Figure @ref(fig:ppvNPVcutoff), as the cutoff increases (becomes more conservative), NPV decreases. As the cutoff decreases (becomes more liberal), NPV decreases.\n\\[\n\\small\n\\begin{aligned}\n  \\text{negative predictive value (NPV)} &= P(\\text{not } C \\mid \\text{not } R) \\\\\n  &= \\frac{\\text{TN}}{\\text{TN} + \\text{FN}} = \\frac{\\text{TN}}{N(\\text{1 - SR})}\\\\\n  &= \\frac{\\text{specificity} \\times (1-{\\text{BR}})}{\\text{specificity} \\times (1-{\\text{BR}}) + [(1 - \\text{sensitivity}) \\times \\text{BR})]}\n\\end{aligned}\n(\\#eq:negativePredictiveValue)\n\\]\n```{r, class.source = “fold-hide”} negativePredictiveValue &lt;- function(TP, TN, FP, FN, BR = NULL, SN, SP){ if(is.null(BR)){ value &lt;- TN/(TN + FN) } else{ value &lt;- (SP * (1 - BR))/(SP * (1 - BR) + (1 - SN) * BR) }\nreturn(value) }\n\n```{r}\nnegativePredictiveValue(\n  TN = TNvalue,\n  FN = FNvalue)\n\nnegativePredictiveValue(\n  BR = baseRate(\n    TP = TPvalue,\n    TN = TNvalue,\n    FP = FPvalue,\n    FN = FNvalue),\n  SN = sensitivity(\n    TP = TPvalue,\n    FN = FNvalue),\n  SP = specificity(\n    TN = TNvalue,\n    FP = FPvalue))\n{r, include = FALSE} negativePredictiveValueValue &lt;- negativePredictiveValue(TN = TNvalue,                                                         FN = FNvalue)\n\n\n80.9.20 False Discovery Rate (FDR)\nMany people confuse the false positive rate (\\(P(R \\mid \\text{not } C)\\)) with its inverse conditional probability, the false discovery rate (\\(P(\\text{not } C \\mid  R)\\)). The false discovery rate (FDR) is the conditional probability of not having the condition given a positive test: \\(P(\\text{not } C \\mid  R)\\). Lower values reflect greater accuracy. The formula for calculating false discovery rate is in Equation @ref(eq:falseDiscoveryRate).\n\\[\n\\begin{aligned}\n  \\text{false discovery rate (FDR)} &= P(\\text{not } C \\mid R) \\\\\n  &= \\frac{\\text{FP}}{\\text{FP} + \\text{TP}} = 1 - \\text{PPV}\n\\end{aligned}\n(\\#eq:falseDiscoveryRate)\n\\]\n```{r, class.source = “fold-hide”} falseDiscoveryRate &lt;- function(TP, TN, FP, FN){ value &lt;- FP/(FP + TP)\nreturn(value) }\n\n```{r}\nfalseDiscoveryRate(\n  TP = TPvalue,\n  FP = FPvalue)\n{r, include = FALSE} falseDiscoveryRateValue &lt;- falseDiscoveryRate(TP = TPvalue, FP = FPvalue)\n\n\n80.9.21 False Omission Rate (FOR)\nMany people confuse the false negative rate (\\(P(\\text{not } R \\mid C)\\)) with its inverse conditional probability, the false omission rate (\\(P(C \\mid \\text{not } R)\\)). The false omission rate (FOR) is the conditional probability of having the condition given a negative test: \\(P(C \\mid \\text{not } R)\\). Lower values reflect greater accuracy. The formula for calculating false omission rate is in Equation @ref(eq:falseOmissionRate).\n\\[\n\\begin{aligned}\n  \\text{false omission rate (FOR)} &= P(C \\mid \\text{not } R) \\\\\n  &= \\frac{\\text{FN}}{\\text{FN} + \\text{TN}} = 1 - \\text{NPV}\n\\end{aligned}\n(\\#eq:falseOmissionRate)\n\\]\n```{r, class.source = “fold-hide”} falseOmissionRate &lt;- function(TP, TN, FP, FN){ value &lt;- FN/(FN + TN)\nreturn(value) }\n\n```{r}\nfalseOmissionRate(\n  TN = TNvalue,\n  FN = FNvalue)\n{r, include = FALSE} falseOmissionRateValue &lt;- falseOmissionRate(TN = TNvalue, FN = FNvalue)\n\n\n80.9.22 Youden’s J Statistic\nYouden’s J statistic is also called Youden’s Index or informedness. Youden’s J statistic is the sum of sensitivity and specificity (and subtracting one). Higher values reflect greater accuracy. The formula for calculating Youden’s J statistic is in Equation @ref(eq:youdenIndex).\n\\[\n\\begin{aligned}\n  \\text{Youden's J statistic} &= \\text{sensitivity} + \\text{specificity} - 1\n\\end{aligned}\n(\\#eq:youdenIndex)\n\\]\n```{r, class.source = “fold-hide”} youdenJ &lt;- function(TP, TN, FP, FN){ SN &lt;- TP/(TP + FN) SP &lt;- TN/(TN + FP) value &lt;- SN + SP - 1\nreturn(value) }\n\n```{r}\nyoudenJ(\n  TP = TPvalue,\n  TN = TNvalue,\n  FP = FPvalue,\n  FN = FNvalue)\n{r, include = FALSE} youdenJValue &lt;- youdenJ(TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue)\n\n\n80.9.23 Balanced Accuracy\nBalanced accuracy is the average of sensitivity and specificity. Higher values reflect greater accuracy. The formula for calculating balanced accuracy is in Equation @ref(eq:balancedAccuracy).\n\\[\n\\begin{aligned}\n  \\text{balanced accuracy} &= \\frac{\\text{sensitivity} + \\text{specificity}}{2}\n\\end{aligned}\n(\\#eq:balancedAccuracy)\n\\]\n```{r, class.source = “fold-hide”} balancedAccuracy &lt;- function(TP, TN, FP, FN){ SN &lt;- TP/(TP + FN) SP &lt;- TN/(TN + FP) value &lt;- (SN + SP) / 2\nreturn(value) }\n\n```{r}\nbalancedAccuracy(\n  TP = TPvalue,\n  TN = TNvalue,\n  FP = FPvalue,\n  FN = FNvalue)\n{r, include = FALSE} balancedAccuracyValue &lt;- balancedAccuracy(TP = TPvalue,                                           TN = TNvalue,                                           FP = FPvalue,                                           FN = FNvalue)\n\n\n80.9.24 F-Score\nThe F-score combines precision (positive predictive value) and recall (sensitivity), where \\(\\beta\\) indicates how many times more important sensitivity is than the positive predictive value. If sensitivity and the positive predictive value are equally important, \\(\\beta = 1\\), and the F-score is called the \\(F_1\\) score. Higher values reflect greater accuracy. The formula for calculating the F-score is in Equation @ref(eq:FScore).\n\\[\n\\begin{aligned}\n  F_\\beta &= (1 + \\beta^2) \\cdot \\frac{\\text{positive predictive value} \\cdot \\text{sensitivity}}{(\\beta^2 \\cdot \\text{positive predictive value}) + \\text{sensitivity}} \\\\\n  &= \\frac{(1 + \\beta^2) \\cdot \\text{TP}}{(1 + \\beta^2) \\cdot \\text{TP} + \\beta^2 \\cdot \\text{FN} + \\text{FP}}\n\\end{aligned}\n(\\#eq:FScore)\n\\]\nThe \\(F_1\\) score is the harmonic mean of sensitivity and positive predictive value. The formula for calculating the \\(F_1\\) score is in Equation @ref(eq:F1Score).\n\\[\n\\begin{aligned}\n  F_1 &= \\frac{2 \\cdot \\text{positive predictive value} \\cdot \\text{sensitivity}}{(\\text{positive predictive value}) + \\text{sensitivity}} \\\\\n  &= \\frac{2 \\cdot \\text{TP}}{2 \\cdot \\text{TP} + \\text{FN} + \\text{FP}}\n\\end{aligned}\n(\\#eq:F1Score)\n\\]\n```{r, class.source = “fold-hide”} fScore &lt;- function(TP, TN, FP, FN, beta = 1){ value &lt;- ((1 + beta^2) * TP) / ((1 + beta^2) * TP + beta^2 * FN + FP)\nreturn(value) }\n\n```{r}\nfScore(\n  TP = TPvalue,\n  FP = FPvalue,\n  FN = FNvalue)\n\nfScore(\n  TP = TPvalue,\n  FP = FPvalue,\n  FN = FNvalue,\n  beta = 2)\n\nfScore(\n  TP = TPvalue,\n  FP = FPvalue,\n  FN = FNvalue,\n  beta = 0.5)\n{r, include = FALSE} f1ScoreValue &lt;- fScore(TP = TPvalue, FP = FPvalue, FN = FNvalue)\n\n\n80.9.25 Matthews Correlation Coefficient (MCC)\nThe Matthews correlation coefficient (MCC) is also called the phi coefficient. It is a correlation coefficient between predicted and observed values from a binary classification. Higher values reflect greater accuracy. The formula for calculating the MCC is in Equation @ref(eq:matthewsCorrelationCoefficient).\n\\[\n\\begin{aligned}\n  \\text{MCC} &= \\frac{\\text{TP} \\times \\text{TN} - \\text{FP} \\times \\text{FN}}{\\sqrt{(\\text{TP} + \\text{FP})(\\text{TP} + \\text{FN})(\\text{TN} + \\text{FP})(\\text{TN} + \\text{FN})}}\n\\end{aligned}\n(\\#eq:matthewsCorrelationCoefficient)\n\\]\n```{r, class.source = “fold-hide”} mcc &lt;- function(TP, TN, FP, FN){ TP &lt;- as.double(TP) TN &lt;- as.double(TN) FP &lt;- as.double(FP) FN &lt;- as.double(FN) value &lt;- ((TP * TN) - (FP * FN)) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\nreturn(value) }\n\n```{r}\nmcc(\n  TP = TPvalue,\n  TN = TNvalue,\n  FP = FPvalue,\n  FN = FNvalue)\n{r, include = FALSE} mccValue &lt;- mcc(TP = TPvalue,                 TN = TNvalue,                 FP = FPvalue,                 FN = FNvalue)\n\n\n80.9.26 Diagnostic Odds Ratio\nThe diagnostic odds ratio is the odds of a positive test among people with the condition relative to the odds of a positive test among people without the condition. Higher values reflect greater accuracy. The formula for calculating the diagnostic odds ratio is in Equation @ref(eq:diagnosticOddsRatio). If the predictor is bad, the diagnostic odds ratio could be less than one, and values can go up from there. If the diagnostic odds ratio is greater than 2, we take the odds ratio seriously because we are twice as likely to predict accurately than inaccurately. However, the diagnostic odds ratio ignores/hides base rates. When interpreting the diagnostic odds ratio, it is important to keep in mind the clinical significance, because otherwise it is not very meaningful. Consider a risk factor that has a diagnostic odds ratio of 3 for tuberculosis, i.e., it puts you at 3 times as likely to develop tuberculosis. The prevalence of tuberculosis is relatively low. Assuming the prevalence of tuberculosis is less than 1/10th of 1%, your risk of developing tuberculosis is still very low even if the risk factor (with a diagnostic odds ratio of 3) is present.\n\\[\n\\begin{aligned}\n  \\text{diagnostic odds ratio} &= \\frac{\\text{TP} \\times \\text{TN}}{\\text{FP} \\times \\text{FN}} \\\\\n  &= \\frac{\\text{sensitivity} \\times \\text{specificity}}{(1 - \\text{sensitivity}) \\times (1 - \\text{specificity})} \\\\\n  &= \\frac{\\text{PPV} \\times \\text{NPV}}{(1 - \\text{PPV}) \\times (1 - \\text{NPV})} \\\\\n  &= \\frac{\\text{LR+}}{\\text{LR}-}\n\\end{aligned}\n(\\#eq:diagnosticOddsRatio)\n\\]\n```{r, class.source = “fold-hide”} diagnosticOddsRatio &lt;- function(TP, TN, FP, FN){ value &lt;- (TP * TN) / (FP * FN)\nreturn(value) }\n\n```{r}\ndiagnosticOddsRatio(\n  TP = TPvalue,\n  TN = TNvalue,\n  FP = FPvalue,\n  FN = FNvalue)\n{r, include = FALSE} diagnosticOddsRatioValue &lt;- diagnosticOddsRatio(TP = TPvalue,                                                 TN = TNvalue,                                                 FP = FPvalue,                                                 FN = FNvalue)\n\n\n80.9.27 Diagnostic Likelihood Ratio\nA likelihood ratio is the ratio of two probabilities. It can be used to compare the likelihood of two possibilities. The diagnostic likelihood ratio is an index of the predictive validity of an instrument: it is the ratio of the probability that a test result is correct to the probability that the test result is incorrect. The diagnostic likelihood ratio is also called the risk ratio. There are two types of diagnostic likelihood ratios: the positive likelihood ratio and the negative likelihood ratio.\n\n80.9.27.1 Positive Likelihood Ratio (LR+)\nThe positive likelihood ratio (LR+) compares the true positive rate to the false positive rate. Positive likelihood ratio values range from 1 to infinity. Higher values reflect greater accuracy, because it indicates the degree to which a true positive is more likely than a false positive. The formula for calculating the positive likelihood ratio is in Equation @ref(eq:positiveLikelihoodRatio).\n\\[\n\\begin{aligned}\n  \\text{positive likelihood ratio (LR+)} &= \\frac{\\text{TPR}}{\\text{FPR}} \\\\\n  &= \\frac{P(R \\mid C)}{P(R \\mid \\text{not } C)} \\\\\n  &= \\frac{P(R \\mid C)}{1 - P(\\text{not } R \\mid \\text{not } C)} \\\\\n  &= \\frac{\\text{sensitivity}}{1 - \\text{specificity}}\n\\end{aligned}\n(\\#eq:positiveLikelihoodRatio)\n\\]\n```{r, class.source = “fold-hide”} positiveLikelihoodRatio &lt;- function(TP, TN, FP, FN){ SN &lt;- TP/(TP + FN) SP &lt;- TN/(TN + FP)\nvalue &lt;- SN/(1 - SP)\nreturn(value) }\n\n```{r}\npositiveLikelihoodRatio(\n  TP = TPvalue,\n  TN = TNvalue,\n  FP = FPvalue,\n  FN = FNvalue)\n{r, include = FALSE} positiveLikelihoodRatioValue &lt;- positiveLikelihoodRatio(TP = TPvalue,                                                         TN = TNvalue,                                                         FP = FPvalue,                                                         FN = FNvalue)\n\n\n80.9.27.2 Negative Likelihood Ratio (LR−)\nThe negative likelihood ratio (LR−) compares the false negative rate to the true negative rate. Negative likelihood ratio values range from 0 to 1. Smaller values reflect greater accuracy, because it indicates that a false negative is less likely than a true negative. The formula for calculating the negative likelihood ratio is in Equation @ref(eq:negativeLikelihoodRatio).\n\\[\n\\begin{aligned}\n  \\text{negative likelihood ratio } (\\text{LR}-) &= \\frac{\\text{FNR}}{\\text{TNR}} \\\\\n  &= \\frac{P(\\text{not } R \\mid C)}{P(\\text{not } R \\mid \\text{not } C)} \\\\\n  &= \\frac{1 - P(R \\mid C)}{P(\\text{not } R \\mid \\text{not } C)} \\\\\n  &= \\frac{1 - \\text{sensitivity}}{\\text{specificity}}\n\\end{aligned}\n(\\#eq:negativeLikelihoodRatio)\n\\]\n```{r, class.source = “fold-hide”} negativeLikelihoodRatio &lt;- function(TP, TN, FP, FN){ SN &lt;- TP/(TP + FN) SP &lt;- TN/(TN + FP)\nvalue &lt;- (1 - SN)/SP\nreturn(value) }\n\n```{r}\nnegativeLikelihoodRatio(\n  TP = TPvalue,\n  TN = TNvalue,\n  FP = FPvalue,\n  FN = FNvalue)\n{r, include = FALSE} negativeLikelihoodRatioValue &lt;- negativeLikelihoodRatio(TP = TPvalue,                                                         TN = TNvalue,                                                         FP = FPvalue,                                                         FN = FNvalue)\n\n\n\n80.9.28 Posttest Odds\nAs presented in Equation @ref(eq:bayes5), the posttest (or posterior) odds are equal to the pretest odds multiplied by the likelihood ratio. The posttest odds and posttest probability can be useful to calculate when the pretest probability is different from the pretest probability (or prevalence) of the classification. For instance, you might use a different pretest probability if a test result is already known and you want to know the updated posttest probability after conducting a second test. The formula for calculating posttest odds is in Equation @ref(eq:posttestOdds).\n\\[\n\\begin{aligned}\n  \\text{posttest odds} &= \\text{pretest odds} \\times \\text{likelihood ratio} \\\\\n\\end{aligned}\n(\\#eq:posttestOdds)\n\\]\nFor calculating the posttest odds of a true positive compared to a false positive, we use the positive likelihood ratio, described later. We would use the negative likelihood ratio if we wanted to calculate the posttest odds of a false negative compared to a true negative.\n```{r, class.source = “fold-hide”} posttestOdds &lt;- function(TP, TN, FP, FN, pretestProb = NULL, SN = NULL, SP = NULL, likelihoodRatio = NULL){ if(!is.null(pretestProb) & !is.null(SN) & !is.null(SP)){ pretestProbability &lt;- pretestProb pretestOdds &lt;- pretestProbability / (1 - pretestProbability)\nlikelihoodRatio &lt;- SN/(1 - SP)\n} else if(!is.null(pretestProb) & !is.null(likelihoodRatio)){ pretestProbability &lt;- pretestProb pretestOdds &lt;- pretestProbability / (1 - pretestProbability)\nlikelihoodRatio &lt;- likelihoodRatio\n} else { N &lt;- TP + TN + FP + FN pretestProbability &lt;- (TP + FN)/N pretestOdds &lt;- pretestProbability / (1 - pretestProbability)\nSN &lt;- TP/(TP + FN)\nSP &lt;- TN/(TN + FP)\nlikelihoodRatio &lt;- SN/(1 - SP)\n}\nvalue &lt;- pretestOdds * likelihoodRatio\nreturn(value) }\n\n```{r}\nposttestOdds(\n  TP = TPvalue,\n  TN = TNvalue,\n  FP = FPvalue,\n  FN = FNvalue)\n\nposttestOdds(\n  pretestProb = baseRate(\n    TP = TPvalue,\n    TN = TNvalue,\n    FP = FPvalue,\n    FN = FNvalue),\n  SN = sensitivity(\n    TP = TPvalue,\n    FN = FNvalue),\n  SP = specificity(\n    TN = TNvalue,\n    FP = FPvalue))\n\nposttestOdds(\n  pretestProb = baseRate(\n    TP = TPvalue,\n    TN = TNvalue,\n    FP = FPvalue,\n    FN = FNvalue),\n  likelihoodRatio = positiveLikelihoodRatio(\n    TP = TPvalue,\n    TN = TNvalue,\n    FP = FPvalue,\n    FN = FNvalue))\n{r, include = FALSE} posttestOddsValue &lt;- posttestOdds(TP = TPvalue,                                   TN = TNvalue,                                   FP = FPvalue,                                   FN = FNvalue)\n\n\n80.9.29 Posttest Probability\nThe posttest probability is the probability of having the disorder given a test result. When the base rate is used as the pretest probability, the posttest probability given a positive test is equal to positive predictive value. To convert odds to a probability, divide the odds by one plus the odds, as is in Equation @ref(eq:posttestProbability).\n\\[\n\\begin{aligned}\n  \\text{posttest probability} &= \\frac{\\text{posttest odds}}{1 + \\text{posttest odds}}\n\\end{aligned}\n(\\#eq:posttestProbability)\n\\]\n```{r, class.source = “fold-hide”} posttestProbability &lt;- function(TP, TN, FP, FN, pretestProb = NULL, SN = NULL, SP = NULL, likelihoodRatio = NULL){ if(!is.null(pretestProb) & !is.null(SN) & !is.null(SP)){ pretestProbability &lt;- pretestProb pretestOdds &lt;- pretestProbability / (1 - pretestProbability)\nlikelihoodRatio &lt;- SN/(1 - SP)\n} else if(!is.null(pretestProb) & !is.null(likelihoodRatio)){ pretestProbability &lt;- pretestProb pretestOdds &lt;- pretestProbability / (1 - pretestProbability)\nlikelihoodRatio &lt;- likelihoodRatio\n} else { N &lt;- TP + TN + FP + FN pretestProbability &lt;- (TP + FN)/N pretestOdds &lt;- pretestProbability / (1 - pretestProbability)\nSN &lt;- TP/(TP + FN)\nSP &lt;- TN/(TN + FP)\nlikelihoodRatio &lt;- SN/(1 - SP)\n}\nposttestOdds &lt;- pretestOdds * likelihoodRatio value &lt;- posttestOdds / (1 + posttestOdds)\nreturn(value) }\n\n```{r}\nposttestProbability(\n  TP = TPvalue,\n  TN = TNvalue,\n  FP = FPvalue,\n  FN = FNvalue)\n\nposttestProbability(\n  pretestProb = baseRate(\n    TP = TPvalue,\n    TN = TNvalue,\n    FP = FPvalue,\n    FN = FNvalue),\n  SN = sensitivity(\n    TP = TPvalue,\n    FN = FNvalue),\n  SP = specificity(\n    TN = TNvalue,\n    FP = FPvalue))\n\nposttestProbability(\n  pretestProb = baseRate(\n    TP = TPvalue,\n    TN = TNvalue,\n    FP = FPvalue,\n    FN = FNvalue),\n  likelihoodRatio = positiveLikelihoodRatio(\n    TP = TPvalue,\n    TN = TNvalue,\n    FP = FPvalue,\n    FN = FNvalue))\nposttestProbabilityValue &lt;- posttestProbability(TP = TPvalue,\n                                                TN = TNvalue,\n                                                FP = FPvalue,\n                                                FN = FNvalue)\nConsider the following example: Assume the base rate of the condition is .03%. We have two tests. Test A has a sensitivity of .95 and a specificity of .80. Test B has a sensitivity of .70 and a specificity of .90. What is the probability of having the condition if a person has a positive test on Test A? Assuming the errors of the two tests are independent, what is the probability of having the condition if the person has a positive test on Test B after having a positive test on Test A?\nprobGivenTestA &lt;- posttestProbability(\n  pretestProb = .003,\n  SN = .95,\n  SP = .80)\n\nprobGivenTestAthenB &lt;- posttestProbability(\n  pretestProb = probGivenTestA,\n  SN = .70,\n  SP = .90)\n\nprobGivenTestA\nprobGivenTestAthenB\nThe probability of having the condition if a person has a positive test on Test A is \\(`r apa(probGivenTestA * 100, decimals = 1)`\\)%. The probability of having the condition if the person has a positive test on Test B after having a positive test on Test A is \\(`r apa(probGivenTestAthenB * 100, decimals = 1)`\\)%.\n\n\n80.9.30 Probability Nomogram\nThe petersenlab package (R-petersenlab?) contains the nomogrammer() function that creates a probability nomogram plot, adapted from https://github.com/achekroud/nomogrammer. In Figure @ref(fig:nomogramPlot), the probability nomogram is generated using the number of true positives, true negatives, false positives, and false negatives at a given cutoff.\n{r nomogramPlot, fig.align = \"center\", fig.cap = \"Probability Nomogram.\"} nomogrammer(   TP = TPvalue,   TN = TNvalue,   FP = FPvalue,   FN = FNvalue )\nThe blue line indicates the posterior probability of the condition given a positive test. The pink line indicates the posterior probability of the condition given a negative test. One can also generate the probability nomogram from the base rate and the sensitivity and specificity of the test at a given cutoff:\n{r, eval = FALSE} nomogrammer(   pretestProb = baseRate(TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue),   SN = sensitivity(TP = TPvalue, FN = FNvalue),   SP = specificity(TN = TNvalue, FP = FPvalue) )\nOne can also generate the probability nomogram from the base rate, positive likelihood ratio, and negative likelihood ratio at a given cutoff:\n{r, eval = FALSE} nomogrammer(   pretestProb = baseRate(TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue),   PLR = positiveLikelihoodRatio(     TP = TPvalue,     TN = TNvalue,     FP = FPvalue,     FN = FNvalue),   NLR = negativeLikelihoodRatio(     TP = TPvalue,     TN = TNvalue,     FP = FPvalue,     FN = FNvalue) )\n\n\n80.9.31 \\(d'\\) Sensitivity from Signal Detection Theory\n\\(d'\\) (\\(d\\) prime) is an index of sensitivity from signal detection theory, as described by (Stanislaw1999?). Higher values reflect greater accuracy. The formula for calculating \\(d'\\) is in Equation @ref(eq:dPrimeSDT).\n\\[\\begin{equation}\nd' = z(\\text{hit rate}) - z(\\text{false alarm rate})\n(\\#eq:dPrimeSDT)\n\\end{equation}\\]\n```{r, class.source = “fold-hide”} dPrimeSDT &lt;- function(TP, TN, FP, FN){ HR &lt;- TP/(TP + FN) FAR &lt;- FP/(FP + TN) value &lt;- qnorm(HR) - qnorm(FAR)\nreturn(value) }\n\n```{r}\ndPrimeSDT(\n  TP = TPvalue,\n  TN = TNvalue,\n  FP = FPvalue,\n  FN = FNvalue)\n{r, include = FALSE} dPrimeValue &lt;- dPrimeSDT(TP = TPvalue,                          TN = TNvalue,                          FP = FPvalue,                          FN = FNvalue)\n\n\n80.9.32 \\(A\\) (Non-Parametric) Sensitivity from Signal Detection Theory\n\\(A\\) is a non-parametric index of sensitivity from signal detection theory, as described by (Zhang2005?). Higher values reflect greater accuracy. The formula for calculating \\(A\\) is in Equation @ref(eq:aSDT).\nhttps://sites.google.com/a/mtu.edu/whynotaprime/ (archived at https://perma.cc/W2M2-39TJ)\n\\[\\begin{equation}\nA =\n\\begin{cases}\n\\frac{3}{4} + \\frac{H - F}{4} - F(1 - H) & \\text{if } F \\leq 0.5 \\leq H ; \\\\\n\\frac{3}{4} + \\frac{H - F}{4} - \\frac{F}{4H} & \\text{if } F \\leq H \\leq 0.5 ;\\\\\n\\frac{3}{4} + \\frac{H - F}{4} - \\frac{1 - H}{4(1 - F)} & \\text{if } 0.5 \\leq F \\leq H .\n\\end{cases}\n(\\#eq:aSDT)\n\\end{equation}\\]\nwhere \\(H\\) is the hit rate and \\(F\\) is the false alarm rate.\n```{r, class.source = “fold-hide”} aSDT &lt;- function(TP, TN, FP, FN){ HR &lt;- TP/(TP + FN) FAR &lt;- FP/(FP + TN)\nifelse(FAR &lt;= .5 & HR &gt;= .5, value &lt;- (3/4) + ((HR - FAR)/4) - (FAR * (1 - HR)), NA) ifelse(FAR &lt;= HR & HR &lt;= .5, value &lt;- (3/4) + ((HR - FAR)/4) - (FAR/(4 * HR)), NA) ifelse(FAR &gt;= .5 & FAR &lt;= HR, value &lt;- (3/4) + ((HR - FAR)/4) - ((1 - HR)/(4 * (1 - FAR))), NA)\nreturn(value) }\n\n```{r, class.source = \"fold-hide\"}\naSDT(\n  TP = TPvalue,\n  TN = TNvalue,\n  FP = FPvalue,\n  FN = FNvalue)\n{r, include = FALSE} Avalue &lt;- aSDT(TP = TPvalue,                TN = TNvalue,                FP = FPvalue,                FN = FNvalue)\n\n\n80.9.33 \\(\\beta\\) Bias from Signal Detection Theory\n\\(\\beta\\) is an index of bias from signal detection theory, as described by (Stanislaw1999?). Smaller values reflect greater accuracy. The formula for calculating \\(\\beta\\) is in Equation @ref(eq:betaSDT).\n\\[\\begin{equation}\n\\beta = e^{\\bigg\\{\\frac{\\big[\\phi^{-1}(F)\\big]^2 - \\big[\\phi^{-1}(H)\\big]}{2}\\bigg\\}^2}\n(\\#eq:betaSDT)\n\\end{equation}\\]\nwhere \\(H\\) is the hit rate, \\(F\\) is the false alarm rate, and \\(\\phi\\) (phi) is a mathematical function that converts a z score to a probability by determining the portion of the normal distribution that lies to the left of the z score.\n```{r, class.source = “fold-hide”} betaSDT &lt;- function(TP, TN, FP, FN){ HR &lt;- TP/(TP + FN) FAR &lt;- FP/(FP + TN) value &lt;- exp(qnorm(FAR)^2/2 - qnorm(HR)^2/2)\nreturn(value) }\n\n```{r}\nbetaSDT(\n  TP = TPvalue,\n  TN = TNvalue,\n  FP = FPvalue,\n  FN = FNvalue)\n{r, include = FALSE} betaValue &lt;- betaSDT(TP = TPvalue,                      TN = TNvalue,                      FP = FPvalue,                      FN = FNvalue)\n\n\n80.9.34 \\(c\\) Bias from Signal Detection Theory\n\\(c\\) is an index of bias from signal detection theory, as described by (Stanislaw1999?). Smaller values reflect greater accuracy. The formula for calculating \\(c\\) is in Equation @ref(eq:cSDT).\n\\[\\begin{equation}\nc = - \\frac{\\phi^{-1}(H) + \\phi^{-1}(F)}{2}\n(\\#eq:cSDT)\n\\end{equation}\\]\nwhere \\(H\\) is the hit rate, \\(F\\) is the false alarm rate, and \\(\\phi\\) (phi) is a mathematical function that converts a z score to a probability by determining the portion of the normal distribution that lies to the left of the z score.\n```{r, class.source = “fold-hide”} cSDT &lt;- function(TP, TN, FP, FN){ HR &lt;- TP/(TP + FN) FAR &lt;- FP/(FP + TN) value &lt;- -(qnorm(HR) + qnorm(FAR))/2\nreturn(value) }\n\n```{r}\ncSDT(\n  TP = TPvalue,\n  TN = TNvalue,\n  FP = FPvalue,\n  FN = FNvalue)\n{r, include = FALSE} cValue &lt;- cSDT(TP = TPvalue,                TN = TNvalue,                FP = FPvalue,                FN = FNvalue)\n\n\n80.9.35 \\(b\\) (Non-Parametric) Bias from Signal Detection Theory\n\\(b\\) is a non-parametric index of bias from signal detection theory, as described by (Zhang2005?). Smaller values reflect greater accuracy. The formula for calculating \\(b\\) is in Equation @ref(eq:bSDT).\n\\[\\begin{equation}\nb =\n\\begin{cases}\n\\frac{5 - 4H}{1 + 4F} & \\text{if } F \\leq 0.5 \\leq H ; \\\\\n\\frac{H^2 + H}{H^2 + F} & \\text{if } F \\leq H \\leq 0.5 ;\\\\\n\\frac{(1 - F)^2 + (1 - H)}{(1 - F)^2 + (1 - F)} & \\text{if } 0.5 \\leq F \\leq H .\n\\end{cases}\n(\\#eq:bSDT)\n\\end{equation}\\]\n```{r, class.source = “fold-hide”} bSDT &lt;- function(TP, TN, FP, FN){ HR &lt;- TP/(TP + FN) FAR &lt;- FP/(FP + TN)\nifelse(FAR &lt;= .5 & HR &gt;= .5, value &lt;-(5 - (4 * HR))/(1 + (4 * FAR)), NA) ifelse(FAR &lt;= HR & HR &lt;= .5, value &lt;- (HR^2 + HR)/(HR^2 + FAR), NA) ifelse(FAR &gt;= .5 & FAR &lt;= HR, value &lt;- ((1 - FAR)^2 + (1 - HR))/((1 - FAR)^2 + (1 - FAR)), NA)\nreturn(value) }\n\n```{r}\nbSDT(\n  TP = TPvalue,\n  TN = TNvalue,\n  FP = FPvalue,\n  FN = FNvalue)\n{r, include = FALSE} bValue &lt;- bSDT(TP = TPvalue,                TN = TNvalue,                FP = FPvalue,                FN = FNvalue)\n\n\n80.9.36 Mean Difference between Predicted Versus Observed Values (Miscalibration)\nThe mean difference between predicted values versus observed values at a given cutoff is an index of miscalibration of predictions at that cutoff. It is called “calibration-in-the-small” (as opposed to calibration-in-the-large, which spans all cutoffs). Values closer to zero reflect greater accuracy. Values above zero indicate that the predicted values are, on average, greater than the observed values. Values below zero indicate that the observed values are, on average, greater than the predicted values.\n```{r, fig.show = “hide”, class.source = “fold-hide”} miscalibration &lt;- function(predicted, actual, cutoff, bins = 10){ data &lt;- data.frame(na.omit(cbind(predicted, actual)))\ncalibrationTable &lt;- mutate( data, bin = cut_number( predicted, n = 10)) %&gt;% group_by(bin) %&gt;% summarise( n = length(predicted), meanPredicted = mean(predicted, na.rm = TRUE), meanObserved = mean(actual, na.rm = TRUE), .groups = “drop”)\ncalibrationTable\\(cutoffMin &lt;- as.numeric(str_replace_all(str_split(\n    calibrationTable\\)bin, pattern = “,”, simplify = TRUE)[,1], “[1\\-\\.]”, ““)) calibrationTable\\(cutoffMax &lt;- as.numeric(str_replace_all(str_split(\n    calibrationTable\\)bin, pattern =”,“, simplify = TRUE)[,2],”[2\\-\\.]“,”“))\ncalibrationTable$inRange &lt;- with( calibrationTable, cutoff &gt;= cutoffMin & cutoff &lt;= cutoffMax)\nif(length(which(calibrationTable\\(inRange == TRUE)) &gt; 0){\n    nearestCutoff &lt;- calibrationTable\\)bin[min(which( calibrationTable\\(inRange == TRUE))]\n    calibrationAtNearestCutoff &lt;- calibrationTable[which(\n      calibrationTable\\)bin == nearestCutoff),] calibrationAtNearestCutoff &lt;- as.data.frame(calibrationTable[max(which( calibrationTable$inRange == TRUE)),])\nmeanPredicted &lt;- calibrationAtNearestCutoff[, \"meanPredicted\"]\nmeanObserved &lt;- calibrationAtNearestCutoff[, \"meanObserved\"]\ndifferenceBetweenPredictedAndObserved &lt;- meanPredicted - meanObserved\n} else{ differenceBetweenPredictedAndObserved &lt;- NA }\nreturn(differenceBetweenPredictedAndObserved) }\n\n```{r, fig.show = \"hide\"}\nmiscalibration(\n  predicted = mydataSDT$predictedProbability,\n  actual = mydataSDT$disorder,\n  cutoff = cutoff)",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#optimalCutoff",
    "href": "chapters/prediction/01_prediction.html#optimalCutoff",
    "title": "80  Predizione",
    "section": "80.10 Optimal Cutoff Specification",
    "text": "80.10 Optimal Cutoff Specification\nThere are two ways to improve diagnostic performance (Swets2000?). One way is to increase the diagnostic accuracy of the assessment. The second way is to increase the utility of the diagnostic decisions that are made, based on where we set the cutoff. The optimal cutoff depends on the differential costs of false positives versus false negatives, as applied in decision theory. When differential costs of false positives versus false negatives cannot be specified, an alternative approach to specifying the optimal cutoff is to use information theory.\n\n80.10.1 Decision Theory\nAccording to the decision theory approach to picking the optimal cutoff, the optimal cutoff depends on the value/importance placed on each of the four decision outcomes [(true positives, true negatives, false positives, false negatives); (Treat2023?)]. Utility is the relative value placed on a specific decision-making outcome (i.e., user-perceived benefit or cost): utilities typically range between zero and one, where a value of zero represents the least desired outcome, and a value of one indicates the most desired outcome. According to the decision theory approach, the optimal cutoff is the cutoff with the highest overall utility.\n\n80.10.1.1 Overall utility of a specific cutoff value\nThe overall utility of a specific cutoff value is a utilities-weighted sum of the probabilities of the four decision-making outcomes (hits, misses, correct rejections, false alarms). That is, overall utility is the sum of the product of the probability of a particular outcome (TP, TN, FP, FN; e.g., \\(\\text{BR} \\times \\text{TP rate}\\)) and the utility of that outcome (e.g., how much we value TPs relative to other outcomes). Higher values reflect greater utility, so you would pick the cutoff with the highest overall utility. The formula for calculating overall utility is in Equation @ref(eq:overallUtility):\n\\[\n\\begin{aligned}\n  U_\\text{overall} = \\ & (\\text{BR})(\\text{HR})(U_\\text{H}) \\\\\n  &+ (\\text{BR})(1 - \\text{HR})(U_\\text{M}) \\\\\n  &+ (1 - \\text{BR})(\\text{FAR})(U_\\text{FA}) \\\\\n  &+ (1 - \\text{BR})(1 - \\text{FAR})(U_\\text{CR})\n\\end{aligned}\n(\\#eq:overallUtility)\n\\]\nwhere \\(\\text{BR} = \\text{base rate}\\), \\(\\text{HR} = \\text{hit rate (true positive rate)}\\), \\(\\text{FAR} = \\text{false alarm rate (false positive rate)}\\), \\(U_\\text{H} = \\text{utility of hits (true positives)}\\), \\(U_\\text{M} = \\text{utility of misses (false negatives)}\\), \\(U_\\text{FA} = \\text{utility of false alarms (false positives)}\\), \\(U_\\text{CR} = \\text{utility of correct rejections (true negatives)}\\).\n{r, class.source = \"fold-hide\"} Uoverall &lt;- function(BR, HR, FAR, UH, UM, UCR, UFA){   (BR*HR*UH) + (BR*(1 - HR)*UM) + ((1 - BR)*FAR*UFA) + ((1 - BR)*(1 - FAR)*(UCR)) }\nUoverall(\n  BR = baseRate(\n    TP = TPvalue,\n    TN = TNvalue,\n    FP = FPvalue,\n    FN = FNvalue),\n  HR = sensitivity(\n    TP = TPvalue,\n    FN = FNvalue),\n  FAR = falsePositiveRate(\n    TN = TNvalue,\n    FP = FPvalue),\n  UH = 1,\n  UM = 0,\n  UCR = 0.75,\n  UFA = 0.25)\n\nUoverall(\n  BR = baseRate(\n    TP = TPvalue,\n    TN = TNvalue,\n    FP = FPvalue,\n    FN = FNvalue),\n  HR = sensitivity(\n    TP = TPvalue,\n    FN = FNvalue),\n  FAR = falsePositiveRate(\n    TN = TNvalue,\n    FP = FPvalue),\n  UH = 1,\n  UM = 0,\n  UCR = 1,\n  UFA = 0)\n\nUoverall(\n  BR = baseRate(\n    TP = TPvalue,\n    TN = TNvalue,\n    FP = FPvalue,\n    FN = FNvalue),\n  HR = sensitivity(\n    TP = TPvalue,\n    FN = FNvalue),\n  FAR = falsePositiveRate(\n    TN = TNvalue,\n    FP = FPvalue),\n  UH = 0.75,\n  UM = 0.25,\n  UCR = 1,\n  UFA = 0)\n\n\n80.10.1.2 Utility ratio\nThe utility ratio is the user-perceived relative importance of decisions about negative versus positive cases. If the utility ratio value is one, it indicates that identifying negative cases and positive cases is equally important. Values above one indicate greater relative importance of identifying negative cases than positive cases. Values below one indicate greater relative importance of identifying positive cases than negative cases. Values of one indicate that you are maximizing percent accuracy. The formula for calculating the utility ratio is in Equation @ref(eq:utilityRatio):\n\\[\\begin{equation}\n\\text{Utility Ratio} = \\frac{U_\\text{CR} - U_\\text{FA}}{U_\\text{H} - U_\\text{M}}\n(\\#eq:utilityRatio)\n\\end{equation}\\]\nwhere \\(U_\\text{H} = \\text{utility of hits (true positives)}\\), \\(U_\\text{M} = \\text{utility of misses (false negatives)}\\), \\(U_\\text{FA} = \\text{utility of false alarms (false positives)}\\), \\(U_\\text{CR} = \\text{utility of correct rejections (true negatives)}\\).\n{r, class.source = \"fold-hide\"} utilityRatio &lt;- function(UH, UM, UCR, UFA){   (UCR - UFA) / (UH - UM) }\nutilityRatio(UH = 1, UM = 0, UCR = 0.75, UFA = 0.25)\nutilityRatio(UH = 1, UM = 0, UCR = 1, UFA = 0)\nutilityRatio(UH = 0.75, UM = 0.25, UCR = 1, UFA = 0)\nDecision theory has key advantages, because it identifies the cutoff that would help you best achieve the goals/purpose of the assessment. However, it can be challenging to specify the relative costs of errors. If you cannot decide values for outcomes (relative importance between FP and FN), you can use information theory to identify the optimal cutoff.\n\n\n\n80.10.2 Information Theory\nWhen the user does not differentially weigh the value/importance of the four decision-making outcomes (hits, misses, correct rejections, false alarms), the information theory approach can be useful for specifying the optimal cutoff. According to the information theory approach, the optimal cutoff is the cutoff that provides the greatest information gain (Treat2023?).\n\n80.10.2.1 Information Gain\nInformation gain (\\(I_\\text{gain}\\)) is the reduction of uncertainty about the true classification of a case that results from administering an assessment or prediction measure (Treat2023?). Greater values reflect greater reduction of uncertainty, so the optimal cutoff can be specified as the cutoff with the highest information gain.\n\n80.10.2.1.1 Formula from (Treat2023?)\nThe formula from (Treat2023?) for calculating information gain is in Equation @ref(eq:informationGain1):\n\\[\n\\begin{aligned}\n  I_\\text{gain} = \\ & (\\text{BR})(\\text{HR})\\bigg[\\log_2\\bigg(\\frac{\\text{HR}}{G}\\bigg)\\bigg] \\\\\n  &+ (\\text{BR})(1 - \\text{HR})\\bigg[\\log_2\\bigg(\\frac{1 - \\text{HR}}{1 - G}\\bigg)\\bigg] \\\\\n  &+ (1 - \\text{BR})(\\text{FAR})\\bigg[\\log_2\\bigg(\\frac{\\text{FAR}}{G}\\bigg)\\bigg] \\\\\n  &+ (1 - \\text{BR})(1 - \\text{FAR})\\bigg[\\log_2\\bigg(\\frac{1 - \\text{FAR}}{1 - G}\\bigg)\\bigg]\n\\end{aligned}\n(\\#eq:informationGain1)\n\\]\nwhere \\(\\text{BR} =\\) base rate, \\(\\text{HR} =\\) hit rate (true positive rate), \\(\\text{FAR} =\\) false alarm rate (false positive rate), \\(G =\\) selection ratio \\(= \\text{BR} (\\text{HR}) + (1 - \\text{BR}) (\\text{FAR})\\), as reported in (Somoza1989?) (see below).\n```{r, class.source = “fold-hide”} Igain &lt;- function(BR, HR, FAR){ G &lt;- BR(HR) + (1 - BR)(FAR)\n(BRHRlog2(HR/G)) + (BR(1 - HR)(log2((1 - HR)/(1 - G)))) + ((1 - BR)FAR(log2(FAR/G))) + ((1 - BR)(1 - FAR)(log2((1 - FAR)/(1 - G)))) }\n\n```{r}\nIgain(\n  BR = baseRate(\n    TP = TPvalue,\n    TN = TNvalue,\n    FP = FPvalue,\n    FN = FNvalue),\n  HR = sensitivity(\n    TP = TPvalue,\n    FN = FNvalue),\n  FAR = falsePositiveRate(\n    TN = TNvalue,\n    FP = FPvalue))\n\n\n80.10.2.1.2 Alternative formula from (Metz1973?)\nThe alternative formula from (Metz1973?) for calculating information gain is in Equation @ref(eq:informationGain2):\n\\[\n\\begin{aligned}\n  I_\\text{gain} = \\ & p(S \\mid s) \\cdot p(s) \\cdot log_2\\Bigg\\{\\frac{p(S \\mid s)}{p(S \\mid s) \\cdot p(s) + p(S \\mid n)[1 - p(s)]}\\Bigg\\} \\\\\n  &+ p(S \\mid n)[1 - p(s)] \\times log_2\\Bigg\\{\\frac{p(S \\mid n)}{p(S \\mid s) \\cdot p(s) + p(S \\mid n)[1 - p(s)]}\\Bigg\\} \\\\\n  &+ [1 - p(S \\mid s)] \\cdot p(s) \\times log_2\\Bigg\\{\\frac{1 - p(S \\mid s)}{1 - p(S \\mid s) \\cdot p(s) - p(S \\mid n)[1 - p(s)]}\\Bigg\\} \\\\\n  &+ [1 - p(S \\mid n)][1 - p(s)] \\times log_2\\Bigg\\{\\frac{1 - p(S \\mid n)}{1 - p(S \\mid s) \\cdot p(s) - p(S \\mid n)[1 - p(s)]}\\Bigg\\}\n\\end{aligned}\n(\\#eq:informationGain2)\n\\]\nwhere \\(p(S \\mid s) =\\) sensitivity (hit rate or true positive rate); i.e., the conditional probability of deciding a signal is present (\\(S\\)) when the signal is in fact present(\\(s\\)), \\(p(S \\mid n) =\\) false positive rate (false alarm rate); i.e., the conditional probability of deciding a signal is present (\\(S\\)) when the signal is in fact absent (\\(n\\)), \\(p(s) =\\) base rate, i.e., the probability that the signal is in fact present (\\(s\\)).\n{r, class.source = \"fold-hide\"} Igain2 &lt;- function(BR, HR, FAR){   HR * BR * log2(HR / ((HR * BR) + (FAR * (1 - BR)))) +     FAR * (1 - BR) * log2(FAR / ((HR * BR) + (FAR * (1 - BR)))) +     (1 - HR) * BR * log2((1 - HR) / (1 - (HR * BR) - (FAR * (1 - BR)))) +     (1 - FAR) * (1 - BR) * log2((1 - FAR) / (1 - (HR * BR) - (FAR * (1 - BR)))) }\nIgain2(\n  BR = baseRate(\n    TP = TPvalue,\n    TN = TNvalue,\n    FP = FPvalue,\n    FN = FNvalue),\n  HR = sensitivity(\n    TP = TPvalue,\n    FN = FNvalue),\n  FAR = falsePositiveRate(\n    TN = TNvalue,\n    FP = FPvalue))\n\n\n80.10.2.1.3 Alternative formula from (Somoza1989?)\nThe alternative formula from (Somoza1989?) for calculating information gain is in Equation @ref(eq:informationGain3):\n\\[\n\\begin{aligned}\n  I_\\text{gain} = \\ & [(\\text{TPR})(\\text{Pr})] \\times \\log_2(\\text{TPR}/G) \\\\\n  &+ [(\\text{FPR})(1 - \\text{Pr})] \\times \\log_2(\\text{FPR}/G) \\\\\n  &+ [(1 - \\text{TPR})(\\text{Pr})] \\times \\log_2\\bigg(\\frac{1 - \\text{TPR}}{1 - G}\\bigg) \\\\\n  &+ [(1 - \\text{FPR})(1 - \\text{Pr})] \\times \\log_2\\bigg(\\frac{1 - \\text{FPR}}{1 - G}\\bigg)\n\\end{aligned}\n(\\#eq:informationGain3)\n\\]\nwhere \\(\\text{TP} =\\) true positive rate (hit rate), \\(\\text{Pr} =\\) prevalence (base rate), \\(\\text{FP} =\\) false positive rate (false alarm rate), \\(G = \\text{Pr} (\\text{TP}) + (1 - \\text{Pr}) (\\text{FP}) =\\) selection ratio\n```{r, class.source = “fold-hide”} Igain3 &lt;- function(BR, HR, FAR){ G &lt;- BR(HR) + (1 - BR)(FAR)\n((HR)(BR))log2((HR/G)) + ((FAR)(1-BR))log2((FAR/G)) + ((1-HR)(BR))log2((1-HR)/(1-G)) + ((1-FAR)(1-BR))log2((1-FAR)/(1-G)) }\n\n```{r}\nIgain3(\n  BR = baseRate(\n    TP = TPvalue,\n    TN = TNvalue,\n    FP = FPvalue,\n    FN = FNvalue),\n  HR = sensitivity(\n    TP = TPvalue,\n    FN = FNvalue),\n  FAR = falsePositiveRate(\n    TN = TNvalue,\n    FP = FPvalue))\n\n\n80.10.2.1.4 Examples\nCase A from Exhibit 38.2 (Treat2023?):\nIgain(HR = (911/1899), FAR = (509/4757), BR = (1899/6656))\nCase B from Exhibit 38.2 (Treat2023?):\nIgain(HR = (1597/3328), FAR = (356/3328), BR = (3328/6656))\nCase C from Exhibit 38.2 (Treat2023?):\nIgain(HR = (2040/3328), FAR = (654/3328), BR = (3328/6656))\nCase B from Exhibit 38.3 (Treat2023?):\nIgain(HR = (1164/1899), FAR = (935/4757), BR = (1899/6656))\n\n\n80.10.2.1.5 Effect of Base Rate\nInformation gain depends on the base rate (Treat2023?), as depicted in Figure @ref(fig:informationGainBaseRate). The maximum reduction of uncertainty (i.e., greatest information) occurs when the base rate is 0.5. A base rate tells us little a priori about the condition if the probability of the condition is 50/50, so the measure can provide more benefit. If the base rate is 0.3 or 0.7, we can do better than going with the base rate. If the base rate is 0.9 or 0.1, it is difficult for our measure to do better than going with the base rate. If the base rate is 0.05 of 0.95 (or more extreme), it is likely that our measure will do almost nothing in terms of information gain.\n```{r informationGainBaseRate, echo = FALSE, results = “hide”, fig.height = 4, out.width = “100%”, fig.align = “center”, fig.cap = “Information Gain as a Function of the Base Rate (BR).”} possibleCutoffs &lt;- unique(na.omit(mydataSDT$testScore)) possibleCutoffs &lt;- possibleCutoffs[order(possibleCutoffs)] possibleCutoffs &lt;- c(possibleCutoffs, max(possibleCutoffs, na.rm = TRUE) + 0.01)\nbaseRates &lt;- c(.05, .1, .3, .5, .7, .9, .95)\npossibleCutoffsBaseRates &lt;- expand_grid(baseRate = baseRates, cutoff = possibleCutoffs)\ninformationGainVars &lt;- c(“cutoff”,“TP”,“TN”,“FP”,“FN”)\ninformationGainStats &lt;- data.frame(matrix(nrow = length(possibleCutoffs), ncol = length(informationGainVars))) names(informationGainStats) &lt;- informationGainVars\nfor(i in 1:length(possibleCutoffs)){ cutoff &lt;- possibleCutoffs[i]\nmydataSDT\\(diagnosis &lt;- NA\n  mydataSDT\\)diagnosis[mydataSDT\\(testScore &lt; cutoff] &lt;- 0\n  mydataSDT\\)diagnosis[mydataSDT$testScore &gt;= cutoff] &lt;- 1\ninformationGainStats[i, “cutoff”] &lt;- cutoff informationGainStats[i, “TP”] &lt;- length(which(mydataSDT\\(diagnosis == 1 & mydataSDT\\)disorder == 1)) informationGainStats[i, “TN”] &lt;- length(which(mydataSDT\\(diagnosis == 0 & mydataSDT\\)disorder == 0)) informationGainStats[i, “FP”] &lt;- length(which(mydataSDT\\(diagnosis == 1 & mydataSDT\\)disorder == 0)) informationGainStats[i, “FN”] &lt;- length(which(mydataSDT\\(diagnosis == 0 & mydataSDT\\)disorder == 1)) }\ninformationGainStats &lt;- full_join(possibleCutoffsBaseRates, informationGainStats, by = “cutoff”)\ninformationGainStats\\(TPrate &lt;- sensitivity(TP = informationGainStats\\)TP, TN = informationGainStats\\(TN, FP = informationGainStats\\)FP, FN = informationGainStats\\(FN)\ninformationGainStats\\)FPrate &lt;- falsePositiveRate(TP = informationGainStats\\(TP, TN = informationGainStats\\)TN, FP = informationGainStats\\(FP, FN = informationGainStats\\)FN)\ninformationGainStats\\(informationGain &lt;- Igain(BR = informationGainStats\\)baseRate, HR = informationGainStats\\(TPrate, FAR = informationGainStats\\)FPrate) informationGainStats &lt;- na.omit(informationGainStats)\nggplot( data = informationGainStats, aes( x = cutoff, y = informationGain, group = as.factor(baseRate), color = as.factor(baseRate))) + geom_line(linewidth = 2) + scale_x_continuous(name = “Cutoff”) + scale_y_continuous(name = “Information Gain”) + scale_color_viridis( name = “Base Rate”, discrete = TRUE, option = “H”) + geom_label_repel( data = informationGainStats %&gt;% filter(cutoff == 0.5), aes(label = paste(“BR =”, baseRate, sep = ““)), nudge_x = 0.10, na.rm = TRUE) + theme_bw() + theme(legend.position =”none”)\n\n\n## Accuracy at Every Possible Cutoff {#accuracyAtEveryPossibleCutoff}\n\n### Specify utility of each outcome {#specifyUtility}\n\n\n```{r}\nutilityHits &lt;- 1\nutilityMisses &lt;- 0\nutilityCorrectRejections &lt;- 0.75\nutilityFalseAlarms &lt;- 0.25\n\n\n\n\n80.10.3 Calculate Accuracy\n```{r, class.source = “fold-hide”} possibleCutoffs &lt;- unique(na.omit(mydataSDT$testScore)) possibleCutoffs &lt;- possibleCutoffs[order(possibleCutoffs)] possibleCutoffs &lt;- c( possibleCutoffs, max(possibleCutoffs, na.rm = TRUE) + 0.01)\naccuracyVariables &lt;- c( “cutoff”, “TP”, “TN”, “FP”, “FN”, “differenceBetweenPredictedAndObserved”)\naccuracyStats &lt;- data.frame( matrix( nrow = length(possibleCutoffs), ncol = length(accuracyVariables))) names(accuracyStats) &lt;- accuracyVariables\nfor(i in 1:length(possibleCutoffs)){ cutoff &lt;- possibleCutoffs[i]\nmydataSDT\\(diagnosis &lt;- NA\n  mydataSDT\\)diagnosis[mydataSDT\\(testScore &lt; cutoff] &lt;- 0\n  mydataSDT\\)diagnosis[mydataSDT$testScore &gt;= cutoff] &lt;- 1\naccuracyStats[i, “cutoff”] &lt;- cutoff accuracyStats[i, “TP”] &lt;- length(which( mydataSDT\\(diagnosis == 1 & mydataSDT\\)disorder == 1)) accuracyStats[i, “TN”] &lt;- length(which( mydataSDT\\(diagnosis == 0 & mydataSDT\\)disorder == 0)) accuracyStats[i, “FP”] &lt;- length(which( mydataSDT\\(diagnosis == 1 & mydataSDT\\)disorder == 0)) accuracyStats[i, “FN”] &lt;- length(which( mydataSDT\\(diagnosis == 0 & mydataSDT\\)disorder == 1))\naccuracyStats[i, “differenceBetweenPredictedAndObserved”] &lt;- miscalibration( predicted = mydataSDT\\(testScore,\n      actual = mydataSDT\\)disorder, cutoff = cutoff) }\naccuracyStats\\(N &lt;- sampleSize(\n  TP = accuracyStats\\)TP, TN = accuracyStats\\(TN,\n  FP = accuracyStats\\)FP, FN = accuracyStats$FN)\naccuracyStats\\(selectionRatio &lt;- selectionRatio(\n  TP = accuracyStats\\)TP, TN = accuracyStats\\(TN,\n  FP = accuracyStats\\)FP, FN = accuracyStats\\(FN)\naccuracyStats\\)baseRate &lt;- baseRate( TP = accuracyStats\\(TP,\n  TN = accuracyStats\\)TN, FP = accuracyStats\\(FP,\n  FN = accuracyStats\\)FN)\naccuracyStats\\(percentAccuracy &lt;- percentAccuracy(\n  TP = accuracyStats\\)TP, TN = accuracyStats\\(TN,\n  FP = accuracyStats\\)FP, FN = accuracyStats\\(FN)\naccuracyStats\\)percentAccuracyByChance &lt;- percentAccuracyByChance( TP = accuracyStats\\(TP,\n  TN = accuracyStats\\)TN, FP = accuracyStats\\(FP,\n  FN = accuracyStats\\)FN) accuracyStats\\(relativeImprovementOverChance &lt;- relativeImprovementOverChance(\n  TP = accuracyStats\\)TP, TN = accuracyStats\\(TN,\n  FP = accuracyStats\\)FP, FN = accuracyStats\\(FN)\naccuracyStats\\)relativeImprovementOverPredictingFromBaseRate &lt;- relativeImprovementOverPredictingFromBaseRate( TP = accuracyStats\\(TP,\n    TN = accuracyStats\\)TN, FP = accuracyStats\\(FP,\n    FN = accuracyStats\\)FN)\naccuracyStats\\(sensitivity &lt;- accuracyStats\\)TPrate &lt;- sensitivity( TP = accuracyStats\\(TP,\n  TN = accuracyStats\\)TN, FP = accuracyStats\\(FP,\n  FN = accuracyStats\\)FN) accuracyStats\\(specificity &lt;- accuracyStats\\)TNrate &lt;- specificity( TP = accuracyStats\\(TP,\n  TN = accuracyStats\\)TN, FP = accuracyStats\\(FP,\n  FN = accuracyStats\\)FN) accuracyStats\\(FNrate &lt;- falseNegativeRate(\n  TP = accuracyStats\\)TP, TN = accuracyStats\\(TN,\n  FP = accuracyStats\\)FP, FN = accuracyStats\\(FN)\naccuracyStats\\)FPrate &lt;- falsePositiveRate( TP = accuracyStats\\(TP,\n  TN = accuracyStats\\)TN, FP = accuracyStats\\(FP,\n  FN = accuracyStats\\)FN)\naccuracyStats\\(youdenJ &lt;- youdenJ(\n  TP = accuracyStats\\)TP, TN = accuracyStats\\(TN,\n  FP = accuracyStats\\)FP, FN = accuracyStats$FN)\naccuracyStats\\(positivePredictiveValue &lt;- positivePredictiveValue(\n  TP = accuracyStats\\)TP, TN = accuracyStats\\(TN,\n  FP = accuracyStats\\)FP, FN = accuracyStats\\(FN)\naccuracyStats\\)negativePredictiveValue &lt;- negativePredictiveValue( TP = accuracyStats\\(TP,\n  TN = accuracyStats\\)TN, FP = accuracyStats\\(FP,\n  FN = accuracyStats\\)FN) accuracyStats\\(falseDiscoveryRate &lt;- falseDiscoveryRate(\n  TP = accuracyStats\\)TP, TN = accuracyStats\\(TN,\n  FP = accuracyStats\\)FP, FN = accuracyStats\\(FN)\naccuracyStats\\)falseOmissionRate &lt;- falseOmissionRate( TP = accuracyStats\\(TP,\n  TN = accuracyStats\\)TN, FP = accuracyStats\\(FP,\n  FN = accuracyStats\\)FN)\naccuracyStats\\(balancedAccuracy &lt;- balancedAccuracy(\n  TP = accuracyStats\\)TP, TN = accuracyStats\\(TN,\n  FP = accuracyStats\\)FP, FN = accuracyStats\\(FN)\naccuracyStats\\)f1Score &lt;- fScore( TP = accuracyStats\\(TP,\n  TN = accuracyStats\\)TN, FP = accuracyStats\\(FP,\n  FN = accuracyStats\\)FN) accuracyStats\\(mcc &lt;- mcc(\n  TP = accuracyStats\\)TP, TN = accuracyStats\\(TN,\n  FP = accuracyStats\\)FP, FN = accuracyStats$FN)\naccuracyStats\\(diagnosticOddsRatio &lt;- diagnosticOddsRatio(\n  TP = accuracyStats\\)TP, TN = accuracyStats\\(TN,\n  FP = accuracyStats\\)FP, FN = accuracyStats\\(FN)\naccuracyStats\\)positiveLikelihoodRatio &lt;- positiveLikelihoodRatio( TP = accuracyStats\\(TP,\n  TN = accuracyStats\\)TN, FP = accuracyStats\\(FP,\n  FN = accuracyStats\\)FN) accuracyStats\\(negativeLikelihoodRatio &lt;- negativeLikelihoodRatio(\n  TP = accuracyStats\\)TP, TN = accuracyStats\\(TN,\n  FP = accuracyStats\\)FP, FN = accuracyStats$FN)\naccuracyStats\\(dPrimeSDT &lt;- dPrimeSDT(\n  TP = accuracyStats\\)TP, TN = accuracyStats\\(TN,\n  FP = accuracyStats\\)FP, FN = accuracyStats\\(FN)\naccuracyStats\\)betaSDT &lt;- betaSDT( TP = accuracyStats\\(TP,\n  TN = accuracyStats\\)TN, FP = accuracyStats\\(FP,\n  FN = accuracyStats\\)FN) accuracyStats\\(cSDT &lt;- cSDT(\n  TP = accuracyStats\\)TP, TN = accuracyStats\\(TN,\n  FP = accuracyStats\\)FP, FN = accuracyStats\\(FN)\naccuracyStats\\)ASDT &lt;- aSDT( TP = accuracyStats\\(TP,\n  TN = accuracyStats\\)TN, FP = accuracyStats\\(FP,\n  FN = accuracyStats\\)FN) accuracyStats\\(bSDT &lt;- bSDT(\n  TP = accuracyStats\\)TP, TN = accuracyStats\\(TN,\n  FP = accuracyStats\\)FP, FN = accuracyStats$FN)\naccuracyStats\\(overallUtility &lt;- Uoverall(\n  BR = accuracyStats\\)baseRate, HR = accuracyStats\\(TPrate,\n  FAR = accuracyStats\\)FPrate, UH = utilityHits, UM = utilityMisses, UCR = utilityCorrectRejections, UFA = utilityFalseAlarms) accuracyStats\\(utilityRatio &lt;- utilityRatio(\n  UH = utilityHits,\n  UM = utilityMisses,\n  UCR = utilityCorrectRejections,\n  UFA = utilityFalseAlarms)\naccuracyStats\\)informationGain &lt;- Igain( BR = accuracyStats\\(baseRate,\n  HR = accuracyStats\\)TPrate, FAR = accuracyStats$FPrate)\n#Replace NaN and INF values with NA is.nan.data.frame &lt;- function(x) do.call(cbind, lapply(x, is.nan))\naccuracyStats[is.nan.data.frame(accuracyStats)] &lt;- NA accuracyStats &lt;- do.call( data.frame, lapply( accuracyStats, function(x) replace(x, is.infinite(x), NA)))\n\n\n### All Accuracy Statistics {#allAccuracyStatistics}\n\nThe [`petersenlab`](https://github.com/DevPsyLab/petersenlab) package [@R-petersenlab] contains an R function that estimates the prediction accuracy at every possible cutoff.\\index{petersenlab package}\\index{cutoff}\n\n\n```{r}\naccuracyAtEachCutoff(\n  predicted = mydataSDT$testScore,\n  actual = mydataSDT$disorder,\n  UH = utilityHits,\n  UM = utilityMisses,\n  UCR = utilityCorrectRejections,\n  UFA = utilityFalseAlarms)\n{r, eval = knitr::is_html_output(excludes = \"epub\"), echo = knitr::is_html_output(excludes = \"epub\")} paged_table(accuracyStats)\n{r, eval = knitr::is_latex_output(), echo = knitr::is_latex_output()} accuracyStats %&gt;%    head %&gt;%    t %&gt;%    round(., 2) %&gt;%    kable(.,       format = \"latex\",       booktabs = TRUE) %&gt;%   kable_styling(latex_options = \"scale_down\")\n{r, eval = knitr::is_html_output(excludes = c(\"markdown\",\"html\",\"html4\",\"html5\",\"revealjs\",\"s5\",\"slideous\",\"slidy\",\"gfm\")), echo = knitr::is_html_output(excludes = c(\"markdown\",\"html\",\"html4\",\"html5\",\"revealjs\",\"s5\",\"slideous\",\"slidy\",\"gfm\"))} accuracyStats %&gt;%    head %&gt;%    t %&gt;%    round(., 2) %&gt;%    kable(., booktabs = TRUE)\n\n\n80.10.4 Youden’s J Statistic\n\n80.10.4.1 Threshold\nThreshold at maximum combination of sensitivity and specificity:\n\\(\\text{max}(\\text{sensitivity} + \\text{specificity})\\)\nyoudenIndex &lt;- coords(roc(data = mydataSDT,\n                          response = disorder,\n                          predictor = testScore,\n                          smooth = FALSE),\n                      x = \"best\",\n                      best.method = \"youden\")[[1]]\n\nyoudenIndex\n\n\n80.10.4.2 Accuracy statistics at cutoff of Youden’s J Statistic\naccuracyStats[head(which(\n  accuracyStats$cutoff &gt;= youdenIndex), 1),]\naccuracyStats[which(\n  accuracyStats$youdenJ == max(accuracyStats$youdenJ, na.rm = TRUE)),]\n\n\n\n80.10.5 Closest to the Top Left of the ROC Curve\n\n80.10.5.1 Threshold\nThreshold where the ROC plot is closest to the Top Left:\nclosestToTheTopLeft &lt;- coords(roc(\n  data = mydataSDT,\n  response = disorder,\n  predictor = testScore,\n  smooth = FALSE),\n  x = \"best\",\n  best.method = \"closest.topleft\")[[1]]\n\n\n80.10.5.2 Accuracy stats at cutoff where the ROC plot is closest to the Top Left\naccuracyStats[head(which(\n  accuracyStats$cutoff &gt;= closestToTheTopLeft), 1),]\n\n\n\n80.10.6 Cutoff that optimizes each of the following criteria:\nThe petersenlab package (R-petersenlab?) contains an R function that identifies the cutoff that optimizes each of various accuracy estimates.\noptimalCutoff(\n  predicted = mydataSDT$testScore,\n  actual = mydataSDT$disorder,\n  UH = utilityHits,\n  UM = utilityMisses,\n  UCR = utilityCorrectRejections,\n  UFA = utilityFalseAlarms)\n\n80.10.6.1 Percent Accuracy\n\naccuracyStats$cutoff[which(\n  accuracyStats$percentAccuracy == max(\n    accuracyStats$percentAccuracy,\n    na.rm = TRUE))]\n\n\n80.10.6.2 Percent Accuracy by Chance\n\naccuracyStats$cutoff[which(\n  accuracyStats$percentAccuracyByChance == max(\n    accuracyStats$percentAccuracyByChance,\n    na.rm = TRUE))]\n\n\n80.10.6.3 Relative Improvement Over Chance (ROIC)\n\naccuracyStats$cutoff[which(\n  accuracyStats$relativeImprovementOverChance == max(\n    accuracyStats$relativeImprovementOverChance,\n    na.rm = TRUE))]\n\n\n80.10.6.4 Relative Improvement Over Predicting from the Base Rate\n\naccuracyStats$cutoff[which(\n  accuracyStats$relativeImprovementOverPredictingFromBaseRate == max(\n    accuracyStats$relativeImprovementOverPredictingFromBaseRate,\n    na.rm = TRUE))]\n\n\n80.10.6.5 Sensitivity\n\naccuracyStats$cutoff[which(\n  accuracyStats$sensitivity == max(\n    accuracyStats$sensitivity,\n    na.rm = TRUE))]\n\n\n80.10.6.6 Specificity\n\naccuracyStats$cutoff[which(\n  accuracyStats$specificity == max(\n    accuracyStats$specificity,\n    na.rm = TRUE))]\n\n\n80.10.6.7 Positive Predictive Value\n\naccuracyStats$cutoff[which(\n  accuracyStats$positivePredictiveValue == max(\n    accuracyStats$positivePredictiveValue,\n    na.rm = TRUE))]\n\n\n80.10.6.8 Negative Predictive Value\n\naccuracyStats$cutoff[which(\n  accuracyStats$negativePredictiveValue == max(\n    accuracyStats$negativePredictiveValue,\n    na.rm = TRUE))]\n\n\n80.10.6.9 Youden’s J Statistic\n\naccuracyStats$cutoff[which(\n  accuracyStats$youdenJ == max(\n    accuracyStats$youdenJ,\n    na.rm = TRUE))]\n\n\n80.10.6.10 Balanced Accuracy\n\naccuracyStats$cutoff[which(\n  accuracyStats$balancedAccuracy == max(\n    accuracyStats$balancedAccuracy,\n    na.rm = TRUE))]\n\n\n80.10.6.11 F1 Score\n\naccuracyStats$cutoff[which(\n  accuracyStats$f1Score == max(\n    accuracyStats$f1Score,\n    na.rm = TRUE))]\n\n\n80.10.6.12 Matthews Correlation Coefficient\n\naccuracyStats$cutoff[which(\n  accuracyStats$mcc == max(\n    accuracyStats$mcc,\n    na.rm = TRUE))]\n\n\n80.10.6.13 Diagnostic Odds Ratio\n\naccuracyStats$cutoff[which(\n  accuracyStats$diagnosticOddsRatio == max(\n    accuracyStats$diagnosticOddsRatio,\n    na.rm = TRUE))]\n\n\n80.10.6.14 Positive Likelihood Ratio\n\naccuracyStats$cutoff[which(\n  accuracyStats$positiveLikelihoodRatio == max(\n    accuracyStats$positiveLikelihoodRatio,\n    na.rm = TRUE))]\n\n\n80.10.6.15 Negative Likelihood Ratio\n\naccuracyStats$cutoff[which(\n  accuracyStats$negativeLikelihoodRatio == min(\n    accuracyStats$negativeLikelihoodRatio,\n    na.rm = TRUE))]\n\n\n80.10.6.16 \\(d'\\) Sensitivity\n\naccuracyStats$cutoff[which(\n  accuracyStats$dPrimeSDT == max(\n    accuracyStats$dPrimeSDT,\n    na.rm = TRUE))]\n\n\n80.10.6.17 \\(A\\) (Non-Parametric) Sensitivity\n\naccuracyStats$cutoff[which(\n  accuracyStats$ASDT == max(\n    accuracyStats$ASDT,\n    na.rm = TRUE))]\n\n\n80.10.6.18 \\(\\beta\\) Bias\n\naccuracyStats$cutoff[which(abs(\n  accuracyStats$betaSDT) == min(abs(\n    accuracyStats$betaSDT),\n    na.rm = TRUE))]\n\n\n80.10.6.19 \\(c\\) Bias\n\naccuracyStats$cutoff[which(abs(\n  accuracyStats$cSDT) == min(abs(\n    accuracyStats$cSDT),\n    na.rm = TRUE))]\n\n\n80.10.6.20 \\(b\\) (Non-Parametric) Bias\n\naccuracyStats$cutoff[which(abs(\n  accuracyStats$bSDT) == min(abs(\n    accuracyStats$bSDT),\n    na.rm = TRUE))]\n\n\n80.10.6.21 Mean difference between predicted and observed values (Miscalibration)\n\naccuracyStats$cutoff[which(abs(\n  accuracyStats$differenceBetweenPredictedAndObserved) == min(abs(\n    accuracyStats$differenceBetweenPredictedAndObserved),\n    na.rm = TRUE))]\n\n\n80.10.6.22 Overall Utility\n\naccuracyStats$cutoff[which(\n  accuracyStats$overallUtility == max(\n    accuracyStats$overallUtility,\n    na.rm = TRUE))]\n\n\n80.10.6.23 Information Gain\n\naccuracyStats$cutoff[which(\n  accuracyStats$informationGain == max(\n    accuracyStats$informationGain,\n    na.rm = TRUE))]",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#regression-prediction",
    "href": "chapters/prediction/01_prediction.html#regression-prediction",
    "title": "80  Predizione",
    "section": "80.11 Regression for Prediction of Continuous Outcomes",
    "text": "80.11 Regression for Prediction of Continuous Outcomes\nWhen predicting a continuous outcome, regression is particularly relevant (or multiple regression, when dealing with multiple predictors). Regression takes the general form in Equation @ref(eq:regression):\n\\[\\begin{equation}\ny = b_0 + b_1 \\cdot x_1 + e\n(\\#eq:regression)\n\\end{equation}\\]\nwhere \\(y\\) is the outcome, \\(b_0\\) is the intercept, \\(b_1\\) is the slope of the association between the predictor (\\(x_1\\)) and outcome, and \\(e\\) is the error term.\n```{r, echo = FALSE} regression1 &lt;- data.frame( “y” = c(7, 13, 29, 10), “x1” = c(1, 2, 7, 2))\nregression2 &lt;- data.frame( “y” = c(7, 13, 29, 10), “x1” = c(1, 2, 7, 2), “x2” = c(3, 5, 1, 2))\nregression1_model &lt;- lm( y ~ x1, data = regression1)\nregression1_intercept &lt;- regression1_model\\(coefficients[[1]]\nregression1_slope &lt;- regression1_model\\)coefficients[[2]] regression1_rsquare &lt;- summary(regression1_model)$r.squared\nregression2_model &lt;- lm(y ~ x1 + x2, data = regression2) regression2_intercept &lt;- regression2_model\\(coefficients[[1]]\nregression2_slope1 &lt;- regression2_model\\)coefficients[[2]] regression2_slope2 &lt;- regression2_model\\(coefficients[[3]]\nregression2_rsquare &lt;- summary(regression2_model)\\)r.squared\n\n\n## Pseudo-Prediction {#pseudoPrediction}\n\nConsider the following example where you have one predictor and one outcome, as shown in Table \\@ref(tab:regression1).\\index{multiple regression}\n\n\n```{r regression1, echo = FALSE}\nkable(regression1,\n      col.names = c(\"y\",\"x1\"),\n      caption = \"Example Data of Predictor (x1) and Outcome (y) Used for Regression Model.\",\n      booktabs = TRUE)\nUsing the data, the best fitting regression model is: \\(y = `r apa(regression1_intercept, decimals = 2)` + `r apa(regression1_slope, decimals = 2)` \\cdot x_1\\). In this example, the \\(R^2\\) is \\(`r apa(regression1_rsquare, decimals = 2)`\\). The equation is not a perfect prediction, but with a single predictor, it captures the majority of the variance in the outcome.\nNow consider the following example where you add a second predictor to the data above, as shown in Table @ref(tab:regression2).\n{r regression2, echo = FALSE} kable(regression2,       col.names = c(\"y\",\"x1\",\"x2\"),       caption = \"Example Data of Predictors (x1 and x2) and Outcome (y) Used for Regression Model.\",       booktabs = TRUE)\nWith the second predictor, the best fitting regression model is: \\(y = `r apa(regression2_intercept, decimals = 2)` + `r apa(regression2_slope1, decimals = 2)` \\cdot x_1 + `r apa(regression2_slope2, decimals = 2)` \\cdot x_2\\). In this example, the \\(R^2\\) is \\(`r apa(regression2_rsquare, decimals = 2)`\\). The equation with the second predictor provides a perfect prediction of the outcome.\nProviding perfect prediction with the right set of predictors is the dream of multiple regression. So, in psychology, we often add predictors to incrementally improve prediction. Knowing how much variance would be accounted for by random chance follows Equation @ref(eq:predictionByChance):\n\\[\\begin{equation}\nE(R^2) = \\frac{K}{n-1}\n(\\#eq:predictionByChance)\n\\end{equation}\\]\nwhere \\(E(R^2)\\) is the expected value of \\(R^2\\) (the proportion of variance explained), \\(K\\) is the number of predictors, and \\(n\\) is the sample size. The formula demonstrates that the more predictors in the regression model, the more variance will be accounted for by chance. With many predictors and a small sample, you can account for a large share of the variance merely by chance—this would be an example of pseudo-prediction.\nAs an example, consider that we have 13 predictors to predict behavior problems for 43 children. Assume that, with 13 predictors, we explain 38% of the variance (\\(R^2 = .38; r = .62\\)). Explaining more than 20–30% of the variance can be a big deal in psychology. We explained a lot of the variance in the outcome, but it is important to consider how much variance could have been explained by random chance: \\(E(R^2) = \\frac{K}{n-1} = \\frac{13}{43 - 1} = .31\\). We expect to explain 31% of the variance, by chance, in the outcome. So, 82% of the variance explained was likely spurious. As the sample size increases, the spuriousness decreases. Adjusted \\(R^2\\) accounts for the number of predictors in the model, based on how much would be expected to be accounted for by chance. But adjusted \\(R^2\\) also has its problems.\n\n80.11.1 Multicollinearity\nMulticollinearity occurs when two or more predictors in a regression model are highly correlated. The problem is that it makes it challenging to estimate the regression coefficients accurately.\nMulticollinearity in multiple regression is depicted conceptually in Figure @ref(fig:multipleRegressionMulticollinearity).\n{r multipleRegressionMulticollinearity, out.width = \"50%\", fig.align = \"center\", fig.cap = \"Conceptual Depiction of Multicollinearity in Multiple Regression.\", echo = FALSE} knitr::include_graphics(\"./Images/multipleRegressionMulticollinearity.png\")\nConsider the following example where you have two predictors and one outcome, as shown in Table @ref(tab:regression3).\n{r, echo = FALSE} regression3 &lt;- data.frame(   \"y\" = c(9, 11, 17, 3, 21, 13),   \"x1\" = c(2, 3, 4, 1, 5, 3.5),   \"x2\" = c(4, 6, 8, 2, 10, 7))\n{r regression3, echo = FALSE} kable(regression3,       col.names = c(\"y\",\"x1\",\"x2\"),       caption = \"Example Data of Predictors (x1 and x2) and Outcome (y) Used for Regression Model.\",       booktabs = TRUE)\nThe second measure is not very good—it is exactly twice the value of the first measure. This means that there are different prediction equation possibilities that are equally good—see Equations in @ref(eq:multicollinearity):\n\\[\n\\begin{aligned}\n  2x_2 &= y \\\\\n  0x_1 + 2x_2 &= y \\\\\n  4x_1 &= y \\\\\n  4x_1 + 0x_2 &= y \\\\\n  2x_1 + 1x_2 &= y \\\\\n  5x_1 - 0.5x_2 &= y \\\\\n  ...\n&= y\n\\end{aligned}\n(\\#eq:multicollinearity)\n\\]\nThen, what are the regression coefficients? We do not know, and we could come up with arbitrary estimates with an enormous standard error around each estimate. Any predictors that have a correlation above ~ \\(r = .30\\) with each other could have an impact on the confidence interval of the regression coefficient. As the correlations among the predictors increase, the chance of getting an arbitrary answer increases, sometimes called “bouncing betas.” So, it is important to examine a correlation matrix of the predictors before putting them in the same regression model. You can also examine indices such as variance inflation factor (VIF).\nGeneralized VIF (GVIF) values are estimated below using the car package (R-car?).\nset.seed(52242)\nmydataSDT$collinearPredictor &lt;- mydataSDT$ndka + \nrnorm(nrow(mydataSDT), sd = 20)\n\ncollinearRegression_model &lt;- lm(\n  s100b ~ ndka + gender + age + wfns + collinearPredictor,\n  data = mydataSDT)\n\nvif(collinearRegression_model)\nTo address multicollinearity, you can drop a redundant predictor or you can also use principal component analysis or factor analysis of the predictors to reduce the predictors down to a smaller number of meaningful predictors. For a meaningful answer in a regression framework that is precise and confident, you need a low level of intercorrelation among predictors, unless you have a very large sample size.\nHowever, multicollinearity does not bias parameter estimates (i.e., multicollinearity does not lead to mean error). Instead, multicollinearity increases the uncertainty (i.e., standard errors) around the parameter estimates (archived at https://perma.cc/DJ7L-TCUK), which makes it more challenging to detect an effect as statistically significant. Some forms of multicollinearity are ignorable (archived at https://perma.cc/2JV5-2QEZ), including when the multicollinearity is among (a) the control variables rather than the variables of interest, (b) the powers (e.g., quadratic term) or products (e.g., interaction term) of other variables, or (c) dummy-coded categories. Ultimately, it is important to examine the question of interest, even if that means inclusion of predictors that are inter-correlated in a regression model (archived at https://perma.cc/DJ7L-TCUK). However, it would not make sense to include two predictors that are perfectly correlated, because they are redundant.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#waysToImprovePredictionAccuracy",
    "href": "chapters/prediction/01_prediction.html#waysToImprovePredictionAccuracy",
    "title": "80  Predizione",
    "section": "80.12 Ways to Improve Prediction Accuracy",
    "text": "80.12 Ways to Improve Prediction Accuracy\nOn the whole, experts’ predictions are inaccurate. Experts’ predictions from many different domains tend to be inaccurate, including political scientists (Tetlock2017?), physicians (Koehler2002?), clinical psychologists (Oskamp1965?), stock market traders and corporate financial officers (Skala2008?), seismologists’ predictions of earthquakes (Hough2016?), economists’ predictions about the economy (Makridakis2009?), lawyers (Koehler2002?), and business managers (Russo1992?). The most common pattern of experts’ predictions is that they show overextremity, that is, their predictions have probability judgments that tend to be too extreme, as described in Section @ref(calibration). Overextremity of experts’ predictions likely reflects over-confidence. The degree of confidence of a person’s predictions is often not a good indicator of the accuracy of their predictions [and confidence and prediction accuracy are sometimes inversely associated; (Silver2012?)]. Cognitive biases including the anchoring bias (Tversky1974?), the confirmation bias (Hoch1985?; Koriat1980?), and base rate neglect (Eddy1982?; Koehler2002?) could contribute to over-confidence of predictions. Poorly calibrated predictions are especially likely when the base rate is very low (e.g., suicide), as is often the case in clinical psychology, or when the base rate is very high (Koehler2002?).\nNevertheless, there are some domains that have shown greater predictive accuracy, from which we may learn what practices may lead to greater accuracy. For instance, experts have shown stronger predictive accuracy in weather forecasting (Murphy1984?), horse race betting (Johnson2001?), and playing the card game of bridge (Keren1987?), but see (Koehler2002?) for exceptions.\nHere are some potential ways to improve the accuracy (and honesty) of predictions and judgments:\n\nProvide appropriate anchoring of your predictions to the base rate of the phenomenon you are predicting. To the extent that the base rate of the event you are predicting is low, more extreme evidence should be necessary to consistently and accurately predict that the event will occur. Applying Bayes’ theorem and Bayesian approaches can help you appropriately weigh base rate and evidence.\nInclude multiple predictors, ideally from different measures and measurement methods. Include the predictors with the strongest validity based on theory of the causal process and based on criterion-related validity.\nWhen possible, aggregate multiple perspectives of predictions, especially predictions made independently (from different people/methods/etc.). The “wisdom of the crowd” is often more accurate than individuals’ predictions, including predictions by so-called “experts” (Silver2012?).\nA goal of prediction is to capture as much signal as possible and as little noise (error) as possible (Silver2012?). Parsimony (i.e., not having too many predictors) can help reduce the amount of error variance captured by the prediction model. However, to accurately model complex systems like human behavior, the brain, etc., complex models may be necessary. Nevertheless, strong theory of the causal processes and dynamics may be necessary to develop accurate complex models.\nAlthough incorporating theory can be helpful, provide more weight to empiricism than to theory, until our theories and measures are stronger. Ideally, we would use theory to design a model that mirrors the causal system, with accurate measures of each process in the system, so we could make accurate predictions. However, as described in Section @ref(theoryEmpiricism), our psychological theories of the causal processes that influence outcomes are not yet very strong. Until we have stronger theories that specify the causal process for a given outcome, and until we have accurate measures of those causal processes, actuarial approaches are likely to be most accurate, as discussed in Chapter @ref(actuarial). At the same time, keep in mind that measures in psychology, and their resulting data, are often noisy. As a result, theoretically (conceptually) informed empirical approaches may lead to more accuracy than empiricism alone.\nUse an empirically validated and cross-validated statistical algorithm to combine information from the predictors in a formalized way. Give each predictor appropriate weight in the statistical algorithm, according to its strength of association with the outcome. Use measures with strong reliability and validity for assessing these processes to be used in the algorithm. Cross-validation will help reduce the likelihood that your model is fitting to noise and will maximize the likelihood that the model predicts accurately when applied to new data (i.e., the model’s predictions accurately generalize), as described in Section @ref(modelAccuracy-actuarial).\nWhen presenting your predictions, acknowledge what you do not know.\nExpress your predictions in terms of probabilistic estimates and present the uncertainty in your predictions with confidence intervals [even though bolder, more extreme predictions tend to receive stronger television ratings; (Silver2012?)].\nQualify your predictions by identifying and noting counter-examples that would not be well fit by your prediction model, such as extreme cases, edge cases, and “broken leg” (Meehl1957?) cases.\nProvide clear, consistent, and timely feedback on the outcomes of the predictions to the people making the predictions (Bolger2004?).\nBe self-critical about your predictions. Update your judgments based on their accuracy, rather than trying to confirm your beliefs (Atanasov2020?).\nIn addition to considering the accuracy of the prediction, consider the quality of the prediction process, especially when random chance is involved to a degree (such as in poker) (Silver2012?).\nWork to identify and mitigate potential blindspots; be aware of cognitive biases, such as confirmation bias and base rate neglect.\nEvaluate for the possibility of bias in the predictions or in the tests from which the predictions are derived. Correct for any test bias.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#conclusion-prediction",
    "href": "chapters/prediction/01_prediction.html#conclusion-prediction",
    "title": "80  Predizione",
    "section": "80.13 Conclusion",
    "text": "80.13 Conclusion\nHuman behavior is challenging to predict. People commonly make cognitive pseudo-prediction errors, such as the confusion of inverse probabilities. People also tend to ignore base rates when making predictions. When the base rate of a behavior is very low or very high, you can be highly accurate in predicting the behavior by predicting from the base rate. Thus, you cannot judge how accurate your prediction is until you know how accurate your predictions would be by random chance. Moreover, maximizing percent accuracy may not be the ultimate goal because different errors have different costs. Though there are many types of accuracy, there are two broad types: discrimination and calibration—and they are orthogonal. Discrimination accuracy is frequently evaluated with the area under the receiver operating characteristic curve, or with sensitivity and specificity, or standardized regression coefficients. Calibration accuracy is frequently evaluated graphically and with various indices. Sensitivity and specificity depend on the cutoff. Therefore, the optimal cutoff depends on the purposes of the assessment and how much one weights the various costs of the different types of errors: false negatives and false positives. It is important to evaluate both discrimination and calibration when evaluating prediction accuracy.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#readings-prediction",
    "href": "chapters/prediction/01_prediction.html#readings-prediction",
    "title": "80  Predizione",
    "section": "80.14 Suggested Readings",
    "text": "80.14 Suggested Readings\n(Steyerberg2010?); (Meehl1955?); (Treat2023?); (Wiggins1973?)",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#exercises-prediction",
    "href": "chapters/prediction/01_prediction.html#exercises-prediction",
    "title": "80  Predizione",
    "section": "80.15 Exercises",
    "text": "80.15 Exercises\n{r, include = FALSE} library(\"MOTE\")\n```{r, include = FALSE} # Load Data —————————————————————\ntitanic &lt;- read_csv(here(“Data”, “titanic.csv”))\n\n```{r, include = FALSE}\n# ROC Curve ---------------------------------------------------------------\n\nrocCurve_ex &lt;- roc(data = titanic, response = survived, predictor = prediction, smooth = FALSE)\nplot(rocCurve_ex, legacy.axes = TRUE)\ncoords(rocCurve_ex, x = \"best\", best.method = \"youden\") #Youden's index (max combination of sensitivity and specificity) is at threshold of 0.205, where sensitivity is 0.65 and specificity is 0.80\ncoords(rocCurve_ex, x = \"best\", best.method = \"closest.topleft\") #The point closest to the top-left part of the ROC plot with perfect sensitivity or specificity: min((1 - sensitivity)^2 + (1 - specificity)^2)\n\nrocCurveSmooth_ex &lt;- roc(data = titanic, response = survived, predictor = prediction, smooth = TRUE)\nplot(rocCurveSmooth_ex, legacy.axes = TRUE)\n```{r, include = FALSE} # Overall Prediction Accuracy ———————————————\nmeanError_ex &lt;- meanError(predicted = titanic\\(prediction, actual = titanic\\)survived) meanAbsoluteError_ex &lt;- meanAbsoluteError(predicted = titanic\\(prediction, actual = titanic\\)survived) meanSquaredError_ex &lt;- meanSquaredError(predicted = titanic\\(prediction, actual = titanic\\)survived) rootMeanSquaredError_ex &lt;- rootMeanSquaredError(predicted = titanic\\(prediction, actual = titanic\\)survived) meanPercentageError_ex &lt;- meanPercentageError(predicted = titanic\\(prediction, actual = titanic\\)survived, dropUndefined = TRUE) meanAbsolutePercentageError_ex &lt;- meanAbsolutePercentageError(predicted = titanic\\(prediction, actual = titanic\\)survived, dropUndefined = TRUE) symmetricMeanAbsolutePercentageError_ex &lt;- symmetricMeanAbsolutePercentageError(predicted = titanic\\(prediction, actual = titanic\\)survived) meanAbsoluteScaledError_ex &lt;- meanAbsoluteScaledError(predicted = titanic\\(prediction, actual = titanic\\)survived) rootMeanSquaredLogError_ex &lt;- rootMeanSquaredLogError(predicted = titanic\\(prediction, actual = titanic\\)survived, dropUndefined = TRUE)\nsummary(lm(survived ~ prediction, data = titanic)) rsquared_ex &lt;- summary(lm(survived ~ prediction, data = titanic))\\(r.squared\nrsquaredAdj_ex &lt;- summary(lm(survived ~ prediction, data = titanic))\\)adj.r.squared predictiveRsquaredValue_ex &lt;- predictiveRSquared(predicted = titanic\\(prediction, actual = titanic\\)survived)\n\n```{r, include = FALSE}\n#Discrimination: Area under the ROC curve (AUC)\nrocCurve_ex$auc\n\nauc_ex &lt;- rocCurve_ex$auc\n{r, include = FALSE} #Calibration Plot (see here: https://perma.cc/6J3J-69G7) val.prob(titanic$prediction, titanic$survived)\n```{r, include = FALSE} g1_ex &lt;- mutate(titanic, bin = cut_number(prediction, 10)) %&gt;% # Bin prediction into 10ths group_by(bin) %&gt;% mutate(n = length(na.omit(prediction)), # Get ests and CIs bin_pred = mean(prediction, na.rm = TRUE), bin_prob = mean(survived, na.rm = TRUE), se = sd(survived, na.rm = TRUE) / sqrt(n), ul = bin_prob + qnorm(.975) * se, ll = bin_prob - qnorm(.975) * se) %&gt;% ungroup() %&gt;% ggplot(aes(x = bin_pred, y = bin_prob, ymin = ll, ymax = ul)) + geom_pointrange(size = 0.5, color = “black”) + scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.1)) + scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.1)) + geom_abline() + # 45 degree line indicating perfect calibration geom_smooth(method = “lm”, se = FALSE, linetype = “dashed”, color = “black”, formula = y~-1 + x) + # straight line fit through estimates geom_smooth(aes(x = prediction, y = survived), color = “red”, se = FALSE, method = “loess”) + # loess fit through estimates xlab(““) + ylab(”Observed Probability”) + theme_minimal()+ xlab(“Predicted Probability”)\ng2_ex &lt;- ggplot(titanic, aes(x = prediction)) + geom_histogram(fill = “black”, bins = 200) + scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.1)) + xlab(“Histogram of Predicted Probability”) + ylab(““) + theme_minimal() + theme(panel.grid.minor = element_blank())\ng_ex &lt;- arrangeGrob(g1_ex, g2_ex, respect = TRUE, heights = c(1, 0.25), ncol = 1) grid.arrange(g_ex)\n\n```{r, include = FALSE}\n#Calibration: Brier Scores\nval.prob(titanic$prediction, titanic$survived, pl = FALSE)[\"Brier\"]\n\n#Calibration: Hosler-Lemeshow Test\nhoslem.test(titanic$survived, titanic$prediction, g = 2)\nhoslem.test(titanic$survived, titanic$prediction, g = 4)\nhoslem.test(titanic$survived, titanic$prediction, g = 6)\nhoslem.test(titanic$survived, titanic$prediction, g = 8)\nhoslem.test(titanic$survived, titanic$prediction, g = 10)\n\n#Calibration: Spiegelhalter's z\nval.prob(titanic$prediction, titanic$survived, pl = FALSE)[\"S:z\"]; val.prob(titanic$prediction, titanic$survived, pl = FALSE)[\"S:p\"]\n\ncalibrationZ_ex &lt;- val.prob(titanic$prediction, titanic$survived, pl = FALSE)[\"S:z\"]\ncalibrationP_ex &lt;- val.prob(titanic$prediction, titanic$survived, pl = FALSE)[\"S:p\"]\n```{r, include = FALSE} # Set a Cutoff ————————————————————\ncutoff_ex &lt;- 0.5\ntitanic\\(diagnosis &lt;- NA\ntitanic\\)diagnosis[titanic\\(prediction &lt; cutoff_ex] &lt;- 0\ntitanic\\)diagnosis[titanic$prediction &gt;= cutoff_ex] &lt;- 1\n\n```{r, include = FALSE}\n# Prediction Accuracy at the Cutoff ---------------------------------------\n\ntable(titanic$diagnosis, titanic$survived)\n\nTPvalue_ex &lt;- length(which(titanic$diagnosis == 1 & titanic$survived == 1))\nTNvalue_ex &lt;- length(which(titanic$diagnosis == 0 & titanic$survived == 0))\nFPvalue_ex &lt;- length(which(titanic$diagnosis == 1 & titanic$survived == 0))\nFNvalue_ex &lt;- length(which(titanic$diagnosis == 0 & titanic$survived == 1))\n\nNvalue_ex &lt;- sampleSize(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n\nselectionRatioValue_ex &lt;- selectionRatio(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\nbaseRateValue_ex &lt;- baseRate(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n\npercentAccuracyValue_ex &lt;- percentAccuracy(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\npercentAccuracyByChanceValue_ex &lt;- percentAccuracyByChance(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\npercentAccuracyPredictingFromBaseRateValue_ex &lt;- percentAccuracyPredictingFromBaseRate(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\nrelativeImprovementOverChanceValue_ex &lt;- relativeImprovementOverChance(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\nrelativeImprovementOverPredictingFromBaseRateValue_ex &lt;- relativeImprovementOverPredictingFromBaseRate(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n\nsensitivityValue_ex &lt;- TPrate_ex &lt;- sensitivity(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\nspecificityValue_ex &lt;- TNrate_ex &lt;- specificity(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n\nFNrateValue_ex &lt;- falseNegativeRate(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\nFPrateValue_ex &lt;- falsePositiveRate(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n\npositivePredictiveValueValue_ex &lt;- positivePredictiveValue(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\nnegativePredictiveValueValue_ex &lt;- negativePredictiveValue(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n\nfalseDiscoveryRateValue_ex &lt;- falseDiscoveryRate(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\nfalseOmissionRateValue_ex &lt;- falseOmissionRate(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n\ndiagnosticOddsRatioValue_ex &lt;- diagnosticOddsRatio(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\npositiveLikelihoodRatioValue_ex &lt;- positiveLikelihoodRatio(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\nnegativeLikelihoodRatioValue_ex &lt;- negativeLikelihoodRatio(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n\nyoudenJValue_ex &lt;- youdenJ(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\nbalancedAccuracyValue_ex &lt;- balancedAccuracy(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\nf1ScoreValue_ex &lt;- fScore(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\nmccValue_ex &lt;- mcc(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n\ndPrimeValue_ex &lt;- dPrimeSDT(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\nbetaValue_ex &lt;- betaSDT(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\ncValue_ex &lt;- cSDT(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\nAValue_ex &lt;- aSDT(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\nbValue_ex &lt;- bSDT(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n\npredictingFromTheBaseRateValue_ex &lt;- max(baseRateValue_ex, 1 - baseRateValue_ex) * 100\nincreasedAccuracyValue_ex &lt;- percentAccuracyValue_ex - predictingFromTheBaseRateValue_ex\n\ndifferenceBetweenPredictedAndObserved_ex &lt;- miscalibration(predicted = titanic$prediction, actual = titanic$survived, cutoff = cutoff_ex)\n```{r, include = FALSE} # Decision Theory Approach to Cutoff Specification ————————\n#Overall utility of a specific cutoff value: utilities-weighted sum of the probabilities of the four decision-making outcomes\nUoverall(BR = baseRateValue_ex, HR = TPrate_ex, FAR = FPrateValue_ex, UH = 1, UM = 0, UCR = 0.75, UFA = 0.25) Uoverall(BR = baseRateValue_ex, HR = TPrate_ex, FAR = FPrateValue_ex, UH = 1, UM = 0, UCR = 1, UFA = 0) Uoverall(BR = baseRateValue_ex, HR = TPrate_ex, FAR = FPrateValue_ex, UH = 0.75, UM = 0.25, UCR = 1, UFA = 0)\nUoverall(BR = baseRateValue_ex, HR = TPrate_ex, FAR = FPrateValue_ex, UH = 1, UM = 0, UCR = 0.25, UFA = 0)\nUoverall_ex &lt;- Uoverall(BR = baseRateValue_ex, HR = TPrate_ex, FAR = FPrateValue_ex, UH = 1, UM = 0, UCR = 0.25, UFA = 0)\n\n```{r, include = FALSE}\n#Utility ratio: user-perceived relative importance of decisions about negative versus positive cases\n\nutilityRatio(UH = 1, UM = 0, UCR = (1/2), UFA = (1/2))\nutilityRatio(UH = 1, UM = 0, UCR = (2/3), UFA = (1/3))\nutilityRatio(UH = 1, UM = 0, UCR = 0.75, UFA = 0.25)\n\nutilityRatio(UH = 1, UM = 0, UCR = 1, UFA = 0)\n\nutilityRatio(UH = 0.75, UM = 0.25, UCR = 1, UFA = 0)\nutilityRatio(UH = (2/3), UM = (1/3), UCR = 1, UFA = 0)\nutilityRatio(UH = (1/2), UM = (1/2), UCR = 1, UFA = 0)\n\nutilityRatio(UH = 1, UM = 0, UCR = (1/3), UFA = 0)\nutilityRatio(UH = 1, UM = 0, UCR = 0.25, UFA = 0)\n\nutilityRatio_ex &lt;- utilityRatio(UH = 1, UM = 0, UCR = 0.25, UFA = 0)\n```{r, include = FALSE} # Information Theory Approach to Cutoff Specification ———————\nIgain(BR = baseRateValue_ex, HR = TPrate_ex, FAR = FPrateValue_ex) Igain_ex &lt;- Igain(BR = baseRateValue_ex, HR = TPrate_ex, FAR = FPrateValue_ex)\n\n```{r, include = FALSE}\n# Accuracy at Every Possible Cutoff ---------------------------------------\n\n#Specify utility of each outcome\nutilityHits_ex &lt;- 1\nutilityMisses_ex &lt;- 0\nutilityCorrectRejections_ex &lt;- 0.25\nutilityFalseAlarms_ex &lt;- 0\n\npossibleCutoffs_ex &lt;- unique(na.omit(titanic$prediction))\npossibleCutoffs_ex &lt;- possibleCutoffs_ex[order(possibleCutoffs_ex)]\npossibleCutoffs_ex &lt;- c(possibleCutoffs_ex, max(possibleCutoffs_ex, na.rm = TRUE) + 0.01)\n\naccuracyVariables_ex &lt;- c(\"cutoff\", \"TP\", \"TN\", \"FP\", \"FN\", \"differenceBetweenPredictedAndObserved\")\n\naccuracyStats_ex &lt;- data.frame(matrix(nrow = length(possibleCutoffs_ex), ncol = length(accuracyVariables_ex)))\nnames(accuracyStats_ex) &lt;- accuracyVariables_ex\n\nfor(i in 1:length(possibleCutoffs_ex)){\n  cutoff &lt;- possibleCutoffs_ex[i]\n  \n  titanic$diagnosis &lt;- NA\n  titanic$diagnosis[titanic$prediction &lt; cutoff] &lt;- 0\n  titanic$diagnosis[titanic$prediction &gt;= cutoff] &lt;- 1\n  \n  accuracyStats_ex[i, \"cutoff\"] &lt;- cutoff\n  accuracyStats_ex[i, \"TP\"] &lt;- length(which(titanic$diagnosis == 1 & titanic$survived == 1))\n  accuracyStats_ex[i, \"TN\"] &lt;- length(which(titanic$diagnosis == 0 & titanic$survived == 0))\n  accuracyStats_ex[i, \"FP\"] &lt;- length(which(titanic$diagnosis == 1 & titanic$survived == 0))\n  accuracyStats_ex[i, \"FN\"] &lt;- length(which(titanic$diagnosis == 0 & titanic$survived == 1))\n  \n  accuracyStats_ex[i, \"differenceBetweenPredictedAndObserved\"] &lt;- miscalibration(predicted = titanic$prediction, actual = titanic$survived, cutoff = cutoff)\n}\n\naccuracyStats_ex$N &lt;- sampleSize(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n\naccuracyStats_ex$selectionRatio &lt;- selectionRatio(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\naccuracyStats_ex$baseRate &lt;- baseRate(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n\naccuracyStats_ex$percentAccuracy &lt;- percentAccuracy(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\naccuracyStats_ex$percentAccuracyByChance &lt;- percentAccuracyByChance(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\naccuracyStats_ex$relativeImprovementOverChance &lt;- relativeImprovementOverChance(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\naccuracyStats_ex$relativeImprovementOverPredictingFromBaseRate &lt;- relativeImprovementOverPredictingFromBaseRate(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n\naccuracyStats_ex$sensitivity &lt;- accuracyStats_ex$TPrate &lt;- sensitivity(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\naccuracyStats_ex$specificity &lt;- accuracyStats_ex$TNrate &lt;- specificity(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\naccuracyStats_ex$FNrate &lt;- falseNegativeRate(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\naccuracyStats_ex$FPrate &lt;- falsePositiveRate(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n\naccuracyStats_ex$positivePredictiveValue &lt;- positivePredictiveValue(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\naccuracyStats_ex$negativePredictiveValue &lt;- negativePredictiveValue(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\naccuracyStats_ex$falseDiscoveryRate &lt;- falseDiscoveryRate(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\naccuracyStats_ex$falseOmissionRate &lt;- falseOmissionRate(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n\naccuracyStats_ex$diagnosticOddsRatio &lt;- diagnosticOddsRatio(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\naccuracyStats_ex$positiveLikelihoodRatio &lt;- positiveLikelihoodRatio(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\naccuracyStats_ex$negativeLikelihoodRatio &lt;- negativeLikelihoodRatio(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n\naccuracyStats_ex$youdenJ &lt;- youdenJ(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\naccuracyStats_ex$balancedAccuracy &lt;- balancedAccuracy(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\naccuracyStats_ex$f1Score &lt;- fScore(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\naccuracyStats_ex$mcc &lt;- mcc(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n\naccuracyStats_ex$dPrimeSDT &lt;- dPrimeSDT(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\naccuracyStats_ex$betaSDT &lt;- betaSDT(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\naccuracyStats_ex$cSDT &lt;- cSDT(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\naccuracyStats_ex$ASDT &lt;- aSDT(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\naccuracyStats_ex$bSDT &lt;- bSDT(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n\naccuracyStats_ex$overallUtility &lt;- Uoverall(BR = accuracyStats_ex$baseRate, HR = accuracyStats_ex$TPrate, FAR = accuracyStats_ex$FPrate, UH = utilityHits_ex, UM = utilityMisses_ex, UCR = utilityCorrectRejections_ex, UFA = utilityFalseAlarms_ex)\naccuracyStats_ex$utilityRatio &lt;- utilityRatio(UH = utilityHits_ex, UM = utilityMisses_ex, UCR = utilityCorrectRejections_ex, UFA = utilityFalseAlarms_ex)\naccuracyStats_ex$informationGain &lt;- Igain(BR = accuracyStats_ex$baseRate, HR = accuracyStats_ex$TPrate, FAR = accuracyStats_ex$FPrate)\n\n#Replace NaN and INF values with NA\nis.nan.data.frame &lt;- function(x)\n  do.call(cbind, lapply(x, is.nan))\n\naccuracyStats_ex[is.nan.data.frame(accuracyStats_ex)] &lt;- NA\naccuracyStats_ex &lt;- do.call(data.frame, lapply(accuracyStats_ex, function(x) replace(x, is.infinite(x), NA)))\n\n#All accuracy stats\naccuracyStats_ex\n```{r, include = FALSE} #Youden Index (maximum combination of sensitivity and specificity) youdenIndex_ex &lt;- coords(roc(data = titanic, response = survived, predictor = prediction, smooth = FALSE), x = “best”, best.method = “youden”)[[1]] closestToTheTopLeft_ex &lt;- coords(roc(data = titanic, response = survived, predictor = prediction, smooth = FALSE), x = “best”, best.method = “closest.topleft”)[[1]]\n#Accuracy stats at cutoff of Youden Index accuracyStats_ex[head(which(accuracyStats_ex\\(cutoff &gt;= youdenIndex_ex), 1),]\naccuracyStats_ex[which(accuracyStats_ex\\)youdenJ == max(accuracyStats_ex$youdenJ, na.rm = TRUE)),]\n\n```{r, include = FALSE}\n#Accuracy stats at cutoff where the ROC plot is closest to the Top Left\naccuracyStats_ex[head(which(accuracyStats_ex$cutoff &gt;= closestToTheTopLeft_ex), 1),]\n{r, include = FALSE} #Cutoff that optimizes: percentAccuracyCutoff_ex &lt;- accuracyStats_ex$cutoff[which(accuracyStats_ex$percentAccuracy == max(accuracyStats_ex$percentAccuracy, na.rm = TRUE))] percentAccuracyByChanceCutoff_ex &lt;- accuracyStats_ex$cutoff[which(accuracyStats_ex$percentAccuracyByChance == max(accuracyStats_ex$percentAccuracyByChance, na.rm = TRUE))] relativeImprovementOverChanceCutoff_ex &lt;- accuracyStats_ex$cutoff[which(accuracyStats_ex$relativeImprovementOverChance == max(accuracyStats_ex$relativeImprovementOverChance, na.rm = TRUE))] relativeImprovementOverPredictingFromBaseRateCutoff_ex &lt;- accuracyStats_ex$cutoff[which(accuracyStats_ex$relativeImprovementOverPredictingFromBaseRate == max(accuracyStats_ex$relativeImprovementOverPredictingFromBaseRate, na.rm = TRUE))] sensitivityCutoff_ex &lt;- accuracyStats_ex$cutoff[which(accuracyStats_ex$sensitivity == max(accuracyStats_ex$sensitivity, na.rm = TRUE))] specificityCutoff_ex &lt;- accuracyStats_ex$cutoff[which(accuracyStats_ex$specificity == max(accuracyStats_ex$specificity, na.rm = TRUE))] positivePredictiveValueCutoff_ex &lt;- accuracyStats_ex$cutoff[which(accuracyStats_ex$positivePredictiveValue == max(accuracyStats_ex$positivePredictiveValue, na.rm = TRUE))] negativePredictiveValueCutoff_ex &lt;- accuracyStats_ex$cutoff[which(accuracyStats_ex$negativePredictiveValue == max(accuracyStats_ex$negativePredictiveValue, na.rm = TRUE))] diagnosticOddsRatioCutoff_ex &lt;- accuracyStats_ex$cutoff[which(accuracyStats_ex$diagnosticOddsRatio == max(accuracyStats_ex$diagnosticOddsRatio, na.rm = TRUE))] positiveLikelihoodRatioCutoff_ex &lt;- accuracyStats_ex$cutoff[which(accuracyStats_ex$positiveLikelihoodRatio == max(accuracyStats_ex$positiveLikelihoodRatio, na.rm = TRUE))] negativeLikelihoodRatioCutoff_ex &lt;- accuracyStats_ex$cutoff[which(accuracyStats_ex$negativeLikelihoodRatio == min(accuracyStats_ex$negativeLikelihoodRatio, na.rm = TRUE))] youdenJCutoff_ex &lt;- accuracyStats_ex$cutoff[which(accuracyStats_ex$youdenJ == max(accuracyStats_ex$youdenJ, na.rm = TRUE))] balancedAccuracyCutoff_ex &lt;- accuracyStats_ex$cutoff[which(accuracyStats_ex$balancedAccuracy == max(accuracyStats_ex$balancedAccuracy, na.rm = TRUE))] f1ScoreCutoff_ex &lt;- accuracyStats_ex$cutoff[which(accuracyStats_ex$f1Score == max(accuracyStats_ex$f1Score, na.rm = TRUE))] mccCutoff_ex &lt;- accuracyStats_ex$cutoff[which(accuracyStats_ex$mcc == max(accuracyStats_ex$mcc, na.rm = TRUE))] dPrimeSDTCutoff_ex &lt;- accuracyStats_ex$cutoff[which(accuracyStats_ex$dPrimeSDT == max(accuracyStats_ex$dPrimeSDT, na.rm = TRUE))] betaSDTCutoff_ex &lt;- accuracyStats_ex$cutoff[which(abs(accuracyStats_ex$betaSDT) == min(abs(accuracyStats_ex$betaSDT), na.rm = TRUE))] cSDTCutoff_ex &lt;- accuracyStats_ex$cutoff[which(abs(accuracyStats_ex$cSDT) == min(abs(accuracyStats_ex$cSDT), na.rm = TRUE))] ASDTCutoff_ex &lt;- accuracyStats_ex$cutoff[which(accuracyStats_ex$ASDT == max(accuracyStats_ex$ASDT, na.rm = TRUE))] bSDTCutoff_ex &lt;- accuracyStats_ex$cutoff[which(abs(accuracyStats_ex$bSDT) == min(abs(accuracyStats_ex$bSDT), na.rm = TRUE))] differenceBetweenPredictedAndObservedCutoff_ex &lt;- accuracyStats_ex$cutoff[which(abs(accuracyStats_ex$differenceBetweenPredictedAndObserved) == min(abs(accuracyStats_ex$differenceBetweenPredictedAndObserved), na.rm = TRUE))] overallUtilityCutoff_ex &lt;- accuracyStats_ex$cutoff[which(accuracyStats_ex$overallUtility == max(accuracyStats_ex$overallUtility, na.rm = TRUE))] informationGainCutoff_ex &lt;- accuracyStats_ex$cutoff[which(accuracyStats_ex$informationGain == max(accuracyStats_ex$informationGain, na.rm = TRUE))]\n```{r, include = FALSE} # Positive and Negative Predictive Value ———————————-\nsensitivity &lt;- .9 specificity &lt;- .95 baseRate &lt;- .05 positivePredictiveValue &lt;- (sensitivitybaseRate)/(sensitivitybaseRate+(1-specificity)(1-baseRate)) negativePredictiveValue &lt;- (specificity(1-baseRate))/(specificity(1-baseRate)+(1-sensitivity)baseRate)\nprYouHadCovid &lt;- 1 - negativePredictiveValue prFriendHadCovid &lt;- positivePredictiveValue\n\n```{r, include = FALSE}\n# Bayes' Theorem ----------------------------------------------------------\n\n#p(C \\mid R)=(p(R \\mid C)∙p(C))/(p(R))\n\n#p(disease \\mid positiveTest) = (p(positiveTest \\mid disease)∙p(disease))/(p(positiveTest))\n\npDisease &lt;- .005\npPositiveTest &lt;- .02\npPositiveTestGivenDisease &lt;- .99\n\npDiseaseGivenPositiveTest &lt;- (pPositiveTestGivenDisease * pDisease)/pPositiveTest * 100\n\n# Posttest Probability\nprobGivenTestA_ex &lt;- posttestProbability(pretestProb = .005, SN = .95, SP = .90)\nprobGivenTestAthenB_ex &lt;- posttestProbability(pretestProb = probGivenTestA_ex, SN = .80, SP = .95)\n\npctGivenTestA_ex &lt;- probGivenTestA_ex * 100\npctGivenTestAthenB_ex &lt;- probGivenTestAthenB_ex * 100\n\n80.15.1 Questions\nNote: Several of the following questions use data from the survivorship of the Titanic accident. The data are publicly available (https://hbiostat.org/data/; archived at https://perma.cc/B4AV-YH4V). The Titanic data file for these exercises is located on the book’s page of the Open Science Framework (https://osf.io/3pwza) and in the GitHub repo (https://github.com/isaactpetersen/Principles-Psychological-Assessment/tree/main/Data). Every record in the data set represents a passenger—including the passenger’s age, sex, passenger class, number of siblings/spouses aboard (sibsp), number of parents/children aboard (parch) and, whether the passenger survived the accident. I used these variables to create a prediction model (based on a logistic regression model using Leave-10-out cross-validation) for whether the passenger survived the accident. The model’s prediction for the passenger’s likelihood of survival are in the variable called “prediction”.\n\nWhat are the two main types of prediction accuracy? Define each. How can you quantify each? What is an example of an index that combines both main types of prediction accuracy?\nProvide the following indexes of overall prediction accuracy for the prediction model in predicting whether a passenger survived the Titanic:\n\nMean error (bias)\nMean absolute error (MAE)\nMean squared error (MSE)\nRoot mean squared error (RMSE)\nMean percentage error (MPE)\nMean absolute percentage error (MAPE)\nSymmetric mean absolute percentage error (sMAPE)\nMean absolute scaled error (MASE)\nRoot mean squared log error (RMSLE)\nCoefficient of determination (\\(R^2\\))\nAdjusted \\(R^2\\) (\\(R^2_{adj}\\))\nPredictive \\(R^2\\)\n\nBased on the mean error for the prediction model you found in 2a, what does this indicate?\nCreate two receiver operating characteristic (ROC) curves for the prediction model in predicting whether a passenger survived the Titanic: one empirical ROC curve and one smooth ROC curve.\nWhat is the area under the ROC curve (AUC) for the prediction model in predicting whether a passenger survived the Titanic? What does this indicate?\nCreate a calibration plot. Are the predictions well-calibrated? Provide empirical evidence and support your inferences with interpretation of the calibration plot. If the predictions are miscalibrated, describe the type of miscalibration present.\nYou predict that a passenger survived the Titanic if their predicted probability of survival is .50 or greater. Create a confusion matrix (2x2 matrix of prediction accuracy) for Titanic survival using this threshold. Make sure to include the marginal cells. Label each cell. Enter the number and proportion in each cell.\nUsing the 2x2 prediction matrix, identify or calculate the following:\n\nSelection ratio\nBase rate\nPercent accuracy\nPercent accuracy by chance\nPercent accuracy predicting from the base rate\nRelative improvement over chance (ROIC)\nRelative improvement over predicting from the base rate\nSensitivity (true positive rate [TPR])\nSpecificity (true negative rate [TNR])\nFalse negative rate (FNR)\nFalse positive rate (FPR)\nPositive predictive value (PPV)\nNegative predictive value (NPV)\nFalse discovery rate (FDR)\nFalse omission rate (FOR)\nDiagnostic odds ratio\nYouden’s J statistic\nBalanced accuracy\n\\(F_1\\) score\nMatthews correlation coefficient (MCC)\nPositive likelihood ratio\nNegative likelihood ratio\n\\(d'\\) sensitivity\n\\(A\\) (non-parametric) sensitivity\n\\(\\beta\\) bias\n\\(c\\) bias\naa. \\(b\\) (non-parametric) bias\nab. Miscalibration (mean difference between predicted and observed values; based on 10 groups)\nac. Information gain\n\nIn terms of percent accuracy, how much more accurate are the predictions compared to predicting from the base rate? What would happen to sensitivity and specificity if you raise the selection ratio? What would happen if you lower the selection ratio?\nFor your assessment goals, it is 4 times more important to identify survivors than to identify non-survivors. Consistent with these assessment goals, you specify the following utility for each of the four outcomes: hits: 1; misses: 0; correct rejections: 0.25; false alarms: 0. What is the utility ratio? What is the overall utility (\\(U_\\text{overall}\\)) of the current cutoff? What cutoff has the highest overall utility?\nFind the optimal cutoff threshold that optimizes each of the following criteria:\n\nYouden’s J statistic\nClosest to the top left of the ROC curve\nPercent accuracy\nPercent accuracy by chance\nRelative improvement over chance (ROIC)\nRelative improvement over predicting from the base rate\nSensitivity (true positive rate [TPR])\nSpecificity (true negative rate [TNR])\nPositive predictive value (PPV)\nNegative predictive value (NPV)\nDiagnostic odds ratio\nBalanced accuracy\n\\(F_1\\) score\nMatthews correlation coefficient (MCC)\nPositive likelihood ratio\nNegative likelihood ratio\n\\(d'\\) sensitivity\n\\(A\\) (non-parametric) sensitivity\n\\(\\beta\\) bias\n\\(c\\) bias\n\\(b\\) (non-parametric) bias\nMiscalibration (mean difference between predicted and observed values; based on 10 groups)\nOverall utility\nInformation gain\n\nA company develops a test that seeks to determine whether someone has been previously infected with a novel coronavirus (COVID-75) based on the presence of antibodies in their blood. You take the test and your test result is negative (i.e., the test says that you have not been infected). Your friend takes the test and their test result is positive for coronavirus (i.e., the test says that your friend has been infected). Assume the prevalence of the coronavirus is 5%, the sensitivity of the test is .90, and the specificity of the test is .95.\n\nWhat is the probability that you actually had the coronavirus?\nWhat is the probability that your friend actually had the coronavirus?\nYour friend thinks that, given their positive test, that they have the antibodies (and thus may have immunity). What is the problem with your friend’s logic?\nWhat logical fallacy is your friend demonstrating?\nWhy is it challenging to interpret a positive test in this situation?\n\nYou just took a screening test for a genetic disease. Your test result is positive (i.e., the tests says that you have the disease). Assume the probability of having the disease is 0.5%, the probability of a positive test is 2%, and the probability of a positive test if you have the disease is 99%. What is the probability that you have the genetic disease?\nYou just took a screening test (Test A) for a virus. Your test result is positive. Assume the base rate of the virus is .05%. Test A has a sensitivity of .95 and a specificity of .90.\n\nWhat is your probability of having the virus after testing positive on Test A?\nAfter getting the results back from Test A, the physician wants greater confidence regarding whether you have the virus given its low base rate, so the physician orders a second test, Test B. You test positive on Test B. Test B has a sensitivity of .80 and a specificity of .95. Assuming the errors of the Tests A and B are independent, what is your updated probability of having the virus?\n\n\n\n\n80.15.2 Answers\n\nThe two main types of prediction accuracy are discrimination and calibration. Discrimination refers to the ability of a prediction model to separate/differentiate data into classes (e.g., survived versus died). Calibration for a prediction model refers to how well the predicted probability of an event (e.g., survival) matches the true probability of an event. You can quantify discrimination with AUC; you can quantify various aspects of discrimination with sensitivity, specificity, positive predictive value, and negative predictive value). You can quantify degree of (poor) calibration with Spiegelhalter’s \\(z\\), though other metrics also exist (e.g., Brier scores and the Hosmer-Lemeshow goodness-of-fit statistic). \\(R^2\\) is an overall index of accuracy that combines both discrimination and calibration.\n\nMean error (bias): \\(`r format(round(meanError_ex, 4), scientific = FALSE)`\\)\nMean absolute error (MAE): \\(`r apa(meanAbsoluteError_ex, decimals = 2)`\\)\nMean squared error (MSE): \\(`r apa(meanSquaredError_ex, decimals = 2)`\\)\nRoot mean squared error (RMSE): \\(`r apa(rootMeanSquaredError_ex, decimals = 2)`\\)\nMean percentage error (MPE): undefined, but when dropping undefined values: \\(`r apa(meanPercentageError_ex, decimals = 2)`\\%\\)\nMean absolute percentage error (MAPE): undefined, but when dropping undefined values: \\(`r apa(meanAbsolutePercentageError_ex, decimals = 2)`\\%\\)\nSymmetric mean absolute percentage error (sMAPE): \\(`r apa(symmetricMeanAbsolutePercentageError_ex, decimals = 2)`\\%\\)\nMean absolute scaled error (MASE): \\(`r apa(meanAbsoluteScaledError_ex, decimals = 2)`\\)\nRoot mean squared log error (RMSLE): \\(`r apa(rootMeanSquaredLogError_ex, decimals = 2)`\\)\nCoefficient of determination (\\(R^2\\)): \\(`r apa(rsquared_ex, decimals = 2, leading = FALSE)`\\)\nAdjusted \\(R^2\\) (\\(R^2_{adj}\\)): \\(`r apa(rsquaredAdj_ex, decimals = 2, leading = FALSE)`\\)\nPredictive \\(R^2\\): \\(`r apa(predictiveRsquaredValue_ex, decimals = 2, leading = FALSE)`\\)\n\nThe small mean error/bias \\((`r format(round(meanError_ex, 4), scientific = FALSE)`)\\) indicates that predictions did not consistently under- or over-estimate the actual values to a considerable degree.\nEmpirical ROC curve:\n\n{r, echo = FALSE, fig.width = 8, fig.height = 8, fig.align = \"center\", fig.cap = c(\"Exercise 4: Empirical Receiver Operating Characteristic Curve.\")} plot(rocCurve_ex, legacy.axes = TRUE, asp = NA)\nSmooth ROC curve:\n{r, echo = FALSE, fig.width = 8, fig.height = 8, fig.align = \"center\", fig.cap = c(\"Exercise 4: Smooth Receiver Operating Characteristic Curve.\")} plot(rocCurveSmooth_ex, legacy.axes = TRUE, asp = NA)\n\nThe AUC is \\(`r apa(auc_ex, decimals = 2, leading = FALSE)`\\). AUC indicates the probability that a randomly selected case has a higher test result than a randomly selected control. Thus, the probability is \\(`r as.integer(auc_ex * 100)`\\%\\) that a randomly selected passenger who survived had a higher predicted probability of survival (based on the prediction model) than a randomly selected passenger who died. The AUC of \\(`r apa(auc_ex, decimals = 2, leading = FALSE)`\\) indicates that the prediction model was moderately accurate in terms of discrimination.\nCalibration plot:\n\n{r, echo = FALSE, fig.width = 8, fig.height = 8, fig.align = \"center\", fig.cap = c(\"Exercise 5: Calibration Plot of Predicted Probability Versus Observed Probability.\")} grid.arrange(g_ex)\n\nIn general, the predictions are well-calibrated. There is not significant miscalibration according to Spiegehalter’s \\(z\\) \\((`r apa(calibrationZ_ex, decimals = 2)`, p = `r apa(calibrationP_ex, decimals = 2, leading = FALSE)`)\\). This is verified graphically in the calibration plot, in which the predicted probabilities fall mostly near the actual/observed probabilities. However, based on the calibration plot, there does appear to be some miscalibration. When the predicted probability of survival was ~60%, the actual probability of survival was lower (~40%). This pattern of miscalibration is known as overprediction, as depicted in Figure 1 of (Lindhiem2020?).\nThe 2x2 prediction matrix is below:\n\n{r twoByTwoMatrix, out.width = \"100%\", fig.align = \"center\", fig.cap = \"Exercise 6: 2x2 Prediction Matrix. TP = true positives; TN = true negatives; FP = false positives; FN = false negatives; BR = base rate; SR = selection ratio.\", echo = FALSE} knitr::include_graphics(\"./Images/2x2-Matrix.png\")\n\n\nSelection ratio: \\(`r apa(selectionRatioValue_ex, decimals = 2, leading = FALSE)`\\)\nBase rate: \\(`r apa(baseRateValue_ex, decimals = 2, leading = FALSE)`\\)\nPercent accuracy: \\(`r as.integer(percentAccuracyValue_ex)`\\%\\)\nPercent accuracy by chance: \\(`r as.integer(percentAccuracyByChanceValue_ex)`\\%\\)\nPercent accuracy predicting from the base rate: \\(`r as.integer(percentAccuracyPredictingFromBaseRateValue_ex)`\\%\\)\nRelative improvement over chance (ROIC): \\(`r apa(relativeImprovementOverChanceValue_ex, decimals = 2, leading = FALSE)`\\)\nRelative improvement over predicting from the base rate: \\(`r apa(relativeImprovementOverPredictingFromBaseRateValue_ex, decimals = 2, leading = FALSE)`\\)\nSensitivity (true positive rate [TPR]): \\(`r apa(sensitivityValue_ex, decimals = 2, leading = FALSE)`\\)\nSpecificity (true negative rate [TNR]): \\(`r apa(specificityValue_ex, decimals = 2, leading = FALSE)`\\)\nFalse negative rate (FNR): \\(`r apa(FNrateValue_ex, decimals = 2, leading = FALSE)`\\)\nFalse positive rate (FPR): \\(`r apa(FPrateValue_ex, decimals = 2, leading = FALSE)`\\)\nPositive predictive value (PPV): \\(`r apa(positivePredictiveValueValue_ex, decimals = 2, leading = FALSE)`\\)\nNegative predictive value (NPV): \\(`r apa(negativePredictiveValueValue_ex, decimals = 2, leading = FALSE)`\\)\nFalse discovery rate (FDR): \\(`r apa(falseDiscoveryRateValue_ex, decimals = 2, leading = FALSE)`\\)\nFalse omission rate (FOR): \\(`r apa(falseOmissionRateValue_ex, decimals = 2, leading = FALSE)`\\)\nDiagnostic odds ratio: \\(`r apa(diagnosticOddsRatioValue_ex, decimals = 2, leading = TRUE)`\\)\nYouden’s J statistic: \\(`r apa(youdenJValue_ex, decimals = 2, leading = FALSE)`\\)\nBalanced accuracy: \\(`r apa(balancedAccuracyValue_ex, decimals = 2, leading = FALSE)`\\)\n\\(F_1\\) score: \\(`r apa(f1ScoreValue_ex, decimals = 2, leading = TRUE)`\\)\nMatthews correlation coefficient (MCC): \\(`r apa(mccValue_ex, decimals = 2, leading = FALSE)`\\)\nPositive likelihood ratio: \\(`r apa(positiveLikelihoodRatioValue_ex, decimals = 2, leading = TRUE)`\\)\nNegative likelihood ratio: \\(`r apa(negativeLikelihoodRatioValue_ex, decimals = 2, leading = TRUE)`\\)\n\\(d'\\) sensitivity: \\(`r apa(dPrimeValue_ex, decimals = 2)`\\)\n\\(A\\) (non-parametric) sensitivity: \\(`r apa(AValue_ex, decimals = 2)`\\)\n\\(\\beta\\) bias: \\(`r apa(betaValue_ex, decimals = 2)`\\)\n\\(c\\) bias: \\(`r apa(cValue_ex, decimals = 2)`\\) aa. \\(b\\) (non-parametric) bias: \\(`r apa(bValue_ex, decimals = 2)`\\) ab. Miscalibration (mean difference between predicted and observed values): \\(`r apa(differenceBetweenPredictedAndObserved_ex, decimals = 2, leading = TRUE)`\\) ac. Information gain: \\(`r apa(Igain_ex, decimals = 2, leading = TRUE)`\\)\n\nPredicting from the base rate would have a percent accuracy of \\(`r as.integer(predictingFromTheBaseRateValue_ex)`\\%\\). So, the predictions increase the percent accuracy by \\(`r as.integer(increasedAccuracyValue_ex)`\\%\\). If you raise the selection ratio (i.e., predict more people survived) sensitivity will increase whereas specificity will decrease. If you lower the selection ratio, specificity will increase and sensitivity will decrease.\nThe utility ratio is \\(`r apa(utilityRatio_ex, decimals = 2)`\\). The overall utility (\\(U_\\text{overall}\\)) of the cutoff is \\(`r apa(Uoverall_ex, decimals = 2)`\\). The cutoff with the highest overall utility is \\(`r apa(overallUtilityCutoff_ex, decimals = 3)`\\).\nThe cutoff that optimizes each of the following criteria:\n\nYouden’s J statistic: \\(`r apa(youdenIndex_ex, decimals = 3)`\\)\nClosest to the top left of the ROC curve: \\(`r apa(closestToTheTopLeft_ex, decimals = 3)`\\)\nPercent accuracy: \\(`r apa(percentAccuracyCutoff_ex, decimals = 3)`\\)\nPercent accuracy by chance: 1 (i.e., predicting from the base rate—that nobody will survive)\nRelative improvement over chance (ROIC): \\(`r apa(relativeImprovementOverChanceCutoff_ex, decimals = 3, leading = FALSE)`\\)\nRelative improvement over predicting from the base rate: \\(`r apa(relativeImprovementOverPredictingFromBaseRateCutoff_ex, decimals = 3, leading = FALSE)`\\)\nSensitivity (true positive rate [TPR]): 0 (i.e., predicting that everyone will survive will minimize false negatives)\nSpecificity: 1 (i.e., predicting that nobody will survive will minimize false positives)\nPositive predictive value (PPP): \\(`r apa(positivePredictiveValueCutoff_ex, decimals = 3)`\\)\nNegative predictive value (NPV): \\(`r apa(negativePredictiveValueCutoff_ex, decimals = 3)`\\)\nDiagnostic odds ratio: \\(`r apa(diagnosticOddsRatioCutoff_ex, decimals = 3)`\\)\nBalanced accuracy: \\(`r apa(balancedAccuracyCutoff_ex, decimals = 3)`\\)\n\\(F_1\\) score: \\(`r apa(f1ScoreCutoff_ex, decimals = 3)`\\)\nMatthews correlation coefficient (MCC): \\(`r apa(mccCutoff_ex, decimals = 3)`\\)\nPositive likelihood ratio: \\(`r apa(positiveLikelihoodRatioCutoff_ex, decimals = 3)`\\)\nNegative likelihood ratio: \\(`r apa(negativeLikelihoodRatioCutoff_ex, decimals = 3)`\\)\nMiscalibration (mean difference between predicted and observed values): \\(`r apa(min(differenceBetweenPredictedAndObservedCutoff_ex), decimals = 3)`–`r apa(max(differenceBetweenPredictedAndObservedCutoff_ex), decimals = 3)`\\)\n\\(d'\\) sensitivity: \\(`r apa(dPrimeSDTCutoff_ex, decimals = 3)`\\)\n\\(A\\) (non-parametric) sensitivity \\(`r apa(ASDTCutoff_ex, decimals = 3)`\\)\n\\(\\beta\\) bias \\(`r apa(betaSDTCutoff_ex, decimals = 3)`\\)\n\\(c\\) bias \\(`r apa(cSDTCutoff_ex, decimals = 3)`\\)\n\\(b\\) (non-parametric) bias \\(`r apa(bSDTCutoff_ex, decimals = 3)`\\)\nOverall utility: \\(`r apa(overallUtilityCutoff_ex, decimals = 3)`\\)\nInformation gain: \\(`r apa(informationGainCutoff_ex, decimals = 3)`\\)\n\n\nBased on negative predictive value (i.e., the probability of no disease given a negative test), the probability that you actually had the coronavirus is less than 1 in 100 \\((`r apa(prYouHadCovid, decimals = 3, leading = FALSE)`)\\).\nBased on positive predictive value (i.e., the probability of disease given a positive test), the probability that your friend actually had the coronavirus is less than 50% \\((`r apa(prFriendHadCovid, decimals = 2, leading = FALSE)`)\\).\nThe problem with your friend’s logic is that your friend is assuming they have the antibodies when chances are more likely that they do not.\nYour friend is likely demonstrating the fallacy of base rate neglect.\nA positive test on a screening device is hard to interpret in this situation because of the low base rate. “Even with a very accurate test, the fewer people in a population who have a condition, the more likely it is that an individual’s positive result is wrong.” For more info, see here: https://www.scientificamerican.com/article/coronavirus-antibody-tests-have-a-mathematical-pitfall/ (archived at https://perma.cc/GL9F-EVPH)\n\nAccording to Bayes’ theorem, the probability that you have the disease is \\(`r apa(pDiseaseGivenPositiveTest, decimals = 2)`\\%\\). For more info, see here: https://www.scientificamerican.com/article/what-is-bayess-theorem-an/ (archived at https://perma.cc/GM6B-5MEP)\n\nAccording to Bayes’ theorem, the probability that you have the virus after testing positive on Test A is \\(`r apa(pctGivenTestA_ex, decimals = 1)`\\%\\).\nAccording to Bayes’ theorem, the updated probability that you have the virus after testing positive on both Tests A and B is \\(`r apa(pctGivenTestAthenB_ex, decimals = 1)`\\%\\).\n\n\n\n\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. CRC Press.",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/prediction/01_prediction.html#footnotes",
    "href": "chapters/prediction/01_prediction.html#footnotes",
    "title": "80  Predizione",
    "section": "",
    "text": ":alnum:↩︎\n:alnum:↩︎",
    "crumbs": [
      "Predizione",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Predizione</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html",
    "href": "chapters/networks/01_networks.html",
    "title": "81  Network psicologici",
    "section": "",
    "text": "81.1 Introduzione\nQuesto capitolo costituisce un semplice riassunto semplificato del capitolo Psychological Networks: A Modern Approach to Analysis of Learning and Complex Learning Processes del testo di Saqr & López-Pernas (2024).",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#la-network-analysis",
    "href": "chapters/networks/01_networks.html#la-network-analysis",
    "title": "81  Network psicologici",
    "section": "81.2 La Network Analysis",
    "text": "81.2 La Network Analysis\nCome messo in evidenza da Saqr & López-Pernas (2024), l’analisi delle reti rappresenta un potente strumento per i ricercatori, utile per mappare relazioni, individuare connessioni e scoprire cluster o comunità tra componenti interagenti. Per questo motivo, è diventata una delle metodologie più importanti per comprendere i sistemi complessi. In psicologia, l’analisi delle reti ha acquisito sempre più rilevanza, non solo per lo studio delle interazioni sociali, ma anche per esplorare sistemi più astratti come i processi cognitivi, emotivi o comportamentali. In questo contesto, si utilizzano le reti probabilistiche, un tipo di rete in cui i nodi rappresentano variabili (come indicatori di costrutti psicologici o punteggi di scala) e gli archi esprimono l’entità delle associazioni probabilistiche tra queste variabili.\nUn esempio particolarmente rilevante in psicologia è costituito dai Gaussian Graphical Models (GGM), nei quali i nodi possono rappresentare costrutti come emozioni, comportamenti, atteggiamenti o tratti psicologici, e gli archi riflettono le correlazioni parziali tra questi nodi. Una correlazione parziale misura la relazione tra due variabili controllando o aggiustando per l’effetto di tutte le altre variabili nella rete, il cosiddetto principio del ceteris paribus.\nAd esempio, immaginiamo di modellare una rete psicologica in cui i nodi rappresentano motivazione, successo accademico, coinvolgimento, autoregolazione e benessere. Se osserviamo un arco che connette benessere e successo accademico, ciò indica che il benessere è associato al successo accademico indipendentemente dagli effetti di motivazione, coinvolgimento e autoregolazione (le altre variabili del sistema). Al contrario, l’assenza di un arco tra due nodi suggerisce che le due variabili sono condizionalmente indipendenti, una volta considerati gli effetti di tutte le altre variabili nella rete.\nLe reti psicologiche offrono anche strumenti rigorosi per valutare l’accuratezza delle stime, la forza degli archi e le misure di centralità attraverso tecniche come il bootstrapping. Inoltre, è possibile simulare la variabilità campionaria per stimare la replicabilità attesa delle reti studiate, aumentando la robustezza dei risultati.\nPer fare un esempio pratico, consideriamo una rete che rappresenta le relazioni tra emozioni negative (ansia, tristezza, rabbia), pensieri disfunzionali e strategie di coping. Se identifichiamo un’interazione tra ansia e pensieri disfunzionali, possiamo interpretarla come un’indicazione che l’ansia è strettamente legata ai pensieri disfunzionali, al netto dell’effetto di altre variabili come tristezza, rabbia e coping. Questo tipo di analisi permette non solo di comprendere le interazioni tra i vari componenti, ma anche di identificare potenziali bersagli per interventi psicologici, come rafforzare il coping per ridurre l’impatto delle emozioni negative sui pensieri disfunzionali.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#tutorial-con-r",
    "href": "chapters/networks/01_networks.html#tutorial-con-r",
    "title": "81  Network psicologici",
    "section": "81.3 Tutorial con R",
    "text": "81.3 Tutorial con R\nIn questa sezione presentiamo un tutorial passo-passo su come utilizzare le reti psicologiche applicandole a dati raccolti in indagini trasversali, così come presentato da Saqr & López-Pernas (2024). Il dataset utilizzato contiene le risposte di 6071 studenti a un questionario che indaga le caratteristiche psicologiche legate al loro benessere durante la pandemia di COVID-19, condotto in Finlandia e Austria.\nLe domande del questionario riguardano i bisogni psicologici fondamentali degli studenti (relazionalità, autonomia e competenza percepita), l’apprendimento autoregolato, le emozioni positive e la motivazione intrinseca verso l’apprendimento. Inoltre, il dataset include variabili demografiche come paese di residenza, genere ed età.\nNel tutorial, mostreremo come costruire e visualizzare una rete che rappresenti le relazioni tra le diverse caratteristiche psicologiche. Successivamente, interpreteremo e valuteremo queste relazioni e confronteremo le differenze nelle reti tra gruppi demografici. Questo approccio consente di esplorare in modo visivo e quantitativo i collegamenti tra i costrutti psicologici, identificando eventuali differenze tra le categorie demografiche.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#importazione-e-preparazione-dei-dati",
    "href": "chapters/networks/01_networks.html#importazione-e-preparazione-dei-dati",
    "title": "81  Network psicologici",
    "section": "81.4 Importazione e Preparazione dei Dati",
    "text": "81.4 Importazione e Preparazione dei Dati\nIl primo passo consiste nell’importare i dati e prepararli, rimuovendo le risposte mancanti o incomplete.\n\nURL &lt;- (\"https://raw.githubusercontent.com/lamethods/data/main/11_universityCovid/data.sav\") \ndf &lt;- import(URL) |&gt;\n    drop_na()\n\nPer rappresentare ciascun costrutto misurato dal questionario, combiniamo tutte le colonne relative agli item dello stesso costrutto calcolando la media delle risposte. Il seguente codice calcola la media per ogni costrutto sommando e mediando tutti gli item associati:\n\naggregated &lt;- df |&gt; rowwise() |&gt; mutate(\n    Competence = rowMeans(cbind\n    (comp1.rec , comp2.rec, comp3.rec),\n    na.rm = T),\n    Autonomy = rowMeans(cbind\n    (auto1.rec , auto2.rec, auto3.rec),\n    na.rm = T),\n    Motivation = rowMeans(cbind\n    (lm1.rec , lm2.rec, lm3.rec),\n    na.rm = T),\n    Emotion = rowMeans(cbind\n    (pa1.rec , pa2.rec, pa3.rec),\n    na.rm = T),\n    Relatedness = rowMeans(cbind\n    (sr1.rec , sr2.rec, sr3.rec),\n    na.rm = T),\n    SRL = rowMeans(cbind\n    (gp1.rec , gp2.rec, gp3.rec),\n    na.rm = T)\n)\n\nDopo aver creato queste nuove colonne, possiamo conservare solo quelle appena generate. Inoltre, creiamo dei sottoinsiemi di dati in base al genere (un dataset per i maschi e uno per le femmine) e al paese (un dataset per l’Austria e un altro per la Finlandia). Utilizzeremo questi dataset successivamente per confronti tra generi e paesi.\n\ncols &lt;- c(\n    \"Relatedness\", \"Competence\", \"Autonomy\",\n    \"Emotion\", \"Motivation\", \"SRL\"\n)\ndplyr::filter(aggregated, country == 1) |&gt;\n    dplyr::select(all_of(cols)) -&gt; finlandData\ndplyr::filter(aggregated, country == 0) |&gt;\n    dplyr::select(all_of(cols)) -&gt; austriaData\ndplyr::filter(aggregated, gender == 1) |&gt;\n    dplyr::select(all_of(cols)) -&gt; femaleData\ndplyr::filter(aggregated, gender == 2) |&gt;\n    dplyr::select(all_of(cols)) -&gt; maleData\ndplyr::select(aggregated, all_of(cols)) -&gt; allData\n\nallData |&gt; glimpse()\n\nRows: 7,160\nColumns: 6\nRowwise: \n$ Relatedness &lt;dbl&gt; 3.00, 5.00, 4.67, 4.33, 5.00, 4.00, 4.67, 2.33, 3.00, ~\n$ Competence  &lt;dbl&gt; 2.33, 3.00, 4.00, 3.33, 4.00, 2.67, 4.33, 3.67, 3.33, ~\n$ Autonomy    &lt;dbl&gt; 2.33, 1.67, 2.67, 3.00, 3.00, 2.67, 3.33, 2.67, 3.00, ~\n$ Emotion     &lt;dbl&gt; 2.00, 2.33, 4.00, 3.33, 4.00, 3.33, 4.67, 5.00, 3.67, ~\n$ Motivation  &lt;dbl&gt; 1.33, 2.00, 3.00, 2.67, 3.00, 2.67, 4.33, 1.33, 2.00, ~\n$ SRL         &lt;dbl&gt; 2.67, 4.33, 4.33, 2.67, 4.33, 5.00, 4.67, 2.67, 4.00, ~",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#controllo-delle-assunzioni",
    "href": "chapters/networks/01_networks.html#controllo-delle-assunzioni",
    "title": "81  Network psicologici",
    "section": "81.5 Controllo delle Assunzioni",
    "text": "81.5 Controllo delle Assunzioni\nCome primo passo dell’analisi, è fondamentale verificare alcune assunzioni per garantire che il dataset e la rete stimata siano adeguati.\nLa matrice di correlazione deve essere definita positiva. Questo significa che le variabili incluse non devono essere combinazioni lineari tra loro. In altre parole, le variabili non devono essere così simili da non aggiungere nuove informazioni al modello. Per verificare questa proprietà, utilizziamo la funzione is.positive.definite() del pacchetto matrixcalc.\nIn pratica, se la matrice di correlazione non fosse definita positiva, potremmo usare l’opzione cor_auto per cercare una matrice positiva definita alternativa (come vedremo più avanti). Nel nostro caso, la matrice è già definita positiva. Inoltre, utilizziamo l’argomento use = \"pairwise.complete.obs\" per includere tutte le osservazioni disponibili per ogni coppia di variabili.\n\ncorrelationMatrix &lt;- cor(x = allData, use = c(\n    \"pairwise.complete.obs\"\n))\nis.positive.definite(correlationMatrix)\n\nTRUE\n\n\nAssenza di ridondanza tra le variabili. La seconda verifica consiste nell’assicurarci che alcune variabili non siano altamente correlate al punto da risultare ridondanti. Questo è importante per garantire che ogni variabile catturi un costrutto unico e non semplicemente una sovrapposizione di altri costrutti già rappresentati.\nPer questa analisi, utilizziamo l’algoritmo goldbricker, che confronta i pattern di correlazione delle variabili con tutte le altre nel dataset. Nello specifico, cerchiamo item che siano fortemente intercorrelati utilizzando i seguenti parametri predefiniti:\n\nCorrelazione alta: \\(r &gt; 0.50\\)\nFrazione significativa: almeno il 25% delle variabili altamente correlate\np-value: 0.05\n\nEseguiamo il controllo con il codice seguente:\n\ngoldbricker(allData,\n    p = 0.05, method = \"hittner2003\",\n    threshold = 0.25, corMin = 0.5, \n    progressbar = FALSE\n)\n\n\n\n\nSuggested reductions: Less than 25 % of correlations are significantly different for the following pairs: \n[1] \"No suggested reductions\"",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#stima-della-rete",
    "href": "chapters/networks/01_networks.html#stima-della-rete",
    "title": "81  Network psicologici",
    "section": "81.6 Stima della Rete",
    "text": "81.6 Stima della Rete\nDopo aver verificato che i dati soddisfano le assunzioni necessarie, possiamo procedere con la stima della rete. La stima consiste nel quantificare le associazioni tra le diverse variabili. Esistono diversi tipi di associazioni che possono essere stimate, ma le più comuni nelle reti psicologiche sono le misure di dipendenza, come le correlazioni. In queste reti, l’obiettivo è analizzare come i livelli o i valori delle variabili si influenzano reciprocamente (ad esempio, se e in che misura alti livelli di motivazione sono associati ad alti livelli di coinvolgimento). Tali associazioni possono essere stimate tramite covarianze, correlazioni semplici, correlazioni parziali o importanza relativa (regressioni).\nIn questo tutorial, ci concentriamo sulla stima delle correlazioni parziali regolarizzate, una delle metodologie più utilizzate.\n\n81.6.1 Correlazioni Parziali Regolarizzate\nLe correlazioni parziali regolarizzate presentano due vantaggi principali:\n\nPermettono di recuperare la struttura reale della rete nella maggior parte dei casi.\nOffrono una rete sparsa e interpretabile che mostra le associazioni condizionate tra le variabili.\n\nLa correlazione parziale rappresenta l’associazione tra due variabili al netto dell’effetto di tutte le altre variabili della rete (ceteris paribus). Questo approccio consente di stimare, ad esempio, l’associazione tra motivazione e coinvolgimento indipendentemente da altre variabili incluse nella rete, come successo, ansia o piacere.\nLa regolarizzazione è il processo di applicare una penalità alla complessità del modello di rete. Questa tecnica è raccomandata per diversi motivi:\n\nAiuta a eliminare archi spuri (associazioni false) che derivano dall’influenza di altri nodi.\nRiduce gli archi di importanza trascurabile a zero, eliminando così gli errori di Tipo 1 (falsi positivi).\nProduce un modello di rete meno complesso, più chiaro e più semplice da interpretare, mantenendo solo le correlazioni forti e significative.\n\nIn sostanza, la regolarizzazione permette di ottenere una struttura della rete più conservativa e fedele alla realtà. Il metodo più comune per applicare questa penalità è il LASSO (Least Absolute Shrinkage and Selection Operator).",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#procedura-di-stima-della-rete",
    "href": "chapters/networks/01_networks.html#procedura-di-stima-della-rete",
    "title": "81  Network psicologici",
    "section": "81.7 Procedura di Stima della Rete",
    "text": "81.7 Procedura di Stima della Rete\nLa stima della rete può essere effettuata con diversi pacchetti. In questo caso, utilizzeremo il pacchetto bootnet e la funzione estimateNetwork(). Per stimare una rete regolarizzata:\n\nSi passa il dataset come argomento alla funzione.\nSi specifica l’opzione default = \"EBICglasso\" per stimare una rete regolarizzata.\nIl tipo di correlazione di default è impostato su cor_auto, che rileva automaticamente la distribuzione delle variabili e applica il coefficiente di correlazione appropriato: policorico, poliseriale o di Pearson.\n\nDi default, la funzione estimateNetwork() calcola 100 modelli di rete, ognuno con un diverso grado di sparsità, ossia con un numero variabile di connessioni (archi) tra i nodi. Tra questi modelli, viene scelto quello con il valore più basso dell’Extended Bayesian Information Criterion (EBIC), un indicatore che bilancia due esigenze opposte: ridurre i falsi positivi (archi spuri) e mantenere gli archi reali della rete.\nQuesto bilanciamento è regolato da un parametro chiamato gamma (γ), il cui valore può variare tra:\n\n0: favorisce modelli con più archi, cioè una rete più densa.\n0.5: favorisce modelli con meno archi, cioè una rete più parsimoniosa.\n\nIl valore predefinito di γ è 0.5, il che significa che il modello risultante tende a includere solo gli archi più robusti e significativi, garantendo una rappresentazione affidabile e parsimoniosa della rete.\n\nallNetwork &lt;- estimateNetwork(allData,\n    default = \"EBICglasso\",\n    corMethod = \"cor_auto\",\n    tuning = 0.5\n)\n\n\nsummary(allNetwork)\n\n\n=== Estimated network ===\nNumber of nodes: 6 \nNumber of non-zero edges: 15 / 15 \nMean weight: 0.14 \nNetwork stored in object$graph \n \nDefault set used: EBICglasso \n \nUse plot(object) to plot estimated network \nUse bootnet(object) to bootstrap edge weights and centrality indices \n\nRelevant references:\n\n    Friedman, J. H., Hastie, T., & Tibshirani, R. (2008). Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 9 (3), 432-441.\n    Foygel, R., & Drton, M. (2010). Extended Bayesian information criteria for Gaussian graphical models. \n    Friedman, J. H., Hastie, T., & Tibshirani, R. (2014). glasso: Graphical lasso estimation of gaussian graphical models. Retrieved from https://CRAN.R-project.org/package=glasso\n    Epskamp, S., Cramer, A., Waldorp, L., Schmittmann, V. D., & Borsboom, D. (2012). qgraph: Network visualizations of relationships in psychometric data. Journal of Statistical Software, 48 (1), 1-18.\n    Epskamp, S., Borsboom, D., & Fried, E. I. (2016). Estimating psychological networks and their accuracy: a tutorial paper. arXiv preprint, arXiv:1604.08462.\n\n\nL’utilizzo delle correlazioni parziali regolarizzate consente di:\n\nEliminare rumore e archi non significativi.\nOttenere una rappresentazione chiara e interpretabile delle relazioni condizionate tra variabili.\nGarantire una rete parsimoniosa, che evidenzia solo le relazioni più forti e significative, riducendo i rischi di sovrastima delle connessioni.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#creazione-del-grafico-della-rete",
    "href": "chapters/networks/01_networks.html#creazione-del-grafico-della-rete",
    "title": "81  Network psicologici",
    "section": "81.8 Creazione del Grafico della Rete",
    "text": "81.8 Creazione del Grafico della Rete\nLa rete può essere visualizzata utilizzando la funzione plot(). Il grafico generato adotta di default un tema adatto ai daltonici, in cui:\n\nGli archi blu rappresentano correlazioni positive.\nGli archi rossi rappresentano correlazioni negative.\nLo spessore degli archi è proporzionale alla magnitudine della correlazione parziale regolarizzata.\n\nNel grafico risultante possiamo osservare, ad esempio, una forte correlazione tra motivazione, autonomia e competenza. Inoltre, le emozioni mostrano una forte associazione con la competenza. Ricordiamo che tutte le correlazioni visualizzate nel grafico sono condizionate, cioè tengono conto degli effetti di tutte le altre variabili nella rete, analogamente a una regressione.\n\nallDataPlot &lt;- plot(allNetwork)\nLX &lt;- allDataPlot$layout\n\n\n\n\n\n\n\n\nPossiamo salvare il grafico in un oggetto R, ad esempio con il nome allDataPlot.\n\nallDataPlot &lt;- plot(\n    allNetwork,\n    title = \"Both countries combined\",\n    vsize = 9,\n    edge.labels = TRUE,\n    cut = 0.10,\n    minimum = 0.05,\n    layout = \"spring\"\n)\n\n\n\n\n\n\n\n\nQuesto è utile perché l’oggetto contiene non solo il grafico, ma anche altri dati utili come:\n\nLa matrice di correlazione.\nI parametri di configurazione del grafico.\nGli argomenti della funzione.\n\nAd esempio, l’oggetto allDataPlot$layout contiene il layout della rete (la disposizione dei nodi), che può essere salvato e riutilizzato per garantire la coerenza visiva tra grafici di reti diverse, facilitandone il confronto.\nNella maggior parte dei casi, è utile personalizzare il grafico per migliorare la leggibilità e l’interpretazione. Alcuni esempi di personalizzazione includono:\n\nTitolo del grafico: Aggiunto con l’argomento title.\nDimensione dei nodi: Regolata con vsize.\nMostrare i pesi degli archi: Impostando edge.labels = TRUE per visualizzare i valori numerici delle correlazioni.\nSoglia di visibilità degli archi:\n\nUtilizzare cut = 0.10 per evidenziare gli archi con correlazioni superiori a 0.10. Gli archi con correlazioni inferiori saranno visualizzati con una minore intensità del colore.\nNascondere (ma non eliminare) archi con valori inferiori a 0.05 con minimum = 0.05. Questo aiuta a ridurre il “rumore” visivo nel grafico.\n\nDisposizione dei nodi (layout): Utilizzare un layout coerente, ad esempio \"spring\" per posizionare automaticamente i nodi.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#interpretare-le-relazioni-nella-rete-predicibilità-dei-nodi",
    "href": "chapters/networks/01_networks.html#interpretare-le-relazioni-nella-rete-predicibilità-dei-nodi",
    "title": "81  Network psicologici",
    "section": "81.9 Interpretare le Relazioni nella Rete: Predicibilità dei Nodi",
    "text": "81.9 Interpretare le Relazioni nella Rete: Predicibilità dei Nodi\nUn’analisi utile consiste nel calcolare la predicibilità di ciascun nodo della rete. La predicibilità rappresenta la proporzione di varianza di un nodo che può essere spiegata dalle sue connessioni con gli altri nodi. In pratica, si tratta di calcolare quanto ciascun nodo è influenzato dalle relazioni con gli altri nodi della rete.\nLa predicibilità di un nodo si basa su una semplice regressione lineare:\n\nPer ciascun nodo, si considera come variabile dipendente.\nTutti gli altri nodi vengono utilizzati come predittori.\nSi calcola il coefficiente \\(R^2\\), che indica la percentuale di varianza spiegata.\n\nQuesto processo viene ripetuto per ogni nodo nella rete.\n\nQuando \\(R^2 = 0\\): Il nodo non è spiegato dalle sue connessioni. In questo caso, è opportuno chiedersi se il nodo è pertinente alla rete o se ci sono problemi nella misurazione di quella variabile.\nQuando \\(R^2 = 1\\): Il nodo è completamente spiegato dalle sue connessioni (100% della varianza). Questo risultato è molto improbabile e potrebbe indicare un errore nel modello o dati altamente ridondanti.\n\nLa predicibilità è anche collegata al concetto di controllabilità, ossia la misura in cui un nodo può influenzare gli altri nodi della rete. Ad esempio, se un intervento è mirato a un nodo con alta predicibilità, potrebbe essere possibile controllare indirettamente altri nodi connessi. Questo è particolarmente utile per identificare target di intervento in contesti applicativi, come la psicologia clinica o l’educazione.\nPer calcolare la predicibilità, utilizziamo la funzione mgm() del pacchetto mgm, che permette di stimare il modello di rete necessario. Viene richiesta la specifica del tipo di ciascuna variabile. Ad esempio, se tutte le variabili sono gaussiane (variabili continue), il parametro sarà: r     type = rep('g', 6)\n\nfitAllData &lt;- mgm(as.matrix(allData), \n    type = c(\"g\", \"g\", \"g\", \"g\", \"g\", \"g\")\n)\n\n  |------------------------------------------------------------------| 100%\nNote that the sign of parameter estimates is stored separately; see ?mgm\n\n\nInterpretazione dei risultati:\n\nUn valore di predicibilità basso (\\(R^2\\) vicino a 0) indica che il nodo è debolmente connesso alla rete e potrebbe essere considerato un “outlier” o mal misurato.\nUn valore moderato (\\(R^2\\) tra 0.3 e 0.7) indica che il nodo è spiegato in parte dalle sue connessioni e contribuisce significativamente alla rete.\nUn valore molto alto (\\(R^2\\) vicino a 1) è raro e potrebbe suggerire che il nodo è fortemente ridondante o che il modello è sovrastimato.\n\nLa predicibilità di ciascuna variabile della rete può essere ottenuta esaminando l’oggetto risultante dal modello di rete. Questo permette di valutare, per ogni nodo, la proporzione di varianza spiegata dalle sue relazioni con gli altri nodi della rete.\n\n# Compute the predictability\npredictAll &lt;- predict(fitAllData, na.omit(allData))\npredictAll$errors$R2\n\n\n0.1390.5180.4420.3150.4580.126\n\n\nOltre alla predicibilità dei singoli nodi, è possibile calcolare la predicibilità media per ottenere una misura complessiva di quanto, in media, i nodi della rete sono spiegati dalle loro connessioni. Questo fornisce un’indicazione generale della qualità esplicativa della rete nel suo insieme.\n\nmean(predictAll$errors$R2) # Mean predictability\n\n0.333\n\n\nUn’altra misura utile è l’Errore Quadratico Medio (RMSE), che indica la differenza tra i valori previsti e quelli osservati per ciascun nodo. Questo valore può essere ottenuto esaminando l’oggetto risultante come segue:\n\nmean(predictAll$errors$RMSE) # Mean RMSE\n\n0.811333333333333\n\n\nInterpretazione\n\nPredicibilità (\\(R^2\\)):\n\nValori elevati (\\(R^2\\) vicino a 1): Il nodo è altamente spiegato dalle sue relazioni con gli altri nodi della rete.\nValori bassi (\\(R^2\\) vicino a 0): Il nodo è poco spiegato dalle sue connessioni e potrebbe richiedere ulteriori verifiche, ad esempio sulla sua pertinenza al modello o sulla qualità della sua misurazione.\n\nErrore Quadratico Medio (RMSE):\n\nUn RMSE basso indica che il modello di rete prevede accuratamente i valori osservati per ciascun nodo.\nUn RMSE alto suggerisce che le connessioni nel modello spiegano meno bene i valori effettivi del nodo, indicando potenziali problemi nel modello o nei dati.\n\n\nPossiamo salvare i risultati in un dataframe:\n\ndata.frame(\n    var = predictAll$errors$Variable,\n    R2 = predictAll$errors$R2,\n    RMSE = predictAll$errors$RMSE\n)\n\n\nA data.frame: 6 x 3\n\n\nvar\nR2\nRMSE\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nRelatedness\n0.139\n0.928\n\n\nCompetence\n0.518\n0.694\n\n\nAutonomy\n0.442\n0.747\n\n\nEmotion\n0.315\n0.828\n\n\nMotivation\n0.458\n0.736\n\n\nSRL\n0.126\n0.935\n\n\n\n\n\nCome mostrano il grafico della rete con la predicibilità rappresentata come grafici a torta nei nodi e il relativo data frame, i nodi con la predicibilità più alta sono quelli relativi a competenza, motivazione e autonomia. Al contrario, i nodi relativi all’apprendimento autoregolato (SRL) e alla relazionalità presentano una predicibilità più bassa, risultando quindi meno spiegati dalle loro connessioni con gli altri nodi della rete.\n\nAlta predicibilità (competenza, motivazione, autonomia): questi nodi sono fortemente influenzati dalle relazioni con gli altri nodi, suggerendo che i loro valori dipendono in larga misura dalle connessioni nella rete. Ciò può indicare che questi costrutti sono centrali e ben integrati nel modello.\nBassa predicibilità (SRL, relazionalità): questi nodi sono debolmente spiegati dalle connessioni, il che può suggerire che:\n\nLe variabili potrebbero essere meno centrali nella rete.\nPotrebbero mancare connessioni significative con altri nodi.\nPotrebbero esserci problemi nella misurazione o nella definizione di questi costrutti.\n\n\nImplicazioni:\n\nI nodi con alta predicibilità possono essere considerati target strategici per interventi, poiché le loro connessioni hanno un impatto significativo sugli altri nodi.\nPer i nodi con bassa predicibilità, potrebbe essere utile approfondire ulteriormente il loro ruolo nel modello, considerando possibili revisioni del questionario o integrazioni con ulteriori variabili esplicative.\n\n\nallDataPlot &lt;- plot(\n    allNetwork,\n    title = \"Both countries combined\",\n    vsize = 9,\n    edge.labels = TRUE,\n    cut = 0.10,\n    minimum = 0.05,\n    pie = predictAll$errors$R2\n)",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#inferenza-sulla-rete-misure-di-centralità",
    "href": "chapters/networks/01_networks.html#inferenza-sulla-rete-misure-di-centralità",
    "title": "81  Network psicologici",
    "section": "81.10 Inferenza sulla Rete: Misure di Centralità",
    "text": "81.10 Inferenza sulla Rete: Misure di Centralità\nCome nelle reti tradizionali, anche nelle reti psicologiche è possibile calcolare le misure di centralità. Queste misure permettono di individuare i nodi più influenti, importanti o centrali all’interno della rete. Tuttavia, l’interpretazione di queste misure dipende da diversi fattori:\n\nLe variabili incluse nella rete.\nIl metodo di stima utilizzato.\nI pesi degli archi (connessioni tra i nodi).\nLe basi teoriche che guidano la struttura della rete.\n\nLe misure di centralità possono essere utili per identificare potenziali bersagli di intervento, ossia nodi il cui cambiamento potrebbe avere un impatto significativo sull’intera rete.\nTra le molteplici misure di centralità disponibili, due sono particolarmente diffuse e raccomandate:\n\nGrado di centralità (degree centrality): Conta il numero di connessioni di un nodo, indipendentemente dal peso degli archi.\nForza di centralità (strength centrality): Somma i pesi assoluti di tutte le connessioni di un nodo (sia positivi che negativi).\nInfluenza attesa (expected influence): Simile alla centralità di forza, ma somma i pesi grezzi delle connessioni, senza considerare i valori assoluti.\n\nAltre misure, come betweenness, closeness e eigenvector centrality, possono essere calcolate, ma la loro interpretazione è meno chiara e non sono generalmente raccomandate per un’analisi standard.\nPer fare un esempio, consideriamo un nodo con le seguenti connessioni: 0.3, -0.1 e 0.5. Le misure di centralità sono:\n\nDegree centrality: 3 (numero delle connessioni).\nStrength centrality: \\(|0.3| + |-0.1| + |0.5| = 0.9\\) (somma dei pesi assoluti).\nExpected influence: \\(0.3 + (-0.1) + 0.5 = 0.7\\) (somma dei pesi grezzi).\n\nNota: Quando la rete non contiene archi negativi, la centralità di forza e l’influenza attesa coincidono.\nPer visualizzare le misure di centralità, utilizziamo la funzione centralityPlot() del pacchetto bootnet. L’argomento include specifica le centralità da calcolare, e l’opzione scale = \"z-scores\" visualizza i valori di centralità standardizzati (z-score) sull’asse x, invece dei valori grezzi.\n\ncentralityPlot(allNetwork,\n    include = c(\"ExpectedInfluence\", \"Strength\"),\n    scale = \"z-scores\"\n)\n\n\n\n\n\n\n\n\nSe si desiderano solo i valori numerici delle centralità, possiamo utilizzare la funzione centralityTable():\n\ncentralityTable(allNetwork)\n\n\nA data.frame: 24 x 5\n\n\ngraph\ntype\nnode\nmeasure\nvalue\n\n\n&lt;chr&gt;\n&lt;lgl&gt;\n&lt;chr&gt;\n&lt;fct&gt;\n&lt;dbl&gt;\n\n\n\n\ngraph 1\nNA\nRelatedness\nBetweenness\n-0.617\n\n\ngraph 1\nNA\nCompetence\nBetweenness\n0.772\n\n\ngraph 1\nNA\nAutonomy\nBetweenness\n1.697\n\n\ngraph 1\nNA\nEmotion\nBetweenness\n-0.617\n\n\ngraph 1\nNA\nMotivation\nBetweenness\n-0.617\n\n\ngraph 1\nNA\nSRL\nBetweenness\n-0.617\n\n\ngraph 1\nNA\nRelatedness\nCloseness\n-1.144\n\n\ngraph 1\nNA\nCompetence\nCloseness\n0.721\n\n\ngraph 1\nNA\nAutonomy\nCloseness\n1.206\n\n\ngraph 1\nNA\nEmotion\nCloseness\n-0.202\n\n\ngraph 1\nNA\nMotivation\nCloseness\n0.578\n\n\ngraph 1\nNA\nSRL\nCloseness\n-1.158\n\n\ngraph 1\nNA\nRelatedness\nStrength\n-1.074\n\n\ngraph 1\nNA\nCompetence\nStrength\n1.224\n\n\ngraph 1\nNA\nAutonomy\nStrength\n0.634\n\n\ngraph 1\nNA\nEmotion\nStrength\n-0.310\n\n\ngraph 1\nNA\nMotivation\nStrength\n0.695\n\n\ngraph 1\nNA\nSRL\nStrength\n-1.170\n\n\ngraph 1\nNA\nRelatedness\nExpectedInfluence\n-1.003\n\n\ngraph 1\nNA\nCompetence\nExpectedInfluence\n1.220\n\n\ngraph 1\nNA\nAutonomy\nExpectedInfluence\n0.649\n\n\ngraph 1\nNA\nEmotion\nExpectedInfluence\n-0.371\n\n\ngraph 1\nNA\nMotivation\nExpectedInfluence\n0.708\n\n\ngraph 1\nNA\nSRL\nExpectedInfluence\n-1.203\n\n\n\n\n\nUn’alternativa per calcolare diverse tipologie di centralità è l’utilizzo del pacchetto NetworkToolbox. Questo pacchetto offre un’ampia gamma di misure di centralità, tra cui:\n\nDegree centrality: Numero di connessioni di un nodo.\nStrength centrality: Somma dei pesi assoluti delle connessioni di un nodo.\nCloseness centrality: Misura della “vicinanza” di un nodo a tutti gli altri nella rete.\nEigenvector centrality: Valuta l’importanza di un nodo tenendo conto anche dell’importanza dei suoi vicini.\nLeverage centrality: Misura il peso relativo delle connessioni di un nodo rispetto ai suoi vicini.\n\nOltre a queste, il pacchetto include molte altre misure di centralità che possono essere esplorate per analisi specifiche. Per ulteriori dettagli sull’utilizzo delle funzioni e sulle opzioni disponibili, è consigliato consultare il manuale del pacchetto NetworkToolbox.\nSebbene NetworkToolbox fornisca una vasta gamma di misure di centralità, è importante ricordare che:\n\nAlcune di queste misure, come closeness, betweenness ed eigenvector centrality, non sono ancora ben comprese o facili da interpretare nel contesto delle reti psicologiche.\nMisure come la strength centrality e l’expected influence sono generalmente preferite perché hanno un’interpretazione più chiara e diretta.\n\n\nDegree &lt;- degree(allNetwork$graph)\nStrength &lt;- strength(allNetwork$graph)\nBetweenness &lt;- betweenness(allNetwork$graph)\nCloseness &lt;- closeness(allNetwork$graph)\nEigenvector &lt;- eigenvector(allNetwork$graph)\nLeverage &lt;- leverage(allNetwork$graph)\ndata.frame(\n    Var = names\n    (Degree), Degree, Strength, Betweenness, Closeness,\n    Eigenvector, Leverage\n)\n\n\nA data.frame: 6 x 7\n\n\n\nVar\nDegree\nStrength\nBetweenness\nCloseness\nEigenvector\nLeverage\n\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nRelatedness\nRelatedness\n5\n0.40\n0\n1.89\n0.217\n-3.429\n\n\nCompetence\nCompetence\n5\n1.07\n6\n3.27\n0.553\n1.349\n\n\nAutonomy\nAutonomy\n5\n0.90\n10\n3.63\n0.475\n1.066\n\n\nEmotion\nEmotion\n5\n0.62\n0\n2.58\n0.363\n-0.425\n\n\nMotivation\nMotivation\n5\n0.91\n0\n3.16\n0.495\n1.105\n\n\nSRL\nSRL\n5\n0.37\n0\n1.88\n0.211\n-5.379",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#altre-opzioni-per-la-stima-delle-reti",
    "href": "chapters/networks/01_networks.html#altre-opzioni-per-la-stima-delle-reti",
    "title": "81  Network psicologici",
    "section": "81.11 Altre Opzioni per la Stima delle Reti",
    "text": "81.11 Altre Opzioni per la Stima delle Reti\nCome accennato in precedenza, oltre alle reti basate su correlazioni parziali regolarizzate, esistono diverse altre opzioni di stima delle reti. Di seguito ne presentiamo alcune, ma per ulteriori dettagli si consiglia di consultare le pagine del manuale della funzione estimateNetwork().\n\n81.11.1 Rete di Associazione\nLa rete di associazione (correlation network) si basa sulle correlazioni semplici tra le variabili. Questo tipo di rete è utile principalmente per esplorazioni preliminari dei dati, ma non è generalmente raccomandata per analisi definitive, poiché tende a produrre reti molto dense con molteplici connessioni non significative.\n\nallNetwork_cor &lt;- estimateNetwork(allData,\n    default = \"cor\", verbose = FALSE\n)\n\n\n\n81.11.2 Metodo ggmModSelect()\nIl metodo ggmModSelect() è particolarmente indicato per dataset di grandi dimensioni con un numero ridotto di nodi. Funziona in questo modo:\n\nParte da una rete regolarizzata come punto di riferimento iniziale.\nStima tutte le possibili reti non regolarizzate.\nSeleziona il modello migliore in base al criterio EBIC (Extended Bayesian Information Criterion), scegliendo quello con il valore più basso.\n\nQuesto approccio combina i vantaggi della regolarizzazione con la flessibilità di reti non regolarizzate, risultando in un modello più accurato per dataset specifici.\n\nallNetwork_mgm &lt;- estimateNetwork(allData,\n    default = \"ggmModSelect\", verbose = FALSE\n)\n\n\n\n81.11.3 Rete di Importanza Relativa\nLa rete di importanza relativa (relimp; Relative Importance Network) stima una rete direzionale basata sull’importanza relativa dei predittori in un modello di regressione lineare. In questa rete:\n\nGli archi rappresentano la magnitudine dell’importanza relativa di ciascun predittore.\nLa direzione degli archi indica come ciascuna variabile influenza le altre, secondo i risultati della regressione.\n\nQuesta rete è utile per identificare relazioni causali teoriche o per evidenziare come alcune variabili predicono altre all’interno del dataset.\n\nallNetwork_relimp &lt;- estimateNetwork(allData,\n    default = \"relimp\", verbose = FALSE\n)\n\n\n\n81.11.4 Confronto tra i Modelli\n\nLa rete ggmModSelect() produce risultati molto simili alla rete regolarizzata standard, con differenze minime nelle connessioni più deboli.\nLa rete di associazione è invece molto densa, poiché include tutte le correlazioni, senza applicare alcuna penalizzazione.\nLa rete di importanza relativa è direzionale e fornisce informazioni aggiuntive su come una variabile influenza le altre.\n\n\nplot(\n    allNetwork_cor, title = \"Correlation\", vsize = 18, edge.labels = TRUE, \n    cut = 0.10, minimum = 0.05, layout = LX\n)\nplot(\n    allNetwork, title = \"EBICglasso\", vsize = 18, edge.labels = TRUE,\n    cut = 0.10, minimum = 0.05, layout = LX\n)\nplot(\n    allNetwork_mgm, title = \"ggmModSelect\", vsize = 18, edge.labels = TRUE,\n    cut = 0.10, minimum = 0.05, layout = LX\n)\nplot(\n    allNetwork_relimp, title = \"Relative importance\", vsize = 18, \n    edge.labels = TRUE, cut = 0.10, minimum = 0.05, layout = LX\n)",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#confronto-tra-reti",
    "href": "chapters/networks/01_networks.html#confronto-tra-reti",
    "title": "81  Network psicologici",
    "section": "81.12 Confronto tra Reti",
    "text": "81.12 Confronto tra Reti\nDopo aver illustrato i passaggi fondamentali per stimare una singola rete, procediamo ora al confronto tra diverse reti. Le reti psicologiche offrono metodi per confrontare le reti nel loro complesso, i pesi degli archi (edge weights) e le misure di centralità.\nNel nostro caso, disponendo di dati provenienti da due paesi (Finlandia e Austria), possiamo stimare due reti separate, una per ciascun paese, e confrontarle per osservare come differiscono.\nPer confrontare le reti, dobbiamo innanzitutto stimarle separatamente per ciascun paese. I passaggi di stima sono gli stessi descritti in precedenza. Per ciascun paese, iniziamo con i passi seguenti:\n\nControllo delle assunzioni per ciascun dataset (ad esempio, verifica che la matrice di correlazione sia positiva definita).\nVerifica con l’algoritmo goldbricker per identificare nodi altamente simili che potrebbero necessitare di essere combinati o ridotti. In questo caso, i risultati mostrano che:\n\nLe matrici di correlazione di entrambe le reti sono già positive definite.\nNon ci sono nodi altamente simili che richiedono riduzioni.\n\n\n\n### Check the assumptions\n## Finland\n# check for positive definitiveness\ncorrelationMatrix &lt;- cor(\n    x = finlandData, \n    use = c(\"pairwise.complete.obs\")\n)\n\nis.positive.definite(correlationMatrix)\n# check for redundancy\ngoldbricker(finlandData,\n    p = 0.05, method = \"hittner2003\",\n    threshold = 0.25, corMin = 0.5, progressbar = FALSE\n)\n\nTRUE\n\n\n\n\n\nSuggested reductions: Less than 25 % of correlations are significantly different for the following pairs: \n[1] \"No suggested reductions\"\n\n\n\n## Austria\n# check for positive definitiveness\ncorrelationMatrix &lt;- cor(\n    x = austriaData, \n    use = c(\"pairwise.complete.obs\")\n)\n\nis.positive.definite(correlationMatrix)\n# check for redundancy\ngoldbricker(austriaData,\n    p = 0.05, method = \"hittner2003\",\n    threshold = 0.25, corMin = 0.5, progressbar = FALSE\n)\n\nTRUE\n\n\n\n\n\nSuggested reductions: Less than 25 % of correlations are significantly different for the following pairs: \n[1] \"No suggested reductions\"\n\n\nStima dei networks:\n\n# Estimate the networks\nfinlandNetwork &lt;- estimateNetwork(\n    finlandData,\n    default = \"EBICglasso\", corMethod = \"cor_auto\", tuning = 0.5\n)\n\naustriaNetwork &lt;- estimateNetwork(\n    austriaData,\n    default = \"EBICglasso\", corMethod = \"cor_auto\", tuning = 0.5\n)\n\nCalcoliamo la predicibilità per ciascun paese.\n\n# Compute the predictability\nfitFinland &lt;- mgm(\n    as.matrix(finlandData), # data\n    c(\"g\", \"g\", \"g\", \"g\", \"g\", \"g\"),\n    # distribution for each var\n    verbatim = TRUE, # hide warnings and progress bar\n    signInfo = FALSE # hide message about signs\n)\n\n\npredictFinland &lt;- predict(fitFinland, na.omit(finlandData))\nmean(predictFinland$errors$R2)\nmean(predictFinland$errors$RMSE)\n\n0.3085\n\n\n0.828333333333333\n\n\n\nfitAustria &lt;- mgm(\n    as.matrix(austriaData), # data\n    c(\"g\", \"g\", \"g\", \"g\", \"g\", \"g\"),\n    # distribution for each var\n    verbatim = TRUE, # hide warnings and progress bar\n    signInfo = FALSE # hide message about signs\n)\n\n\npredictAustria &lt;- predict(fitAustria, na.omit(austriaData))\nmean(predictAustria$errors$R2)\nmean(predictAustria$errors$RMSE)\n\n0.343666666666667\n\n\n0.803833333333333\n\n\nDopo aver stimato le reti, possiamo visualizzarle per facilitare il confronto. Questo approccio consente di identificare rapidamente le differenze e le somiglianze tra le reti dei due paesi.\n\nAverageLayout &lt;- averageLayout(finlandNetwork, austriaNetwork)\n\n\nplot(finlandNetwork, # input network\n    title = \"Finland\", # plot title\n    vsize = 19, # size of the nodes\n    edge.labels = TRUE, # label the edge weights\n    cut = 0.10, # saturate edges &gt; .10\n    minimum = 0.05, # remove edges &lt; .05\n    pie = predictFinland$errors$R2, # put R2 as pie\n    layout = LX\n) # set the layout\n\n\n\n\n\n\n\n\n\nplot(austriaNetwork, # input network\n    title = \"Austria\", # plot title\n    vsize = 19, # size of the nodes\n    edge.labels = TRUE, # label the edge weights\n    cut = 0.10, # saturate edges &gt; .10\n    minimum = 0.05, # remove edges &lt; .05\n    pie = predictAustria$errors$R2, # put R2 as pie\n    layout = LX\n) # set the layout\n\n\n\n\n\n\n\n\nPossiamo rappresentare graficamente la differenza tra le due reti usando qgraph(). La funzione qgraph() richiede come input una rete stimata o una matrice. Per creare una rete di differenza, è necessario sottrarre le due matrici delle connessioni delle reti (ad esempio, finlandNetwork$graph - austriaNetwork$graph).\nIl seguente codice mostra come visualizzare la rete di differenza che evidenzia le variazioni nei pesi delle connessioni tra le due reti.\n\nqgraph(finlandNetwork$graph - abs(austriaNetwork$graph),\n    title = \"Difference\", # plot title\n    theme = allDataPlot$Arguments$theme,\n    vsize = 19, # size of the nodes\n    edge.labels = TRUE, # label the edge weights\n    labels = allDataPlot$Arguments$labels, # node labels\n    cut = 0.10, # saturate edges &gt; .10\n    layout = LX\n) # set the layout\n\n\n\n\n\n\n\n\nIl confronto tra le reti evidenzia differenze tra i due paesi:\n\nFinlandia:\n\nConnessione più forte tra competenza ed emozione.\nConnessione più forte tra motivazione e relazionalità.\n\nAustria:\n\nConnessioni più forti tra:\n\nMotivazione e competenza.\nMotivazione ed emozione.\nCompetenza e autonomia.\nAutonomia e relazionalità.\n\n\n\nQueste differenze suggeriscono che le relazioni psicologiche tra le variabili possono essere influenzate da fattori specifici di ciascun contesto culturale o sociale. La rete di differenza rappresenta un utile strumento visivo per evidenziare queste variazioni e guidare l’interpretazione.\nUn confronto visivo delle centralità può essere effettuato nello stesso modo descritto in precedenza. Per farlo, forniamo le reti che vogliamo confrontare come una lista e specifichiamo le misure di centralità da calcolare.\nI risultati mostrano che:\n\nNella rete dell’Austria, la variabile con il valore di centralità più alto è la motivazione, indicando che la motivazione è il fattore principale che guida la connettività nella rete.\nNella rete della Finlandia, la variabile più centrale è la competenza, che risulta essere il driver principale della connettività della rete.\n\nQuesto confronto mette in evidenza come le variabili centrali differiscano tra i due contesti, suggerendo che fattori culturali o ambientali possono influenzare il ruolo delle variabili psicologiche nella struttura della rete.\n\ncentralityPlot(\n    list(\n        Finland = finlandNetwork,\n        Austria = austriaNetwork\n    ),\n    include = c(\"ExpectedInfluence\", \"Strength\")\n)",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#confronto-statistico-tra-reti",
    "href": "chapters/networks/01_networks.html#confronto-statistico-tra-reti",
    "title": "81  Network psicologici",
    "section": "81.13 Confronto Statistico tra Reti",
    "text": "81.13 Confronto Statistico tra Reti\nPer confrontare in modo rigoroso le reti, è necessario utilizzare un test statistico che permetta di stabilire quali differenze nei pesi degli archi o nelle misure di centralità siano significative e non dovute al caso. Il Network Comparison Test (NCT) è uno strumento per effettuare un confronto dettagliato della struttura delle reti, dei pesi degli archi e delle centralità.\nL’NCT utilizza un approccio basato su permutazioni:\n\nGenera un grande numero di reti permutate a partire dalle reti originali, creando una distribuzione di riferimento.\nConfronta le reti originali con quelle permutate per determinare se le differenze osservate sono statisticamente significative.\n\nPer eseguire il test, dobbiamo:\n\nFornire le due reti da confrontare.\nSpecificare il numero di iterazioni (almeno 1000 è raccomandato per risultati affidabili).\nTestare gli archi con test.edges = TRUE e edges = 'all' per verificare tutte le connessioni.\nTestare le centralità con test.centrality = TRUE (poiché non vengono testate di default).\n\n\nset.seed(1337)\n\nCompared &lt;- NCT(\n    finlandNetwork, # network 1\n    austriaNetwork, # network 2\n    verbose = FALSE, # hide warnings and progress bar\n    it = 1000, # number of iterations\n    abs = TRUE,\n    binary.data = FALSE, # set data distribution\n    test.edges = TRUE, # test edge differences\n    edges = 'all', # which edges to test\n    test.centrality = TRUE, # test centrality\n    progressbar = FALSE # progress bar\n)\n\n\nCompared$glstrinv.sep # Separate global strength values of the individual networks\n\n\n2.148451146668562.1724996996597\n\n\n\nCompared$einv.pvals # Holm-Bonferroni adjusted p-values for each edge\n\n\nA data.frame: 15 x 4\n\n\n\nVar1\nVar2\np-value\nTest statistic E\n\n\n\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n7\nRelatedness\nCompetence\n0.017982\n0.0681\n\n\n13\nRelatedness\nAutonomy\n0.001998\n0.0952\n\n\n14\nCompetence\nAutonomy\n0.000999\n0.1348\n\n\n19\nRelatedness\nEmotion\n0.165834\n0.0414\n\n\n20\nCompetence\nEmotion\n0.000999\n0.1713\n\n\n21\nAutonomy\nEmotion\n0.009990\n0.0767\n\n\n25\nRelatedness\nMotivation\n0.000999\n0.1371\n\n\n26\nCompetence\nMotivation\n0.000999\n0.1605\n\n\n27\nAutonomy\nMotivation\n0.003996\n0.0876\n\n\n28\nEmotion\nMotivation\n0.001998\n0.1287\n\n\n31\nRelatedness\nSRL\n0.077922\n0.0467\n\n\n32\nCompetence\nSRL\n0.000999\n0.1258\n\n\n33\nAutonomy\nSRL\n0.688312\n0.0123\n\n\n34\nEmotion\nSRL\n0.073926\n0.0530\n\n\n35\nMotivation\nSRL\n0.301698\n0.0306\n\n\n\n\n\n\nCompared$diffcen.real # Difference in centralities\n\n\nA matrix: 6 x 2 of type dbl\n\n\n\nstrength\nexpectedInfluence\n\n\n\n\nRelatedness\n0.105\n0.105\n\n\nCompetence\n0.070\n0.070\n\n\nAutonomy\n-0.229\n-0.229\n\n\nEmotion\n0.127\n0.214\n\n\nMotivation\n-0.209\n-0.209\n\n\nSRL\n0.088\n0.175\n\n\n\n\n\n\nCompared$diffcen.pval # Holm-Bonferroni adjusted p-values for each centrality\n\n\nA matrix: 6 x 2 of type dbl\n\n\n\nstrength\nexpectedInfluence\n\n\n\n\nRelatedness\n0.006993\n0.006993\n\n\nCompetence\n0.127872\n0.127872\n\n\nAutonomy\n0.000999\n0.000999\n\n\nEmotion\n0.009990\n0.000999\n\n\nMotivation\n0.000999\n0.000999\n\n\nSRL\n0.132867\n0.000999\n\n\n\n\n\n\n81.13.1 Interpretazione dei Risultati\nForza Globale delle Reti. Finlandia: 2.15, Austria: 2.17. La differenza tra le due reti (\\(\\Delta = 0.024\\)) non è statisticamente significativa, indicando che le reti hanno una connettività complessiva simile.\nDifferenze nei Pesi degli Archi. Gli archi Competence-Autonomy, Competence-Emotion, Competence-Motivation, Relatedness-Competence e Relatedness-Motivation mostrano differenze statisticamente significative (\\(p &lt; 0.05\\)). Archi come Relatedness-Emotion e Autonomy-SRL non mostrano differenze significative.\nDifferenze nelle Centralità. Autonomy e Motivation hanno differenze significative sia in strength che in expected influence (\\(p &lt; 0.001\\)), suggerendo che il loro ruolo nella rete varia notevolmente tra i due paesi. Relatedness e Emotion mostrano differenze significative (\\(p &lt; 0.01\\)). Competence e SRL non presentano differenze significative nelle centralità.\nIn conclusione, le reti di Finlandia e Austria hanno una forza globale simile, ma mostrano differenze significative in alcune connessioni chiave e centralità. Competence, Autonomy, e Motivation giocano ruoli diversi nei due contesti culturali. Le differenze nei pesi degli archi e nelle centralità suggeriscono influenze specifiche di fattori contestuali o culturali.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#la-rete-della-variabilità",
    "href": "chapters/networks/01_networks.html#la-rete-della-variabilità",
    "title": "81  Network psicologici",
    "section": "81.14 La Rete della Variabilità",
    "text": "81.14 La Rete della Variabilità\nLa rete della variabilità fornisce un’indicazione di come i pesi degli archi (le connessioni tra nodi) variano tra le reti. In altre parole, questa rete riflette il grado di variabilità o le differenze individuali presenti nella popolazione analizzata.\n\nArchi con bassa variabilità: Indicano che le connessioni sono simili tra le reti, ovvero stabili e consistenti.\nArchi con alta variabilità: Indicano che le connessioni differiscono significativamente tra le reti, suggerendo la presenza di differenze individuali o contestuali.\n\nPer costruire la rete della variabilità, calcoliamo la deviazione standard dei pesi degli archi tra le due reti. Il processo include:\n\nLa creazione di due matrici, una per ciascuna rete.\nUn ciclo che calcola la deviazione standard per ogni arco tra le due reti.\n\n\n# Construct a network where edges are standard deviations across edge weights of networks\nedgeMeanJoint &lt;- matrix(0, 6, 6)\nedgeSDJoint &lt;- matrix(0, 6, 6)\nfor (i in 1:6) {\n    for (j in 1:6) {\n        vector &lt;- c(getWmat(finlandNetwork)[i, j], getWmat\n        (austriaNetwork)[i, j])\n        edgeMeanJoint[i, j] &lt;- mean(vector)\n        edgeSDJoint[i, j] &lt;- sd(vector)\n    }\n}\n\nSuccessivamente, tracciamo le reti in cui i pesi degli archi rappresentano le deviazioni standard di tutti gli archi.\n\nqgraph(edgeSDJoint,\n    layout = LX, edge.labels = TRUE,\n    labels = allDataPlot$Arguments$labels, vsize = 9,\n    cut = 0.09, minimum = 0.01, theme = \"colorblind\"\n)\n\n\n\n\n\n\n\n\nAllo stesso modo in cui abbiamo confrontato i paesi, possiamo confrontare i generi. Come mostrato nel seguente blocco di codice, stimiamo la rete per il gruppo maschile, quella per il gruppo femminile e la rete di differenza. Le differenze risultano molto piccole o addirittura trascurabili.\n\nmaleNetwork &lt;- estimateNetwork(maleData, default = \"EBICglasso\")\nfemaleNetwork &lt;- estimateNetwork(femaleData, default = \"EBICglasso\")\n\nplot(maleNetwork,\n    title = \"Male\", vsize = 9, edge.labels = TRUE,\n    cut = 0.10, minimum = 0.05, layout = LX\n)\n\nplot(femaleNetwork,\n    title = \"Female\", vsize = 9,\n    edge.labels = TRUE,\n    cut = 0.10, minimum = 0.05, layout = LX\n)\n\nqgraph(femaleNetwork$graph - maleNetwork$graph,\n    title =\n        \"Difference\", cut = 0.1,\n    labels = allDataPlot$Arguments$labels, vsize = 9,\n    minimum = 0.01,\n    edge.labels = TRUE, layout = LX, theme =\n        \"colorblind\"\n)\n\nDi seguito eseguiamo il Network Comparison Test (NCT) e osserviamo che i valori \\(p\\) relativi alle differenze tra tutti gli archi non sono statisticamente significativi.\n\nComparedGender &lt;- NCT(\nmaleNetwork, # network 1\nfemaleNetwork, # network 2\nverbose = FALSE, # hide warnings and progress bar\nit = 1000, # number of iterations\nabs = T, # test strength or expected influence?\nbinary.data = FALSE, # set data distribution\ntest.edges = TRUE, # test edge differences\nedges = 'all', # which edges to test\nprogressbar = FALSE) # progress bar\nComparedGender$einv.pvals # Holm-Bonferroni adjusted p-values for each edge",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#valutazione-della-robustezza-e-accuratezza",
    "href": "chapters/networks/01_networks.html#valutazione-della-robustezza-e-accuratezza",
    "title": "81  Network psicologici",
    "section": "81.15 Valutazione della Robustezza e Accuratezza",
    "text": "81.15 Valutazione della Robustezza e Accuratezza\nIl metodo più comune per valutare la stabilità e l’accuratezza delle reti stimate è il bootstrapping. Questo procedimento prevede la creazione di un grande numero di reti bootstrappate (almeno 1000) a partire dai dati originali.\nPassaggi del Bootstrapping:\n\nSi generano un numero elevato di reti bootstrappate basate sui dati originali.\nSi calcolano i pesi degli archi per ciascuna di queste reti.\nSi utilizzano i pesi degli archi delle reti bootstrappate per costruire intervalli di confidenza che rappresentano l’accuratezza degli archi.\n\nInterpretazione:\n\nPer ogni arco nella rete stimata, i pesi vengono confrontati con gli intervalli di confidenza generati dalle reti bootstrappate.\nUn arco è considerato statisticamente significativo se i limiti superiore e inferiore dell’intervallo di confidenza non includono lo zero.\nAl contrario, un arco è non significativo se uno dei limiti dell’intervallo di confidenza attraversa la linea dello zero.\n\nQuesto approccio consente di identificare quali archi sono robusti e quali potrebbero essere il risultato di variabilità casuale nei dati.\n\nnCores &lt;- parallel::detectCores() - 1\n# Non-parametric bootstrap for stability of edges and of edge differences\n\nallBoot &lt;- bootnet(\n    allNetwork, # network input\n    default = \"EBICglasso\", # method\n    nCores = nCores, # number of cores for parallelization\n    computeCentrality = FALSE, # estimate centrality?\n    statistics = \"edge\" # what statistics do we want?\n)\n\n\nplot(allBoot,\n    plot = \"area\", order = \"sample\", legend = FALSE\n)\n\n\n\n\n\n\n\n\nCome mostrato nella Figura precedente, solo gli archi autonomy-emotion ed emotion-SRL attraversano la linea dello zero e, pertanto, non sono significativi.\nPossiamo inoltre tracciare il grafico delle differenze tra gli archi, che verifica se i pesi degli archi differiscono significativamente tra loro.\n\nplot(allBoot,\n    plot = \"difference\", order = \"sample\",\n    onlyNonZero = FALSE, labels = TRUE\n)\n\nInterpretazione del Grafico delle Differenze tra gli Archi\n\nQuadrati grigi: Indicano che l’intervallo di confidenza al 95% ottenuto dal bootstrapping per la differenza tra due archi attraversa la linea dello zero, suggerendo che la differenza non è statisticamente significativa.\nQuadrati neri: Indicano che l’intervallo di confidenza non attraversa lo zero, quindi la differenza tra i due archi è significativa.\n\nAd esempio:\n\nGli archi autonomy-emotion ed emotion-SRL presentano un quadrato grigio, indicando una differenza non significativa.\nGli archi emotion-SRL e relatedness-SRL presentano un quadrato nero, indicando che i due archi differiscono significativamente.\n\nL’accuratezza delle misure di centralità viene valutata tramite il case dropping test. In questo test, vengono eliminate diverse proporzioni di casi dai dati, e si calcola la correlazione tra la misura di centralità osservata e quella ottenuta dai dati ridotti. Se la correlazione diminuisce significativamente dopo l’eliminazione di un piccolo sottoinsieme di casi, la misura di centralità è considerata non affidabile.\n\nset.seed(1)\ncentBoot &lt;- bootnet(\n    allNetwork, # network input\n    default = \"EBICglasso\", # method\n    type = \"case\", # method for testing centrality stability\n    nCores = nCores, # number of cores\n    computeCentrality = TRUE, # compute centrality\n    statistics = c(\"strength\", \"expectedInfluence\"),\n    nBoots = 19000, # number of bootstraps\n    caseMin = .05, # min cases to drop\n    caseMax = .95 # max cases to drop\n)\n\nIl coefficiente di stabilità della correlazione è una metrica utilizzata per valutare la stabilità delle misure di centralità attraverso il case dropping test. Esso viene stimato come la massima proporzione di casi che può essere eliminata mantenendo una correlazione di almeno 0.7 con il campione originale.\n\ncorStability(centBoot)\n\n=== Correlation Stability Analysis === \n\nSampling levels tested:\n   nPerson Drop%    n\n1      358    95 1954\n2     1074    85 1918\n3     1790    75 1865\n4     2506    65 1928\n5     3222    55 1826\n6     3938    45 1939\n7     4654    35 1891\n8     5370    25 1930\n9     6086    15 1927\n10    6802     5 1822\n\nMaximum drop proportions to retain correlation of 0.7 in at least 95% of the samples:\n\nexpectedInfluence: 0.95 (CS-coefficient is highest level tested)\n  - For more accuracy, run bootnet(..., caseMin = 0.85, caseMax = 1) \n\nstrength: 0.95 (CS-coefficient is highest level tested)\n  - For more accuracy, run bootnet(..., caseMin = 0.85, caseMax = 1) \n\nAccuracy can also be increased by increasing both 'nBoots' and 'caseN'.\n\n\nSe tracciamo i risultati, possiamo osservare che il coefficiente di stabilità della correlazione è pari a 0.95, un valore molto elevato che indica un’alta stabilità degli archi.\n\nplot(centBoot)",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#riflessioni-conclusive",
    "href": "chapters/networks/01_networks.html#riflessioni-conclusive",
    "title": "81  Network psicologici",
    "section": "81.16 Riflessioni Conclusive",
    "text": "81.16 Riflessioni Conclusive\nIl campo delle reti psicologiche è in rapida espansione, con metodi che vengono continuamente perfezionati. In questo capitolo, abbiamo illustrato i passaggi fondamentali per analizzare una rete psicologica, visualizzarne i risultati e confrontare diverse reti. Inoltre, abbiamo mostrato come utilizzare metodi statistici robusti per il confronto delle reti e come testare l’accuratezza delle reti stimate attraverso il bootstrapping.\nUn’importante questione è il confronto tra le reti psicologiche e altri metodi utilizzati nel campo educativo, come l’Epistemic Network Analysis (ENA). ENA, ad esempio:\n\nNon offre strumenti per testare se i pesi degli archi sono significativamente diversi dal caso.\nNon include metodi rigorosi per confrontare le reti o verificare i pesi degli archi.\nNon prevede misure di centralità o altre metriche tipiche delle reti.\n\nDi fatto, l’Epistemic Network Analysis (ENA) non ha una connessione diretta con i metodi propri dell’analisi delle reti. Strumenti fondamentali come la verifica casuale o il calcolo delle metriche di rete non possono essere applicati. Analogamente, il process mining, che genera reti di transizione, presenta limitazioni significative, offrendo pochi test statistici per confermare la validità dei modelli generati o per confrontarli in modo rigoroso con altri modelli.\nL’analisi delle reti sociali (SNA) è forse il metodo più vicino alle reti psicologiche. Tuttavia, SNA si limita solitamente a tipi specifici di archi (come co-occorrenze, risposte o interazioni) che sono quasi sempre non direzionati e positivi, concentrandosi principalmente su interazioni sociali o semantiche.\nVantaggi delle Reti Psicologiche\nLe reti psicologiche offrono una prospettiva molto più ampia sulle interazioni e le dipendenze tra le variabili, grazie a un vasto numero di metodi di stima e tecniche di ottimizzazione disponibili. Inoltre, il campo è supportato da una comunità scientifica dinamica che continua a migliorare e innovare i metodi esistenti.\nUn altro punto di forza è che le reti psicologiche non richiedono una teoria preesistente o forti assunzioni sulle variabili modellate, rendendole strumenti analitici flessibili e potenti. Come affermato da Borsboom et al., le reti psicologiche “formano un ponte naturale tra l’analisi dei dati e la formazione di teorie basate sui principi della scienza delle reti” e possono quindi essere utilizzate per “generare ipotesi causali”.\nIn conclusione, le reti psicologiche si dimostrano uno strumento potente e versatile per l’analisi dei sistemi complessi, offrendo nuovi modi di esplorare le interazioni tra variabili psicologiche e generare nuove ipotesi teoriche.\nRisorse Raccomandate\n\nEpskamp, S., Borsboom, D., & Fried, E. I. (2018). Estimating psychological networks and their accuracy: A tutorial paper. Behavior Research Methods, 50(1), 195–212. https://doi.org/10.3758/s13428-017-0862-1\nEpskamp, S., & Fried, E. I. (2018). A tutorial on regularized partial correlation networks. Psychological Methods, 23(4), 617–634. https://doi.org/10.1037/met0000167\nVan Borkulo, C. D., Van Bork, R., Boschloo, L., Kossakowski, J. J., Tio, P., Schoevers, R. A., & Waldorp, L. J. (2022). Comparing network structures on three aspects: A permutation test. Psychological Methods. https://doi.org/10.1037/met0000427\nBorsboom, D., Deserno, M. K., Rhemtulla, M., Epskamp, S., Fried, E. I., McNally, R. J., & Waldorp, L. J. (2021). Network analysis of multivariate data in psychological science. Nature Reviews Methods Primers, 1(1), 58. https://doi.org/10.1038/s43586-021-00055-w\nBringmann, L. F., Elmer, T., Epskamp, S., Krause, R. W., Schoch, D., Wichers, M., & Snippe, E. (2019). What do centrality measures measure in psychological networks? Journal of Abnormal Psychology, 128(8), 892–903. https://doi.org/10.1037/abn0000446",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/networks/01_networks.html#informazioni-sullambiente-di-sviluppo",
    "href": "chapters/networks/01_networks.html#informazioni-sullambiente-di-sviluppo",
    "title": "81  Network psicologici",
    "section": "Informazioni sull’Ambiente di Sviluppo",
    "text": "Informazioni sull’Ambiente di Sviluppo\n\nsessionInfo()\n\nR version 4.4.2 (2024-10-31)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.1.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] C\n\ntime zone: Europe/Rome\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] matrixcalc_1.0-6            mgm_1.2-14                 \n [3] qgraph_1.9.8                NetworkComparisonTest_2.2.2\n [5] NetworkToolbox_1.4.2        networktools_1.5.2         \n [7] bootnet_1.6                 rio_1.2.3                  \n [9] MASS_7.3-61                 viridis_0.6.5              \n[11] viridisLite_0.4.2           ggpubr_0.6.0               \n[13] ggExtra_0.10.1              gridExtra_2.3              \n[15] patchwork_1.3.0             bayesplot_1.11.1           \n[17] semTools_0.5-6              semPlot_1.1.6              \n[19] lavaan_0.6-19               psych_2.4.6.26             \n[21] scales_1.3.0                markdown_1.13              \n[23] knitr_1.49                  lubridate_1.9.3            \n[25] forcats_1.0.0               stringr_1.5.1              \n[27] dplyr_1.1.4                 purrr_1.0.2                \n[29] readr_2.1.5                 tidyr_1.3.1                \n[31] tibble_3.2.1                ggplot2_3.5.1              \n[33] tidyverse_2.0.0             here_1.0.1                 \n\nloaded via a namespace (and not attached):\n  [1] splines_4.4.2       later_1.3.2         pbdZMQ_0.3-13      \n  [4] R.oo_1.27.0         XML_3.99-0.17       rpart_4.1.23       \n  [7] lifecycle_1.0.4     rstatix_0.7.2       doParallel_1.0.17  \n [10] rprojroot_2.0.4     lattice_0.22-6      survey_4.4-2       \n [13] rockchalk_1.8.157   backports_1.5.0     magrittr_2.0.3     \n [16] openxlsx_4.2.7.1    Hmisc_5.2-0         rmarkdown_2.29     \n [19] plotrix_3.8-4       IsingFit_0.4        httpuv_1.6.15      \n [22] zip_2.3.1           DBI_1.2.3           pbapply_1.7-2      \n [25] minqa_1.2.8         RColorBrewer_1.1-3  multcomp_1.4-26    \n [28] abind_1.4-8         quadprog_1.5-8      R.utils_2.12.3     \n [31] nnet_7.3-19         TH.data_1.1-2       sandwich_3.1-1     \n [34] relaimpo_2.2-7      gdata_3.0.1         ellipse_0.5.0      \n [37] openintro_2.5.0     arm_1.14-4          airports_0.1.0     \n [40] codetools_0.2-20    shape_1.4.6.1       tidyselect_1.2.1   \n [43] farver_2.1.2        IsingSampler_0.2.3  lme4_1.1-35.5      \n [46] stats4_4.4.2        base64enc_0.1-3     eigenmodel_1.11    \n [49] jsonlite_1.8.9      e1071_1.7-16        mitml_0.4-5        \n [52] Formula_1.2-5       iterators_1.0.14    survival_3.7-0     \n [55] emmeans_1.10.5      foreach_1.5.2       tools_4.4.2        \n [58] snow_0.4-4          Rcpp_1.0.13-1       glue_1.8.0         \n [61] pan_1.9             mnormt_2.1.1        xfun_0.49          \n [64] IRdisplay_1.1       withr_3.0.2         fastmap_1.2.0      \n [67] mitools_2.4         boot_1.3-31         fansi_1.0.6        \n [70] digest_0.6.37       mi_1.1              timechange_0.3.0   \n [73] R6_2.5.1            mime_0.12           estimability_1.5.1 \n [76] mice_3.16.0         colorspace_2.1-1    Cairo_1.6-2        \n [79] gtools_3.9.5        jpeg_0.1-10         weights_1.0.4      \n [82] R.methodsS3_1.8.2   utf8_1.2.4          generics_0.1.3     \n [85] data.table_1.16.2   corpcor_1.6.10      class_7.3-22       \n [88] usdata_0.3.1        htmlwidgets_1.6.4   pkgconfig_2.0.3    \n [91] sem_3.1-16          gtable_0.3.6        htmltools_0.5.8.1  \n [94] carData_3.0-5       cocor_1.1-4         png_0.1-8          \n [97] wordcloud_2.6       rstudioapi_0.17.1   tzdb_0.4.0         \n[100] reshape2_1.4.4      uuid_1.2-1          curl_6.0.1         \n[103] coda_0.19-4.1       checkmate_2.3.2     nlme_3.1-166       \n[106] nloptr_2.1.1        proxy_0.4-27        repr_1.1.7         \n[109] zoo_1.8-12          parallel_4.4.2      miniUI_0.1.1.1     \n[112] foreign_0.8-87      pillar_1.9.0        grid_4.4.2         \n[115] vctrs_0.6.5         promises_1.3.0      jomo_2.7-6         \n[118] car_3.1-3           OpenMx_2.21.13      xtable_1.8-4       \n[121] cluster_2.1.6       htmlTable_2.4.3     evaluate_1.0.1     \n[124] pbivnorm_0.6.0      mvtnorm_1.3-2       cli_3.6.3          \n[127] kutils_1.73         compiler_4.4.2      rlang_1.1.4        \n[130] crayon_1.5.3        smacof_2.1-7        ggsignif_0.6.4     \n[133] labeling_0.4.3      fdrtool_1.2.18      plyr_1.8.9         \n[136] stringi_1.8.4       nnls_1.6            munsell_0.5.1      \n[139] lisrelToR_0.3       glmnet_4.1-8        pacman_0.5.1       \n[142] Matrix_1.7-1        IRkernel_1.3.2      hms_1.1.3          \n[145] glasso_1.11         shiny_1.9.1         haven_2.5.4        \n[148] igraph_2.1.1        broom_1.0.7         RcppParallel_5.1.9 \n[151] cherryblossom_0.1.0 polynom_1.4-1      \n\n\n\n\n\n\nSaqr, M., & López-Pernas, S. (2024). Learning analytics methods and tutorials: A practical guide using R. Springer Nature.",
    "crumbs": [
      "Networks",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Network psicologici</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a1_intro_r.html",
    "href": "chapters/appendix/a1_intro_r.html",
    "title": "Appendice A — Linguaggio R",
    "section": "",
    "text": "\\(\\mathsf{R}\\) è un linguaggio di programmazione per l’analisi dei dati, il calcolo e la visualizzazione grafica. È open source ed estensibile, il che significa che il codice sorgente è disponibile per essere esaminato e riutilizzato. Può essere scaricato gratuitamente dal sito web del Comprehensive R Archive Network (CRAN) ed è disponibile per PC, MacOS e sistemi operativi Linux/Unix. Gran parte del core-R è scritto in Fortran o C++, ma molti pacchetti per \\(\\mathsf{R}\\) sono scritti in \\(\\mathsf{R}\\) stesso. Chiunque può aggiungere pacchetti al CRAN o ad altri repository come GitHub o BioConductor. CRAN ha test di garanzia della qualità per garantire che i programmi contribuiti abbiano una documentazione coerente e non falliscano durante l’esecuzione degli esempi forniti. Al momento ci sono migliaia di pacchetti disponibili per \\(\\mathsf{R}\\) e questo numero aumenta quotidianamente.\nPer programmare in \\(\\mathsf{R}\\), è importante seguire le regole sintattiche del linguaggio. Se una riga di codice non è scritta correttamente, l’interprete di \\(\\mathsf{R}\\) segnalerà un errore. Questo può essere difficile per i principianti, ma ci sono due vantaggi nell’atto di scrivere codice.\n\nPrima di poter risolvere un problema con il codice, è necessario comprenderlo e analizzarlo in modo preciso. Questo aiuta a sviluppare una comprensione più profonda del problema e a identificare soluzioni efficaci.\nInoltre, se un programma non funziona correttamente, il programmatore che lo ha scritto è l’unico responsabile. Questo aiuta i programmatori a sviluppare una maggiore autoconsapevolezza e responsabilità nei confronti del proprio lavoro.\n\nInoltre, su Internet è disponibile una vasta gamma di materiali utili per avvicinarsi all’ambiente \\(\\mathsf{R}\\) e aiutare l’utente nell’apprendimento di questo software statistico. Tra le tante introduzioni al linguaggio \\(\\mathsf{R}\\), si veda ad esempio, Introduction to R di Venables, Smith, and the R development core team (2023).",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Linguaggio R</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a2_sums.html",
    "href": "chapters/appendix/a2_sums.html",
    "title": "Appendice B — Simbolo di somma",
    "section": "",
    "text": "B.1 Manipolazione di somme\nÈ conveniente utilizzare le seguenti regole per semplificare i calcoli che coinvolgono l’operatore della sommatoria.",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Simbolo di somma</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a2_sums.html#manipolazione-di-somme",
    "href": "chapters/appendix/a2_sums.html#manipolazione-di-somme",
    "title": "Appendice B — Simbolo di somma",
    "section": "",
    "text": "B.1.1 Proprietà 1\nLa sommatoria di \\(n\\) valori tutti pari alla stessa costante \\(a\\) è pari a \\(n\\) volte la costante stessa:\n\\[\n\\sum_{i=1}^{n} a = \\underbrace{a + a + \\dots + a} = {n\\text{ volte } a} = n a.\n\\]\n\n\nB.1.2 Proprietà 2 (proprietà distributiva)\nNel caso in cui l’argomento contenga una costante, è possibile riscrivere la sommatoria. Ad esempio con\n\\[\n\\sum_{i=1}^{n} a x_i = a x_1 + a x_2 + \\dots + a x_n\n\\]\nè possibile raccogliere la costante \\(a\\) e fare \\(a(x_1 +x_2 + \\dots + x_n)\\). Quindi possiamo scrivere\n\\[\n\\sum_{i=1}^{n} a x_i = a \\sum_{i=1}^{n} x_i.\n\\]\n\n\nB.1.3 Proprietà 3 (proprietà associativa)\nNel caso in cui\n\\[\n\\sum_{i=1}^{n} (a + x_i) = (a + x_1) + (a + x_1) + \\dots  (a + x_n)\n\\]\nsi ha che\n\\[\n\\sum_{i=1}^{n} (a + x_i) = n a + \\sum_{i=1}^{n} x_i.\n\\]\nÈ dunque chiaro che in generale possiamo scrivere\n\\[\n\\sum_{i=1}^{n} (x_i + y_i) = \\sum_{i=1}^{n} x_i + \\sum_{i=1}^{n} y_i.\n\\]\n\n\nB.1.4 Proprietà 4\nSe deve essere eseguita un’operazione algebrica (innalzamento a potenza, logaritmo, ecc.) sull’argomento della sommatoria, allora tale operazione algebrica deve essere eseguita prima della somma. Per esempio,\n\\[\n\\sum_{i=1}^{n} x_i^2 = x_1^2 + x_2^2 + \\dots + x_n^2 \\neq \\left(\\sum_{i=1}^{n} x_i \\right)^2.\n\\]\n\n\nB.1.5 Proprietà 5\nNel caso si voglia calcolare \\(\\sum_{i=1}^{n} x_i y_i\\), il prodotto tra i punteggi appaiati deve essere eseguito prima e la somma dopo:\n\\[\n\\sum_{i=1}^{n} x_i y_i = x_1 y_1 + x_2 y_2 + \\dots + x_n y_n,\n\\]\ninfatti, \\(a_1 b_1 + a_2 b_2 \\neq (a_1 + a_2)(b_1 + b_2)\\).",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Simbolo di somma</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a2_sums.html#doppia-sommatoria",
    "href": "chapters/appendix/a2_sums.html#doppia-sommatoria",
    "title": "Appendice B — Simbolo di somma",
    "section": "B.2 Doppia sommatoria",
    "text": "B.2 Doppia sommatoria\nÈ possibile incontrare la seguente espressione in cui figurano una doppia sommatoria e un doppio indice:\n\\[\n\\sum_{i=1}^{n}\\sum_{j=1}^{m} x_{ij}.\n\\]\nLa doppia sommatoria comporta che per ogni valore dell’indice esterno, \\(i\\) da \\(1\\) ad \\(n\\), occorre sviluppare la seconda sommatoria per \\(j\\) da \\(1\\) ad \\(m\\). Quindi,\n\\[\n\\sum_{i=1}^{3}\\sum_{j=4}^{6} x_{ij} = (x_{1, 4} + x_{1, 5} + x_{1, 6}) + (x_{2, 4} + x_{2, 5} + x_{2, 6}) + (x_{3, 4} + x_{3, 5} + x_{3, 6}).\n\\]\nUn caso particolare interessante di doppia sommatoria è il seguente:\n\\[\n\\sum_{i=1}^{n}\\sum_{j=1}^{n} x_i y_j\n\\]\nSi può osservare che nella sommatoria interna (quella che dipende dall’indice \\(j\\)), la quantità \\(x_i\\) è costante, ovvero non dipende dall’indice (che è \\(j\\)). Allora possiamo estrarre \\(x_i\\) dall’operatore di sommatoria interna e scrivere\n\\[\n\\sum_{i=1}^{n} \\left( x_i \\sum_{j=1}^{n} y_j \\right).\n\\]\nAllo stesso modo si può osservare che nell’argomento della sommatoria esterna la quantità costituita dalla sommatoria in \\(j\\) non dipende dall’indice \\(i\\) e quindi questa quantità può essere estratta dalla sommatoria esterna. Si ottiene quindi\n\\[\n\\sum_{i=1}^{n}\\sum_{j=1}^{n} x_i y_j = \\sum_{i=1}^{n} \\left( x_i \\sum_{j=1}^{n} y_j \\right) = \\sum_{i=1}^{n} x_i \\sum_{j=1}^{n} y_j.\n\\]\nFacciamo un esercizio. Verifichiamo quanto detto sopra nel caso particolare di \\(x = \\{2, 3, 1\\}\\) e \\(y = \\{1, 4, 9\\}\\), svolgendo prima la doppia sommatoria per poi verificare che quanto così ottenuto sia uguale al prodotto delle due sommatorie.\n\\[\n\\begin{align}\n\\sum_{i=1}^3 \\sum_{j=1}^3 x_i y_j &= x_1y_1 + x_1y_2 + x_1y_3 +\nx_2y_1 + x_2y_2 + x_2y_3 +\nx_3y_1 + x_3y_2 + x_3y_3 \\notag\\\\\n&= 2 \\times (1+4+9) + 3 \\times (1+4+9) + 2 \\times (1+4+9) = 84,\\notag\n\\end{align}\n\\]\novvero\n\\[\n(2 + 3 + 1) \\times (1+4+9) = 84.\n\\]",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Simbolo di somma</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a3_calculus.html",
    "href": "chapters/appendix/a3_calculus.html",
    "title": "Appendice C — Per liberarvi dai terrori preliminari",
    "section": "",
    "text": "C.1 Introduzione ai logaritmi\nIl logaritmo è una funzione matematica che risponde alla domanda: “quante volte devo moltiplicare un certo numero (chiamato”base”) per ottenere un altro numero?” Matematicamente, questo è espresso come:\n\\[\n\\log_b(a) = x \\iff b^x = a\n\\]\nAd esempio, \\(\\log_2(8) = 3\\) perché \\(2^3 = 8\\).\nNel contesto dei logaritmi, i valori molto piccoli (compresi tra 0 e 1) diventano più grandi (in termini assoluti) e negativi quando applichiamo una funzione logaritmica. Questo è utile per stabilizzare i calcoli, specialmente quando lavoriamo con prodotti di numeri molto piccoli che potrebbero portare a problemi di underflow.\nPer esempio: - \\(\\log(1) = 0\\) - \\(\\log(0.1) = -1\\) - \\(\\log(0.01) = -2\\) - \\(\\log(0.001) = -3\\)\nCome si può vedere, i valori assoluti dei logaritmi crescono man mano che il numero originale si avvicina a zero.\nUna delle proprietà più utili dei logaritmi è che consentono di trasformare un prodotto in una somma:\n\\[\n\\log_b(a \\times c) = \\log_b(a) + \\log_b(c)\n\\]\nQuesta proprietà è estremamente utile in calcoli complessi, come nella statistica bayesiana, dove il prodotto di molte probabilità potrebbe diventare un numero molto piccolo e causare problemi numerici.\nUn’altra proprietà utile dei logaritmi è che un rapporto tra due numeri diventa la differenza dei loro logaritmi:\n\\[\n\\log_b\\left(\\frac{a}{c}\\right) = \\log_b(a) - \\log_b(c)\n\\]\nAnche questa proprietà è molto utilizzata in matematica, specialmente in situazioni in cui è necessario normalizzare i dati.\nIn sintesi, i logaritmi sono strumenti potenti per semplificare e stabilizzare i calcoli matematici. Essi consentono di lavorare più agevolmente con numeri molto grandi o molto piccoli e di trasformare operazioni complesse come prodotti e divisioni in somme e differenze, rendendo i calcoli più gestibili e meno inclini a errori numerici.",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Per liberarvi dai terrori preliminari</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a4_linear_alg.html",
    "href": "chapters/appendix/a4_linear_alg.html",
    "title": "Appendice D — Elementi di algebra lineare",
    "section": "",
    "text": "D.1 Vettori",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a4_linear_alg.html#vettori",
    "href": "chapters/appendix/a4_linear_alg.html#vettori",
    "title": "Appendice D — Elementi di algebra lineare",
    "section": "",
    "text": "D.1.1 Vettori nello spazio euclideo\nUn vettore geometrico è un segmento orientato dotato di una lunghezza, una direzione e un verso. Spesso viene rappresentato con una freccia. Dato che i vettori non hanno posizione (ma solo direzione, verso e intensità), sono possibili rappresentazioni multiple dello stesso vettore. Nella discussione seguente, considereremo soltanto vettori che hanno origine nel punto (0, 0). Questo verrà chiarito dall’esempio seguente. La posizione di un punto nel piano può essere espressa nei termini di una coppia ordinata di numeri (\\(x, y\\)), le coordinate di quel punto. Tale coppia di valori rappresenta la distanza verticale dal punto a ciascuno degli assi coordinati.\nPossiamo anche definire il punto \\(P\\) specificando la distanza e la direzione di \\(P\\) dall’origine, ovvero nei termini del vettore \\(\\overrightarrow{OP}\\). A sua volta, questo vettore può essere espresso nei termini delle sue componenti nelle direzioni orizzontali e verticali:\n\\[\n\\overrightarrow{OP} = \\left[ \\begin{array}{c}\n2\\\\\n3\n\\end{array}\n\\right]\n\\]\nSe volessimo specificare un punto in uno spazio a 3 dimensioni, avremmo:\n\\[\n\\overrightarrow{OP} = \\left[ \\begin{array}{c}\nx\\\\\ny\\\\\nz\n\\end{array}\n\\right]\n\\]\nIn generale, un punto \\(P\\) in uno spazio a \\(n\\)-dimensioni sarà specificato da:\n\\[\n\\overrightarrow{OP} = \\left[ \\begin{array}{c}\nv_1\\\\\nv_2\\\\\n\\dots\\\\\nv_n\n\\end{array}\n\\right]\n\\]\nDal punto di vista geometrico, dunque, un vettore rappresenta un punto in uno spazio \\(n\\)-dimensionale.\nIn \\(\\mathsf{R}\\), un vettore è definito come\n\na &lt;- c(1, 3, 2) \na |&gt; print()\n\n[1] 1 3 2\n\n\n\ndf &lt;- data.frame(\n    x = c(0, 3), # Start points for arrows\n    y = c(0, 1), # Start points for arrows\n    xend = c(2, 5), # End points for arrows\n    yend = c(3, 4) # End points for arrows\n)\n\n# Plot arrows using geom_segment with the updated 'linewidth' aesthetic\nggplot(df, aes(x = x, y = y)) +\n    geom_segment(aes(xend = xend, yend = yend),\n        arrow = arrow(length = unit(0.3, \"cm\")), linewidth = 1\n    ) +\n    geom_hline(yintercept = 0, linetype = \"dashed\") +\n    geom_vline(xintercept = 0, linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\n\nD.1.2 Somma e differenza di vettori\nLa somma di due vettori è definita come\n\\[\n(a_1, a_2) + (b_1, b_2) = (a_1 + b_1, a_2 + b_2).\n\\]\nIn \\(\\mathsf{R}\\) abbiamo\n\na &lt;- c(1, 3, 2) \nb &lt;- c(2, 8, 9) \na + b |&gt; print()\n\n[1] 2 8 9\n\n\n\n31111\n\n\nLa differenza di due vettori è\n\\[\n(a_1, a_2) - (b_1, b_2) = (a_1 - b_1, b_2 - b_2).\n\\]\nIn \\(\\mathsf{R}\\) abbiamo\n\na &lt;- c(1, 3, 2) \nb &lt;- c(2, 8, 9) \na - b |&gt; print()\n\n[1] 2 8 9\n\n\n\n-1-5-7\n\n\n\n\nD.1.3 Moltiplicazione scalare\nLa moltiplicazione scalare di un vettore per un numero reale (o scalare) è data da\n\\[\n\\rho (a_1, a_2) = (\\rho a_1, \\rho a_2)\n\\]\nDal punto di vista geometrico, la moltiplicazione scalare effettua una estensione o contrazione del vettore \\(\\boldsymbol{a}\\), preservandone la direzione.\nIn \\(\\mathsf{R}\\) abbiamo\n\na &lt;- 2\nx &lt;- c(2, 8, 9) \na * x |&gt; print()\n\n[1] 2 8 9\n\n\n\n41618\n\n\n\n\nD.1.4 Combinazione lineare\nSe \\(\\mathbf{v}_{1}, \\dots, \\mathbf{v}_{n}\\) sono vettori e \\(a_1, \\dots, a_n\\) sono scalari, allora la combinazione lineare di questi vettori con questi coefficienti scalari è data da\n\\[\n{\\displaystyle a_{1}\\mathbf {v} _{1}+a_{2}\\mathbf {v} _{2}+a_{3}\\mathbf {v} _{3}+\\cdots +a_{n}\\mathbf {v} _{n}.}\n\\] Per esempio, in \\(\\mathsf{R}\\) possiamo aver\n\na &lt;- c(2, 3, 4)\nv1 &lt;- c(2, 8, 3) \nv2 &lt;- c(4, 5, 1) \nv3 &lt;- c(1, 3, 2) \ny &lt;- a[1] * v1 + a[2] * v2 + a[3] * v3\ny |&gt; print()\n\n[1] 20 43 17\n\n\n\n\nD.1.5 Vettore 0 e vettore 1\nIl vettore 0 è costituito da \\(n\\) elementi, tutti uguali a 0. Il vettore 1 è costituito da \\(n\\) elementi, tutti uguali a 1.\nIn \\(\\mathsf{R}\\) abbiamo\n\nx &lt;- rep(0, 5) \nx |&gt; print()\ny &lt;- rep(1, 5)\ny |&gt; print()\n\n[1] 0 0 0 0 0\n[1] 1 1 1 1 1\n\n\n\n\nD.1.6 Ortogonalità tra vettori\nDue vettori si dicono ortogonali, e si scrive \\(\\boldsymbol{a} \\bot \\boldsymbol{b}\\), se e solo se il loro prodotto scalare è nullo:\n\\[\n\\boldsymbol{a}'\\boldsymbol{b} = 0.\n\\]\nIn \\(\\mathsf{R}\\) abbiamo\n\nv1 &lt;- c(1, 1)\nv2 &lt;- c(-1, 1)\nsum(v1 * v2) |&gt; print()\n\n[1] 0\n\n\n\n\nD.1.7 Trasposta di un vettore\nIn un vettore trasposto gli indici delle righe prendono il posto degli indici delle colonne, e viceversa.\nIn \\(\\mathsf{R}\\) abbiamo\n\nv1 &lt;- c(1, 3, 7) %&gt;% \n  as.matrix()\nv1 |&gt; print()\n\n     [,1]\n[1,]    1\n[2,]    3\n[3,]    7\n\n\nLe dimensioni di v1 sono\n\ndim(v1)\n\n\n31\n\n\nLa trasposta di v1 è\n\nv2 &lt;- t(v1)\nv2 |&gt; print()\n\n     [,1] [,2] [,3]\n[1,]    1    3    7\n\n\ndi dimensioni\n\ndim(v2) |&gt; print()\n\n[1] 1 3\n\n\n\n\nD.1.8 Norma o lunghezza di un vettore\nPer il teorema di Pitagora, la norma di un vettore \\((a_1,\na_2)\\) è \\(\\sqrt{a_1^2 + a_2^2}\\) ed è denotata da \\(\\| (a_1,\na_2) \\|\\). Infatti, se un vettore \\(\\boldsymbol{a}\\) (l’ipotenusa) è la somma di due vettori ortogonali \\(\\boldsymbol{a}_1\\) e \\(\\boldsymbol{a}_2\\) (i cateti), allora la lunghezza al quadrato di \\(\\boldsymbol{a}\\) è uguale alla somma dei quadrati delle lunghezze di \\(\\boldsymbol{a}_1\\) e \\(\\boldsymbol{a}_2\\).\nViene detta norma di \\(\\boldsymbol{a}\\) la radice del prodotto scalare di un vettore per se stesso:\n\\[\n\\| \\boldsymbol{a} \\| = \\sqrt{\\boldsymbol{a}'\\boldsymbol{a}}.\n\\]\nIn \\(\\mathsf{R}\\) abbiamo\n\nsqrt(t(v1) %*% v1) |&gt; print()\n\n         [,1]\n[1,] 7.681146",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a4_linear_alg.html#matrici",
    "href": "chapters/appendix/a4_linear_alg.html#matrici",
    "title": "Appendice D — Elementi di algebra lineare",
    "section": "D.2 Matrici",
    "text": "D.2 Matrici\nUna matrice costituisce un insieme rettangolare di scalari ordinati per riga e colonna. Può anche essere vista come la raccolta di \\(m\\) vettori colonna di dimensione \\(n\\) o come la raccolta di \\(n\\) vettori riga di dimensione \\(m\\). Per esempio:\n\\[\n\\boldsymbol{A} =  \\left[ \\begin{array}{c c c}\na_{11} & a_{12} & a_{13}\\\\\na_{21} & a_{22} & a_{23} \\end{array} \\right]\n\\]\nIn \\(\\mathsf{R}\\) abbiamo\n\nM &lt;- matrix(\n  c(1, 2, 3, 4, 5, 6), \n  ncol = 3, \n  byrow = FALSE)\nM |&gt; print()\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\n\nD.2.1 Dimensioni della matrice\nI numeri interi \\(m\\) ed \\(n\\) si dicono dimensioni della matrice, ovvero \\(\\boldsymbol{A}\\) si dice matrice di dimensioni \\(m \\times n\\) o di ordine \\(m \\times n\\). Nel caso presente, la matrice \\(\\boldsymbol{A}\\) ha dimensioni \\(2 \\times 3\\).\n\ndim(M) |&gt; print()\n\n[1] 2 3\n\n\n\n\nD.2.2 Matrice trasposta\nSi definisce matrice trasposta di \\(\\boldsymbol{A}\\), e si denota con \\(\\boldsymbol{A}'\\) oppure \\(\\boldsymbol{A}'\\), la matrice \\(\\boldsymbol{B} = \\boldsymbol{A}'\\) di ordine \\(n \\times m\\) cui elementi sono:\n\\[\nb_{ij} = a_{ji},  \\quad        i = 1 \\dots m, j = 1 \\dots n\n\\]\nPer esempio,\n\\[\n\\left[ \\begin{array}{c c}\n-2 & 5\\\\\n3 & 1\\\\\n7 & -6\n\\end{array}\n\\right]'=\n\\left[ \\begin{array}{c c c}\n-2 & 3 & 7\\\\\n5 & 1 & -6\n\\end{array}\n\\right]\n\\]\n\n\nD.2.3 Matrice simmetrica\nSe accade che \\(\\boldsymbol{A} = \\boldsymbol{A}'\\) allora la matrice è detta simmetrica.\n\\[\n\\boldsymbol{A} =  \\left[ \\begin{array}{c c c}\n7 & 1 & 2\\\\\n1 & 8 & 3\\\\\n2 & 3 & 9 \\end{array} \\right]\n\\]\n\n\\((\\boldsymbol{A} + \\boldsymbol{B})' = (\\boldsymbol{A})' +\n(\\boldsymbol{B})'\\)\n\\((\\boldsymbol{A} - \\boldsymbol{B})' = (\\boldsymbol{A})' -\n(\\boldsymbol{B})'\\)\n\\((\\boldsymbol{a} + \\boldsymbol{b})' = (\\boldsymbol{a})' +\n(\\boldsymbol{b})'\\)\n\\((\\boldsymbol{a} - \\boldsymbol{b})' = (\\boldsymbol{a})' -\n(\\boldsymbol{b})'\\)\n\n\n\nD.2.4 Matrice quadrata o rettangolare\nSe \\(m = n\\) allora la matrice \\(\\boldsymbol{A}\\) si dice quadrata di dimensione \\(n\\) o di ordine \\(n\\) altrimenti si dice rettangolare. Le righe di \\(\\boldsymbol{A}\\) sono \\([a_{11}\\ a_{12}\\ a_{13}]\\) e \\([a_{21}\\ a_{22}\\ a_{23}]\\). Le colonne di \\(\\boldsymbol{A}\\) sono \\(\\left[\\begin{array}{c} a_{11} \\\\ a_{21} \\end{array} \\right]\\), \\(\\left[\n\\begin{array}{c} a_{12} \\\\ a_{22} \\end{array} \\right]\\) e \\(\\left[\n\\begin{array}{c} a_{13} \\\\ a_{23} \\end{array} \\right]\\).\n\n\nD.2.5 Diagonale principale\nSe \\(i\\) e \\(j\\) sono numeri interi con \\(1 \\leq i \\leq m\\) e \\(1 \\leq j\n\\leq n\\) allora l’elemento della matrice \\(\\boldsymbol{A}\\) di dimensione \\(m \\times n\\) che si trova in posizione (\\(i, j\\)) viene indicato con \\(a_{ij}\\). Gli elementi \\(a_{ij}\\) di una matrice quadrata \\(\\boldsymbol{A}\\) di ordine \\(n\\) tali che \\(i = j\\) sono detti elementi principali o diagonali e formano la cosiddetta diagonale principale di \\(\\boldsymbol{A}\\).\n\\[\n\\boldsymbol{A} =  \\left[ \\begin{array}{c c c}\na_{11} & a_{12} & a_{13}\\\\\na_{21} &  a_{22} & a_{23}\\\\\na_{31} & a_{32} & a_{33} \\end{array} \\right]\n\\]\n\n\nD.2.6 Matrice diagonale\nSe gli elementi \\(a_{ij}\\) di una matrice quadrata \\(\\boldsymbol{A}\\) sono tali che \\(a_{ij} =0\\) e \\(a_{ii} \\neq 0\\), allora la matrice \\(\\boldsymbol{A}\\) viene detta matrice diagonale.\n\\[\n\\boldsymbol{A} =  \\left[ \\begin{array}{c c c}\na_{11} & 0 & 0\\\\\n0 & a_{22} & 0\\\\\n0 & 0 & a_{33} \\end{array} \\right]\n\\]\n\n\nD.2.7 Matrice identità\nSi definisce matrice identità di ordine \\(n\\) la matrice quadrata diagonale \\(\\boldsymbol{I}_n\\) avente tutti gli elementi principali uguali a \\(1\\):\n\\[\n\\boldsymbol{I}_3 =  \\left[ \\begin{array}{c c c}\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 1 \\end{array} \\right]\n\\]\nLa matrice identità ha la stessa funzione del numero “1” nel sistema dei numeri reali.\n\n\nD.2.8 Matrici diagonali e triangolari\nGli elementi di una matrice che si trovano al di sopra della diagonale principale sono detti sopradiagonali, mentre quelli che si trovano al di sotto della stessa diagonale principale sono detti sottodiagonali. Se una matrice ha tutti gli elementi sopradiagonali e sottodiagonali uguali a zero viene detta matrice diagonale. Se invece ha solo gli elementi sopradiagonali nulli allora viene detta triangolare inferiore. Se ha gli elementi sottodiagonali nulli allora è detta triangolare superiore.\n\n\nD.2.9 Somma e sottrazione\nLa somma e la sottrazione di due matrici sono operazioni definite elemento per elemento. Per sommare due matrici sommiamo gli elementi corrispondenti. Per sottrarre due matrici sottraiamo gli elementi corrispondenti. Si noti che queste operazioni hanno senso solo se le due matrici hanno le stesse dimensioni (altrimenti queste operazioni non sono definite). Per esempio,\n\\[\n\\left[ \\begin{array}{c c}\n-2 & 5\\\\\n3 & 1\\\\\n7 & -6\n\\end{array}\n\\right]+\n\\left[ \\begin{array}{c c}\n3 & -2\\\\\n4 & 5\\\\\n10 & -3\n\\end{array}\n\\right]=\n\\left[ \\begin{array}{c c}\n1 & 3\\\\\n7 & 6\\\\\n17 & -9\n\\end{array}\n\\right]\n\\]\n\nA &lt;- matrix(\n  c(-2, 5, 3, 1, 7, -6), nrow = 3, byrow = TRUE\n)\nA |&gt; print()\n\n     [,1] [,2]\n[1,]   -2    5\n[2,]    3    1\n[3,]    7   -6\n\n\n\nB &lt;- matrix(\n  c(3, -2, 4, 5, 10, -3), nrow = 3, byrow = TRUE\n)\nB |&gt; print()\n\n     [,1] [,2]\n[1,]    3   -2\n[2,]    4    5\n[3,]   10   -3\n\n\n\n(A + B) |&gt; print()\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    7    6\n[3,]   17   -9\n\n\n\\[\n\\left[ \\begin{array}{c c}\n-2 & 5\\\\\n3 & 1\\\\\n7 & -6\n\\end{array}\n\\right]-\n\\left[ \\begin{array}{c c}\n3 & -2\\\\\n4 & 5\\\\\n10 & -3\n\\end{array}\n\\right]=\n\\left[ \\begin{array}{c c}\n-5 & 7\\\\\n-1 & -4\\\\\n-3 & -3\n\\end{array}\n\\right]\n\\]\n\n(A - B) |&gt; print()\n\n     [,1] [,2]\n[1,]   -5    7\n[2,]   -1   -4\n[3,]   -3   -3\n\n\n\n\nD.2.10 Moltiplicazione di scalari e matrici\nL’effetto della moltiplicazione di una matrice \\(\\boldsymbol{A}\\) di qualsiasi dimensione per un numero reale b (scalare) è quello di moltiplicare ciascun elemento in \\(\\boldsymbol{A}\\) per b. Questo è equivalente a sommare \\(\\boldsymbol{A}\\) a se stessa b volte. Per esempio,\n\\[\n3 \\left[ \\begin{array}{c c}\n-2 & 5\\\\\n3 & 1\\\\\n7 & -6\n\\end{array}\n\\right]=\n\\left[ \\begin{array}{c c}\n-6 & 15\\\\\n9 & 3\\\\\n21 & -18\n\\end{array}\n\\right]\n\\]\n\n(3 * A) |&gt; print()\n\n     [,1] [,2]\n[1,]   -6   15\n[2,]    9    3\n[3,]   21  -18\n\n\n\n\nD.2.11 Proprietà della somma e differenza\nSiano \\(A\\), \\(B\\) e \\(C\\) matrici di dimensioni \\(m \\times n\\), e siano \\(k\\) e \\(p\\) scalari appartenenti a un campo \\(F\\). Allora, valgono le seguenti proprietà:\n\n\\(A + B = B + A\\) (commutatività)\n\\(A + (B + C) = (A + B) + C\\) (associatività)\n\\(0 + A = A\\) (0 è l’identità additiva)\n\\(A + (-A) = 0\\) ( (-A) è l’inverso additivo di \\(A\\))\n\\(k(A + B) = kA + kB\\) (la moltiplicazione scalare è distributiva rispetto all’addizione di matrici)\n\\((k + p)A = kA + pA\\) (la moltiplicazione scalare è distributiva rispetto all’addizione di scalari)\n\\((kp)A = k(pA)\\) (associatività della moltiplicazione scalare)\n\\(1A = A\\) (la moltiplicazione per 1 preserva la matrice)\n\n\n\nD.2.12 Moltiplicazione matrice-vettore\nSia \\(A\\) una matrice \\(m \\times n\\) con colonne \\(a_1, a_2, \\dots, a_n\\):\n\\[\nA = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{bmatrix}\n\\]\ne sia \\(x\\) un vettore colonna di dimensione \\(n \\times 1\\), cioè \\(x \\in \\mathbb{R}^n\\):\n\\[\nx = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}.\n\\]\nIl prodotto matrice-vettore \\(Ax\\) è definito come la combinazione lineare delle colonne di \\(A\\) con i coefficienti dati dalle componenti di \\(x\\):\n\\[\nAx = x_1 a_1 + x_2 a_2 + \\dots + x_n a_n = \\sum_{j=1}^n a_{ij}x_j.\n\\]\nInterpretazione geometrica: La moltiplicazione di un vettore per una matrice può essere vista come una trasformazione lineare dello spazio.\nEsempio: Consideriamo le matrici:\n\\[\nA =\n\\begin{bmatrix}\n2 & -1 & 0 \\\\\n3 & 1/2 & \\pi \\\\\n-2 & 1 & 1 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\quad \\text{e} \\quad x =\n\\begin{bmatrix}\n2 \\\\\n1 \\\\\n2\n\\end{bmatrix}.\n\\]\nAllora:\n\\[\nAx = 2 \\begin{bmatrix} 2 \\\\ 3 \\\\ -2 \\\\ 0 \\end{bmatrix} + 1 \\begin{bmatrix} -1 \\\\ 1/2 \\\\ 1 \\\\ 0 \\end{bmatrix} + 2 \\begin{bmatrix} 0 \\\\ \\pi \\\\ 1 \\\\ 0 \\end{bmatrix}\n=\n\\begin{bmatrix} -3 \\\\ -5/2 + 2\\pi \\\\ 5 \\\\ 0 \\end{bmatrix}.\n\\]\n\n\nD.2.13 Prodotto di matrici\nLa moltiplicazione di matrici è un’operazione fondamentale in algebra lineare, ma richiede una certa attenzione. A differenza della somma, il prodotto di matrici non è commutativo e la sua definizione è un po’ più complessa.\nCondizione di conformabilità: Due matrici \\(\\boldsymbol{A}\\) e \\(\\boldsymbol{B}\\) si dicono conformabili per il prodotto \\(\\boldsymbol{AB}\\) se il numero di colonne di \\(\\boldsymbol{A}\\) è uguale al numero di righe di \\(\\boldsymbol{B}\\).\nDefinizione: Siano \\(\\boldsymbol{A}\\) una matrice \\(m \\times p\\) e \\(\\boldsymbol{B}\\) una matrice \\(p \\times n\\). Il prodotto \\(\\boldsymbol{C} = \\boldsymbol{AB}\\) è una matrice \\(m \\times n\\) tale che:\n\\[\nc_{ij} = \\sum_{k=1}^{p} a_{ik}b_{kj}.\n\\]\nIn altre parole, l’elemento \\(c_{ij}\\) di \\(\\boldsymbol{C}\\) si ottiene moltiplicando gli elementi corrispondenti della i-esima riga di \\(\\boldsymbol{A}\\) per gli elementi della j-esima colonna di \\(\\boldsymbol{B}\\) e sommando i prodotti ottenuti.\nProprietà:\n\nAssociatività: \\((AB)C = A(BC)\\).\nNon commutatività: In generale, \\(AB \\neq BA\\).\nDistributività rispetto alla somma: \\(A(B+C) = AB + AC\\) e \\((A+B)C = AC + BC\\).\nMoltiplicazione per uno scalare: \\(k(AB) = (kA)B = A(kB)\\).\nMoltiplicazione per la matrice identità: Se \\(I\\) è la matrice identità, allora \\(AI = A\\) e \\(IA = A\\).\n\nAd esempio, siano \\(\\boldsymbol{A}\\) e \\(\\boldsymbol{B}\\) le seguenti matrici\n\\[\n\\left[ \\begin{array}{c c c}\n-2 & 1 & 1\\\\\n1 & 1 & 4\\\\\n2 & -3 & 2\n\\end{array}\n\\right] \\quad \\text{e} \\quad\n\\left[ \\begin{array}{c c c}\n3 & -2 &1\\\\\n4 & 5 & 0\\\\\n1 & -3 & 1\n\\end{array}\n\\right]\n\\]\nCalcoliamo la matrice \\(\\boldsymbol{C} = \\boldsymbol{AB}\\). L’elemento \\(c_{ij}\\) è uguale alla somma dei prodotti degli elementi della i-esima riga di \\(\\boldsymbol{A}\\) per la j-esima colonna di \\(\\boldsymbol{B}\\).\n\\(c_{11} = (-2) \\cdot 3 + 1 \\cdot 4 + 1 \\cdot 1 = -1\\)\n\\(c_{12} = (-2) \\cdot (-2) + 1 \\cdot 5 + 1 \\cdot (-3) = 6\\)\n\\(c_{13} = (-2) \\cdot 3 + 1 \\cdot 0 + 1 \\cdot 1 = -1\\)\n\\(c_{21} = 1 \\cdot 3 + 1 \\cdot 4 + 4 \\cdot 1 = 11\\)\n\\(c_{22} = 1 \\cdot (-2) + 1 \\cdot 5 + 4 \\cdot (-3) = -9\\)\n\\(c_{23} = 1 \\cdot 3 + 1 \\cdot 0 + 4 \\cdot 1 = 5\\)\n\\(c_{31} = 2 \\cdot 3 +(-3) \\cdot 4 + 2 \\cdot 1 = -4\\)\n\\(c_{32} = 2 \\cdot (-2) +(-3) \\cdot 5 + 2 \\cdot (-3) = -25\\)\n\\(c_{33} = 2 \\cdot 1 + (-3) \\cdot 0 + 2 \\cdot 1 = 4\\)\nIn definitiva\n\\[\n\\boldsymbol{C} =  \\left[ \\begin{array}{c c c}\n-1 & 6 & -1\\\\\n11 & -9 & 5\\\\\n-4 & -25 & 4\n\\end{array}\n\\right].\n\\]\nPossiamo facilmente svolgere i calcoli precedenti in R nel modo seguente:\n\nA = matrix(\n  c(-2, 1, 1, 1, 1, 4, 2, -3, 2), \n  nrow = 3,\n  byrow = TRUE\n)\nA |&gt; print()\n\n     [,1] [,2] [,3]\n[1,]   -2    1    1\n[2,]    1    1    4\n[3,]    2   -3    2\n\n\n\nB = matrix(\n  c(3, -2, 1, 4, 5, 0, 1, -3, 1), \n  nrow = 3,\n  byrow = TRUE\n)\nB |&gt; print()\n\n     [,1] [,2] [,3]\n[1,]    3   -2    1\n[2,]    4    5    0\n[3,]    1   -3    1\n\n\n\n(A %*% B) |&gt; print()\n\n     [,1] [,2] [,3]\n[1,]   -1    6   -1\n[2,]   11   -9    5\n[3,]   -4  -25    4\n\n\nCalcolando il prodotto \\(\\boldsymbol{D} = \\boldsymbol{BA}\\) si trova invece:\n\\[\n\\boldsymbol{D} =  \\left[ \\begin{array}{c c c}\n-6 & -2 & -3\\\\\n-3 & 9 & 24\\\\\n-3 & -5 & -9\n\\end{array}\n\\right]\n\\]\nda cui risulta evidente che \\(\\boldsymbol{AB} \\neq \\boldsymbol{BA}\\).\n\n(B %*% A) |&gt; print()\n\n     [,1] [,2] [,3]\n[1,]   -6   -2   -3\n[2,]   -3    9   24\n[3,]   -3   -5   -9\n\n\n\n\nD.2.14 Casi particolari\nLa matrice identità è l’elemento neutro per il prodotto, cioè se \\(\\boldsymbol{I}\\) è una matrice \\(n \\times n\\) si ha\n\\[\n\\boldsymbol{A} \\boldsymbol{I}_n = \\boldsymbol{I}_n \\boldsymbol{A}\n= \\boldsymbol{A}.\n\\]\nPer esempio,\n\\[\n\\boldsymbol{IA} = \\left(%\n\\begin{array}{cc}\n  1 & 0 \\\\\n  0 & 1 \\\\\n\\end{array}%\n\\right)\n\\left(%\n\\begin{array}{ccc}\n  2 & 3 & -1 \\\\\n  1 & 4 & 7 \\\\\n\\end{array}%\n\\right)=\n\\left(%\n\\begin{array}{ccc}\n  2 & 3 & -1 \\\\\n  1 & 4 & 7 \\\\\n\\end{array}%\n\\right).\n\\]\nIn R la matrice identità si crea nel modo seguente.\n\nprint(diag(2)) \n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n\nDunque\n\nA &lt;- matrix(\n  c(2, 3, -1, 1, 4, 7),\n  nrow = 2, byrow = TRUE\n)\nA |&gt; print()\n\n     [,1] [,2] [,3]\n[1,]    2    3   -1\n[2,]    1    4    7\n\n\n\n(diag(2) %*% A) |&gt; print()\n\n     [,1] [,2] [,3]\n[1,]    2    3   -1\n[2,]    1    4    7\n\n\n\n(A %*% diag(3)) |&gt; print()\n\n     [,1] [,2] [,3]\n[1,]    2    3   -1\n[2,]    1    4    7\n\n\nUn secondo caso particolare si verifica quando una matrice è costituita da un’unica colonna o un’unica riga. Se la matrice \\(\\boldsymbol{A}\\) si riduce ad una sola colonna (o una sola riga) e viene detta vettore colonna (o riga) ad \\(m\\) elementi o componenti. Un vettore colonna è una matrice \\(n \\times 1\\); un vettore riga è una matrice \\(1 \\times m\\). Se \\(\\boldsymbol{a}\\) è un vettore colonna di \\(m\\) elementi allora \\(\\boldsymbol{a}'\\) è un vettore riga sempre di \\(m\\) elementi.\nPer le operazioni tra vettori valgono le stesse regole viste per le matrici, cioè la somma e la differenza sono possibili tra vettori dello stesso tipo e con lo stesso numero di componenti. La moltiplicazione è possibile tra una matrice e un vettore di dimensioni appropriate, e tra due vettori di dimensioni appropriate. In questo secondo caso, distinguiamo tra prodotto interno e prodotto esterno.\n\n\nD.2.15 Operazioni tra vettori\nIl prodotto interno (o scalare) di un vettore \\(\\boldsymbol{a}'\\) \\(1 \\times n\\) che premoltiplica un vettore \\(\\boldsymbol{b}\\) \\(n \\times 1\\) produce uno scalare:\n\\[\n\\boldsymbol{a}'\\boldsymbol{b} = \\sum_{i=1}^{n}a_i b_i.\n\\]\nDati due vettori \\(\\boldsymbol{a}\\), \\(\\boldsymbol{b}\\) di ordini \\(n\n\\times 1\\) e \\(m \\times 1\\), il prodotto esterno \\(\\boldsymbol{C} = \\boldsymbol{ab}'\\) è una matrice \\(n \\times m\\) di elementi \\(c_{ij} = a_i b_j\\).\n\n\nD.2.16 Prodotto interno\nSiano \\(\\boldsymbol{a}\\) e \\(\\boldsymbol{b}\\) i seguenti vettori:\n\\[\n\\left[ \\begin{array}{c}\n1 \\\\\n2 \\\\\n3\n\\end{array}\n\\right] \\quad e \\quad\n\\left[ \\begin{array}{c}\n-1 \\\\\n-2 \\\\\n4\n\\end{array}\n\\right].\n\\]\nIl prodotto interno è:\n\\[\n\\boldsymbol{a}'\\boldsymbol{b}= 1 \\cdot (-1) + 2 \\cdot (-2) + 3\n\\cdot 4 = 7.\n\\]\nOsserviamo che tale operazione gode della proprietà commutativa, poichè \\(\\boldsymbol{b}'\\boldsymbol{a}=7\\).\n\na &lt;- matrix(\n  c(1, 2, 3), \n  nrow = 3, \n  byrow = TRUE\n)\na |&gt; print()\n\n     [,1]\n[1,]    1\n[2,]    2\n[3,]    3\n\n\n\nb &lt;- matrix(\n  c(-1, -2, 4), \n  nrow = 3, \n  byrow = TRUE\n)\nb |&gt; print()\n\n     [,1]\n[1,]   -1\n[2,]   -2\n[3,]    4\n\n\n\n(t(a) %*% b) |&gt; print()\n\n     [,1]\n[1,]    7\n\n\n\n(t(b) %*% a) |&gt; print()\n\n     [,1]\n[1,]    7\n\n\n\n\nD.2.17 Prodotto esterno\nIl prodotto esterno è la matrice\n\\[\n\\boldsymbol{C} = \\boldsymbol{a}\\boldsymbol{b}'= \\left[\n\\begin{array}{c c c}\n-1 & -2 & 4\\\\\n-2 & -4 & 8\\\\\n-3 & -6 & 12\n\\end{array}\n\\right].\n\\]\n\na %*% t(b) |&gt; print()\n\n     [,1] [,2] [,3]\n[1,]   -1   -2    4\n[2,]   -2   -4    8\n[3,]   -3   -6   12\n\n\nTale prodotto non gode della proprietà commutativa, infatti:\n\\[\n\\boldsymbol{D} = \\boldsymbol{b}\\boldsymbol{a}'= \\left[\n\\begin{array}{c c c}\n-1 & -2 & -3\\\\\n-2 & -4 & -6\\\\\n4 & 8 & 12\n\\end{array}\n\\right]\n\\]\n\nb %*% t(a) |&gt; print()\n\n     [,1] [,2] [,3]\n[1,]   -1   -2   -3\n[2,]   -2   -4   -6\n[3,]    4    8   12\n\n\n\n\nD.2.18 Traccia di una matrice\nSi definisce traccia di una matrice quadrata \\(\\boldsymbol{A}\\) \\(n \\times n\\), e si denota con \\(tr(\\boldsymbol{A})\\) la somma degli elementi sulla diagonale principale di \\(\\boldsymbol{A}\\):\n\\[\ntr(\\boldsymbol{A}) = \\sum_{i=1}^{n} a_{ii}.\n\\]\nLa traccia gode delle seguenti proprietà:\n\\[\n\\begin{aligned}\n&tr(\\rho \\boldsymbol{A}) = \\rho tr( \\boldsymbol{A}) \\notag \\\\\n&tr(\\boldsymbol{A} + \\boldsymbol{B}) =  tr( \\boldsymbol{A})+tr( \\boldsymbol{B}) \\notag \\\\\n&tr(\\boldsymbol{A}') =  tr( \\boldsymbol{A}) \\notag \\\\\n&tr(\\boldsymbol{AB}) =  tr( \\boldsymbol{BA}) \\notag\\end{aligned}\n\\]\nPer esempio, sia\n\\[\n\\boldsymbol{A} =  \\left[ \\begin{array}{c c c}\n7 & 1 & 2\\\\\n1 & 8 & 3\\\\\n2 & 3 & 9 \\end{array} \\right]\n\\]\nallora\n\\[\ntr(\\boldsymbol{A}) = 7 + 8 + 9 = 24.\n\\]\n\nA &lt;- matrix(\n  c(7,1, 2, 1, 8, 3, 2, 3, 9),\n  nrow = 3,\n  byrow = TRUE\n)\nA |&gt; print()\n\n     [,1] [,2] [,3]\n[1,]    7    1    2\n[2,]    1    8    3\n[3,]    2    3    9\n\n\n\nsum(diag(A)) |&gt; print()\n\n[1] 24\n\n\n\n\nD.2.19 Dipendenza lineare\nSi consideri la matrice\n\\[\n\\boldsymbol{A}=\n\\left(%\n\\begin{array}{ccc}\n  1 & 1 & 1 \\\\\n  3 & 1 & 5 \\\\\n  2 & 3 & 1 \\\\\n\\end{array}%\n\\right).\n\\]\nSiano \\(\\boldsymbol{c}_1\\), \\(\\boldsymbol{c}_2\\), \\(\\boldsymbol{c}_3\\) le colonne di \\(\\boldsymbol{A}\\). Si noti che\n\\[\n2\\boldsymbol{c}_1 + -\\boldsymbol{c}_2 + - \\boldsymbol{c}_3 =\n\\boldsymbol{0}\n\\]\ndove \\(\\boldsymbol{0}\\) è un vettore (\\(3 \\times 1\\)) di zeri.\nDato che le 3 colonne di \\(\\boldsymbol{A}\\) possono essere combinate linearmente in modo da produrre un vettore \\(\\boldsymbol{0}\\) vi è chiaramente una qualche forma di relazione, o dipendenza, tra le informazioni nelle colonne. Detto in un altro modo, sembra esserci una qualche duplicazione delle informazione nelle colonne. In generale, si dice che \\(k\\) colonne \\(\\boldsymbol{c}_1, \\boldsymbol{c}_2,\n\\dots \\boldsymbol{c}_k\\) di una matrice sono linearmente dipendenti se esiste un insieme di valori scalari \\(\\lambda_1,\n\\dots, \\lambda_k\\) tale per cui\n\\[\n\\lambda_1 \\boldsymbol{c}_1 + \\dots + \\lambda_k \\boldsymbol{c}_k=\\boldsymbol{0}\n\\]\ne almeno uno dei valori \\(\\lambda_i\\) non è uguale a 0.\nLa dipendenza lineare implica che ciascun vettore colonna è una combinazione degli altri. Per esempio\n\\[\n\\boldsymbol{c}_k= -(\\lambda_1 \\boldsymbol{c}_1 + \\dots + \\lambda_{k-1}\n   \\boldsymbol{c}_{k-1})/\\lambda_k.\n\\]\nQuesto implica che tutta “l’informazione” della matrice è contenuta in un sottoinsieme delle colonne – se \\(k-1\\) colonne sono conosciute, l’ultima resta determinata. È in questo senso che abbiamo detto che l’informazione della matrice veniva “duplicata”.\nSe l’unico insieme di valori scalari \\(\\lambda_i\\) che soddisfa l’equazione\n\\[\n\\lambda_1 \\boldsymbol{c}_1 + \\dots + \\lambda_k \\boldsymbol{c}_k=\\boldsymbol{0}\n\\]\nè un vettore di zeri, allora questo significa che non vi è alcuna relazione tra le colonne della matrice. Le colonne si dicono linearmente indipendenti, nel senso che non contengono alcuna “duplicazione” di informazione.\n\n\nD.2.20 Rango di una matrice\nIl rango della matrice è il massimo numero di vettori colonna linearmente indipendenti che possono essere selezionati dalla matrice. In maniera equivalente, il rango di una matrice può essere definito come il massimo numero di vettori riga linermente indipendenti. Il rango minimo di una matrice è 1, il che significa che vi è una colonna tale per cui le altre colonne sono dei multipli di questa. Per l’esempio precedente, il rango della matrice \\(\\boldsymbol{A}\\) è 2.\nSe la matrice è quadrata, \\(\\boldsymbol{A}_{n \\times n}\\), ed è costituita da vettori tutti indipendenti tra di loro, allora il suo rango è \\(n\\). Se, invece, la matrice è rettangolare, \\(\\boldsymbol{A}_{m \\times n}\\), allora il suo rango può essere al massimo il più piccolo tra i due valori m ed n, cioè:\n\\[\nr(\\boldsymbol{A}_{m \\times n}) \\leq min(m,n).\n\\]",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a4_linear_alg.html#matrice-inversa",
    "href": "chapters/appendix/a4_linear_alg.html#matrice-inversa",
    "title": "Appendice D — Elementi di algebra lineare",
    "section": "D.3 Matrice inversa",
    "text": "D.3 Matrice inversa\nL’inversa di una matrice quadrata è un concetto analogo al reciproco di un numero. Tuttavia, a differenza dei numeri, non tutte le matrici hanno un’inversa. Una matrice che non ha inversa è detta singolare.\nSia \\(\\boldsymbol{A}\\) una matrice quadrata di dimensione \\(n\\). La matrice inversa di \\(\\boldsymbol{A}\\), indicata con \\(\\boldsymbol{A}^{-1}\\), è una matrice tale che:\n\\[\n\\boldsymbol{A}\\boldsymbol{A}^{-1}=\\boldsymbol{A}^{-1}\\boldsymbol{A}=\\boldsymbol{I}\n\\]\ndove \\(\\boldsymbol{I}\\) è la matrice identità di dimensione \\(n\\).\nCondizione di esistenza: Una matrice quadrata \\(\\boldsymbol{A}\\) ha un’inversa se e solo se il suo determinante è diverso da zero. Il determinante è una funzione che associa a ogni matrice quadrata un numero.\nInterpretazione geometrica: Geometricamente, l’inversa di una matrice rappresenta la trasformazione inversa della trasformazione rappresentata dalla matrice stessa. Ad esempio, se una matrice rappresenta una rotazione, la sua inversa rappresenta la rotazione opposta.\nCalcolo dell’inversa:\n\nMatrici diagonali: Per le matrici diagonali, l’inversa si ottiene invertendo gli elementi sulla diagonale principale.\nMetodo di Gauss-Jordan: Questo è un algoritmo sistematico per calcolare l’inversa di una matrice. Si basa su operazioni elementari sulle righe della matrice aumentata \\([\\boldsymbol{A} | \\boldsymbol{I}]\\), dove \\(\\boldsymbol{I}\\) è la matrice identità.\nAltri metodi: Esistono altri metodi, come l’uso della matrice aggiunta, ma sono generalmente meno efficienti.\n\nProprietà:\n\nUnicità: Se una matrice ha un’inversa, questa è unica.\nInversa del prodotto: \\((AB)^{-1} = B^{-1}A^{-1}\\) (a condizione che le inverse esistano).\nInversa della trasposta: \\((A^T)^{-1} = (A^{-1})^T\\).\n\nPer esempio, sia\n\\[\n\\boldsymbol{A} =  \\left[ \\begin{array}{c c}\n3 & 4 \\\\\n2 & 6\n\\end{array}\n\\right]\n\\]\nallora\n\\[\n\\boldsymbol{A}^{-1} =  \\left[ \\begin{array}{c c}\n.6 & -.4 \\\\\n-.2 & .3\n\\end{array}\n\\right]\n\\]\ne\n\\[\n\\boldsymbol{A}\\boldsymbol{A}^{-1} =\\left[ \\begin{array}{c c}\n3 & 4 \\\\\n2 & 6\n\\end{array}\n\\right]\n\\left[ \\begin{array}{c c}\n.6 & -.4 \\\\\n-.2 & .3\n\\end{array}\n\\right] =\n\\left[ \\begin{array}{c c}\n1 & 0 \\\\\n0 & 1\n\\end{array}\n\\right]\n\\]\n\nA &lt;- matrix(\n  c(3, 4, 2, 6),\n  nrow = 2,\n  byrow = TRUE\n)\nA |&gt; print()\n\n     [,1] [,2]\n[1,]    3    4\n[2,]    2    6\n\n\n\nsolve(A) |&gt; print()\n\n     [,1] [,2]\n[1,]  0.6 -0.4\n[2,] -0.2  0.3\n\n\n\nA %*% solve(A) |&gt; print()\n\n             [,1] [,2]\n[1,] 1.000000e+00    0\n[2,] 5.551115e-17    1\n\n\n\nsolve(A) %*% A |&gt; print()\n\n     [,1]         [,2]\n[1,]    1 1.110223e-16\n[2,]    0 1.000000e+00\n\n\nAbbiamo detto sopra che, se \\(\\boldsymbol{A}\\) e \\(\\boldsymbol{B}\\) sono due matrici non singolari aventi le stesse dimensioni, allora l’inversa del loro prodotto è uguale al prodotto delle loro inverse nella sequenza opposta:\n\\[\n(\\boldsymbol{AB})^{-1}=\\boldsymbol{B}^{-1}\\boldsymbol{A}^{-1}.\n\\]\nEsaminiamo un esempio numerico.\n\nB &lt;- matrix(\n  c(1, 2, 9, 7),\n  nrow = 2,\n  byrow = TRUE\n)\nB |&gt; print()\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    9    7\n\n\n\nB %*% solve(B) \n\n\nA matrix: 2 × 2 of type dbl\n\n\n1\n0\n\n\n0\n1\n\n\n\n\n\n\nsolve(A %*% B) |&gt; print()\n\n           [,1]       [,2]\n[1,] -0.4181818  0.3090909\n[2,]  0.5090909 -0.3545455\n\n\n\nsolve(B) %*% solve(A) |&gt; print()\n\n           [,1]       [,2]\n[1,] -0.4181818  0.3090909\n[2,]  0.5090909 -0.3545455\n\n\nL’inversa della trasposta di una matrice non singolare è uguale alla trasposta dell’inversa:\n\\[\n(\\boldsymbol{A}')^{-1}=(\\boldsymbol{A}^{-1})'.\n\\]\nConsideriamo un esempio numerico.\n\nsolve(t(A)) |&gt; print()\n\n     [,1] [,2]\n[1,]  0.6 -0.2\n[2,] -0.4  0.3\n\n\n\nt(solve(A)) |&gt; print()\n\n     [,1] [,2]\n[1,]  0.6 -0.2\n[2,] -0.4  0.3",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a4_linear_alg.html#determinante-di-una-matrice",
    "href": "chapters/appendix/a4_linear_alg.html#determinante-di-una-matrice",
    "title": "Appendice D — Elementi di algebra lineare",
    "section": "D.4 Determinante di una matrice",
    "text": "D.4 Determinante di una matrice\nIl determinante di una matrice quadrata \\(\\boldsymbol{A}\\), indicato con \\(|\\boldsymbol{A}|\\), è uno scalare che fornisce importanti informazioni sulla matrice stessa. Geometricamente, il valore assoluto del determinante rappresenta il volume del parallelepipedo generato dalle colonne della matrice (nel caso di una matrice 3x3). Per matrici di dimensioni inferiori, il determinante ha analoghe interpretazioni geometriche: ad esempio, per una matrice 2x2 rappresenta l’area del parallelogramma determinato dalle sue colonne.\n\nD.4.1 Calcolo del determinante\n\nMatrice diagonale: Per una matrice diagonale \\(\\boldsymbol{D} = diag(d_1, \\dots, d_n)\\), il determinante è semplicemente il prodotto degli elementi sulla diagonale: \\(|\\boldsymbol{D}| = d_1 \\cdot d_2 \\cdots d_n\\).\nMatrice 2x2: Per una matrice\n\\[\n  \\boldsymbol{A} =  \\left[ \\begin{array}{c c}\n  a_{11}& a_{12} \\\\\n  a_{21} & a_{22} \\end{array} \\right]\n  \\]\nil determinante è dato da:\n\\[\n  |\\boldsymbol{A}| =  a_{11}a_{22}-a_{12}a_{21}\n  \\]\nMatrici di dimensioni superiori: Per matrici di dimensioni superiori, esistono diversi metodi per calcolare il determinante, tra cui lo sviluppo di Laplace e la regola di Sarrus (per matrici 3x3).\n\nInterpretazioni e applicazioni:\n\nInvertibilità: Una matrice quadrata è invertibile se e solo se il suo determinante è diverso da zero.\nSistemi lineari: Il teorema di Cramer lega il determinante al numero di soluzioni di un sistema lineare.\n\nConsideriamo un esempio numerico nel caso più semplice di una matrice 2$$2.\n\nA &lt;- matrix(\n  c(1, -2, 3, 9),\n  nrow = 2, \n  byrow = TRUE\n)\nA |&gt; print()\n\n     [,1] [,2]\n[1,]    1   -2\n[2,]    3    9\n\n\n\ndet(A) |&gt; print()\n\n[1] 15\n\n\n\n\nD.4.2 Determinante e inversa\nVi è una relazione tra il determinante e l’inversa di una matrice. Se la matrice \\(\\boldsymbol{A}\\) ha dimensioni \\(2 \\times 2\\) l’inversa di \\(\\boldsymbol{A}\\) si trova nel modo seguente\n\\[\n\\boldsymbol{A}^{-1} = \\frac{1}{|\\boldsymbol{A}|} \\left[\n\\begin{array}{c c}\na_{22} & -a_{12} \\\\\n-a_{21} & a_{11}\n\\end{array}\n\\right].\n\\]\nAnche per le matrici di dimensioni maggiori la matrice inversa è definita nei termini del determinante, ma le formule di calcolo sono molto più complesse.\nPer esempio, sia\n\\[\n\\boldsymbol{A} = \\left[ \\begin{array}{c c}\n3 & 4 \\\\\n2 & 6\n\\end{array}\n\\right]\n\\]\nallora\n\\[\n\\boldsymbol{A}^{-1} = \\frac{1}{10} \\left[\n\\begin{array}{c c}\n6 & -4 \\\\\n-2 & 3\n\\end{array}\n\\right]= \\left[ \\begin{array}{c c}\n.6 & -.4 \\\\\n-.2 & .3\n\\end{array}\n\\right].\n\\]\nIn precedenza abbiamo detto che, in alcuni casi, una matrice “si comporta come lo 0.” Il determinante di una matrice è ci dice quando una matrice “si comporta come lo 0.” \\(|\\boldsymbol{A}| = 0\\), infatti, se una riga (o una colonna) è una combinazione lineare di due (o più) righe (o colonne) di \\(\\boldsymbol{A}\\).\nPer esempio, nel caso di una matrice (\\(2 \\times 2\\))\n\\[\n\\boldsymbol{A} =  \\left( \\begin{array}{c c}\na_{11}& a_{12} \\\\\na_{21} & a_{22} \\end{array} \\right)\n\\]\nsupponiamo che\n\\[\n\\left(%\n\\begin{array}{c}\n  a_{11} \\\\\n  a_{21} \\\\\n\\end{array}%\n\\right)=2\n\\left(%\n\\begin{array}{c}\n  a_{12} \\\\\n  a_{22} \\\\\n\\end{array}%\n\\right).\n\\]\nAllora\n\\[\n\\boldsymbol{A} =  \\left( \\begin{array}{c c}\n2a_{12}& a_{12} \\\\\n2a_{22} & a_{22} \\end{array} \\right)\n\\]\ne\n\\[\n|\\boldsymbol{A}| = 2a_{12}a_{22}-2a_{12}a_{22}=0.\n\\]\nIn conclusione, se il determinante è uguale a zero, allora la matrice inversa non esiste. Nel caso di una matrice (\\(2 \\times 2\\)), infatti, la formula dell’inversa richiede la divisione per \\(a_{11}a_{22}-a_{12}a_{21}\\) che, nel caso di una matrice singolare, è uguale a zero.\n\nA &lt;- matrix(\n  c(2, 4, 3, 6),\n  nrow = 2,\n  byrow = TRUE\n)\nA |&gt; print()\n\n     [,1] [,2]\n[1,]    2    4\n[2,]    3    6\n\n\n\ndet(A) |&gt; print()\n\n[1] -6.661338e-16\n\n\n\n\nD.4.3 Proprietà del determinante\n\n\\(|\\boldsymbol{A}'| = |\\boldsymbol{A}|\\).\nSe \\(\\boldsymbol{A}\\) contiene una colonna o una riga i cui elementi sono tutti 0, allora \\(|\\boldsymbol{A}|=0\\).\nSe \\(\\boldsymbol{A}\\) contiene due colonne (o righe) identiche, allora \\(|\\boldsymbol{A}|=0\\).\n\\(|\\boldsymbol{A}| = 0\\) se una riga (o una colonna) è combinazione lineare di due (o più) righe (o colonne) di \\(\\boldsymbol{A}\\).\n\\(|\\boldsymbol{A}| = 1/|\\boldsymbol{A}^{-1}|\\).\n\\(|\\boldsymbol{I}| = 1\\).\n\\(|\\boldsymbol{A} \\boldsymbol{B}| = |\\boldsymbol{A}| |\\boldsymbol{B}|\\).\n\nPer una matrice quadrata \\(\\boldsymbol{A}\\), le seguenti affermazioni sono equivalenti: \\(\\boldsymbol{A}\\) è non singolare, \\(|\\boldsymbol{A}|\\neq 0\\), \\(\\boldsymbol{A}^{-1}\\) esiste.",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a4_linear_alg.html#radici-e-vettori-latenti",
    "href": "chapters/appendix/a4_linear_alg.html#radici-e-vettori-latenti",
    "title": "Appendice D — Elementi di algebra lineare",
    "section": "D.5 Radici e vettori latenti",
    "text": "D.5 Radici e vettori latenti\nDal determinante di una matrice si possono ricavare le radici latenti o autovalori (denotati da \\(\\lambda_i\\)) e i vettori latenti o autovettori della matrice. Alle nozioni di autovalore e autovettore verrà qui fornita un’interpretazione geometrica.\nSimuliamo di dati di due variabili associate tra loro:\n\nset.seed(123456)\noptions(repr.plot.width = 8, repr.plot.height = 8)\n\nnpoints &lt;- 20\nx &lt;- as.numeric(scale(rnorm(npoints, 0, 1)))\ny &lt;- as.numeric(scale(3 * x + rnorm(npoints, 0, 2)))\nmean(x) |&gt; print()\nmean(y) |&gt; print()\ncor(x, y) |&gt; print()\n\n[1] -2.775558e-17\n[1] -7.771561e-17\n[1] 0.8291033\n\n\nDisegnamo il diagramma di dispersione con un ellisse che contiene la nube di punti:\n\nY &lt;- cbind(x, y)\ncar::dataEllipse(\n  Y[, 1], Y[, 2],\n  levels = 0.95,\n  lty = 2,\n  ylim = c(-3, 3),\n  xlim = c(-3, 3)\n)\n\n\n\n\n\n\n\n\nSe racchiudiamo le osservazioni (\\(v_1, v_2\\)) con un’ellisse, allora la lunghezza dei semiassi maggiori e minori dell’ellisse sarà proporzionale a \\(\\sqrt{\\lambda_1}\\) e \\(\\sqrt{\\lambda_2}\\). L’asse maggiore è la linea passante per il punto (\\(\\bar{v_1}, \\bar{v_2}\\)) nella direzione determinata dal primo autovettore \\(\\boldsymbol{a}_1'\\) con pendenza uguale a \\(a_{12}/a_{11}\\). L’asse minore è la linea passante per il punto (\\(\\bar{v_1}, \\bar{v_2}\\)) nella direzione determinata dal secondo autovettore \\(\\boldsymbol{a}_2\\).\nCalcoliamo ora gli autovettori e gli autovalori:\n\ns &lt;- cov(Y)\nee &lt;- eigen(s)\nee |&gt; print()\n\neigen() decomposition\n$values\n[1] 1.8291033 0.1708967\n\n$vectors\n          [,1]       [,2]\n[1,] 0.7071068 -0.7071068\n[2,] 0.7071068  0.7071068\n\n\n\n\n# First eigenvector \nev_1 &lt;- ee$vectors[, 1]\n\n# Slope of the first eigenvector\nev1_m &lt;- ev_1[2] / ev_1[1]\n\n# Second eigenvector \nev_2 &lt;- ee$vectors[, 2]\n\n# Slope of the second eigenvector\nev2_m &lt;- ev_2[2] / ev_2[1]\n\n\n# Scatter plot showing the span of both eigenvectors \ndata.frame(zx=scale(x), zy=scale(y))  |&gt;\nggplot(aes(x = zx, y = zy)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  geom_abline(slope = ev1_m, color = \"blue\", linewidth = 0.7) +\n  geom_abline(slope = ev2_m, color = \"red\", linewidth = 0.7) \n\n\n\n\n\n\n\n\nGli autovettori sono ortogonali:\n\n# Multiply both eigenvectors \nprint(ev_1 %*% ev_2)\n\n             [,1]\n[1,] 2.237114e-17\n\n\nGeneriamo uno Scree Plot.\n\n# Calculate the estimated variance for each eigenvalue\ne_var &lt;- ee$values / (length(x) - 1)\n\n# Data frame with variance percentages\nvar_per &lt;- tibble(\n  PC  = c(\"PC1\", \"PC2\"),\n  PER = c(e_var) * 100 / sum(e_var) # Calculate the percentage\n    )\n\n# Scree plot \nggplot(var_per, aes(x = PC, y = PER)) +\n  geom_col(width = 0.5, color = \"black\") +\n  xlab(\"Principal component\") +\n  ylab(\"Percentage of variation (%)\") \n\n\n\n\n\n\n\n\nVerifichiamo che la somma degli autovalori sia uguale alla varianza totale.\n\nvar(x) + var(y)\n\n2\n\n\n\nee$values |&gt; sum()\n\n2\n\n\nGli autovettori ottenuti utilizzando la funzione eigen() sono normalizzati. Ciò significa che la loro lunghezza è uguale a 1:\n\nt(as.matrix(ee$vectors[, 1])) %*% as.matrix(ee$vectors[, 1]) |&gt; print()\n\n     [,1]\n[1,]    1\n\n\nUtilizziamo le informazioni degli autovettori e degli autovalori per disegnare gli assi dell’ellisse:\n\ngli autovettori determinano la direzione degli assi;\nla radice quadrata degli autovalori è proporzionale alla lunghezza degli assi dell’ellisse.\n\n\ncar::dataEllipse(\n  Y[, 1], Y[, 2],\n  levels = 0.95,\n  lty = 2,\n  ylim = c(-3, 3),\n  xlim = c(-3, 3)\n)\nk &lt;- 2.65\narrows(\n  0, 0, \n  k * sqrt(ee$values[1]) * ee$vectors[1],\n  k * sqrt(ee$values[1]) * ee$vectors[2],\n  code = 2, \n  col = \"red\", \n  lwd = 2\n)\narrows(\n  0, 0, \n  k * sqrt(ee$values[2]) * ee$vectors[1],\n  k * sqrt(ee$values[2]) * -ee$vectors[2],\n  code = 2, \n  col = \"red\", \n  lwd = 2\n)\n\n\n\n\n\n\n\n\nTale analisi si può estendere a qualunque numero di variabili. Per esempio, nel caso di tre variabili, possiamo pensare di disegnare un ellisoide attorno ad una nube di punti nello spazio tridimensionale. Anche in questo caso, gli autovalori e gli associati autovettori corrisponderanno agli assi dell’elissoide.",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a4_linear_alg.html#scomposizione-spettrale-di-una-matrice",
    "href": "chapters/appendix/a4_linear_alg.html#scomposizione-spettrale-di-una-matrice",
    "title": "Appendice D — Elementi di algebra lineare",
    "section": "D.6 Scomposizione spettrale di una matrice",
    "text": "D.6 Scomposizione spettrale di una matrice\nData una matrice quadrata e simmetrica di dimensione \\(n\\), \\(\\boldsymbol{A}\\), esistono una matrice diagonale \\(\\boldsymbol{\\Lambda}\\) e una matrice ortogonale \\(\\boldsymbol{V}\\) tali che\n\\[\\boldsymbol{A} =\\boldsymbol{V} \\boldsymbol{\\Lambda} \\boldsymbol{V}',\\] dove\n\n\\(\\boldsymbol{\\Lambda}\\) è una matrice diagonale i cui elementi sono gli autovalori di \\(\\boldsymbol{A}\\): \\(\\boldsymbol{\\Lambda} = diag(\\lambda_1, \\lambda_2,\n    \\dots, \\lambda_n)\\);\n\\(\\boldsymbol{V}\\) è una matrice ortogonale le cui colonne \\((v_1, v_2, \\dots, v_p)\\) sono gli autovettori di \\(\\boldsymbol{A}\\) associati ai rispettivi autovalori.\n\nIn maniera equivalente\n\\[\\boldsymbol{A} \\boldsymbol{V} =  \\boldsymbol{\\Lambda} \\boldsymbol{V}'.\\]\nPremoltiplicando entrambi i membri per \\(\\boldsymbol{V}'\\) si ottiene\n\\[\\boldsymbol{V}'\\boldsymbol{A} \\boldsymbol{V} =\n\\boldsymbol{\\Lambda},\\]\nda cui l’affermazione che la matrice degli autovettori diagonalizza \\(\\boldsymbol{A}\\).\nPer esempio,\n\nsigma &lt;- matrix(\n  data = c(1, 0.5, 0.5, 1.25), \n  nrow = 2, \n  ncol = 2\n)\nsigma |&gt; print()\n\n     [,1] [,2]\n[1,]  1.0 0.50\n[2,]  0.5 1.25\n\n\n\nout &lt;- eigen(sigma)\nout |&gt; print()\n\neigen() decomposition\n$values\n[1] 1.6403882 0.6096118\n\n$vectors\n          [,1]       [,2]\n[1,] 0.6154122 -0.7882054\n[2,] 0.7882054  0.6154122\n\n\n\n\nLambda &lt;- diag(out$values)\nLambda |&gt; print()\n\n         [,1]      [,2]\n[1,] 1.640388 0.0000000\n[2,] 0.000000 0.6096118\n\n\n\nU &lt;- out$vectors\nU |&gt; print()\n\n          [,1]       [,2]\n[1,] 0.6154122 -0.7882054\n[2,] 0.7882054  0.6154122\n\n\n\nU %*% Lambda %*% t(U) |&gt; print()\n\n     [,1] [,2]\n[1,]  1.0 0.50\n[2,]  0.5 1.25\n\n\n\nD.6.1 Autovalori e determinante\nIl determinante di una matrice è il prodotto degli autovalori:\n\\[\\begin{aligned}\n    |\\boldsymbol{A}| &= \\prod_{i=1}^{p} \\lambda_i. \\notag\n    \\end{aligned}\\]\nLa traccia di una matrice è uguale alla somma degli autovalori:\n\\[\\begin{aligned}\n    tr(\\boldsymbol{A}) &= \\sum_{i=1}^{p} \\lambda_i. \\notag\n    \\end{aligned}\\]\n\nsigma &lt;- matrix(data = c(1, 0.5, 0.5, 2), nrow = 2, ncol = 2)\nsigma |&gt; print()\n\nout &lt;- eigen(sigma)\nout |&gt; print()\n\n     [,1] [,2]\n[1,]  1.0  0.5\n[2,]  0.5  2.0\neigen() decomposition\n$values\n[1] 2.2071068 0.7928932\n\n$vectors\n          [,1]       [,2]\n[1,] 0.3826834 -0.9238795\n[2,] 0.9238795  0.3826834\n\n\n\nLa traccia di una matrice è uguale alla somma degli autovalori:\n\nsum(out$values) |&gt; print()\n\n[1] 3\n\n\nIl determinante di una matrice è il prodotto degli autovalori:\n\ndet(sigma) |&gt; print()\n(out$values[1] * out$values[2]) |&gt; print()\n\n[1] 1.75\n[1] 1.75\n\n\nGli autovalori di \\(\\boldsymbol{A}^{-1}\\) sono i reciproci degli autovalori di \\(\\boldsymbol{A}\\); gli autovettori sono coincidenti.",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a4_linear_alg.html#la-distanza-euclidea",
    "href": "chapters/appendix/a4_linear_alg.html#la-distanza-euclidea",
    "title": "Appendice D — Elementi di algebra lineare",
    "section": "D.7 La distanza euclidea",
    "text": "D.7 La distanza euclidea\nPer calcolare la distanza euclidea tra due punti utilizzando l’algebra matriciale, consideriamo i punti come vettori in uno spazio euclideo.\n\nD.7.1 In due dimensioni:\nSiano dati i vettori:\n\nx = \\(\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\),\ny = \\(\\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix}\\).\n\nLa distanza euclidea tra x e y è la norma del vettore differenza (\\(\\mathbf{x} - \\mathbf{y}\\)):\n\\[ d(\\mathbf{x}, \\mathbf{y}) = \\| \\mathbf{x} - \\mathbf{y} \\| \\].\nIn termini di algebra matriciale, questa norma è calcolata come:\n\\[ d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ (\\mathbf{x} - \\mathbf{y})^\\top (\\mathbf{x} - \\mathbf{y}) } \\]\nPassaggi dettagliati:\n\nCalcolo del vettore differenza:\n\\[\n\\mathbf{x} - \\mathbf{y} = \\begin{bmatrix} x_1 - y_1 \\\\ x_2 - y_2 \\end{bmatrix}\n\\]\nCalcolo del prodotto scalare:\n\\[\n(\\mathbf{x} - \\mathbf{y})^\\top (\\mathbf{x} - \\mathbf{y}) = (x_1 - y_1)^2 + (x_2 - y_2)^2\n\\]\nCalcolo della radice quadrata:\n\\[\nd(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ (x_1 - y_1)^2 + (x_2 - y_2)^2 }\n\\]\n\nQuesto risultato corrisponde alla formula classica per la distanza tra due punti nel piano cartesiano.\n\n\nD.7.2 Estensione a più dimensioni\nPer vettori in uno spazio $ n $-dimensionale:\n\nx = \\(\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\\)\ny = \\(\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\\)\n\nLa distanza euclidea diventa:\n\\[ d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ (\\mathbf{x} - \\mathbf{y})^\\top (\\mathbf{x} - \\mathbf{y}) } \\]\nChe si espande in:\n\\[ d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ \\sum_{i=1}^{n} (x_i - y_i)^2 } \\]\nIn conclusione:\nl’utilizzo dell’algebra matriciale permette di esprimere in modo compatto e generalizzato il calcolo della distanza euclidea tra due punti in qualsiasi dimensione, sfruttando operazioni matriciali come la trasposizione e il prodotto scalare.",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/a4_linear_alg.html#la-distanza-di-mahalanobis",
    "href": "chapters/appendix/a4_linear_alg.html#la-distanza-di-mahalanobis",
    "title": "Appendice D — Elementi di algebra lineare",
    "section": "D.8 La distanza di Mahalanobis",
    "text": "D.8 La distanza di Mahalanobis\nLa distanza euclidea presuppone che le variabili siano non correlate e su scale comparabili. Tuttavia, in molti casi, le variabili possono avere scale diverse e possono essere correlate tra loro. La distanza di Mahalanobis tiene conto di queste differenze utilizzando la matrice di covarianza, permettendo una misurazione della distanza che considera sia la scala che la correlazione tra le variabili.\n\nD.8.1 Definizione\nLa distanza di Mahalanobis tra due vettori \\(\\mathbf{x}\\) e \\(\\mathbf{y}\\) è definita come:\n\\[\nd_M(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ (\\mathbf{x} - \\mathbf{y})^\\top \\mathbf{S}^{-1} (\\mathbf{x} - \\mathbf{y}) }\n\\]\ndove:\n\n\\(\\mathbf{S}\\) è la matrice di covarianza delle variabili.\n\\(\\mathbf{S}^{-1}\\) è l’inversa della matrice di covarianza.\n\n\n\nD.8.2 Perché è necessaria la matrice di covarianza?\n\nScala delle variabili: Se le variabili hanno varianze diverse, la matrice di covarianza normalizza queste differenze, evitando che variabili con varianze maggiori dominino la misura della distanza.\nCorrelazione tra variabili: La matrice di covarianza tiene conto delle correlazioni tra le variabili, riducendo l’influenza delle variabili altamente correlate sulla distanza totale.\n\n\n\nD.8.3 Esempio numerico\nSupponiamo di avere due punti in uno spazio bidimensionale:\n\nPunto A: \\(\\mathbf{x} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}\\)\nPunto B: \\(\\mathbf{y} = \\begin{bmatrix} 5 \\\\ 7 \\end{bmatrix}\\)\n\nE una matrice di covarianza stimata:\n\\[\n\\mathbf{S} = \\begin{bmatrix}\n4 & 2 \\\\\n2 & 3\n\\end{bmatrix}\n\\]\n\nD.8.3.1 Passaggi per il calcolo\n\nCalcolo del vettore differenza \\(\\mathbf{d}\\):\n\\[\n\\mathbf{d} = \\mathbf{x} - \\mathbf{y} = \\begin{bmatrix} 2 - 5 \\\\ 3 - 7 \\end{bmatrix} = \\begin{bmatrix} -3 \\\\ -4 \\end{bmatrix}\n\\]\nCalcolo dell’inversa della matrice di covarianza \\(\\mathbf{S}^{-1}\\):\n\nDeterminante di \\(\\mathbf{S}\\):\n\\[\n\\det(\\mathbf{S}) = (4)(3) - (2)(2) = 12 - 4 = 8\n\\]\nMatrice aggiunta (comatrice trasposta) di \\(\\mathbf{S}\\):\n\\[\n\\text{adj}(\\mathbf{S}) = \\begin{bmatrix}\n3 & -2 \\\\\n-2 & 4\n\\end{bmatrix}\n\\]\nInversa di \\(\\mathbf{S}\\):\n\\[\n\\mathbf{S}^{-1} = \\frac{1}{\\det(\\mathbf{S})} \\text{adj}(\\mathbf{S}) = \\frac{1}{8} \\begin{bmatrix}\n3 & -2 \\\\\n-2 & 4\n\\end{bmatrix} = \\begin{bmatrix}\n0.375 & -0.25 \\\\\n-0.25 & 0.5\n\\end{bmatrix}\n\\]\n\nCalcolo della distanza di Mahalanobis:\n\\[\nd_M(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ \\mathbf{d}^\\top \\mathbf{S}^{-1} \\mathbf{d} }\n\\]\n\nCalcolo del prodotto \\(\\mathbf{S}^{-1} \\mathbf{d}\\):\n\\[\n\\mathbf{S}^{-1} \\mathbf{d} = \\begin{bmatrix}\n0.375 & -0.25 \\\\\n-0.25 & 0.5\n\\end{bmatrix} \\begin{bmatrix} -3 \\\\ -4 \\end{bmatrix} = \\begin{bmatrix}\n(-0.375 \\times 3) + (0.25 \\times 4) \\\\\n(0.25 \\times 3) + (-0.5 \\times 4)\n\\end{bmatrix} = \\begin{bmatrix}\n-0.125 \\\\\n-1.25\n\\end{bmatrix}\n\\]\nCalcolo del prodotto scalare:\n\\[\n\\mathbf{d}^\\top (\\mathbf{S}^{-1} \\mathbf{d}) = \\begin{bmatrix} -3 & -4 \\end{bmatrix} \\begin{bmatrix} -0.125 \\\\ -1.25 \\end{bmatrix} = (-3)(-0.125) + (-4)(-1.25) = 0.375 + 5 = 5.375\n\\]\nCalcolo della distanza:\n\\[\nd_M(\\mathbf{x}, \\mathbf{y}) = \\sqrt{5.375} \\approx 2.318\n\\]\n\n\n\n\n\nD.8.4 Confronto con la distanza euclidea\nLa distanza euclidea tra gli stessi punti è:\n\\[\nd_E(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ (-3)^2 + (-4)^2 } = \\sqrt{9 + 16} = \\sqrt{25} = 5\n\\]\nCome si può notare, la distanza di Mahalanobis (\\(\\approx 2.318\\)) è diversa dalla distanza euclidea (5) a causa della considerazione delle varianze e delle correlazioni tra le variabili.\n\n\nD.8.5 Interpretazione\n\nVarianze diverse: Se una variabile ha una varianza elevata, le differenze lungo quella direzione avranno meno peso nella distanza totale.\nCorrelazioni: Se due variabili sono altamente correlate, la distanza di Mahalanobis riduce l’importanza delle differenze lungo la direzione in cui le variabili sono correlate.\n\nIn conclusione, la distanza di Mahalanobis è particolarmente utile in contesti multivariati dove le variabili hanno scale diverse e possono essere correlate. Essa fornisce una misura di distanza che è invariante rispetto alle trasformazioni lineari dei dati, rendendola ideale per l’analisi di dati statistici e il rilevamento di outlier.\nNota: È importante assicurarsi che la matrice di covarianza \\(\\mathbf{S}\\) sia non singolare (invertibile). In pratica, quando si lavora con campioni di dati, \\(\\mathbf{S}\\) viene stimata dai dati stessi.",
    "crumbs": [
      "Appendici",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Elementi di algebra lineare</span>"
    ]
  },
  {
    "objectID": "chapters/appendix/solutions_probability.html",
    "href": "chapters/appendix/solutions_probability.html",
    "title": "Appendice E — Probabilità",
    "section": "",
    "text": "# Standard library imports\nimport os\n\n# Third-party imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport arviz as az\nimport scipy.stats as stats\nfrom scipy.special import expit  # Funzione logistica\nimport math\nfrom cmdstanpy import cmdstan_path, CmdStanModel\n\n# Configuration\nseed = sum(map(ord, \"stan_poisson_regression\"))\nrng = np.random.default_rng(seed=seed)\naz.style.use(\"arviz-darkgrid\")\n%config InlineBackend.figure_format = \"retina\"\n\n# Define directories\nhome_directory = os.path.expanduser(\"~\")\nproject_directory = f\"{home_directory}/_repositories/psicometria\"\n\n# Print project directory to verify\nprint(f\"Project directory: {project_directory}\")\n\nProject directory: /Users/corradocaudek/_repositories/psicometria\n\n\n\n?sec-prob-on-general-spaces\n?exr-prob-on-general-spaces-1\nPer calcolare questa probabilità in maniera analitica, utilizziamo la seguente uguaglianza:\n\\[\nP(\\text{almeno 2 psicologi clinici}) = 1 - P(\\text{nessun psicologo clinico}) - P(\\text{1 psicologo clinico}).\n\\]\nIl numero totale di modi per selezionare 5 persone dal gruppo di 20 è dato da:\n\\[\n\\binom{20}{5} = \\frac{20!}{5!(15!)} = 15,504.\n\\]\nIl numero di modi per avere nessun psicologo clinico nella commissione (ovvero, selezionare solo psicologi del lavoro) è:\n\\[\n\\binom{10}{0} \\times \\binom{10}{5} = 1 \\times 252 = 252.\n\\]\nQuindi, la probabilità di avere nessun psicologo clinico è:\n\\[\nP(\\text{nessun psicologo clinico}) = \\frac{252}{15,504} \\approx 0.016.\n\\]\nIl numero di modi per avere esattamente 1 psicologo clinico nella commissione è:\n\\[\n\\binom{10}{1} \\times \\binom{10}{4} = 10 \\times 210 = 2,100.\n\\]\nQuindi, la probabilità di avere esattamente 1 psicologo clinico è:\n\\[\nP(\\text{1 psicologo clinico}) = \\frac{2,100}{15,504} \\approx 0.135.\n\\]\nLa probabilità di avere almeno 2 psicologi clinici nella commissione è quindi:\n\\[\n\\begin{align}\nP(\\text{almeno 2 psicologi clinici}) &= 1 - P(\\text{nessun psicologo clinico}) - P(\\text{1 psicologo clinico}) \\notag\\\\\n&= 1 - 0.016 - 0.135 \\notag\\\\\n&= 0.848.\\notag\n\\end{align}\n\\]\nQuindi, la probabilità che almeno 2 psicologi clinici siano nella commissione è circa 0.848.\n\n# Funzione per calcolare le combinazioni\ndef nCk(n, k):\n    return math.factorial(n) // (math.factorial(k) * math.factorial(n - k))\n\n\n# Calcolo delle probabilità per il problema della commissione\ntotal_ways = nCk(20, 5)\nno_clinical = nCk(10, 0) * nCk(10, 5)\none_clinical = nCk(10, 1) * nCk(10, 4)\n\np_no_clinical = no_clinical / total_ways\np_one_clinical = one_clinical / total_ways\n\np_at_least_two_clinical = 1 - p_no_clinical - p_one_clinical\n\nprint(f\"Probabilità di almeno 2 psicologi clinici: {p_at_least_two_clinical:.3f}\")\n\nProbabilità di almeno 2 psicologi clinici: 0.848\n\n\nIn maniera più intuitiva, possiamo risolvere il problema con una simulazione Monte Carlo.\n\nimport random\n\n# Numero di simulazioni\nsimulations = 1000000\n\n# Numero di successi (almeno 2 psicologi clinici nella commissione)\nsuccess_count = 0\n\n# Creiamo una lista che rappresenta il gruppo di 20 persone\n# 1 rappresenta un psicologo clinico, 0 rappresenta un psicologo del lavoro\ngroup = [1] * 10 + [0] * 10\n\n# Simulazione Monte Carlo\nfor _ in range(simulations):\n    # Estrai casualmente 5 persone dal gruppo\n    committee = random.sample(group, 5)\n\n    # Conta quanti psicologi clinici ci sono nella commissione\n    num_clinical_psychologists = sum(committee)\n\n    # Verifica se ci sono almeno 2 psicologi clinici\n    if num_clinical_psychologists &gt;= 2:\n        success_count += 1\n\n# Calcola la probabilità\nprobability = success_count / simulations\n\n# Mostra il risultato\nprint(\n    f\"La probabilità che almeno 2 psicologi clinici siano nella commissione è: {probability:.4f}\"\n)\n\nLa probabilità che almeno 2 psicologi clinici siano nella commissione è: 0.8482\n\n\n\n\n?sec-simulations\n?exr-prob-simulation-1\nPer calcolare le deviazioni standard delle distribuzioni gaussiane date le percentuali di studenti che ottengono meno di 18, possiamo utilizzare le proprietà della distribuzione normale e i quantili della distribuzione normale standard (distribuzione normale con media 0 e deviazione standard 1).\nLe distribuzioni normali hanno la proprietà che possiamo trasformare qualsiasi valore \\(X\\) della distribuzione \\(N(\\mu, \\sigma)\\) nella distribuzione normale standard \\(N(0, 1)\\) tramite la formula:\n\\[ Z = \\frac{X - \\mu}{\\sigma}, \\]\ndove \\(Z\\) è il quantile standardizzato.\nPer trovare il valore di \\(\\sigma\\) dato un certo percentile, utilizziamo l’inverso della funzione di distribuzione cumulativa (CDF) della distribuzione normale standard. Per un dato percentile \\(p\\), \\(z_p\\) è tale che:\n\\[ p = P(Z \\leq z_p) \\]\nQuindi possiamo trovare \\(\\sigma\\) risolvendo per \\(\\sigma\\) nella formula:\n\\[ z_p = \\frac{X - \\mu}{\\sigma}, \\]\n\\[ \\sigma = \\frac{X - \\mu}{z_p}, \\]\ndove:\n\n\\(X\\) è il punteggio di soglia (18 in questo caso).\n\\(\\mu\\) è la media della distribuzione.\n\\(z_p\\) è il quantile della distribuzione normale standard per il percentile \\(p\\).\n\nI quantili della distribuzione normale standard per i percentili desiderati sono:\n\nPer il 15%, il quantile è \\(z_{0.15} \\approx -1.036\\).\nPer il 10%, il quantile è \\(z_{0.10} \\approx -1.281\\).\nPer il 5%, il quantile è \\(z_{0.05} \\approx -1.645\\).\n\nPrima Prova\n\nMedia: \\(\\mu = 24\\)\nPercentuale che ottiene meno di 18: 15%\nQuantile: \\(z_{0.15} = -1.036\\)\nSoglia: \\(X = 18\\)\n\n\\[ \\sigma_1 = \\frac{24 - 18}{1.036} \\approx 5.79 \\]\nSeconda Prova\n\nMedia: \\(\\mu = 25\\)\nPercentuale che ottiene meno di 18: 10%\nQuantile: \\(z_{0.10} = -1.281\\)\nSoglia: \\(X = 18\\)\n\n\\[ \\sigma_2 = \\frac{25 - 18}{1.281} \\approx 5.46 \\]\nTerza Prova\n\nMedia: \\(\\mu = 26\\)\nPercentuale che ottiene meno di 18: 5%\nQuantile: \\(z_{0.05} = -1.645\\)\nSoglia: \\(X = 18\\)\n\n\\[ \\sigma_3 = \\frac{26 - 18}{1.645} \\approx 4.86 \\]\n\n# Funzione per calcolare la deviazione standard data la media, la soglia e il quantile\ndef calculate_std(mean, threshold, quantile):\n    return abs((mean - threshold) / quantile)\n\n\n# Parametri delle distribuzioni gaussiane per le tre prove\nmean_test1 = 24\nstd_test1 = calculate_std(mean_test1, 18, -1.036)\nmean_test2 = 25\nstd_test2 = calculate_std(mean_test2, 18, -1.281)\nmean_test3 = 26\nstd_test3 = calculate_std(mean_test3, 18, -1.645)\n\n# Numero di studenti\nn_students = 220\n\n# Percentuale di studenti che non fa le prove\ndrop_test1 = 0.10\ndrop_test2 = 0.05\n\n# Seed per il generatore di numeri casuali basato sulla stringa \"simulation\"\nseed = sum(map(ord, \"simulation\"))\nrng = np.random.default_rng(seed=seed)\n\n# Generazione dei voti per le tre prove\n# Genera i voti solo per gli studenti che partecipano alla prova\ntest1_scores = np.where(\n    rng.random(n_students) &gt; drop_test1,\n    rng.normal(mean_test1, std_test1, n_students),\n    np.nan,\n)\ntest2_scores = np.where(\n    rng.random(n_students) &gt; drop_test2,\n    rng.normal(mean_test2, std_test2, n_students),\n    np.nan,\n)\ntest3_scores = rng.normal(mean_test3, std_test3, n_students)\n\n# Calcola il voto finale solo per gli studenti che hanno partecipato a tutte e tre le prove\nfinal_scores = np.nanmean(\n    np.column_stack((test1_scores, test2_scores, test3_scores)), axis=1\n)\n\n# Filtra gli studenti che non hanno partecipato a tutte e tre le prove\nvalid_final_scores = final_scores[~np.isnan(final_scores)]\n\n# Visualizzazione della distribuzione finale dei voti\nplt.hist(valid_final_scores, bins=30, edgecolor=\"black\")\nplt.title(\"Distribuzione dei voti finali\")\nplt.xlabel(\"Voto finale\")\nplt.ylabel(\"Frequenza\")\nplt.show()\n\n# Statistiche descrittive dei voti finali\nmean_final_score = np.mean(valid_final_scores)\nmedian_final_score = np.median(valid_final_scores)\nstd_final_score = np.std(valid_final_scores)\n\nprint(f\"Media dei voti finali: {mean_final_score:.2f}\")\nprint(f\"Mediana dei voti finali: {median_final_score:.2f}\")\nprint(f\"Deviazione standard dei voti finali: {std_final_score:.2f}\")",
    "crumbs": [
      "Appendici",
      "Soluzioni degli esercizi",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Probabilità</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Testing Psicologico",
    "section": "",
    "text": "Benvenuti\nL’insegnamento di Testing Psicologico si propone quale stimolo e guida per l’apprendimento delle basi dell’assessment psicologico.",
    "crumbs": [
      "Benvenuti"
    ]
  },
  {
    "objectID": "index.html#informazioni-sullinsegnamento",
    "href": "index.html#informazioni-sullinsegnamento",
    "title": "Testing Psicologico",
    "section": "Informazioni sull’insegnamento",
    "text": "Informazioni sull’insegnamento\n\nCodice: B033288 - Testing Psicologico\nModulo: B033288 - Testing Psicologico (Cognomi L-Z)\nCorso di laurea: Laurea Magistrale: Psicologia Clinica e della Salute e Neuropsicologia\nAnno Accademico: 2024-2025\nCalendario: Il corso si terrà dal 4 marzo al 31 maggio 2025.\nOrario delle lezioni: Le lezioni si svolgeranno il martedì dalle 10:30 alle 13:30 e il giovedì dalle 8:30 alle 11:30.\nLuogo: Le lezioni si terranno presso il Plesso didattico La Torretta.\nModalità di svolgimento della didattica: Le lezioni ed esercitazioni saranno svolte in modalità frontale.\n\n\n\n\n\n\n\nQuesto sito web è la fonte ufficiale per tutte le informazioni relative al programma dell’insegnamento B033288 - Testing Psicologico (Cognomi A-K) per l’A.A. 2024-2025 e le modalità d’esame.",
    "crumbs": [
      "Benvenuti"
    ]
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Testing Psicologico",
    "section": "Syllabus",
    "text": "Syllabus\nIl Syllabus può essere scaricato utilizzando questo link.",
    "crumbs": [
      "Benvenuti"
    ]
  },
  {
    "objectID": "prefazione.html",
    "href": "prefazione.html",
    "title": "Prefazione",
    "section": "",
    "text": "Definizione di misurazione\nLa misurazione psicologica è un pilastro fondamentale nella comprensione e nell’analisi del comportamento umano, fornendo un mezzo quantitativo per esplorare le dinamiche della mente e della personalità. La definizione di misurazione proposta da Stevens (1951), uno dei pionieri della teoria della misurazione, stabilisce che essa consiste nell’assegnare numeri a oggetti o eventi secondo regole definite. Tuttavia, è ormai ampiamente accettato che questa visione sia troppo semplicistica e che la misurazione richieda un approccio più sofisticato. Si concorda comunemente sul fatto che la misurazione debba essere considerata come un processo di creazione di modelli che rappresentano i fenomeni di interesse, principalmente in forma quantitativa.\nDi conseguenza, la misurazione si basa su regole che attribuiscono scale o valori alle entità che rappresentano i costrutti di interesse. Come avviene per tutti i modelli, quelli di misurazione, come i test, le scale o le variabili, devono semplificare la realtà per risultare utili. Pertanto, è fondamentale specificare chiaramente i modelli di misurazione per poterli valutare, confutare e migliorare.\nInoltre, anziché chiedersi se un modello sia vero o corretto, è più utile sviluppare diversi modelli alternativi plausibili e porre domande del tipo: quale modello è meno inaccurato? Questo approccio al confronto dei modelli rappresenta la strategia migliore per valutare e perfezionare le procedure di misurazione, consentendo un’analisi più approfondita e accurata delle variabili coinvolte.\nPer illustrare l’approccio alla misurazione come descritto, prendiamo in considerazione un esempio concreto: la valutazione dell’intelligenza attraverso il test del quoziente intellettivo (QI).\nIniziamo definendo il concetto di interesse, ovvero l’intelligenza, che può essere concepita come la capacità di apprendere, comprendere e applicare conoscenze, risolvere problemi e adattarsi a nuove situazioni. Tuttavia, trattandosi di un concetto astratto, è necessario operazionalizzarlo in modo misurabile.\nPer misurare l’intelligenza, si crea un test di QI che comprende una serie di compiti e domande progettati per valutare diverse dimensioni della capacità cognitiva, quali la memoria, il ragionamento logico e la comprensione verbale.\nCiascun compito nel test di QI è associato a un punteggio. I risultati individuali vengono quindi calcolati e confrontati con una norma statistica per attribuire un punteggio di QI.\nSuccessivamente, il test di QI viene sottoposto a diverse analisi per verificare la sua validità (ovvero se misura effettivamente l’intelligenza) e affidabilità (se fornisce risultati consistenti nel tempo).\nTuttavia, esistono diverse teorie dell’intelligenza, come ad esempio quella delle intelligenze multiple di Gardner, che suggeriscono modelli alternativi di misurazione. Confrontando il modello del QI con questi approcci alternativi, gli psicologi possono valutare quale modello è meno distorto o più adatto per specifici scopi.\nIn risposta alle critiche, alle nuove scoperte e ai cambiamenti culturali e sociali, il modello del QI viene regolarmente rivisto e adattato per assicurare che continui a essere uno strumento utile di misurazione.\nQuesto esempio mostra come la misurazione in psicologia non sia semplicemente un atto di assegnare numeri a un costrutto, ma piuttosto un processo complesso che implica la creazione, la valutazione e il continuo perfezionamento di modelli teorici.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "prefazione.html#temi-centrali-nellapproccio-psicometrico",
    "href": "prefazione.html#temi-centrali-nellapproccio-psicometrico",
    "title": "Prefazione",
    "section": "Temi Centrali nell’Approccio Psicometrico",
    "text": "Temi Centrali nell’Approccio Psicometrico\n\nAffidabilità: Questo concetto si riferisce alla capacità di un test di produrre risultati consistenti nel tempo e in contesti diversi, costituendo una base fondamentale per la misurazione psicologica.\nValidazione del Costrutto e Test dei Modelli: L’evoluzione della psicometria ha portato a una sempre maggiore enfasi sulla validazione dei costrutti e sull’importanza dei test di modelli, utilizzando tecniche come i modelli a equazioni strutturali (SEM) per verificare la coerenza e la validità dei costrutti psicologici.\nDimensionalità e Validità Strutturale: La dimensionalità viene considerata un elemento fondamentale nella valutazione della validità strutturale, poiché permette di esplorare come i diversi aspetti di un costrutto si manifestano e interagiscono all’interno del modello di misurazione.\nCostruzione dei Questionari: La progettazione e la formulazione degli item dei questionari rivestono un ruolo cruciale, in quanto influenzano direttamente l’affidabilità e la validità dei risultati ottenuti. La scelta degli item, il loro ordine e la chiarezza della formulazione sono tutti aspetti che contribuiscono alla qualità e all’efficacia della misurazione psicologica.\n\nAttraverso questi approcci, la misurazione psicologica si adatta alle sfide uniche poste dalla natura astratta e complessa dei costrutti psicologici, cercando di fornire strumenti validi e affidabili per la loro esplorazione e comprensione.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "prefazione.html#affidabilità-e-generalizzabilità-nelle-misure-psicologiche",
    "href": "prefazione.html#affidabilità-e-generalizzabilità-nelle-misure-psicologiche",
    "title": "Prefazione",
    "section": "Affidabilità e Generalizzabilità nelle Misure Psicologiche",
    "text": "Affidabilità e Generalizzabilità nelle Misure Psicologiche\nNel contesto della misurazione psicologica, così come in altre discipline, è cruciale considerare le variabili che possono influenzare la precisione delle misure. L’affidabilità di uno strumento di misurazione psicologica si riferisce alla sua consistenza nel produrre risultati replicabili nel tempo e in contesti diversi. Gli indici di affidabilità sono utilizzati per quantificare il grado di riproducibilità e l’assenza di errori casuali nelle misurazioni.\n\nTeoria Classica dei Test\nL’approccio più ampiamente utilizzato nello studio dell’affidabilità delle misure psicologiche è rappresentato dalla teoria classica dei test, come descritto da Lord e Novick (1968). Secondo questa teoria, ogni misurazione (\\(X\\)) è composta da due componenti distintive: un punteggio “vero” (\\(T\\)) e un errore di misurazione (\\(e\\)). Il concetto di misurazione accurata, o “vera”, può essere rappresentato come \\(X - e\\), evidenziando il fatto che ogni misurazione può essere decomposta in tali elementi distinti.\nLa teoria classica dei test enfatizza l’importanza di condurre misurazioni ripetute per valutare l’affidabilità. Un concetto fondamentale è quello dei test paralleli, che consistono in due test con medie, varianze e distribuzioni identiche, e che mostrano una correlazione simile con variabili esterne. In questa prospettiva, il punteggio vero e l’errore di misurazione sono considerati indipendenti. Di conseguenza, la varianza dei punteggi osservati (Varianza \\(X\\)) è la somma della varianza dei punteggi veri (Varianza \\(T\\)) e della varianza dell’errore di misurazione (Varianza \\(e\\)).\nL’affidabilità è quindi definita come il rapporto tra la varianza del punteggio vero e la varianza del punteggio osservato:\n\\[\n\\text{Affidabilità} = \\frac{\\text{Varianza}(T)}{\\text{Varianza}(X)}.\n\\]\nIn termini pratici, un’affidabilità di 1 indicherebbe l’assenza di errori, mentre un’affidabilità di 0 implicherebbe che i punteggi derivano esclusivamente dall’errore. La correlazione tra il punteggio osservato e il punteggio vero è la radice quadrata dell’affidabilità, fornendo una stima della precisione della misurazione.\nQuesto framework fornisce una solida base per comprendere e quantificare l’affidabilità nelle misure psicologiche, sottolineando l’importanza di considerare sia i punteggi veri sia gli errori di misurazione per ottenere misurazioni precise e affidabili.\n\n\nEvidenze Multiple di Affidabilità\nNonostante la teoria classica dei test fornisca una definizione matematica dei test paralleli, non fornisce dettagliate linee guida sulle procedure specifiche per costruirli. Tuttavia, a partire dagli anni ’50, sono stati sviluppati diversi metodi che consentono di valutare empiricamente l’affidabilità delle misurazioni:\n\nTest-Retest: Questo approccio implica la somministrazione dello stesso test ai partecipanti in due momenti diversi. L’obiettivo è valutare la stabilità dei punteggi nel tempo. Una correlazione elevata tra i punteggi ottenuti nei due momenti indica una buona affidabilità del test-retest.\nEquivalenza di Forme Parallele: Questo metodo prevede l’utilizzo di due versioni diverse del test, ma che coprono lo stesso contenuto, somministrate simultaneamente ai partecipanti. Una forte correlazione tra i punteggi ottenuti dalle due versioni suggerisce che entrambe misurano il medesimo costrutto in modo affidabile.\nSplit-Half e Coerenza Interna:\n\nSplit-Half: I partecipanti completano una sola versione del test, la quale è divisa in due parti equivalenti. Si calcola poi la correlazione tra i punteggi delle due metà. Questo metodo valuta la coerenza interna del test.\nCoerenza Interna (ad esempio, Omega di McDonals): Valuta la correlazione tra tutti gli elementi del test. Un alto valore di coerenza interna indica che tutti gli elementi del test misurano aspetti simili del costrutto.\n\nValutazione da Giudici Multipli: In questo caso, i partecipanti sono valutati da più giudici in un’unica occasione. Un alto grado di accordo tra i giudici fornisce un’indicazione dell’affidabilità delle valutazioni.\n\nCiascuno di questi approcci fornisce indicazioni sull’affidabilità di un test, ma è fondamentale considerare che alcuni potrebbero essere più appropriati di altri in base alla natura del test e del costrutto misurato. L’affidabilità è pertanto un concetto multidimensionale che richiede l’impiego di diversi approcci per una valutazione completa delle misurazioni psicologiche.\n\n\nIl Ruolo del Coefficiente Alpha nella Misurazione Psicologica\nIl coefficiente alpha, introdotto da Cronbach nel 1951, è diventato un importante indicatore di coerenza interna nella letteratura psicologica, principalmente grazie alla sua facilità di calcolo. A differenza dell’affidabilità test-retest, che richiede dati raccolti in due momenti diversi, o dell’affidabilità delle forme parallele, che richiede la costruzione di due versioni alternative di un test, il coefficiente alpha può essere calcolato utilizzando un unico set di dati, rendendolo estremamente pratico come indice di affidabilità.\nTuttavia, è importante correggere un comune malinteso riguardo al coefficiente alpha: esso non misura direttamente l’omogeneità delle intercorrelazioni tra gli elementi o conferma la unidimensionalità di una scala. In realtà, il coefficiente alpha non fornisce informazioni dirette su questi aspetti strutturali della scala.\nPer affrontare la questione della unidimensionalità, è necessario ricorrere a approcci più sofisticati come l’analisi fattoriale confermativa e i modelli di equazioni strutturali (SEM). Questi metodi consentono di testare quanto bene la struttura di correlazione degli elementi si adatti a un modello con un singolo fattore rispetto a modelli multifattoriali, valutando se le correlazioni tra gli elementi possono essere meglio spiegate da un singolo costrutto sottostante.\nNel contesto delle analisi SEM, le saturazioni degli item indicano quanto della varianza di un item sia condivisa con gli altri (e quindi generalizzabile), mentre la varianza residua dell’item cattura l’errore unico associato a quell’item. La presenza di multidimensionalità emerge dalla capacità di un modello multifattoriale di adattarsi meglio ai dati rispetto a un modello a singolo fattore.\nQuando un test è considerato multidimensionale, è ancora appropriato utilizzare il coefficiente alpha come indice di affidabilità? La risposta è negativa. In presenza di multidimensionalità, il coefficiente alpha tende a sottostimare l’affidabilità. Pertanto, è consigliabile, in tali casi, utilizzare altri metodi per valutare l’affidabilità, anziché basarsi esclusivamente sul coefficiente alpha.\n\n\nIl Fenomeno dell’Attenuazione in Relazione all’Affidabilità\nAll’interno del contesto della teoria classica dei test, come delineato da Lord e Novick (1968), l’affidabilità svolge un ruolo cruciale poiché influisce sulla forza della correlazione che una misura può mostrare con altre variabili, come un criterio esterno. Secondo questa teoria, se l’errore nelle misurazioni è genuinamente casuale, il massimo teorico della correlazione tra una misura e un’altra variabile non è 1.0, ma piuttosto la radice quadrata dell’affidabilità di quella misura.\nCiò implica che, in presenza di un’affidabilità meno che ottimale, la correlazione effettiva tra una misura e qualsiasi altra variabile viene sistematicamente sottostimata, fenomeno noto come attenuazione. Questa attenuazione è direttamente proporzionale all’inadeguatezza dell’affidabilità: più bassa è l’affidabilità di una misura, maggiore sarà la sottostima della sua correlazione con altre variabili. Pertanto, per ottenere stime accurate delle correlazioni e comprendere veramente le relazioni tra diverse variabili, è fondamentale garantire che le misure utilizzate siano il più affidabili possibile. Questa considerazione enfatizza l’importanza dell’accuratezza e della precisione nelle procedure di misurazione psicologica.\n\n\nLa Teoria della Generalizzabilità\nLa Teoria della Generalizzabilità propone un approccio più completo e flessibile per comprendere l’affidabilità delle misure psicologiche rispetto alla classificazione tradizionale delle tipologie di affidabilità. Invece di limitarsi a categorizzare le misure in base a criteri specifici come test-retest, affidabilità interna o inter-valutatori, la Teoria della Generalizzabilità considera una serie di dimensioni che possono influenzare l’affidabilità in contesti diversi.\nUna delle principali criticità della teoria classica dei test è la sua presunzione di uniformità e parallelismo delle misurazioni e degli errori casuali. La Teoria della Generalizzabilità, al contrario, riconosce che l’affidabilità dipende dalla specifica dimensione di generalizzazione considerata. Ad esempio, un test potrebbe essere affidabile per misurare una certa caratteristica in un contesto, ma non altrettanto affidabile in un contesto diverso o per una caratteristica correlata ma non identica.\nPer superare le limitazioni della teoria classica dei test, l’American Psychological Association ha proposto l’adozione della Teoria della Generalizzabilità. Tuttavia, nonostante questa proposta, la pratica nei campi di ricerca non si è adeguatamente evoluta e la teoria della generalizzabilità non ha ancora completamente sostituito le nozioni più semplicistiche popolari in psicologia.\nLa Teoria della Generalizzabilità esamina diverse dimensioni che influenzano l’affidabilità, tra cui la dimensione temporale, delle forme, degli item e dei giudici o osservatori. Questa teoria enfatizza l’importanza di estendere le osservazioni a un’ampia varietà di situazioni e identificare l’impatto specifico delle fonti di varianza nei punteggi dei test in contesti particolari.\nInvece dei tradizionali coefficienti di affidabilità come il coefficiente di stabilità o il coefficiente alfa, la Teoria della Generalizzabilità suggerisce l’uso di misure più ampie di affidabilità, come il coefficiente di correlazione intraclasse, per esaminare specifici aspetti dell’affidabilità. Questo approccio è particolarmente utile in ricerche con dati strutturati in maniera nidificata e dove diverse dimensioni possono influenzare l’affidabilità, come nei metodi di valutazione ecologica momentanea.### La Teoria della Risposta agli Item\nLa Teoria della Risposta agli Item (IRT) rappresenta un avanzamento rispetto alla teoria classica dei test, offrendo un approccio più sofisticato per analizzare le risposte degli individui agli item e la loro relazione con un costrutto latente. Questa teoria stabilisce un collegamento tra le risposte degli individui a un particolare item e il costrutto latente utilizzando una funzione chiamata “curva caratteristica dell’item”.\nLa curva caratteristica dell’item mostra la probabilità che individui con differenti livelli del costrutto latente rispondano correttamente all’item, fornendo inoltre informazioni sulla capacità dell’item di distinguere tra individui con livelli elevati e bassi del tratto latente, oltre a misurare la sua difficoltà. Queste informazioni sono cruciali per identificare eventuali distorsioni negli item, noto come bias. Secondo la IRT, un item è privo di bias nel misurare un costrutto se individui con lo stesso livello del tratto ottengono punteggi attesi simili sull’item, indipendentemente da caratteristiche non rilevanti come genere, etnia o background culturale.\nLa Teoria della Risposta agli Item offre diversi vantaggi nel processo di creazione e valutazione di scale psicometriche:\n\nSelezione degli Item: Permette di selezionare gli item in base alla loro difficoltà e alla capacità di discriminazione, superando così la limitazione della teoria classica che si basa esclusivamente sulle correlazioni tra gli item e il punteggio totale.\nTesting Adattivo Computerizzato: La IRT facilita la valutazione della posizione di un individuo su un costrutto latente senza la necessità di somministrare l’intero test, grazie a tecniche come il testing adattivo computerizzato.\n\nIn conclusione, la Teoria della Risposta agli Item fornisce strumenti quantitativi per esaminare approfonditamente la relazione tra un item specifico e il costrutto latente, attraverso parametri di difficoltà e discriminazione.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "prefazione.html#evoluzione-e-comprensione-della-validità-nelle-misure-psicologiche",
    "href": "prefazione.html#evoluzione-e-comprensione-della-validità-nelle-misure-psicologiche",
    "title": "Prefazione",
    "section": "Evoluzione e Comprensione della Validità nelle Misure Psicologiche",
    "text": "Evoluzione e Comprensione della Validità nelle Misure Psicologiche\nLa nostra comprensione della validità nelle misure psicologiche ha subito un notevole sviluppo nel corso del tempo, passando da una visione iniziale più frammentata a un approccio più olistico e dinamico. Inizialmente, la validità veniva suddivisa in diversi tipi, tra cui la validità di contenuto, di facciata, orientata al criterio e di costrutto.\nLa validità di contenuto si riferisce alla rappresentatività degli item di un test rispetto al costrutto che si intende misurare, mentre la validità di facciata valuta se superficialmente gli item sembrano idonei a misurare il costrutto, sebbene questa non sia considerata un indice rigoroso di validità. La validità orientata al criterio si divide ulteriormente in predittiva e concorrente, che valutano la capacità del test di prevedere comportamenti futuri o di correlare con criteri esterni contemporaneamente misurati. Infine, la validità di costrutto indaga se il test misura effettivamente il costrutto in questione, richiedendo una comprensione approfondita sia del costrutto sia della metodologia del test.\nTuttavia, queste distinzioni sono state gradualmente considerate limitate e frammentarie. Un punto di svolta è stato rappresentato dall’approccio olistico di Samuel Messick, che ha enfatizzato che la validità va oltre la misura stessa, coinvolgendo l’interpretazione e l’uso dei punteggi del test. Messick ha sottolineato l’importanza di considerare le evidenze di validità da molteplici fonti e di assicurare la coerenza delle interpretazioni dei punteggi del test con le teorie psicologiche sottostanti.\nUn’importante correzione concettuale è stata l’idea che la validità non sia un attributo statico dei test, ma piuttosto un processo continuo di accumulo di evidenze e giustificazioni teoriche. Questo processo di validazione riflette l’evoluzione delle teorie psicologiche e delle metodologie di misurazione, sottolineando che la validità è dinamica e contestuale.\nIn sintesi, l’evoluzione della concezione di validità nelle misure psicologiche sottolinea l’importanza di un approccio comprensivo, teoricamente informato e basato sull’evidenza per valutare, interpretare e utilizzare i punteggi dei test. Questo approccio moderno incoraggia i ricercatori e i praticanti a considerare la validità come un concetto ampio che incorpora molteplici aspetti della progettazione, dell’implementazione e dell’interpretazione dei test psicologici.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "prefazione.html#approfondimento-su-tecniche-di-validazione-di-costrutto-e-costruzione-di-scale",
    "href": "prefazione.html#approfondimento-su-tecniche-di-validazione-di-costrutto-e-costruzione-di-scale",
    "title": "Prefazione",
    "section": "Approfondimento su Tecniche di Validazione di Costrutto e Costruzione di Scale",
    "text": "Approfondimento su Tecniche di Validazione di Costrutto e Costruzione di Scale\nLa discussione sulla evoluzione della validità nelle misure psicologiche può proseguire con l’esame delle tecniche che vengono usate per la validazione di costrutto e per la costruzione di scale. In particolare, gli strumenti maggiormente usati dagli psicometristi sono l’Analisi Fattoriale Confermativa (CFA) e i Modelli di Equazioni Strutturali (SEM).\nL’Analisi Fattoriale Confermativa (CFA) rappresenta un approccio metodologico rigoroso, basato sull’ipotesi che un insieme di osservazioni possa essere spiegato da pochi costrutti latenti. A differenza dell’Analisi Fattoriale Esplorativa, che non prevede ipotesi a priori sui fattori, la CFA richiede che i ricercatori definiscano anticipatamente un modello teorico. Questo specifica le relazioni tra le variabili osservabili e i costrutti latenti, permettendo di testare l’adeguatezza del modello ai dati. La capacità della CFA di confrontare diversi modelli offre un mezzo potente per identificare la struttura che meglio rappresenta i dati.\nNel contesto della valutazione della coerenza interna di una scala, l’utilizzo della CFA supera i limiti dei metodi basati sulla teoria classica dei test, fornendo una valutazione più dettagliata e strutturata delle relazioni tra item e costrutti latenti.\nI Modelli di Equazioni Strutturali (SEM) estendono le possibilità offerte dalla CFA, abilitando l’analisi delle relazioni di regressione non solo tra variabili manifeste e latenti, ma anche tra i costrutti latenti stessi. Questa caratteristica rende i SEM strumenti eccezionalmente potenti per esplorare le interazioni complesse tra variabili in uno studio psicometrico.\nL’esame della dimensionalità di un costrutto attraverso la CFA e i SEM consente di testare con precisione le ipotesi sulla struttura dimensionale dei costrutti, verificando se l’organizzazione teorizzata degli item in fattori latenti corrisponde ai dati. Questi strumenti sono quindi fondamentali per confermare la struttura di un costrutto come ipotizzato dalla teoria sottostante.\nIn aggiunta, l’approccio Multitrait-Multimethod (MTMM) per esaminare la validità esterna, incorporando la validità convergente e discriminante, arricchisce ulteriormente la comprensione della misura. L’uso del disegno MTMM permette di distinguere efficacemente tra costrutti correlati ma distinti, assicurando che le misure non solo riflettano accuratamente il costrutto target, ma siano anche discriminanti rispetto ad altri costrutti.\nIn sintesi, l’integrazione di CFA e SEM nel processo di validazione di costrutti e nella costruzione di scale psicometriche rappresenta un avanzamento metodologico significativo. Questi approcci non solo migliorano la precisione e la comprensione delle relazioni tra variabili osservabili e latenti, ma contribuiscono anche a elevare la qualità e l’affidabilità delle misure psicologiche. Attraverso un uso attento e informato di queste tecniche, i ricercatori possono arricchire la validità e l’utilità delle scale psicometriche. Chi volesse approfondire ulteriormente questi argomenti, può fare riferimento al testo di John & Benet-Martinez (2014).\n\n\n\n\nJohn, O. P., & Benet-Martinez, V. (2014). Measurement: Reliability, construct validation, and scale construction. In H. T. Reis & C. M. Judd (A c. Di), Handbook of research methods in social and personality psychology (2nd ed., pp. 473–503). Cambridge University Press.",
    "crumbs": [
      "Prefazione"
    ]
  },
  {
    "objectID": "cal_testing_psic_2025.html",
    "href": "cal_testing_psic_2025.html",
    "title": "1  Calendario delle lezioni",
    "section": "",
    "text": "Il calendario didattico prevede 14 incontri di 3 ore ciascuno, con una verifica tramite Quiz Moodle e le presentazioni finali degli studenti negli ultimi due incontri.\n\n\n\n\n\n\n\n\n\nIncontro\nData\nArgomento\nOrario\n\n\n\n\n1\n4 marzo 2025\nPresentazione del corso; introduzione a \\(\\mathsf{R}\\)\n10:30-13:30\n\n\n2\n6 marzo 2025\nPunteggi e scale; punteggi derivati; scaling Likert; codifica inversa; imputazione; calcolo del punteggio totale; ottimizzazione dello scoring dei dati ordinali; scaling di Thurstone; sviluppo dello strumento; equating nei test psicologici; modello lineare\n8:30-11:30\n\n\n3\n11 marzo 2025\nTeoria Classica dei Test; relazione con il modello lineare; errore standard della misurazione; misure congeneriche, tau-equivalenti, parallele; affidabilità; stima del punteggio vero e errore standard della stima; applicazioni\n10:30-13:30\n\n\n4\n13 marzo 2025\nModello di regressione logistica; Mokken Scale Analysis\n8:30-11:30\n\n\n5\n18 marzo 2025\nItem Response Theory; validità.\n10:30-13:30\n\n\n6\n20 marzo 2025\nPath Analysis; tutorial di Clement & Bradley-Garcia (2022); network analysis\n8:30-11:30\n\n\n7\n25 marzo 2025\nElementi di algebra lineare; analisi delle componenti principali\n10:30-13:30\n\n\n8\n27 marzo 2025\nAnalisi fattoriale esplorativa; il modello statistico delll’analisi fattoriale\n8:30-11:30\n\n\n9\n1 aprile 2025\nEstrazione dei fattori; rotazione\n10:30-13:30\n\n\n10\n3 aprile 2025\nAnalisi fattoriale confermativa\n8:30-11:30\n\n\n11\n8 aprile 2025\nIntroduzione ai modelli di equazioni strutturali\n10:30-13:30\n\n\n12\n10 aprile 2025\nModelli multilivello; attendibilità dei giudici; modelli di crescita latente\n8:30-11:30\n\n\n13\n15 aprile 2025\nVerifica tramite Quiz Moodle; presentazioni finali degli studenti\n10:30-13:30\n\n\n14\n17 aprile 2025\nPresentazioni finali degli studenti\n8:30-11:30\n\n\n\n\n\n\n\n\nClement, L. M., & Bradley-Garcia, M. (2022). A step-by-step tutorial for performing a moderated mediation analysis using PROCESS. The Quantitative Methods for Psychology, 18(3), 258–271.",
    "crumbs": [
      "Programmazione",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Calendario delle lezioni</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/11_lgm_wais.html",
    "href": "chapters/lgm/11_lgm_wais.html",
    "title": "\n79  Lo Sviluppo dell’Intelligenza\n",
    "section": "",
    "text": "79.1 Introduzione\nQuesta discussione riproduce il tutorial presentato nel Workshop on Latent Growth Modeling in Lavaan tenuto al Donders Institute nel novembre 2024. Questo tutorial riprende in un unico studio i concetti che avevamo esaminato nei capitoli precedenti. Verranno utilizzati dei dati longitudinali relativi al WISC-V forniti dagli autori a 6, 7, 9 e 11 anni.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>79</span>  <span class='chapter-title'>Lo Sviluppo dell'Intelligenza</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/11_lgm_wais.html#dati",
    "href": "chapters/lgm/11_lgm_wais.html#dati",
    "title": "\n79  Lo Sviluppo dell’Intelligenza\n",
    "section": "\n79.2 Dati",
    "text": "79.2 Dati\nIl WISC-V Test (Wechsler Intelligence Scale for Children) è un test del QI somministrato a bambini di età compresa tra 6 e 16 anni. Fornisce cinque punteggi indici principali, ovvero Indice di Comprensione Verbale, Indice Visuo-Spaziale, Indice di Ragionamento Fluido, Indice di Memoria di Lavoro e Indice di Velocità di Elaborazione. Nel workshop gli autori discutono su un sottoinsieme contenente: Indice di Comprensione Verbale, Indice di Velocità di Elaborazione e il totale.\n\nwisc &lt;- rio::import(\n  here::here(\n    \"data\", \"wisc.csv\"\n  )\n)[,-1]\n\nhead(wisc)         #first 6 rows\n\n  ID Verbal_T6 Verbal_T7 Verbal_T9 Verbal_T11 Pspeed_T6 Pspeed_T7 Pspeed_T9\n1  0  24.41964  26.97917  39.61310   55.63988 19.835937  22.96932  43.90245\n2  1  12.44048  14.37500  21.91964   37.81250  5.898771  13.43888  18.28666\n3  2  32.42560  33.51190  34.30060   50.17857 27.638263  45.02193  46.99432\n4  3  22.69345  28.39286  42.15774   44.71726 33.158129  29.67703  45.97218\n5  4  28.22917  37.81250  41.05655   70.95238 27.638263  44.41775  65.47530\n6  5  16.05655  20.11905  38.02083   39.94048  8.445562  15.77845  26.98717\n  Pspeed_T11   Total_6  Total_7  Total_9 Total_11   age_T6 sex race mo_edu\n1   44.18684 22.127790 24.97424 41.75777 49.91336 5.833333   1    1      4\n2   40.38395  9.169624 13.90694 20.10315 39.09822 5.916667   2    2      6\n3   77.71613 30.031929 39.26692 40.64746 63.94735 6.333333   1    1      2\n4   61.66409 27.925791 29.03495 44.06496 53.19067 6.333333   2    1      2\n5   64.21768 27.933715 41.11512 53.26593 67.58503 6.166667   1    1      3\n6   39.08200 12.251055 17.94875 32.50400 39.51124 5.666667   1    1      2\n  mo_educat fa_edu fa_educat\n1         0      4         0\n2         0      5         0\n3         2      3         1\n4         2      2         2\n5         1      3         1\n6         2      2         2\n\n\n\ndim(wisc)          #number of rows and columns\n\n[1] 204  20\n\n\nGli autori si concentrano sull’analisi dei dati del subtest verbale.\n\nwisc_verbal &lt;- wisc[,c(\"ID\",\"Verbal_T6\",\"Verbal_T7\",\"Verbal_T9\",\"Verbal_T11\")]\nglimpse(wisc_verbal)\n\nRows: 204\nColumns: 5\n$ ID         &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 14, 15, 16, 17, 21, 23, 2…\n$ Verbal_T6  &lt;dbl&gt; 24.419643, 12.440476, 32.425595, 22.693452, 28.229167, 16.0…\n$ Verbal_T7  &lt;dbl&gt; 26.97917, 14.37500, 33.51190, 28.39286, 37.81250, 20.11905,…\n$ Verbal_T9  &lt;dbl&gt; 39.61310, 21.91964, 34.30060, 42.15774, 41.05655, 38.02083,…\n$ Verbal_T11 &lt;dbl&gt; 55.63988, 37.81250, 50.17857, 44.71726, 70.95238, 39.94048,…\n\n\nI dati vanno trasformati nel formato long.\n\nwisc_verbal_long &lt;- wisc_verbal %&gt;% \n  pivot_longer(!ID, names_to = \"wave\", values_to = \"verbal\")\n\nwisc_verbal_long |&gt; head()\n\n# A tibble: 6 × 3\n     ID wave       verbal\n  &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt;\n1     0 Verbal_T6    24.4\n2     0 Verbal_T7    27.0\n3     0 Verbal_T9    39.6\n4     0 Verbal_T11   55.6\n5     1 Verbal_T6    12.4\n6     1 Verbal_T7    14.4\n\n\nUn grafico dei dati si ottiene nel modo seguente.\n\nwisc_verbal_long$wave = factor(wisc_verbal_long$wave, levels=c(\"Verbal_T6\",\"Verbal_T7\",\"Verbal_T9\",\"Verbal_T11\"))\n\nggplot(wisc_verbal_long, aes(wave, verbal, group=ID, fill=ID, color=ID)) +\n  geom_point() + \n  geom_line() +\n  theme_classic(base_size = 15) + # adding a classic theme; https://ggplot2.tidyverse.org/reference/ggtheme.html\n  theme(legend.position = \"none\") + # getting rid of legend\n  labs(x = \"Wave\", y = \"Score on Verbal Subtest\")",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>79</span>  <span class='chapter-title'>Lo Sviluppo dell'Intelligenza</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/11_lgm_wais.html#modello-lineare",
    "href": "chapters/lgm/11_lgm_wais.html#modello-lineare",
    "title": "\n79  Lo Sviluppo dell’Intelligenza\n",
    "section": "\n79.3 Modello lineare",
    "text": "79.3 Modello lineare\nIl modello più semplice è quello di crescita lineare.\n\n# Create LGM\nlinear_growth_model &lt;- '\n  i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11\n  s =~ 0*Verbal_T6 + 1*Verbal_T7 + 2*Verbal_T9 + 3*Verbal_T11'\n\nAdattiamo il modello ai dati ed esaminiamo i risultati.\n\n# Fit LGM\nfit_linear_growth_model &lt;- growth(linear_growth_model, data=wisc_verbal,missing='fiml')\n# Output results\nsummary(fit_linear_growth_model, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n\nlavaan 0.6-19 ended normally after 65 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n\n  Number of observations                           204\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                                      \n  Test statistic                               100.756\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               585.906\n  Degrees of freedom                                 6\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.835\n  Tucker-Lewis Index (TLI)                       0.802\n                                                      \n  Robust Comparative Fit Index (CFI)             0.835\n  Robust Tucker-Lewis Index (TLI)                0.802\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2530.194\n  Loglikelihood unrestricted model (H1)      -2479.816\n                                                      \n  Akaike (AIC)                                5078.388\n  Bayesian (BIC)                              5108.251\n  Sample-size adjusted Bayesian (SABIC)       5079.736\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.306\n  90 Percent confidence interval - lower         0.256\n  90 Percent confidence interval - upper         0.360\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n                                                      \n  Robust RMSEA                                   0.306\n  90 Percent confidence interval - lower         0.256\n  90 Percent confidence interval - upper         0.360\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080             1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.113\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i =~                                                                  \n    Verbal_T6         1.000                               4.354    0.775\n    Verbal_T7         1.000                               4.354    0.681\n    Verbal_T9         1.000                               4.354    0.583\n    Verbal_T11        1.000                               4.354    0.417\n  s =~                                                                  \n    Verbal_T6         0.000                               0.000    0.000\n    Verbal_T7         1.000                               1.251    0.196\n    Verbal_T9         2.000                               2.502    0.335\n    Verbal_T11        3.000                               3.752    0.360\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i ~~                                                                  \n    s                 5.081    1.079    4.709    0.000    0.933    0.933\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    i                18.760    0.391   48.006    0.000    4.308    4.308\n    s                 7.291    0.192   38.007    0.000    5.829    5.829\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Verbal_T6        12.600    2.175    5.793    0.000   12.600    0.399\n   .Verbal_T7        10.213    1.463    6.982    0.000   10.213    0.250\n   .Verbal_T9        10.243    1.941    5.277    0.000   10.243    0.184\n   .Verbal_T11       45.410    5.781    7.855    0.000   45.410    0.417\n    i                18.961    3.154    6.012    0.000    1.000    1.000\n    s                 1.565    0.658    2.379    0.017    1.000    1.000\n\nR-Square:\n                   Estimate\n    Verbal_T6         0.601\n    Verbal_T7         0.750\n    Verbal_T9         0.816\n    Verbal_T11        0.583\n\n\nIl modello non si adatta bene ai dati.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>79</span>  <span class='chapter-title'>Lo Sviluppo dell'Intelligenza</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/11_lgm_wais.html#crescita-non-lineare",
    "href": "chapters/lgm/11_lgm_wais.html#crescita-non-lineare",
    "title": "\n79  Lo Sviluppo dell’Intelligenza\n",
    "section": "\n79.4 Crescita non lineare",
    "text": "79.4 Crescita non lineare\nNell’analisi precedente, abbiamo modellato un modello di crescita lineare. Tuttavia, è anche possibile modellare una crescita non lineare in lavaan come una traiettoria quadratica. Per fare ciò, è necessario aggiungere un terzo parametro chiamato termine quadratico che avrà gli stessi loadings del coefficiente angolare, ma al quadrato.\nPer fare questo, è necessario specificare un’altra variabile latente nel modello chiamata termine quadratico. Al termine quadratico vengono assegnati loadings che sono i quadrati dei loadings del coefficiente angolare.\n\n# Create quadratic growth model\nquad_growth_model &lt;- 'i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11\n                      s =~ 0*Verbal_T6 + 1*Verbal_T7 + 2*Verbal_T9 + 3*Verbal_T11\n                      q =~ 0*Verbal_T6 + 1*Verbal_T7 + 4*Verbal_T9 + 9*Verbal_T11'\n# Fit model\nfit_quad_growth_model &lt;- growth(quad_growth_model, data=wisc_verbal,missing='fiml')\n# Output results\nsummary(fit_quad_growth_model, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n\nlavaan 0.6-19 ended normally after 99 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n\n  Number of observations                           204\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                                      \n  Test statistic                                 6.176\n  Degrees of freedom                                 1\n  P-value (Chi-square)                           0.013\n\nModel Test Baseline Model:\n\n  Test statistic                               585.906\n  Degrees of freedom                                 6\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.991\n  Tucker-Lewis Index (TLI)                       0.946\n                                                      \n  Robust Comparative Fit Index (CFI)             0.991\n  Robust Tucker-Lewis Index (TLI)                0.946\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2482.904\n  Loglikelihood unrestricted model (H1)      -2479.816\n                                                      \n  Akaike (AIC)                                4991.808\n  Bayesian (BIC)                              5034.943\n  Sample-size adjusted Bayesian (SABIC)       4993.755\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.159\n  90 Percent confidence interval - lower         0.059\n  90 Percent confidence interval - upper         0.289\n  P-value H_0: RMSEA &lt;= 0.050                    0.039\n  P-value H_0: RMSEA &gt;= 0.080                    0.910\n                                                      \n  Robust RMSEA                                   0.159\n  90 Percent confidence interval - lower         0.059\n  90 Percent confidence interval - upper         0.289\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.039\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.910\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.023\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i =~                                                                  \n    Verbal_T6         1.000                               4.883    0.843\n    Verbal_T7         1.000                               4.883    0.800\n    Verbal_T9         1.000                               4.883    0.668\n    Verbal_T11        1.000                               4.883    0.459\n  s =~                                                                  \n    Verbal_T6         0.000                                  NA       NA\n    Verbal_T7         1.000                                  NA       NA\n    Verbal_T9         2.000                                  NA       NA\n    Verbal_T11        3.000                                  NA       NA\n  q =~                                                                  \n    Verbal_T6         0.000                                  NA       NA\n    Verbal_T7         1.000                                  NA       NA\n    Verbal_T9         4.000                                  NA       NA\n    Verbal_T11        9.000                                  NA       NA\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i ~~                                                                  \n    s                -0.564    6.937   -0.081    0.935   -0.064   -0.064\n    q                 2.014    1.811    1.112    0.266    0.738    0.738\n  s ~~                                                                  \n    q                 1.518    1.719    0.883    0.377    1.500    1.500\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    i                19.697    0.409   48.124    0.000    4.033    4.033\n    s                 4.051    0.354   11.439    0.000       NA       NA\n    q                 1.284    0.130    9.861    0.000       NA       NA\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Verbal_T6         9.730    6.209    1.567    0.117    9.730    0.290\n   .Verbal_T7        11.059    2.297    4.814    0.000   11.059    0.297\n   .Verbal_T9         9.542    2.677    3.564    0.000    9.542    0.179\n   .Verbal_T11       29.417   11.518    2.554    0.011   29.417    0.260\n    i                23.848    6.558    3.636    0.000    1.000    1.000\n    s                -3.277    7.053   -0.465    0.642       NA       NA\n    q                -0.312    0.656   -0.476    0.634       NA       NA\n\nR-Square:\n                   Estimate\n    Verbal_T6         0.710\n    Verbal_T7         0.703\n    Verbal_T9         0.821\n    Verbal_T11        0.740\n\n\nÈ anche possibile modellare una crescita non lineare in lavaan senza alcuna ipotesi sulla forma. Per farlo, si fissano i loadings della prima e dell’ultima misurazione, ma si stimano liberamente quelli intermedi.\n\n# Create non-linear growth model\nbasis_growth_model &lt;- 'i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11\n                       s =~ 0*Verbal_T6 + Verbal_T7 + Verbal_T9 + 1*Verbal_T11'\n# Fit model\nfit_basis_growth_model &lt;- growth(basis_growth_model, data=wisc_verbal,missing='fiml')\n# Output results\nsummary(fit_basis_growth_model, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n\nlavaan 0.6-19 ended normally after 109 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        11\n\n  Number of observations                           204\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                                      \n  Test statistic                                 5.893\n  Degrees of freedom                                 3\n  P-value (Chi-square)                           0.117\n\nModel Test Baseline Model:\n\n  Test statistic                               585.906\n  Degrees of freedom                                 6\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.995\n  Tucker-Lewis Index (TLI)                       0.990\n                                                      \n  Robust Comparative Fit Index (CFI)             0.995\n  Robust Tucker-Lewis Index (TLI)                0.990\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2482.763\n  Loglikelihood unrestricted model (H1)      -2479.816\n                                                      \n  Akaike (AIC)                                4987.525\n  Bayesian (BIC)                              5024.024\n  Sample-size adjusted Bayesian (SABIC)       4989.173\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.069\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.151\n  P-value H_0: RMSEA &lt;= 0.050                    0.275\n  P-value H_0: RMSEA &gt;= 0.080                    0.491\n                                                      \n  Robust RMSEA                                   0.069\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.151\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.275\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.491\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.043\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i =~                                                                  \n    Verbal_T6         1.000                               4.610    0.825\n    Verbal_T7         1.000                               4.610    0.731\n    Verbal_T9         1.000                               4.610    0.620\n    Verbal_T11        1.000                               4.610    0.446\n  s =~                                                                  \n    Verbal_T6         0.000                               0.000    0.000\n    Verbal_T7         0.237    0.012   20.223    0.000    1.300    0.206\n    Verbal_T9         0.536    0.012   43.295    0.000    2.937    0.395\n    Verbal_T11        1.000                               5.484    0.531\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i ~~                                                                  \n    s                15.072    3.133    4.811    0.000    0.596    0.596\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    i                19.634    0.391   50.221    0.000    4.259    4.259\n    s                24.180    0.565   42.773    0.000    4.409    4.409\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Verbal_T6         9.974    1.645    6.063    0.000    9.974    0.319\n   .Verbal_T7         9.704    1.307    7.422    0.000    9.704    0.244\n   .Verbal_T9         9.217    1.513    6.093    0.000    9.217    0.167\n   .Verbal_T11       25.340    4.317    5.870    0.000   25.340    0.237\n    i                21.255    2.898    7.335    0.000    1.000    1.000\n    s                30.076    6.788    4.430    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    Verbal_T6         0.681\n    Verbal_T7         0.756\n    Verbal_T9         0.833\n    Verbal_T11        0.763\n\n\n\n# Compare model fit\nanova(fit_linear_growth_model, fit_quad_growth_model)\n\n\nChi-Squared Difference Test\n\n                        Df    AIC    BIC    Chisq Chisq diff   RMSEA Df diff\nfit_quad_growth_model    1 4991.8 5034.9   6.1758                           \nfit_linear_growth_model  5 5078.4 5108.3 100.7562      94.58 0.33317       4\n                        Pr(&gt;Chisq)    \nfit_quad_growth_model                 \nfit_linear_growth_model  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIl modello non lineare in lavaan senza alcuna ipotesi sulla forma e il modello quadratico non sono annidati. Pertanto un test del rapporto di verosimiglianza non è possibile. Tuttavia, gli indici di bontà di adattamento del modello senza ipotesi sulla forma sono migliori del modello quadratico, per cui sarà quello il modello prescelto.",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>79</span>  <span class='chapter-title'>Lo Sviluppo dell'Intelligenza</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/11_lgm_wais.html#predizioni",
    "href": "chapters/lgm/11_lgm_wais.html#predizioni",
    "title": "\n79  Lo Sviluppo dell’Intelligenza\n",
    "section": "\n79.5 Predizioni",
    "text": "79.5 Predizioni\nSi potrebbe essere interessati a ciò che predice i punteggi di base e/o il cambiamento. Per valutare questo, si possono aggiungere predittori nel modello di crescita. Un’ipotesi potrebbe essere che il livello di istruzione della madre predica lo sviluppo della comprensione verbale.\n\n# Specify model\nbasis_growth_model_cov &lt;- ' \n  i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11\n  s =~ 0*Verbal_T6 + Verbal_T7 + Verbal_T9 + 1*Verbal_T11 \n  s~mo_edu\n  i~mo_edu\n  '\n\n\n# Fit model\nfit_basis_growth_model_cov &lt;- growth(basis_growth_model_cov, data=wisc,missing='fiml')\n# Output results\nsummary(fit_basis_growth_model_cov, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n\nlavaan 0.6-19 ended normally after 118 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n\n  Number of observations                           204\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                                      \n  Test statistic                                 6.498\n  Degrees of freedom                                 5\n  P-value (Chi-square)                           0.261\n\nModel Test Baseline Model:\n\n  Test statistic                               650.266\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.998\n  Tucker-Lewis Index (TLI)                       0.995\n                                                      \n  Robust Comparative Fit Index (CFI)             0.998\n  Robust Tucker-Lewis Index (TLI)                0.995\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2450.885\n  Loglikelihood unrestricted model (H1)      -2447.636\n                                                      \n  Akaike (AIC)                                4927.770\n  Bayesian (BIC)                              4970.906\n  Sample-size adjusted Bayesian (SABIC)       4929.718\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.038\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.110\n  P-value H_0: RMSEA &lt;= 0.050                    0.520\n  P-value H_0: RMSEA &gt;= 0.080                    0.210\n                                                      \n  Robust RMSEA                                   0.038\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.110\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.520\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.210\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.038\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i =~                                                                  \n    Verbal_T6         1.000                               4.615    0.826\n    Verbal_T7         1.000                               4.615    0.732\n    Verbal_T9         1.000                               4.615    0.619\n    Verbal_T11        1.000                               4.615    0.447\n  s =~                                                                  \n    Verbal_T6         0.000                               0.000    0.000\n    Verbal_T7         0.237    0.012   20.312    0.000    1.306    0.207\n    Verbal_T9         0.535    0.012   43.171    0.000    2.949    0.396\n    Verbal_T11        1.000                               5.508    0.534\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  s ~                                                                   \n    mo_edu           -1.724    0.414   -4.165    0.000   -0.313   -0.394\n  i ~                                                                   \n    mo_edu           -1.943    0.259   -7.503    0.000   -0.421   -0.531\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .i ~~                                                                  \n   .s                 9.676    2.721    3.556    0.000    0.489    0.489\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .i                26.302    0.958   27.446    0.000    5.700    5.700\n   .s                30.098    1.527   19.710    0.000    5.464    5.464\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Verbal_T6         9.923    1.622    6.117    0.000    9.923    0.318\n   .Verbal_T7         9.607    1.281    7.500    0.000    9.607    0.242\n   .Verbal_T9         9.443    1.501    6.291    0.000    9.443    0.170\n   .Verbal_T11       24.956    4.288    5.820    0.000   24.956    0.234\n   .i                15.298    2.309    6.624    0.000    0.718    0.718\n   .s                25.619    6.352    4.033    0.000    0.844    0.844\n\nR-Square:\n                   Estimate\n    Verbal_T6         0.682\n    Verbal_T7         0.758\n    Verbal_T9         0.830\n    Verbal_T11        0.766\n    i                 0.282\n    s                 0.156\n\n\nI risultati indicano come il livello di educazione della madre influenza sia il valore di base delle abilità verbali del bambino, sia il tasso di crescita.\nAggiungiamo ora la velocità di elaborazione a 11 anni come esito dei cambiamenti nella comprensione verbale. In altre parole, verifichiamo se le pendenze del cambiamento verbale predicono il livello di velocità di elaborazione a 11.\n\n# Specify model\nbasis_growth_model_covO &lt;- ' \n  i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11\n  s =~ 0*Verbal_T6 + Verbal_T7 + Verbal_T9 + 1*Verbal_T11 \n  Pspeed_T11~s\n  Pspeed_T11~1\n'\n\n# Fit model\nfit_basis_growth_model_covO &lt;- growth(basis_growth_model_covO, data=wisc,missing='fiml')\n# Output results\nsummary(fit_basis_growth_model_covO, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n\nlavaan 0.6-19 ended normally after 142 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        14\n\n  Number of observations                           204\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                                      \n  Test statistic                                14.016\n  Degrees of freedom                                 6\n  P-value (Chi-square)                           0.029\n\nModel Test Baseline Model:\n\n  Test statistic                               685.769\n  Degrees of freedom                                10\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.988\n  Tucker-Lewis Index (TLI)                       0.980\n                                                      \n  Robust Comparative Fit Index (CFI)             0.988\n  Robust Tucker-Lewis Index (TLI)                0.980\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3240.780\n  Loglikelihood unrestricted model (H1)      -3233.772\n                                                      \n  Akaike (AIC)                                6509.560\n  Bayesian (BIC)                              6556.014\n  Sample-size adjusted Bayesian (SABIC)       6511.657\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.081\n  90 Percent confidence interval - lower         0.024\n  90 Percent confidence interval - upper         0.137\n  P-value H_0: RMSEA &lt;= 0.050                    0.151\n  P-value H_0: RMSEA &gt;= 0.080                    0.566\n                                                      \n  Robust RMSEA                                   0.081\n  90 Percent confidence interval - lower         0.024\n  90 Percent confidence interval - upper         0.137\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.151\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.566\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.043\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i =~                                                                  \n    Verbal_T6         1.000                               4.476    0.804\n    Verbal_T7         1.000                               4.476    0.714\n    Verbal_T9         1.000                               4.476    0.598\n    Verbal_T11        1.000                               4.476    0.434\n  s =~                                                                  \n    Verbal_T6         0.000                               0.000    0.000\n    Verbal_T7         0.237    0.012   19.886    0.000    1.220    0.195\n    Verbal_T9         0.534    0.013   42.125    0.000    2.752    0.367\n    Verbal_T11        1.000                               5.157    0.500\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Pspeed_T11 ~                                                          \n    s                 1.683    0.219    7.690    0.000    8.680    0.697\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i ~~                                                                  \n    s                17.195    2.513    6.842    0.000    0.745    0.745\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Pspeed_T11       10.214    5.368    1.903    0.057   10.214    0.820\n    i                19.648    0.390   50.431    0.000    4.389    4.389\n    s                24.194    0.554   43.672    0.000    4.691    4.691\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Verbal_T6        10.997    1.451    7.578    0.000   10.997    0.354\n   .Verbal_T7         9.658    1.299    7.436    0.000    9.658    0.246\n   .Verbal_T9        10.154    1.529    6.640    0.000   10.154    0.181\n   .Verbal_T11       25.254    3.694    6.836    0.000   25.254    0.238\n   .Pspeed_T11       79.657   10.996    7.244    0.000   79.657    0.514\n    i                20.036    2.643    7.580    0.000    1.000    1.000\n    s                26.598    5.701    4.666    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    Verbal_T6         0.646\n    Verbal_T7         0.754\n    Verbal_T9         0.819\n    Verbal_T11        0.762\n    Pspeed_T11        0.486\n\n\nI dati mostrano come le pendenze del cambiamento verbale effettivamente predicono il livello di velocità di elaborazione a 11 anni.\nI predittori tempo-invarianti sono predittori delle differenze individuali nelle intercette e nelle pendenze. Sono spesso misurati al basale (ad esempio, reddito familiare) o sono caratteristiche specifiche della persona il cui valore è costante nel tempo (ad esempio, sesso biologico, paese di origine). Ad esempio, nelle analisi precedenti, il livello di istruzione della madre e la velocità di elaborazione a 6 anni sono predittori tempo-invarianti.\nI predittori tempo-varianti sono predittori dell’esito in ogni punto temporale. Nel nostro esempio, ad esempio, avremmo bisogno di misurazioni a T6, T7, T9 e T11.\nIn questo ultimo modello useremo la velocità di elaborazione come predittore tempo-variante della misurazione verbale in ogni punto temporale. Ci chiediamo le seguenti domande. Come sono l’intercetta e la pendenza delle misure verbali? La velocità di elaborazione predice le misure verbali allo stesso modo in tutti i punti temporali?\n\n# Specify model\nbasis_growth_model_tvp &lt;- ' \n  i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11\n  s =~ 0*Verbal_T6 + Verbal_T7 + Verbal_T9 + 1*Verbal_T11 \n  Verbal_T6~Pspeed_T6\n  Verbal_T7~Pspeed_T7\n  Verbal_T9~Pspeed_T9\n  Verbal_T11~Pspeed_T11\n  '\n# Fit LGM\nfit_basis_growth_model_tvp &lt;- growth(basis_growth_model_tvp, data=wisc,missing='fiml')\n# Output results\nsummary(fit_basis_growth_model_tvp, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n\nlavaan 0.6-19 ended normally after 96 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        15\n\n  Number of observations                           204\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                                      \n  Test statistic                                90.277\n  Degrees of freedom                                15\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               754.793\n  Degrees of freedom                                22\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.897\n  Tucker-Lewis Index (TLI)                       0.849\n                                                      \n  Robust Comparative Fit Index (CFI)             0.897\n  Robust Tucker-Lewis Index (TLI)                0.849\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2440.511\n  Loglikelihood unrestricted model (H1)      -2395.373\n                                                      \n  Akaike (AIC)                                4911.022\n  Bayesian (BIC)                              4960.794\n  Sample-size adjusted Bayesian (SABIC)       4913.269\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.157\n  90 Percent confidence interval - lower         0.127\n  90 Percent confidence interval - upper         0.189\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n                                                      \n  Robust RMSEA                                   0.157\n  90 Percent confidence interval - lower         0.127\n  90 Percent confidence interval - upper         0.189\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080             1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.194\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i =~                                                                  \n    Verbal_T6         1.000                               3.628    0.710\n    Verbal_T7         1.000                               3.628    0.627\n    Verbal_T9         1.000                               3.628    0.535\n    Verbal_T11        1.000                               3.628    0.386\n  s =~                                                                  \n    Verbal_T6         0.000                               0.000    0.000\n    Verbal_T7         0.297    0.071    4.209    0.000    1.337    0.231\n    Verbal_T9         0.703    0.108    6.531    0.000    3.161    0.466\n    Verbal_T11        1.000                               4.498    0.479\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Verbal_T6 ~                                                           \n    Pspeed_T6         0.243    0.038    6.467    0.000    0.243    0.397\n  Verbal_T7 ~                                                           \n    Pspeed_T7         0.230    0.034    6.853    0.000    0.230    0.397\n  Verbal_T9 ~                                                           \n    Pspeed_T9         0.220    0.036    6.130    0.000    0.220    0.332\n  Verbal_T11 ~                                                          \n    Pspeed_T11        0.319    0.039    8.233    0.000    0.319    0.423\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i ~~                                                                  \n    s                 5.873    2.723    2.157    0.031    0.360    0.360\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    i                15.271    0.754   20.245    0.000    4.209    4.209\n    s                12.341    1.994    6.190    0.000    2.744    2.744\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Verbal_T6         8.814    1.705    5.170    0.000    8.814    0.338\n   .Verbal_T7         9.806    1.311    7.478    0.000    9.806    0.292\n   .Verbal_T9         9.583    1.978    4.846    0.000    9.583    0.208\n   .Verbal_T11       27.351    4.282    6.387    0.000   27.351    0.310\n    i                13.165    2.347    5.609    0.000    1.000    1.000\n    s                20.235    6.197    3.265    0.001    1.000    1.000\n\nR-Square:\n                   Estimate\n    Verbal_T6         0.662\n    Verbal_T7         0.708\n    Verbal_T9         0.792\n    Verbal_T11        0.690",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>79</span>  <span class='chapter-title'>Lo Sviluppo dell'Intelligenza</span>"
    ]
  },
  {
    "objectID": "chapters/lgm/11_lgm_wais.html#interazione-tra-pendenza-e-intercetta",
    "href": "chapters/lgm/11_lgm_wais.html#interazione-tra-pendenza-e-intercetta",
    "title": "\n79  Lo Sviluppo dell’Intelligenza\n",
    "section": "\n79.6 Interazione tra pendenza e intercetta",
    "text": "79.6 Interazione tra pendenza e intercetta\nOra che sappiamo come stimare la traiettoria di una variabile, siamo in grado di stimare la traiettoria di due variabili e vedere come interagiscono.\nNell’analisi successiva, creiamo due modelli di crescita non lineari, uno per la comprensione verbale e uno per la velocità di elaborazione. Correliamo i cambiamenti delle due metriche e ci chiediamo se loro pendenze sono correlate.\n\n# Specify model\nbasis_growth_model_cor_ver_pro &lt;- ' \n  i_verbal =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11\n  s_verbal =~ 0*Verbal_T6 + Verbal_T7 + Verbal_T9 + 1*Verbal_T11 \n  i_processpeed =~ 1*Pspeed_T6 + 1*Pspeed_T7 + 1*Pspeed_T9 + 1*Pspeed_T11\n  s_processpeed =~ 0*Pspeed_T6 + Pspeed_T7 + Pspeed_T9 + 1*Pspeed_T11 \n  s_verbal ~~ s_processpeed\n'\n\n# Fit LGM\nfit_basis_growth_model_cor_ver_pro &lt;- growth(basis_growth_model_cor_ver_pro, data=wisc,missing='fiml')\n# Output results\nsummary(fit_basis_growth_model_cor_ver_pro, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n\nlavaan 0.6-19 ended normally after 211 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        26\n\n  Number of observations                           204\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                                      \n  Test statistic                                29.305\n  Degrees of freedom                                18\n  P-value (Chi-square)                           0.045\n\nModel Test Baseline Model:\n\n  Test statistic                              1423.083\n  Degrees of freedom                                28\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.992\n  Tucker-Lewis Index (TLI)                       0.987\n                                                      \n  Robust Comparative Fit Index (CFI)             0.992\n  Robust Tucker-Lewis Index (TLI)                0.987\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5124.285\n  Loglikelihood unrestricted model (H1)      -5109.632\n                                                      \n  Akaike (AIC)                               10300.570\n  Bayesian (BIC)                             10386.841\n  Sample-size adjusted Bayesian (SABIC)      10304.465\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.055\n  90 Percent confidence interval - lower         0.009\n  90 Percent confidence interval - upper         0.091\n  P-value H_0: RMSEA &lt;= 0.050                    0.367\n  P-value H_0: RMSEA &gt;= 0.080                    0.137\n                                                      \n  Robust RMSEA                                   0.055\n  90 Percent confidence interval - lower         0.009\n  90 Percent confidence interval - upper         0.091\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.367\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.137\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.048\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i_verbal =~                                                           \n    Verbal_T6         1.000                               4.637    0.832\n    Verbal_T7         1.000                               4.637    0.734\n    Verbal_T9         1.000                               4.637    0.617\n    Verbal_T11        1.000                               4.637    0.451\n  s_verbal =~                                                           \n    Verbal_T6         0.000                               0.000    0.000\n    Verbal_T7         0.237    0.012   20.495    0.000    1.349    0.213\n    Verbal_T9         0.533    0.012   43.283    0.000    3.037    0.404\n    Verbal_T11        1.000                               5.694    0.554\n  i_processpeed =~                                                      \n    Pspeed_T6         1.000                               7.604    0.902\n    Pspeed_T7         1.000                               7.604    0.799\n    Pspeed_T9         1.000                               7.604    0.713\n    Pspeed_T11        1.000                               7.604    0.617\n  s_processpeed =~                                                      \n    Pspeed_T6         0.000                               0.000    0.000\n    Pspeed_T7         0.298    0.011   26.220    0.000    1.841    0.194\n    Pspeed_T9         0.648    0.012   53.375    0.000    4.005    0.376\n    Pspeed_T11        1.000                               6.183    0.502\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  s_verbal ~~                                                           \n    s_processpeed    16.813    4.806    3.499    0.000    0.478    0.478\n  i_verbal ~~                                                           \n    s_verbal         14.606    3.112    4.694    0.000    0.553    0.553\n    i_processpeed    26.537    3.572    7.430    0.000    0.753    0.753\n    s_processpeed     2.520    3.154    0.799    0.424    0.088    0.088\n  s_verbal ~~                                                           \n    i_processpeed    25.796    4.875    5.291    0.000    0.596    0.596\n  i_processpeed ~~                                                      \n    s_processpeed    14.974    5.451    2.747    0.006    0.319    0.319\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    i_verbal         19.642    0.390   50.360    0.000    4.236    4.236\n    s_verbal         24.200    0.561   43.138    0.000    4.250    4.250\n    i_processpeed    17.949    0.590   30.419    0.000    2.360    2.360\n    s_processpeed    32.986    0.615   53.609    0.000    5.335    5.335\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Verbal_T6         9.574    1.581    6.055    0.000    9.574    0.308\n   .Verbal_T7         9.698    1.269    7.643    0.000    9.698    0.243\n   .Verbal_T9        10.149    1.491    6.806    0.000   10.149    0.180\n   .Verbal_T11       22.419    4.039    5.551    0.000   22.419    0.212\n   .Pspeed_T6        13.286    2.911    4.565    0.000   13.286    0.187\n   .Pspeed_T7        20.338    2.534    8.026    0.000   20.338    0.225\n   .Pspeed_T9        20.430    2.945    6.937    0.000   20.430    0.180\n   .Pspeed_T11       25.840    5.200    4.969    0.000   25.840    0.170\n    i_verbal         21.502    2.893    7.432    0.000    1.000    1.000\n    s_verbal         32.425    6.990    4.639    0.000    1.000    1.000\n    i_processpeed    57.821    6.889    8.393    0.000    1.000    1.000\n    s_processpeed    38.226    8.968    4.263    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    Verbal_T6         0.692\n    Verbal_T7         0.757\n    Verbal_T9         0.820\n    Verbal_T11        0.788\n    Pspeed_T6         0.813\n    Pspeed_T7         0.775\n    Pspeed_T9         0.820\n    Pspeed_T11        0.830\n\n\n\n# Specify model\nbasis_growth_model_pred_ver_pro &lt;- ' \n  i_verbal =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11\n  s_verbal =~ 0*Verbal_T6 + Verbal_T7 + Verbal_T9 + 1*Verbal_T11 \n  i_processpeed =~ 1*Pspeed_T6 + 1*Pspeed_T7 + 1*Pspeed_T9 + 1*Pspeed_T11\n  s_processpeed =~ 0*Pspeed_T6 + Pspeed_T7 + Pspeed_T9 + 1*Pspeed_T11 \n  s_verbal ~ i_processpeed\n  s_processpeed ~ i_verbal'\n\n# Fit LGM\nfit_basis_growth_model_pred_ver_pro &lt;- growth(basis_growth_model_pred_ver_pro, data=wisc,missing='fiml')\n# Output results\nsummary(fit_basis_growth_model_pred_ver_pro, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)\n\nlavaan 0.6-19 ended normally after 175 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        24\n\n  Number of observations                           204\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                                      \n  Test statistic                                38.158\n  Degrees of freedom                                20\n  P-value (Chi-square)                           0.008\n\nModel Test Baseline Model:\n\n  Test statistic                              1423.083\n  Degrees of freedom                                28\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.987\n  Tucker-Lewis Index (TLI)                       0.982\n                                                      \n  Robust Comparative Fit Index (CFI)             0.987\n  Robust Tucker-Lewis Index (TLI)                0.982\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5128.711\n  Loglikelihood unrestricted model (H1)      -5109.632\n                                                      \n  Akaike (AIC)                               10305.422\n  Bayesian (BIC)                             10385.057\n  Sample-size adjusted Bayesian (SABIC)      10309.018\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.067\n  90 Percent confidence interval - lower         0.033\n  90 Percent confidence interval - upper         0.099\n  P-value H_0: RMSEA &lt;= 0.050                    0.181\n  P-value H_0: RMSEA &gt;= 0.080                    0.268\n                                                      \n  Robust RMSEA                                   0.067\n  90 Percent confidence interval - lower         0.033\n  90 Percent confidence interval - upper         0.099\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.181\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.268\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.055\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i_verbal =~                                                           \n    Verbal_T6         1.000                               4.809    0.856\n    Verbal_T7         1.000                               4.809    0.760\n    Verbal_T9         1.000                               4.809    0.647\n    Verbal_T11        1.000                               4.809    0.477\n  s_verbal =~                                                           \n    Verbal_T6         0.000                               0.000    0.000\n    Verbal_T7         0.238    0.011   21.065    0.000    1.421    0.225\n    Verbal_T9         0.534    0.012   43.708    0.000    3.189    0.429\n    Verbal_T11        1.000                               5.977    0.593\n  i_processpeed =~                                                      \n    Pspeed_T6         1.000                               7.861    0.929\n    Pspeed_T7         1.000                               7.861    0.831\n    Pspeed_T9         1.000                               7.861    0.756\n    Pspeed_T11        1.000                               7.861    0.662\n  s_processpeed =~                                                      \n    Pspeed_T6         0.000                               0.000    0.000\n    Pspeed_T7         0.299    0.011   26.953    0.000    2.078    0.220\n    Pspeed_T9         0.648    0.012   53.898    0.000    4.495    0.432\n    Pspeed_T11        1.000                               6.940    0.585\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  s_verbal ~                                                            \n    i_processpeed     0.408    0.071    5.782    0.000    0.537    0.537\n  s_processpeed ~                                                       \n    i_verbal          0.143    0.142    1.013    0.311    0.099    0.099\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i_verbal ~~                                                           \n    i_processpeed    26.674    3.649    7.310    0.000    0.706    0.706\n .s_verbal ~~                                                           \n   .s_processpeed    10.954    5.052    2.168    0.030    0.315    0.315\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    i_verbal         19.630    0.393   49.926    0.000    4.082    4.082\n   .s_verbal         16.877    1.368   12.333    0.000    2.823    2.823\n    i_processpeed    17.944    0.592   30.305    0.000    2.283    2.283\n   .s_processpeed    30.167    2.843   10.612    0.000    4.347    4.347\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Verbal_T6         8.445    1.407    6.000    0.000    8.445    0.267\n   .Verbal_T7         9.715    1.287    7.547    0.000    9.715    0.243\n   .Verbal_T9        10.375    1.493    6.951    0.000   10.375    0.188\n   .Verbal_T11       21.001    3.922    5.355    0.000   21.001    0.207\n   .Pspeed_T6         9.777    2.479    3.943    0.000    9.777    0.137\n   .Pspeed_T7        21.117    2.640    8.000    0.000   21.117    0.236\n   .Pspeed_T9        21.159    3.017    7.014    0.000   21.159    0.196\n   .Pspeed_T11       23.241    5.106    4.552    0.000   23.241    0.165\n    i_verbal         23.129    2.847    8.125    0.000    1.000    1.000\n   .s_verbal         25.426    5.629    4.517    0.000    0.712    0.712\n    i_processpeed    61.801    6.889    8.971    0.000    1.000    1.000\n   .s_processpeed    47.685    8.194    5.820    0.000    0.990    0.990\n\nR-Square:\n                   Estimate\n    Verbal_T6         0.733\n    Verbal_T7         0.757\n    Verbal_T9         0.812\n    Verbal_T11        0.793\n    Pspeed_T6         0.863\n    Pspeed_T7         0.764\n    Pspeed_T9         0.804\n    Pspeed_T11        0.835\n    s_verbal          0.288\n    s_processpeed     0.010",
    "crumbs": [
      "LGM",
      "<span class='chapter-number'>79</span>  <span class='chapter-title'>Lo Sviluppo dell'Intelligenza</span>"
    ]
  },
  {
    "objectID": "99-references.html",
    "href": "99-references.html",
    "title": "Bibliografia",
    "section": "",
    "text": "Allen, M. J., & Yen, W. M. (2001). Introduction to measurement\ntheory. Waveland Press.\n\n\nAlphen, T. van, Jak, S., Jansen in de Wal, J., Schuitema, J., &\nPeetsma, T. (2022). Determining reliability of daily measures: An\nillustration with data on teacher stress. Applied Measurement in\nEducation, 35(1), 63–79.\n\n\nAmerican Educational Research Association, American Psychological\nAssociation, & National Council on Measurement in Education. (2014).\nStandards for educational and psychological testing. American\nEducational Research Association.\n\n\nArias, A. (2024). A short tutorial on validation in educational and\npsychological assessment. Teaching Quantitative Methods\nVignettes, 20(3).\n\n\nBandalos, D. L. (2018). Measurement theory and applications for the\nsocial sciences. Guilford Publications.\n\n\nBarrett, P. (2007). Structural equation modelling: Adjudging model fit.\nPersonality and Individual Differences, 42(5),\n815–824.\n\n\nBates, D., Mächler, M., Bolker, B., & Walker, S. (2014). Fitting\nlinear mixed-effects models using lme4. arXiv Preprint\narXiv:1406.5823.\n\n\nBlampied, N. M. (2022). Reliable change and the reliable change index:\nStill useful after all these years? The Cognitive Behaviour\nTherapist, 15, e50.\n\n\nBlume, F., Buhr, L., Kuehnhausen, J., Köpke, R., Weber, L. A.,\nFallgatter, A. J., Ethofer, T., & Gawrilow, C. (2020). Validation of\nthe self-report version of the german strengths and weaknesses of ADHD\nsymptoms and normal behavior scale (SWAN-DE-SB). Assessment,\n10731911241236699.\n\n\nBollen, K., & Lennox, R. (1991). Conventional wisdom on measurement:\nA structural equation perspective. Psychological Bulletin,\n110(2), 305–314.\n\n\nBonifay, W., Winter, S. D., Skoblow, H. F., & Watts, A. L. (2024).\nGood fit is weak evidence of replication: Increasing rigor through prior\npredictive similarity checking. Assessment, 10731911241234118.\n\n\nBrown, A. (2023). Psychometrics in exercises using r and RStudio:\nTextbook and data resource. https://bookdown.org/annabrown/psychometricsR\n\n\nBrown, T. A. (2015). Confirmatory factor analysis for applied\nresearch. Guilford publications.\n\n\nBuchberger, E. S., Ngo, C. T., Peikert, A., Brandmaier, A. M., &\nWerkle-Bergner, M. (2024). Estimating statistical power for structural\nequation models in developmental cognitive science: A tutorial in r:\nPower simulation for SEMs. Behavior Research Methods, 1–18.\n\n\nByrne, B. M. (2013). Structural equation modeling with mplus: Basic\nconcepts, applications, and programming. routledge.\n\n\nCharter, R. A. (1996). Revisiting the standard errors of measurement,\nestimate, and prediction and their application to test scores.\nPerceptual and Motor Skills, 82(3), 1139–1144.\n\n\nChen, D.-G., & Yung, Y.-F. (2023). Structural equation modeling\nusing r/SAS: A step-by-step approach with real data analysis. CRC\nPress.\n\n\nClement, L. M., & Bradley-Garcia, M. (2022). A step-by-step tutorial\nfor performing a moderated mediation analysis using PROCESS. The\nQuantitative Methods for Psychology, 18(3), 258–271.\n\n\nDebelak, R., Strobl, C., & Zeigenfuse, M. D. (2022). An\nintroduction to the rasch model with examples in r. CRC Press.\n\n\nDudek, F. J. (1979). The continuing misinterpretation of the standard\nerror of measurement. Psychological Bulletin, 86(2),\n335--337.\n\n\nGrimm, K. J., Ram, N., & Estabrook, R. (2016). Growth modeling:\nStructural equation and multilevel modeling approaches. Guilford\nPublications.\n\n\nHargrave, T. D., & Hammer, M. Y. (2016). Restoration of\nrelationships after affairs. In Techniques for the couple\ntherapist (pp. 190–193). Routledge.\n\n\nHaslbeck, J., & Bork, R. van. (2022). Estimating the number of\nfactors in exploratory factor analysis via out-of-sample prediction\nerrors. Psychological Methods.\n\n\nHayduk, L. A. (2014). Shame for disrespecting evidence: The personal\nconsequences of insufficient respect for structural equation model\ntesting. BMC Medical Research Methodology, 14, 1–10.\n\n\nHirsch, M. E., Thompson, A., Kim, Y., & Lansford, K. L. (2022). The\nreliability and validity of speech-language pathologists’ estimations of\nintelligibility in dysarthria. Brain Sciences, 12(8),\n1011.\n\n\nHu, L., & Bentler, P. M. (1998). Fit indices in covariance structure\nmodeling: Sensitivity to underparameterized model misspecification.\nPsychological Methods, 3(4), 424--453.\n\n\nJohn, O. P., & Benet-Martinez, V. (2014). Measurement: Reliability,\nconstruct validation, and scale construction. In H. T. Reis & C. M.\nJudd (Eds.), Handbook of research methods in social and personality\npsychology (2nd ed., pp. 473–503). Cambridge University Press.\n\n\nKan, K.-J., Maas, H. L. van der, & Levine, S. Z. (2019). Extending\npsychometric network analysis: Empirical evidence against g in favor of\nmutualism? Intelligence, 73, 52–62.\n\n\nKline, P. (2013). Handbook of psychological testing. Routledge.\n\n\nKline, R. B. (2023). Principles and practice of structural equation\nmodeling. Guilford publications.\n\n\nKomaroff, E. (1997). Effect of simultaneous violations of essential/g=\nt/-equivalence and uncorrelated error on coefficient/g= a. Applied\nPsychological Measurement, 21(4), 337–348.\n\n\nLittle, T. D. (2023). Longitudinal structural equation\nmodeling. Guilford Press.\n\n\nLord, F. M., & Novick, M. R. (1968). Statistical theories of\nmental test scores. Addison-Wesley.\n\n\nMarsh, H., & Alamer, A. (2024). When and how to use set-exploratory\nstructural equation modelling to test structural models: A tutorial\nusing the r package lavaan. British Journal of Mathematical and\nStatistical Psychology.\n\n\nMcDonald, R. P. (2013). Test theory: A unified treatment.\nPsychology Press.\n\n\nMcElreath, R. (2020). Statistical rethinking: A\nBayesian course with examples in R and\nStan (2nd Edition). CRC Press.\n\n\nNunnally, J. C. (1994). Psychometric theory. McGraw-Hill.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With\napplied examples in r. CRC Press.\n\n\nRandall, J. (2021). “Color-neutral” is not a thing:\nRedefining construct definition and representation through a\njustice-oriented critical antiracist lens. Educational Measurement:\nIssues and Practice, 40(4), 82–90.\n\n\nRandall, J., Slomp, D., Poe, M., & Oliveri, E. (2023). Disrupting\nwhite supremacy in assessment: Toward a justice-oriented, antiracist\nvalidity framework. In Twin pandemics (pp. 78–86). Routledge.\n\n\nRencher, A. (2002). Methods of multivariate analysis. 2002.\nWiley Publications.\n\n\nReynolds, C. R., & Livingston, R. (2021). Mastering modern\npsychological testing. Springer.\n\n\nRoberts, S., & Pashler, H. (2000). How persuasive is a good fit? A\ncomment on theory testing. Psychological Review,\n107(2), 358–367.\n\n\nRosseel, Y. (2020). Small sample solutions for structural equation\nmodeling. In Small sample size solutions: A guide for applied\nresearchers and practitioners (pp. 226–238). Routledge.\n\n\nSaqr, M., & López-Pernas, S. (2024). Learning analytics methods\nand tutorials: A practical guide using r. Springer Nature.\n\n\nSpearman, C. (1904). General intelligence objectively determined and\nmeasured. American Journal of Psychology, 15, 201–293.\n\n\nSvetina, D., Rutkowski, L., & Rutkowski, D. (2020). Multiple-group\ninvariance with categorical outcomes using updated guidelines: An\nillustration using m plus and the lavaan/semtools packages.\nStructural Equation Modeling: A Multidisciplinary Journal,\n27(1), 111–130.\n\n\nWaller, N. G., & Meehl, P. E. (2002). Risky tests, verisimilitude,\nand path analysis. Psychological Methods, 7(3),\n323–337. https://doi.org/10.1037/1082-989X.7.3.323\n\n\nWechsler, D. (2008). Wechsler adult intelligence scale–fourth edition\n(WAIS–IV). San Antonio, TX: NCS Pearson, 22(498),\n816–827.\n\n\nWind, S. A. (2017). An instructional module on mokken scale analysis.\nEducational Measurement: Issues and Practice, 36(2),\n50–66.\n\n\nWu, H., & Estabrook, R. (2016). Identification of confirmatory\nfactor analysis models of different levels of invariance for ordered\ncategorical outcomes. Psychometrika, 81(4), 1014–1045.\n\n\nXia, Y., & Yang, Y. (2019). RMSEA, CFI, and TLI in structural\nequation modeling with ordered categorical data: The story they tell\ndepends on the estimation methods. Behavior Research Methods,\n51(1), 409–428.",
    "crumbs": [
      "Bibliografia"
    ]
  }
]