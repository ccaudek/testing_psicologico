# Analisi delle componenti principali {#sec-pca}

::: callout-note
## In questo capitolo imparerai a

- eseguire la PCA usando l'algebra lineare;
- eseguire la PCA usando R.
:::

::: callout-tip
## Prerequisiti

- Leggere il capitolo *Factor Analysis and Principal Component Analysis* del testo di @petersen2024principles. 
:::

::: callout-important
## Preparazione del Notebook

```{r}
here::here("code", "_common.R") |> 
  source()

# Load packages
if (!requireNamespace("pacman")) install.packages("pacman")
pacman::p_load(readr)
```
:::


## Introduzione

L'**Analisi delle Componenti Principali (PCA)** √® una tecnica statistica utilizzata per **ridurre la dimensionalit√† di un insieme di dati**. Il suo obiettivo √® quello di semplificare analisi complesse **conservando quanta pi√π informazione possibile**, cio√® spiegando la maggior parte della **varianza** presente nei dati originali.

La PCA trasforma un insieme di variabili iniziali, spesso **correlate tra loro**, in un nuovo insieme di variabili **non correlate** dette **componenti principali**. Queste componenti sono **combinazioni lineari** delle variabili originali e sono ordinate in base alla quantit√† di varianza che riescono a spiegare.

## Perch√© Usare la PCA in Psicologia?

In psicologia, ci troviamo spesso a lavorare con dati ad alta dimensionalit√†: questionari con decine di item, batterie di test, o set di dati raccolti tramite studi longitudinali. La PCA √® utile perch√©:

- **semplifica l‚Äôinterpretazione dei dati**, riducendo molte variabili a poche dimensioni latenti;
- **elimina ridondanze**: se due o pi√π variabili sono altamente correlate, la PCA pu√≤ rappresentarle con un'unica componente;
- **favorisce la visualizzazione**, soprattutto nei casi in cui si riescano a ridurre i dati a 2 o 3 componenti principali;
- **prepara i dati per analisi successive** come regressioni o modelli strutturali, evitando collinearit√† tra predittori.

> üß© *Esempio tipico*: un questionario su tratti di personalit√† con 50 item pu√≤ essere ridotto a 5 componenti principali che riflettono le dimensioni dei Big Five.

## Cos'√® la Varianza Totale?

La **varianza totale** rappresenta la somma della variabilit√† presente in ciascuna variabile del dataset.

Se abbiamo tre variabili, la varianza totale sar√†:

$$
\text{Varianza Totale} = \sigma_1^2 + \sigma_2^2 + \sigma_3^2 ,
$$

dove $\sigma_i^2$ √® la varianza della variabile $i$-esima.

Nella PCA:

- ogni **autovalore** (o *eigenvalue*) rappresenta la quantit√† di varianza spiegata da una componente principale;
- la **somma degli autovalori** √® pari alla varianza totale del dataset (dopo la standardizzazione, essa sar√† uguale al numero di variabili).


## Un Nuovo Sistema di Coordinate

La PCA pu√≤ essere intesa come una **rotazione del sistema di riferimento** nello spazio delle variabili:

1. la **prima componente principale (PC1)** √® la direzione lungo cui la varianza dei dati √® massima;
2. la **seconda componente (PC2)** √® perpendicolare alla prima (cio√® ortogonale) e spiega la massima varianza residua;
3. le successive componenti seguono lo stesso principio.

Questo nuovo sistema √® costruito in modo tale che le componenti siano **non correlate tra loro** (ortogonali) e spiegano, progressivamente, meno varianza.

> üí° *Nota importante*: la PCA non elimina variabili, ma le **riorganizza** in modo da ridurre la complessit√† informativa.

## Riduzione della Dimensionalit√†

La PCA consente di rappresentare i dati originali in un nuovo spazio, pi√π compatto, ma informativamente ricco.

### Cosa significa "ridurre la dimensionalit√†"?

- **Riduzione**: da *p* variabili iniziali (es. 30 item), possiamo ottenere *k* componenti principali (es. 3 o 5), dove *k < p*;
- **Obiettivo**: mantenere una soglia prefissata di varianza spiegata, ad esempio il 70% o l‚Äô80%.

> üéØ *Esempio concreto*: In uno studio sui Big Five, la PCA pu√≤ ridurre un set di 100 item a 5 componenti, ciascuna interpretabile come una delle dimensioni di personalit√†.


## Interpretazione dei Risultati della PCA

La PCA produce due insiemi principali di risultati:

1. **Punteggi delle componenti principali (scores)**  
Per ogni partecipante (o unit√† osservata), si ottiene un punteggio per ciascuna componente. Questi punteggi possono essere utilizzati come **nuove variabili sintetiche**.

2. **Varianza spiegata (eigenvalues)**  
L‚Äôimportanza di ciascuna componente √® misurata dalla proporzione di varianza che essa spiega:

$$
\text{Proporzione di varianza spiegata dalla PC}_i = \frac{\lambda_i}{\sum_{j=1}^{p} \lambda_j} ,
$$

  dove $\lambda_i$ √® l'autovalore della componente *i*.

> üîç *Interpretazione tipica*: Se la PC1 spiega il 50% della varianza e la PC2 un ulteriore 30%, possiamo dire che **le prime due componenti spiegano l'80% della variabilit√† totale**.


## Geometria della PCA: Assi, Autovalori e Autovettori

La PCA si basa su concetti fondamentali dell‚Äôalgebra lineare:

- **autovalori (eigenvalues)**: quantificano la varianza spiegata da ogni componente;
- **autovettori (eigenvectors)**: definiscono le direzioni lungo cui si osserva la varianza massima nei dati.

Nel piano bidimensionale:

- ogni autovettore √® un **asse di una nuova base ortogonale**;
- la **lunghezza dell‚Äôasse** √® proporzionale alla **radice quadrata dell‚Äôautovalore** corrispondente;
- le osservazioni vengono **proiettate ortogonalmente** su questi assi per ottenere i punteggi delle componenti principali.


## Visualizzazione: Scree Plot e Biplot

### Scree Plot

Lo **Scree Plot** √® un grafico che mostra gli autovalori ordinati per componente:

- permette di **determinare quante componenti mantenere**;
- un "gomito" nel grafico indica il punto oltre il quale le componenti aggiuntive spiegano poca varianza.

### Biplot

Il **biplot** mostra simultaneamente:

- **i punteggi delle osservazioni** (punti);
- **i contributi delle variabili originali** (frecce).

> üéì *Guida all'interpretazione del biplot*:  
> - **Correlazione positiva**: variabili con frecce vicine tra loro (angolo piccolo tra i vettori);  
> - **Correlazione negativa**: variabili con frecce dirette in versi opposti (angolo ‚âà 180¬∞);  
> - **Nessuna correlazione**: variabili con frecce quasi perpendicolari (angolo ‚âà 90¬∞).  
> 
> *Ogni angolo riflette l'intensit√† della relazione tra le variabili analizzate.*


## Tutorial in R 

### Obiettivo del tutorial

Applicare la PCA passo dopo passo su un dataset simulato con due variabili correlate, comprendendo ogni fase del processo:

- creazione dei dati e visualizzazione;
- standardizzazione;
- calcolo della PCA con algebra lineare;
- interpretazione geometrica (autovalori, autovettori);
- visualizzazioni: ellissi, componenti, scree plot;
- proiezione dei dati nello spazio delle componenti;
- verifica con `prcomp()`;
- costruzione del biplot.

### Passo 1: Creare un dataset

Generiamo un dataset con due variabili correlate.

```{r}
# Generiamo due variabili correlate
set.seed(123)
X <- data.frame(
  x1 = rnorm(100, mean = 5, sd = 2),
  x2 = rnorm(100, mean = 10, sd = 3)
)

# Aggiungiamo una correlazione lineare tra x1 e x2
X$x2 <- 0.8 * X$x1 + rnorm(100, mean = 0, sd = 1)
```

### Passo 2: Standardizzare i dati

La standardizzazione √® essenziale nella PCA quando le variabili hanno scale diverse. Dopo la standardizzazione, ogni variabile ha media = 0 e deviazione standard = 1.

```{r}
# Standardizziamo le variabili
X_scaled <- scale(X)
```

Visualizziamo i dati standardizzati:

```{r}
#| fig.asp: 1
#| fig.height: 6
#| fig.width: 6
#| 
ggplot(X_scaled, aes(x = x1, y = x2)) +
  geom_point(shape = 19) +
  coord_fixed(ratio = 1) + # Imposta l'aspect ratio a 1
  labs(
    x = "x1 standardizzata",
    y = "x2 standardizzata",
    title = "Dati standardizzati (asp = 1)"
  )
```

Aggiungiamo un‚Äôellisse di confidenza per mostrare la distribuzione:

```{r}
#| fig-asp: 1
#| fig.height: 6
#| fig.width: 6
#| 
car::dataEllipse(
  X_scaled[, 1], X_scaled[, 2],
  levels = 0.95, lty = 2,
  asp = 1,
  xlab = "x1", ylab = "x2"
)
```

### Passo 3: Calcolare la matrice di covarianza

```{r}
cov_matrix <- cov(X_scaled)
cov_matrix
```

### Passo 4: Calcolare autovalori e autovettori

```{r}
eigen_decomp <- eigen(cov_matrix)
eigenvalues <- eigen_decomp$values       # Varianza spiegata (autovalori)
eigenvectors <- eigen_decomp$vectors     # Direzioni principali (autovettori)
```

Stampiamo i risultati:

```{r}
eigenvalues
```

```{r}
eigenvectors
```

Verifichiamo che gli autovettori siano ortogonali (prodotto scalare = 0):

```{r}
t(eigenvectors[,1]) %*% eigenvectors[,2]
```

### Visualizzare le direzioni principali

Calcoliamo le pendenze degli autovettori:

```{r}
ev1_slope <- eigenvectors[2, 1] / eigenvectors[1, 1]
ev2_slope <- eigenvectors[2, 2] / eigenvectors[1, 2]
```

Visualizziamo il grafico con gli autovettori sovrapposti:

```{r}
#| fig-asp: 1
#| fig.height: 6
#| fig.width: 6
#| 
data.frame(zx = X_scaled[, 1], zy = X_scaled[, 2]) |>
  ggplot(aes(x = zx, y = zy)) +
  geom_point(size = 2) +
  geom_vline(xintercept = 0, linewidth = .5) +
  geom_hline(yintercept = 0, linewidth = .5) +
  geom_abline(slope = ev1_slope, color = "red", linewidth = 1.2) +
  geom_abline(slope = ev2_slope, color = "blue", linewidth = 1.2) +
  ggtitle("Autovettori: direzioni delle componenti principali") 
```

### Scree Plot ‚Äì Percentuale di varianza spiegata

```{r}
#| fig-asp: 1
#| fig.height: 6
#| fig.width: 6
#| 
# Percentuale di varianza spiegata
var_per <- tibble(
  PC = c("PC1", "PC2"),
  Percent = eigenvalues / sum(eigenvalues) * 100
)

ggplot(var_per, aes(x = PC, y = Percent)) +
  geom_col(fill = "skyblue", color = "black", width = 0.5) +
  ylab("Varianza spiegata (%)") +
  xlab("Componente Principale") +
  ggtitle("Scree Plot") 
```

Verifica: somma degli autovalori ‚âà somma delle varianze

```{r}
sum(eigenvalues)
sum(apply(X_scaled, 2, var))  # Varianza totale
```

### Passo 5: Visualizzazione geometrica (ellisse + assi principali)

```{r}
#| fig-asp: 1
#| fig.height: 6
#| fig.width: 6
#| 
car::dataEllipse(
  X_scaled[, 1], X_scaled[, 2],
  levels = 0.95, lty = 2,
  xlim = c(-3, 3), ylim = c(-3, 3),
  asp = 1,
  xlab = "x1", ylab = "x2"
)

# Disegniamo gli assi in base agli autovettori
k <- 2.5
arrows(0, 0,
       k * sqrt(eigenvalues[1]) * eigenvectors[1, 1],
       k * sqrt(eigenvalues[1]) * eigenvectors[2, 1],
       col = "red", lwd = 2, code = 2)

arrows(0, 0,
       k * sqrt(eigenvalues[2]) * eigenvectors[1, 2],
       k * sqrt(eigenvalues[2]) * eigenvectors[2, 2],
       col = "red", lwd = 2, code = 2)
```

### Passo 6: Proiezione dei dati (calcolo dei punteggi)

```{r}
pc_scores <- as.matrix(X_scaled) %*% eigenvectors
colnames(pc_scores) <- c("PC1", "PC2")

head(pc_scores)  # Mostra le prime osservazioni nel nuovo spazio
```

Visualizziamo le osservazioni nello spazio delle componenti:

```{r}
pc_df <- as.data.frame(pc_scores)

# Grafico ggplot dei punteggi delle componenti principali
ggplot(pc_df, aes(x = PC1, y = PC2)) +
  geom_point(color = "blue", size = 2) +
  labs(
    title = "Punteggi delle Componenti Principali",
    x = "PC1",
    y = "PC2"
  ) +
  coord_fixed()
```

### Passo 7: Confronto con `prcomp`

```{r}
# Calcolo automatico della PCA
pca_auto <- prcomp(X, scale. = TRUE)
summary(pca_auto)
```

Verifica: confronta punteggi calcolati a mano con quelli di `prcomp`

```{r}
head(pca_auto$x)  # Punteggi calcolati da prcomp
```

```{r}
head(pc_scores)   # Punteggi calcolati a mano
```

### Passo 8: Biplot

Il **biplot** permette di visualizzare contemporaneamente:

- le **osservazioni** (rappresentate da numeri), proiettate nello spazio delle componenti principali;
- le **variabili originali** (`x1`, `x2`), rappresentate come **frecce rosse**, che indicano come ciascuna variabile contribuisce alla definizione delle componenti.

```{r}
#| fig-asp: 1
biplot(pca_auto, scale = 0,
       main = "Biplot delle Componenti Principali",
       xlab = "PC1", ylab = "PC2")
```

**Distribuzione delle osservazioni**

- I numeri neri rappresentano le 100 osservazioni.
- Le osservazioni si distribuiscono principalmente **lungo la direzione della prima componente principale (PC1)**, che si estende orizzontalmente.
- C‚Äô√® **relativamente poca variabilit√† lungo la seconda componente (PC2)**, che √® verticale. Questo conferma che **quasi tutta la varianza** √® catturata da PC1, come osservato nei passaggi precedenti (Scree Plot, autovalori).

**Frecce delle variabili originali**

Nel biplot vediamo due frecce:

- la freccia **x1** punta verso l‚Äôalto a destra;
- la freccia **x2** punta verso il basso a destra;
- entrambe le frecce sono **allineate in parte con l'asse orizzontale (PC1)**, ma puntano in **direzioni opposte lungo PC2**.

Interpretazione geometrica:

- **Entrambe le variabili contribuiscono positivamente a PC1**, perch√© le componenti orizzontali delle frecce sono entrambe > 0.
- La componente **verticale di x1 √® positiva**, quella di **x2 √® negativa**, quindi le due variabili sono **positivamente correlate in generale**, ma **divergono leggermente lungo PC2**.

Questo pattern √® coerente con i coefficienti degli autovettori:

```r
pca_auto$rotation
```

Es.:

```
           PC1      PC2
x1       0.71     0.71
x2       0.71    -0.71
```

Cosa significa?

- La **PC1** √® la somma bilanciata di `x1` e `x2`, e rappresenta la **dimensione comune** tra le due variabili.
- La **PC2** √® la loro differenza: rappresenta una direzione lungo cui `x1` e `x2` si muovono in modo opposto. Ma in questo caso, la varianza lungo PC2 √® molto piccola ‚Üí **questo secondo asse √® poco informativo**.

Lunghezza delle frecce:

- le frecce hanno **lunghezza simile** ‚Üí entrambe le variabili sono **ben rappresentate** dallo spazio PC1‚ÄìPC2;
- in un biplot, la lunghezza di una freccia indica **quanto bene quella variabile √® spiegata dalle componenti principali**.


**Conclusioni sull'interpretazione del biplot.**

- Il biplot mostra che la struttura dei dati pu√≤ essere **riassunta da una sola dimensione latente (PC1)**.
- Le due variabili `x1` e `x2` sono **entrambe fortemente associate a PC1**, e **debolmente differenziate** da PC2.


## Riflessioni Conclusive

Abbiamo completato un'analisi PCA manuale e automatica, verificando ogni passaggio attraverso:

- algebra lineare (autovalori/autovettori);
- visualizzazione geometrica (ellisse, autovettori);
- scree plot;
- proiezione dei dati;
- biplot.

Questa struttura permette di **collegare i concetti teorici alla loro implementazione pratica**, fornendo agli studenti una comprensione pi√π profonda della PCA.


## ‚úçÔ∏è Suggerimenti per gli Studenti

- Quando standardizzare? **Sempre**, se le variabili sono su scale diverse (es. punteggi da 0 a 10, da 1 a 100);
- Quando usare la PCA? Quando **l‚Äôobiettivo √® descrittivo**, esplorare la struttura latente o semplificare l‚Äôanalisi;
- Quando non usarla? Se le variabili **non sono correlate**: la PCA non sar√† utile;
- Come interpretare le componenti? Serve **analizzare i coefficienti** degli autovettori per capire il contributo delle variabili originali a ciascuna componente.


## Informazioni sull'Ambiente di Sviluppo {.unnumbered} 

```{r}
sessionInfo()
```

