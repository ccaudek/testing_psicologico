# Applicazioni della CTT {#sec-ctt-applications}

**Preparazione del Notebook**

```{r}
here::here("code", "_common.R") |> source()
pacman::p_load(
  lavaan, modelsummary, foreach, ggridges, truncnorm, 
  cmdstanr, doParallel
)
```

## Introduzione

In questo capitolo vengono esplorate alcune tra le più importanti applicazioni della CTT, con particolare attenzione alle implicazioni operative che derivano dalla sua struttura teorica di base. Il percorso che seguiremo tocca diversi aspetti fondamentali:

1. **Affidabilità e lunghezza del test.** Verrà mostrato come stimare il numero di item necessari per ottenere un certo livello di affidabilità, grazie all’impiego di formule specifiche (ad esempio, l’adattamento della formula di Spearman-Brown). Questo consente di progettare strumenti di misura calibrati sulle esigenze di precisione richieste.

2. **Attenuazione della correlazione.** Illustreremo il fenomeno per cui l’errore di misurazione “nasconde” o riduce la vera correlazione tra due variabili, analizzando i metodi proposti per “disattenuare” tale correlazione (ad esempio, la formula di correzione in funzione dell’affidabilità).

3. **Stima dei punteggi veri a livello individuale.** Saranno presentate le procedure per migliorare l’accuratezza dei punteggi osservati, in particolare mediante la regressione di Kelley, che permette di avvicinare i punteggi individuali alla media di gruppo in proporzione all’affidabilità del test. 

4. **Modelli gerarchici bayesiani.** Concluderemo evidenziando come l’approccio classico possa essere esteso o rivisto in un’ottica pienamente probabilistica attraverso modelli gerarchici bayesiani, che offrono una naturale interpretazione “shrinkante” degli stessi concetti, senza necessitare di un coefficiente di affidabilità calcolato a priori.

Nel complesso, lo scopo di questo capitolo è fornire una visione articolata su come la CTT, con i suoi strumenti e le sue formule, possa guidare non solo la costruzione e la valutazione di un test, ma anche l’interpretazione dei dati ottenuti. Al tempo stesso, il richiamo all’approccio bayesiano evidenzia come molte procedure classiche trovino un corrispettivo in modelli moderni, sottolineando la continuità tra i due paradigmi.

## Stimare un Cambiamento Clinicamente Significativo

Un tema fondamentale in psicologia clinica è stabilire se un individuo abbia effettivamente manifestato un *cambiamento clinicamente significativo* nel tempo. I metodi per valutarlo rientrano per lo più in due categorie: i metodi “basati su ancoraggi” e quelli “basati sulla distribuzione” [@blampied2022reliable].

- **Metodi basati su ancoraggi.** Stabiliscono che un cambiamento clinicamente significativo debba corrispondere a un evento o a un segnale “clinicamente rilevante” esterno allo strumento di misura. In altre parole, invece di concentrarsi solo su quanti punti sono cambiati in una scala, questi metodi pongono l’accento sulla **qualità** di quel cambiamento: la variazione di punteggio deve essere associata a un miglioramento (o peggioramento) tangibile nello stato clinico del paziente, tale da essere riconosciuto come rilevante dal punto di vista clinico o terapeutico.

    **Esempio pratico.**  
    Supponiamo di valutare l’efficacia di una psicoterapia per la depressione. Un ricercatore vuole stabilire se i pazienti hanno ottenuto un “cambiamento clinicamente significativo” nel loro stato depressivo, misurato con un questionario standard (ad es. un punteggio su una scala da 0 a 60).
  
    1. **Definizione dell’ancoraggio clinico**  
       Si sceglie come *ancora* il fatto che un paziente non soddisfi più i criteri clinici per la diagnosi di depressione secondo un colloquio diagnostico strutturato (es. MINI, SCID). Questo “evento” costituisce un passaggio riconosciuto come clinicamente rilevante.
    
    2. **Collegamento fra scala e ancoraggio**  
       Il passo successivo è stabilire quale differenza di punteggio sul questionario corrisponda tipicamente a questa transizione “da depresso a non depresso”. Ad esempio, attraverso dati esistenti o nuove analisi, si individua che la maggior parte dei pazienti che escono dalla diagnosi presenta una riduzione di almeno 10 punti sulla scala.
    
    3. **Soglia di significatività**  
       Il ricercatore definisce dunque come “clinicamente significativo” un miglioramento di 10 o più punti sul questionario, *purché* sia coerente con il superamento della soglia diagnostica. In questo modo, la variazione osservata non è solo una questione di numeri, bensì è “ancorata” a un cambiamento concreto: il passaggio effettivo dallo stato di diagnosi di depressione allo stato di remissione clinica.
    
    Grazie a questo collegamento, l’approccio basato su ancoraggi permette di interpretare la variazione dei punteggi alla luce di uno scenario clinico reale e tangibile.

- **Metodi basati sulla distribuzione.** Utilizzano gli indici di errore di misurazione per stabilire se la variazione del punteggio sia sufficientemente grande da non poter essere spiegata dal solo errore. Uno dei primi e più noti metodi di questo tipo è il *Reliable Change Index* (RCI), introdotto da Jacobson e Truax. Alla base dell’RCI vi è l’errore standard della differenza (SED), a sua volta derivato dall’errore standard di misura (SEM). Quest’ultimo si ottiene dalla seguente formula:

  $$
  SEM = s_x \sqrt{1 - r_{xx'}},
  $$

  dove $s_x$ è la deviazione standard dei punteggi al pre-test e $r_{xx'}$ l’affidabilità dello strumento. Se la variazione osservata nel punteggio di un individuo supera l’errore di misurazione intrinseco, si può concludere che il cambiamento sia *clinicamente significativo* e non soltanto frutto della variabilità casuale legata alla misura.

### Calcolo del Reliable Change Index (RCI)

La logica dell’RCI si basa su una suddivisione di qualunque punteggio osservato ($X$) in due componenti: il *punteggio vero* ($T$) e un *errore di misurazione* ($E$):

$$
X = T \pm E.
$$

Se una persona subisce un vero cambiamento tra due tempi di osservazione ($t1$ e $t2$), ciò dovrebbe riflettersi in una differenza fra i relativi punteggi veri. Tuttavia, la presenza dell’errore di misurazione rende possibile osservare differenze anche in assenza di un mutamento reale. Di conseguenza, occorre stabilire *quanto* debba essere ampio un cambiamento per poterlo definire genuino e non dovuto a fluttuazioni casuali.

Jacobson e Truax (1991), riprendendo concetti introdotti da Jacobson e colleghi (1984), sfruttarono la distribuzione degli errori di misurazione (che ha media zero e varianza pari al $SEM^2$) per definire un indice che quantifica l’entità di un cambiamento in termini di *unità di errore standard*.

### Calcolo della differenza tra punteggi e distribuzione di errore

Il modo più diretto per rilevare un cambiamento consiste nel calcolare il *punteggio di differenza* (o *change score*) tra due tempi di misura:

$$
C_i = X_{t1} - X_{t2},
$$

dove $C_i$ rappresenta il cambiamento per l’individuo $i$. Poiché ogni punteggio osservato comprende una componente di errore, il punteggio di differenza conterrà anche l’errore combinato delle due misurazioni.

La distribuzione di questi *errori di differenza* ha anch’essa media zero, ma una deviazione standard più grande, chiamata deviazione standard della differenza ($SD_{Diff}$). Per due misure indipendenti o non correlate dal punto di vista dell’errore, essa è data da:

$$
SD_{Diff} = \sqrt{2 \times SEM^2}.
$$

Questo nasce dalla proprietà secondo cui la varianza della differenza fra due variabili indipendenti si ottiene sommando le loro varianze. Anche se i punteggi veri al pre- e post- possono essere correlati (perché appartengono alla stessa persona), si assume in psicometria che gli *errori di misurazione* siano indipendenti fra loro.

### Definizione dell’RCI

Una volta calcolati il punteggio di differenza $C_i$ e la sua deviazione standard $SD_{Diff}$, l’RCI è semplicemente il *punteggio di differenza standardizzato*:

$$
C_i(\text{Standardized}) = \frac{C_i}{SD_{Diff}}.
$$

Analogamente a uno *z-score*, l’RCI indica di quante *unità di errore standard* differisce il cambiamento osservato dal valore zero (che corrisponderebbe a nessun cambiamento reale). Più alto è l’RCI in valore assoluto, più ci si può ragionevolmente aspettare che la variazione misurata rifletta un mutamento vero e non solo un artefatto della misura.  

Un RCI che superi una determinata soglia (spesso ±1.96 per p < .05) suggerisce un cambiamento talmente ampio da poter essere considerato *clinicamente significativo*, cioè non spiegabile interamente dall’errore di misurazione.

## Affidabilità e Lunghezza del Test

Un modo per determinare quanti item servano a raggiungere un dato livello di affidabilità è sfruttare la formula di **Spearman-Brown**, adattandola per calcolare la lunghezza del test desiderata. Nel caso di un *item medio* con affidabilità stimata $\rho_1$ e di un test esteso il cui obiettivo è raggiungere l’affidabilità complessiva $\rho_p$, la formula che restituisce il numero di item $p$ necessari è:

$$
p = \frac{\rho_p (1 - \rho_1)}{\rho_1 (1 - \rho_p)}.
\tag{1}
$$

Supponiamo, per esempio, che un test composto da 5 item abbia un’affidabilità di 0.824 e che $\rho_1$ (affidabilità dell’item medio) sia pari a 0.479. Se desideriamo raggiungere un’affidabilità complessiva $\rho_p$ di 0.95, applicando la formula $(1)$ si ottiene un valore di $p$ di circa 21 item. In altre parole, servirebbe un totale di 21 item per raggiungere una stima di affidabilità pari a 0.95.

```{r}
# Esempio di calcolo in R
rho_1 <- 0.479
desired_rho <- 0.95
n_items_needed <- (desired_rho * (1 - rho_1)) / (rho_1 * (1 - desired_rho))
n_items_needed
```

## Attenuazione

### Attenuazione e Correlazioni Disattenuate

Un fenomeno cruciale nell’analisi statistica è **l’attenuazione**, ovvero la riduzione della correlazione osservata fra due variabili dovuta alla presenza di errore di misurazione in una o entrambe le scale. Se l’errore di misurazione è elevato, la correlazione empirica tende a essere sottostimata rispetto a quella vera, dando luogo al cosiddetto *effetto di attenuazione*.

Come discusso da Lord e Novick (1967), quando si creano scale di misura per rappresentare due costrutti e si vuole valutarne la relazione, l’errore inevitabilmente incluso in ciascuna scala fa sì che la correlazione osservata sia inferiore alla correlazione *vera* fra i costrutti. Per “recuperare” questa correlazione vera (o latente), si applicano formule di **correzione per l’attenuazione** che sfruttano i coefficienti di affidabilità delle due misure.

Se $X$ e $Y$ sono i punteggi osservati e $T_X$ e $T_Y$ i punteggi veri di due costrutti, la **correlazione disattenuata** tra i punteggi veri si calcola come:

$$
\rho(T_X, T_Y) = \frac{\rho_{XY}}{\sqrt{\rho_{XX^\prime} \,\rho_{YY^\prime}}},
\tag{2}
$$

dove $\rho_{XY}$ è la correlazione osservata tra $X$ e $Y$, mentre $\rho_{XX^\prime}$ e $\rho_{YY^\prime}$ sono le affidabilità delle due scale.

Allo stesso modo, se si desidera la correlazione tra i punteggi osservati di un test $X$ e i *punteggi veri* di un secondo test $T_Y$, la formula è:

$$
\rho(X, T_Y) = \frac{\rho_{XY}}{\sqrt{\rho_{YY^\prime}}}.
\tag{3}
$$

Queste formule, radicate nella Teoria Classica dei Test (CTT), permettono di “risalire” a quanto due costrutti latenti siano effettivamente correlati, depurando il contributo dell’errore di misurazione.

#### Intervalli di confidenza per la correlazione corretta

Il calcolo degli intervalli di confidenza per la **correlazione disattenuata** richiede qualche cautela: un approccio comune è applicare la *formula di disattenuazione* agli estremi dell’intervallo di confidenza della correlazione osservata, consapevoli che tale procedura è approssimativa. In pratica, si stima l’intervallo di confidenza della correlazione osservata e se ne calcola una versione “corretta” sostituendo in formula i valori limite dell’intervallo.

Ad esempio, si consideri un caso in cui la correlazione osservata $\rho_{XY} = 0.50$, con affidabilità $\rho_{XX^\prime} = 0.70$ e $\rho_{YY^\prime} = 0.80$. Applicando la formula $(2)$, la correlazione disattenuata è:

```{r}
r_osservata <- 0.5
rho_X <- 0.7
rho_Y <- 0.8

r_corretta <- r_osservata / sqrt(rho_X * rho_Y)
r_corretta
```

Se l’intervallo di confidenza della correlazione osservata andasse, ad esempio, da 0.40 a 0.60, si otterrebbero due estremi per la correlazione corretta applicando la stessa operazione:

```{r}
CI_lower_observed <- 0.4
CI_upper_observed <- 0.6

CI_lower_corrected <- CI_lower_observed / sqrt(rho_X * rho_Y)
CI_upper_corrected <- CI_upper_observed / sqrt(rho_X * rho_Y)

cat("Intervallo di confidenza corretto: da",
    CI_lower_corrected, "a", CI_upper_corrected)
```

È importante sottolineare che, sebbene questa procedura offra una stima veloce dei limiti di confidenza disattenuati, non costituisce un metodo pienamente rigoroso per calcolare l’incertezza attorno alle correlazioni latenti.

#### Esempio classico di Spearman

L’utilizzo delle correlazioni disattenuate risale a Spearman (1904), il quale evidenziò come la correlazione empirica tra una misura di **discriminazione dell’altezza del suono** ($X$) e **l’intelligenza valutata dall’insegnante** ($Y$) fosse pari a $\hat{\rho}_{XY} = 0.38$. Notando che le due misure avevano affidabilità basse ($\hat{\rho}_{XX'} = 0.25$ e $\hat{\rho}_{YY'} = 0.55$), Spearman corresse la correlazione per l’attenuazione, ottenendo addirittura un valore di poco superiore a 1. Questo esempio evidenzia un limite ben noto: la formula di disattenuazione, se applicata a stime di affidabilità troppo basse o a correlazioni molto elevate, può dare risultati paradossali (correlazioni > 1).

Già all’epoca, ciò innescò un vivace dibattito con Pearson, il quale non accettò l’idea di “quantità non osservabili” e restò scettico verso una correlazione superiore a 1. Ciononostante, Spearman proseguì i suoi studi, contribuendo in maniera determinante allo sviluppo dell’**analisi fattoriale** e mostrando come spesso le correlazioni disattenuate si avvicinassero all’unità nel caso di variabili fortemente collegate allo stesso fenomeno.

McDonald (1999) sottolinea l’importanza di un uso cauto di queste correlazioni corrette, suggerendo che i modelli di **equazioni strutturali** (SEM) forniscono stime più robuste, in quanto permettono di modellare direttamente le variabili latenti senza dover ricorrere a correzioni ex post.

## Usare l’Affidabilità per Migliorare l’Inferenza a Livello Individuale

Un altro uso cruciale dell’affidabilità è il miglioramento delle stime dei *punteggi veri* di ciascun individuo. **Kelley** (1920; 1947) mostrò come tale stima possa essere effettuata regredendo i punteggi osservati sulla stima dell’affidabilità:

$$
\hat{T} = \bar{X} + r_{xx'} \,(X - \bar{X}),
$$

dove:
- $\bar{X}$ è la *media dei punteggi osservati* nel campione,
- $r_{xx'}$ è il coefficiente di affidabilità tra i punteggi osservati e quelli veri.

In sostanza, più lo strumento è affidabile (valore elevato di $r_{xx'}$), più il **punteggio osservato** $X$ avrà peso nel determinare la stima $\hat{T}$; viceversa, quando l’affidabilità è bassa, la stima $\hat{T}$ si avvicina maggiormente alla media del gruppo $\bar{X}$.

### L’Errore Standard della Stima del Vero Punteggio

Oltre a indicare come stimare il punteggio vero, Kelley fornì una formula per il relativo **errore standard**:

$$
\sigma_{\hat{T}} 
= \sigma_X \,\sqrt{\rho_{XX^\prime}\,(1 - \rho_{XX^\prime})},
$$

dove $\sigma_X$ è la deviazione standard dei punteggi osservati e $\rho_{XX^\prime}$ l’affidabilità. Tale errore standard risulta inferiore all’errore standard di misura (SEM) dei semplici punteggi osservati:

$$
SE_{X} = \sigma_{X}\,\sqrt{1 - \rho_{XX'}},
$$

e dimostra come, conoscendo l’affidabilità, si possa ridurre l’incertezza della stima $\hat{T}$.

### Legami con gli stimatori di James-Stein e i Metodi Bayesiani

Le equazioni di Kelley anticipano di molti anni i principi alla base degli **stimatori di James-Stein**, che prevedono anch’essi un aggiustamento delle stime individuali verso la media del gruppo, in modo da ottenere stime complessivamente più accurate. In un’ottica bayesiana, se si assume che i punteggi veri seguano una distribuzione *a priori* (ad esempio, una normale centrata sulla media del campione), la **media a posteriori** coincide con la stima di Kelley (de Gruijter & van der Kamp, 2008). Ciò mostra un nesso concettuale fra la psicometria classica e l’inferenza bayesiana, sottolineando come l’informazione di gruppo possa migliorare significativamente la stima dei punteggi individuali.

In pratica, lo stesso principio viene sfruttato nei modelli multilivello (per esempio tramite il pacchetto `lme4` in R), i quali, in modo “bayesiano empirico”, riducono l’errore di stima dei singoli soggetti integrando informazioni sull’intero gruppo.

### Simulazione: Stima dei Punteggi Veri con Pooling

Per illustrare concretamente l’effetto di tale “pooling” verso la media, riportiamo un esempio in R, ispirato al codice di Nathaniel Haines. La simulazione genera dati per 20 soggetti con probabilità di successo media di 0.7, variando il numero di item (10, 30, 100) per osservare come cambia l’affidabilità (e di conseguenza la stima dei punteggi veri).

```{r}
set.seed(43202)

# Numero di soggetti e di item
n_subj <- 20
n_items <- c(10, 30, 100)

# Campione casuale di punteggi veri intorno a 0.7
theta <- rnorm(n_subj, .7, .1)

# Funzione per stimare l'errore standard di misura (al quadrato)
est_se2 <- function(x) {
    p <- mean(x)
    q <- 1 - p
    n <- length(x)
    # Varianza dell'errore per item binari
    sig2_ep_i <- (p * q) / (n - 1)
    sig2_ep_i
}

# Simulazione
dis_dat <- foreach(i = seq_along(n_items), .combine = "rbind") %do% {
    # Genera dati osservati (binari) per ciascun soggetto
    X_all <- foreach(t = seq_along(theta), .combine = "rbind") %do% {
        rbinom(n_items[i], 1, prob = theta[t])
    }
    # Media di gruppo
    X_bar <- mean(rowMeans(X_all))

    # Calcolo dell'affidabilità
    X <- rowMeans(X_all)
    sig2_ep <- mean(apply(X_all, 1, est_se2))
    sig2_X <- var(X)
    rho <- 1 - (sig2_ep / sig2_X)

    # Stima dei punteggi veri secondo la formula di Kelley
    foreach(t = seq_along(theta), .combine = "rbind") %do% {
        X_i <- mean(X_all[t, ])
        data.frame(
            subj_num = t,
            n_items = n_items[i],
            theta = theta[t],
            rho = rho,
            X = X_i,
            se_obs = sd(X) * sqrt(1 - rho),
            se_hat = sd(X) * sqrt(1 - rho) * sqrt(rho),
            theta_hat = (1 - rho) * X_bar + rho * X_i
        )
    }
}

# Visualizzazione
dis_dat %>%
  mutate(subj_num = reorder(subj_num, theta)) %>%
  ggplot(aes(x = subj_num, y = theta)) +
  geom_point() +
  geom_point(aes(y = X),
             color = "gray",
             position = position_jitter(width = .2, height = 0, seed = 1)) +
  geom_linerange(
    aes(ymin = X - 1.96 * se_obs, ymax = X + 1.96 * se_obs),
    color = "gray",
    position = position_jitter(width = .2, height = 0, seed = 1)
  ) +
  geom_point(aes(y = theta_hat),
             color = "red",
             position = position_jitter(width = .2, height = 0, seed = 2)) +
  geom_linerange(
    aes(ymin = theta_hat - 1.96 * se_hat, ymax = theta_hat + 1.96 * se_hat),
    color = "red",
    position = position_jitter(width = .2, height = 0, seed = 2)
  ) +
  geom_hline(yintercept = mean(dis_dat$X), linetype = 2, color = "black") +
  facet_wrap(~ n_items, nrow = 1) +
  ggtitle("Stime del punteggio vero basate sulla regressione di Kelley") +
  xlab("Soggetto") +
  ylab("Valore") +
  theme_minimal(base_size = 14) +
  theme(panel.grid = element_blank(),
        axis.text.x.bottom = element_blank())
```

Tre aspetti principali emergono da questo esempio:

1. Le **stime puntuali** ottenute tramite la formula di Kelley ($\hat{T}$) si trovano sempre *più vicine alla media del gruppo* (linea tratteggiata) rispetto ai punteggi osservati.
2. L’effetto di “**pooling**” verso la media è tanto più marcato quanto più bassa è l’affidabilità. Nel codice, la manipolazione dell’affidabilità avviene variando il numero di item.
3. Gli **intervalli di confidenza** (IC) attorno a $\hat{T}$ risultano *più stretti* rispetto agli IC dei punteggi osservati, a conferma di come la stima di Kelley riduca effettivamente l’incertezza legata all’errore di misurazione.

Questa simulazione mette in luce come la *regressione verso la media* – unita alla conoscenza dell’affidabilità – permetta di ottenere stime più accurate dei punteggi veri, specialmente quando il numero di item (e dunque la precisione) è limitato. Allo stesso tempo, evidenzia come la disponibilità di un maggior numero di item o di misure più affidabili riduca la necessità di “shrinkage” verso la media, poiché i punteggi osservati rispecchiano più da vicino i punteggi veri.

### Approccio Bayesiano

In questa sezione mostreremo come i risultati ottenuti con la **regressione di Kelley** possano essere riprodotti mediante un **modello gerarchico bayesiano**, applicato a dati provenienti da questionari con risposte dicotomiche (vero/falso, corretto/errato).

#### Introduzione: Modello Bernoulliano e Logit

Quando abbiamo un questionario a risposte dicotomiche, ogni risposta può essere vista come un *esperimento di Bernoulli*. Indichiamo con $X$ la variabile che vale 1 (successo) con probabilità $p$ e 0 (insuccesso) con probabilità $1-p$.

Per collegare questa probabilità di successo a un parametro di “abilità” $\theta$, utilizziamo la **funzione logistica**:

$$
p = \frac{1}{1 + e^{-\theta}},
$$

dove $\theta$ può variare da $-\infty$ a $+\infty$. Inversamente,

$$
\theta = \log\biggl(\frac{p}{1 - p}\biggr).
$$

Questo modello costituisce una forma molto semplificata della **Teoria della Risposta all’Item (IRT)**, in cui:
- Ogni individuo è caratterizzato da un unico parametro di abilità $\theta$.
- Tutti gli item hanno difficoltà e discriminazione uguali (per semplicità fissate a 1).

#### Ipotesi di Gruppo e Distribuzione a Priori

Nel contesto bayesiano, bisogna specificare come i parametri individuali $\theta_i$ siano generati a livello di gruppo. Possiamo assumere una distribuzione normale:

$$
\theta_i \sim \mathcal{N}(\mu,\sigma),
$$

dove $\mu$ e $\sigma$ sono i parametri di gruppo (rispettivamente media e deviazione standard). Ad esempio, in un modello semplice si potrebbe fissare $\mu = 0$ e porre $\sigma \sim \text{HalfNormal}(1)$, rendendo esplicito che $\sigma$ debba essere positiva.

Il risultato è un **modello gerarchico**, in cui:

1. Ogni individuo ha il proprio $\theta_i$.
2. I $\theta_i$ a loro volta derivano da una distribuzione di gruppo definita da $\mu$ e $\sigma$.
3. I dati osservati (risposte di ciascun soggetto) consentono di stimare simultaneamente i parametri a livello individuale e quelli di gruppo, migliorando la stabilità delle stime.

Quest’idea rispecchia quanto avviene nella stima dei punteggi veri nella teoria classica dei test, dove i punteggi individuali vengono “shrinkati” (avvicinati) verso la media di gruppo, riducendo così la varianza dovuta all’errore di misurazione.

### Parametrizzazione Non Centrata in Stan

Nel codice Stan che segue, si usa una **parametrizzazione non centrata** per il livello di gruppo, opzione che tende a migliorare la convergenza delle catene MCMC (in particolare con Hamiltonian Monte Carlo):

1. **Parametri di Gruppo**  
   $\mu_\theta$ e $\sigma_\theta$, entrambi con priori appropriati (es. $\mu_\theta \sim \mathcal{N}(0,1)$, $\sigma_\theta \sim \text{HalfNormal}(1)$).
2. **Parametri Non Centrati**  
   $\theta_{pr}$ sono variabili standardizzate ($\mathcal{N}(0,1)$) che verranno poi trasformate in $\theta$ effettive con la formula
   $$
   \theta = \mu_\theta + \sigma_\theta \times \theta_{pr}.
   $$
3. **Verosimiglianza Bernoulliana**  
   Ogni risposta $\text{Y}_{ij}$ (soggetto $i$, item $j$) segue Bernoulli$(p_{ij})$ con $p_{ij} = \text{logit}^{-1}(\theta_i)$.

Questa strategia non cambia il modello statistico ma spesso ne rende più efficiente la stima.

### Esempio di Simulazione (un solo soggetto)

```{r}
file <- file.path("hbern.stan")
mod <- cmdstan_model(file)
mod$print()
```

Di seguito, uno schema in R che illustra il flusso di lavoro per un singolo soggetto con 30 item:

```{r}
#| output: false
#| tags: [hide-output]
#| 
# Esempio semplificato: un soggetto e 30 item
n_subj <- 1
n_items <- 30

# Generazione del "vero" theta
theta <- rnorm(n_subj, 0.7, 0.1)

# Risposte simulate in base a theta
Y <- rbinom(n_items, 1, prob = theta)

# Adattamento del modello in Stan (codice non mostrato qui)
fit_bernoulli <- mod$sample(
    data = list(
        N = n_subj,
        N_items = n_items,
        Y = matrix(Y, nrow = 1)
    ),
    iter_sampling = 2500,
    iter_warmup = 500,
    chains = 4,
    parallel_chains = 4,
    seed = 43202
)

# Estrazione stime a posteriori per p
bayes_est_p <- as.vector(fit_bernoulli$draws(variables = "p"))
bayes_theta_est <- mean(bayes_est_p)
hdi_bounds <- quantile(bayes_est_p, probs = c(0.025, 0.975))

# Prepare the results with a single HDI for 'p'
results <- data.frame(
    subj_num = 1,
    n_items = n_items,
    theta = theta,
    bayes_theta = bayes_theta_est,
    bayes_lo = hdi_bounds[1], # Lower bound of HDI
    bayes_hi = hdi_bounds[2] # Upper bound of HDI
)
```


```{r}
# Print the results
print(results)
```

Il risultato fornisce la stima puntuale di $p$ (cioè la probabilità di successo) e un intervallo di credibilità al 95%.


### Estensione a Più Soggetti e Diversi Numeri di Item

Per illustrare come varia la stima di $\theta$ in base al numero di item e al numero di soggetti, possiamo ripetere la stessa procedura per:

- $n_{subj} = 20$ soggetti.
- Diversi valori di $n_{items}$ (ad es. 10, 30, 100).

```{r}
#| output: false
#| tags: [hide-output]

set.seed(43202)

n_subj <- 20
n_items_vec <- c(10, 30, 100)

# Placeholder for results
results <- list()

for (n_items in n_items_vec) {
    for (subj in 1:n_subj) {
        # Generate "true" theta for the subject
        theta <- rnorm(1, .7, .1)

        # Generate observed data for the subject using "true" theta
        Y <- rbinom(n_items, 1, prob = theta)

        # Fit the model
        fit_bernoulli <- mod$sample(
            data = list(
                N = 1,
                N_items = n_items,
                Y = matrix(Y, nrow = 1) # Ensure Y is a matrix
            ),
            iter_sampling = 2500,
            iter_warmup = 500,
            chains = 4,
            parallel_chains = 4,
            seed = 43202
        )

        # Extract and process posterior samples for 'p'
        bayes_est_p <- as.vector(fit_bernoulli$draws(variables = "p"))
        bayes_theta_est <- mean(bayes_est_p)
        hdi_bounds <- quantile(bayes_est_p, probs = c(0.025, 0.975))

        # Collect results
        results[[paste(subj, n_items)]] <- data.frame(
            subj_num = subj,
            n_items = n_items,
            theta = theta,
            bayes_theta = bayes_theta_est,
            bayes_lo = hdi_bounds[1],
            bayes_hi = hdi_bounds[2]
        )
    }
}
```

Otterremo una tabella con, per ogni soggetto e ogni condizione (numero di item), la stima bayesiana di $p$ o $\theta$, insieme ai relativi intervalli di credibilità.

```{r}
all_results <- bind_rows(results)
all_results |> head()
```

Alla fine, possiamo riunire tutti i risultati in un unico `data.frame` e tracciarli con **ggplot2**, confrontando i valori “veri” $\theta$ con le stime ottenute dal modello. Per esempio:

```{r}
ggplot(all_results, aes(x = theta, y = bayes_theta)) +
    geom_point() +
    geom_errorbar(aes(ymin = bayes_lo, ymax = bayes_hi), width = 0.02) +
    geom_hline(yintercept = 0.7, linetype = "dashed", color = "gray") +
    facet_wrap(~ n_items, nrow = 1) +
    labs(x = "True Theta", y = "Estimated p") +
    ggtitle("Estimated p vs. True Theta\nfor Different Numbers of Items")
```

### Confronto con la Regressione di Kelley

Proprio come nella teoria classica dei test, dove la **regressione di Kelley** stima i punteggi veri in base all’affidabilità e al punteggio osservato, un modello bayesiano gerarchico produce stime a posteriori che convergono a risultati analoghi, senza dover calcolare esplicitamente l’affidabilità.

1. **Stima dei Punteggi Veri (Kelley)**  
   $$
   \text{Punteggio Vero} = \bar{X} + r_{xx'} \times (X - \bar{X}),
   $$
   dove $r_{xx'}$ è l’affidabilità, $\bar{X}$ la media campionaria e $X$ il punteggio osservato.

2. **Intervallo di Confidenza di Kelley**  
   Se $\sigma$ è la deviazione standard dei punteggi osservati,
   $$
   \text{SEM} = \sigma \sqrt{1 - r_{xx'}}, \quad
   \text{CI}_{95\%} = \text{Punteggio Vero} \pm 1.96 \times \text{SEM}.
   $$

3. **Stima Bayesiana**  
   Si basa su:
   - Una **distribuzione a priori** per $\mu_\theta$ e $\sigma_\theta$.
   - Una verosimiglianza Bernoulliana per i dati.
   - L’uscita del modello sono le **medie a posteriori** (e intervalli di credibilità) per $\theta$.

Quando si confrontano graficamente i due metodi, spesso si nota un’ottima sovrapposizione delle stime puntuali e una corrispondenza tra gli intervalli (di confidenza in Kelley, di credibilità in Bayes). Questo riflette il fondamento comune: *la regressione di Kelley corrisponde alla stima bayesiana in un semplice modello normale con priori non informativi*. 

L’approccio bayesiano offre alcuni vantaggi:
- Non richiede il calcolo esplicito dell’affidabilità.
- Integra in modo naturale le informazioni di gruppo (pooling).
- Fornisce un quadro pienamente probabilistico (inclusi gli intervalli di credibilità).

In sintesi, l’esempio mostra come i **modelli gerarchici bayesiani** possano produrre stime di “punteggi veri” del tutto affini a quelle ottenute dalla teoria classica dei test (Kelley). La similitudine tra i due approcci non deve stupire: in entrambi i casi, si applica un principio di “regressione verso la media” per contrastare l’errore di misurazione. 

La differenza principale è che, nell’approccio bayesiano, la *stima dell’errore di misurazione* e la correzione verso la media avvengono all’interno di un unico quadro gerarchico, evitando il calcolo di un coefficiente di affidabilità esterno. Inoltre, la modellazione bayesiana fornisce naturalmente stime posteriori, compresi gli intervalli di credibilità, che ne facilitano l’interpretazione probabilistica e la flessibilità in scenari più complessi.

## Riflessioni Conclusive 

In questo capitolo, abbiamo esaminato alcune tra le più rilevanti applicazioni della Teoria Classica dei Test (CTT), approfondendo sia aspetti teorici sia implicazioni pratiche nella misurazione psicologica. In particolare, abbiamo:

- **Esplorato il problema dell’attenuazione** e mostrato come l’errore di misurazione possa “nascondere” la reale entità di una correlazione tra costrutti psicologici.  
- **Analizzato la formula di Spearman-Brown**, utile per determinare il numero di item richiesto per raggiungere un livello desiderato di affidabilità, evidenziando così il ruolo strategico della progettazione dei test.  
- **Confrontato due differenti modalità di stima dei punteggi veri individuali**: da un lato, la regressione di Kelley, che fa leva sui principi della CTT; dall’altro, un approccio bayesiano gerarchico, che offre un quadro probabilistico più generale pur arrivando, nei casi semplici, a risultati concettualmente simili.

La prospettiva fornita dalla CTT rimane uno strumento fondamentale nella cassetta degli attrezzi dello psicometrista, consentendo di formulare regole chiare per la costruzione, la valutazione e l’uso dei test. Al tempo stesso, la sovrapposizione con il mondo bayesiano rivela come molti dei principi classici—quali la regressione verso la media e la correzione dell’errore di misurazione—trovino un corrispettivo naturale all’interno di modelli gerarchici e approcci statistici più evoluti.

In definitiva, la padronanza di questi concetti fornisce basi solide per:
1. **Progettare strumenti di misura** con un grado di affidabilità adeguato rispetto allo scopo dell’indagine.  
2. **Evitare sottostime o sovrastime** delle relazioni tra variabili, grazie alla consapevolezza dell’errore e alla correzione per l’attenuazione.  
3. **Migliorare la stima dei punteggi veri** in modo accurato e rigoroso, sia con procedure tradizionali sia tramite modelli bayesiani di ultima generazione.

La CTT rappresenta dunque un punto di partenza indispensabile per chiunque desideri affrontare le sfide della misurazione in psicologia. La possibilità di integrare queste tecniche con metodologie avanzate—come la Teoria della Risposta all’Item (IRT) o i modelli di equazioni strutturali—apre a sviluppi futuri in cui l’accuratezza e la versatilità delle nostre misurazioni possono essere ulteriormente incrementate, a beneficio tanto della ricerca di base quanto dell’applicazione clinica.
## Session Info

```{r}
sessionInfo()
```

