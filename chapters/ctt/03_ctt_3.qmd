# Metodi di stima dell'affidabilità {#sec-ctt-methods-reliability}

**Preparazione del Notebook**

```{r}
here::here("code", "_common.R") |> source()
pacman::p_load(modelsummary, ltm)
```

## Introduzione

I punteggi ottenuti da test psicologici possono variare per differenze tra item, occasioni di somministrazione o modalità di valutazione. La CTT affronta questo problema introducendo il concetto di affidabilità, intesa come stabilità e coerenza delle misure ottenute, distinguendo tra "punteggio vero" e "errore di misurazione".

Nel capitolo precedente abbiamo visto che l'affidabilità riflette la proporzione della varianza dovuta al punteggio vero rispetto alla varianza totale. Il problema successivo è stimare accuratamente l'affidabilità, considerando diverse modalità di errore.

## Come Stimare l'Affidabilità

Per stimare l'affidabilità ($\rho^2_{XT}$), dobbiamo affrontare la difficoltà legata all'impossibilità di osservare direttamente il punteggio vero o l'errore di misurazione. La strategia di stima dipende da come definiamo e interpretiamo l'errore di misurazione ($\sigma^2_E$):
  
1. **Affidabilità delle Forme Parallele**:
    - considera l’errore come differenza tra punteggi ottenuti da forme equivalenti del test.

2. **Consistenza Interna**:
    - valuta quanto gli item all’interno dello stesso test siano omogenei rispetto al costrutto misurato (es. Alpha di Cronbach).

3. **Affidabilità Test-Retest (Coerenza Temporale)**:
    - misura l’errore come variazione dei punteggi nel tempo, attraverso somministrazioni ripetute dello stesso test.

La principale differenza tra questi metodi è nella definizione operativa e nel calcolo della varianza d’errore ($\sigma^2_E$).


## Affidabilità come Consistenza Interna
  
  La consistenza interna valuta l’omogeneità tra item. Nella CTT, esistono tre principali modelli teorici che descrivono come i punteggi veri di diversi item si relazionano tra loro (cfr. @sec-ctt-foundations):

### Item paralleli
  
**Definizione in CTT**:  
  
- $\tau$-equivalenti con intercetta nulla e fattore di scala pari a 1,  
- stessa varianza dell’errore di misura.

Formule generiche:  

  $$
    \begin{cases}
    X_1 = T + E_1 \\
    X_2 = T + E_2 \\
    \end{cases}
    $$
    
con $\mathbb{E}[E_1] = \mathbb{E}[E_2] = 0$ e $\mathrm{Var}(E_1) = \mathrm{Var}(E_2)$.

In questo caso, la **correlazione $\mathrm{corr}(X_1, X_2)$ coincide con l’affidabilità**, data da

$$
  \rho_{X_1 X_2} \;=\;
  \frac{\sigma_T^2}{\sigma_T^2 + \sigma_E^2}.
  $$

**Esempio in R.**

```{r}
set.seed(123)

n <- 100000 # numero di osservazioni

# 1) Generiamo il punteggio vero T
T <- rnorm(n, mean = 50, sd = 10)

# 2) Generiamo errori di misura con stessa varianza
E1 <- rnorm(n, mean = 0, sd = 5)
E2 <- rnorm(n, mean = 0, sd = 5)

# 3) Costruiamo i due item paralleli
X1_parallel <- T + E1
X2_parallel <- T + E2

# 4) Calcoliamo la correlazione tra i due item
corr_par <- cor(X1_parallel, X2_parallel)

# 5) Calcoliamo la "affidabilità" teorica di uno qualunque di questi item
#    secondo CTT: var(T) / var(X1_parallel)
rel_par <- var(T) / var(X1_parallel)

cat("Item paralleli:\n")
cat("Correlazione (X1, X2):", round(corr_par, 2), "\n")
cat("Affidabilità teorica  :", round(rel_par, 2), "\n")
```

**Risultato**: La correlazione tra $X_1$ e $X_2$ dovrebbe approssimare molto da vicino la stima dell’affidabilità: se i due item sono paralleli, “vedono” lo stesso punteggio vero con la stessa qualità di misura.


### Item $\tau$-equivalenti
  
**Definizione in CTT**:  

- Stesso coefficiente di pendenza ($b_{ij} = 1$),  
- Possibile **shift costante** (intercetta diversa) o differenze nelle varianze d’errore,  
- In generale, $\mathrm{corr}(X_1, X_2)$ non è uguale all’affidabilità, anche se i punteggi veri differiscono solo per un termine costante.

Formule generiche (possibilità di shift di media e/o errori differenti):  

  $$
    \begin{cases}
    X_1 = T + E_1,\\
    X_2 = (T + c) + E_2,
    \end{cases}
    $$
dove $c \neq 0$ è uno shift nella media o $\mathrm{Var}(E_1)\neq \mathrm{Var}(E_2)$.

**Esempio in R.**

```{r}
# 1) Punteggio vero T
T <- rnorm(n, mean = 50, sd = 10)

# 2) Generiamo errori con varianze diverse
E1_tau <- rnorm(n, mean = 0, sd = 4)
E2_tau <- rnorm(n, mean = 0, sd = 6)

# 3) Aggiungiamo un piccolo shift (c)
c_shift <- 5

# 4) Costruiamo i due item tau-equivalenti (stessa "pendenza" = 1)
X1_tau <- T + E1_tau
X2_tau <- (T + c_shift) + E2_tau

# 5) Correlazione
corr_tau <- cor(X1_tau, X2_tau)

# 6) "Affidabilità" teorica di X1_tau (ad es.)
rel_tau <- var(T) / var(X1_tau)

cat("\nItem tau-equivalenti:\n")
cat("Correlazione (X1, X2):", round(corr_tau, 3), "\n")
cat("Affidabilità teorica  :", round(rel_tau, 3), "\n")
```

**Risultato**: Qui la correlazione tra $X_1$ e $X_2$ non coincide con la stima di affidabilità, perché abbiamo violato una delle condizioni di parallelismo (stessa varianza d’errore e/o zero shift). Pur avendo la stessa “pendenza” ($b=1$), lo shift e la differente varianza d’errore fanno sì che $\mathrm{corr}(X_1, X_2)\neq \mathrm{var}(T)/\mathrm{var}(X_1)$.

###  Item congenerici
  
**Definizione in CTT**: 

- Gli item possono avere **pendenze (slopes)** diverse e **intercette** diverse.  
- In formula: $X_1 = a_1 + b_1 T + E_1$, $X_2 = a_2 + b_2 T + E_2$.  
- I rapporti tra varianze e correlazioni diventano più complessi, e non è possibile che la mera correlazione tra $X_1$ e $X_2$ coincida con l’affidabilità di uno dei due, se non per casi particolari.

**Esempio in R.**

```{r}
# 1) Punteggio vero T
T <- rnorm(n, mean = 50, sd = 10)

# 2) Generiamo errori con varianze (o distribuzioni) diverse
E1_cong <- rnorm(n, mean = 0, sd = 4)
E2_cong <- rnorm(n, mean = 0, sd = 8)

# 3) Fattori di scala e intercette diverse
a1 <- 10
b1 <- 1.2
a2 <- -5
b2 <- 0.8

# 4) Costruiamo i due item congenerici
X1_cong <- a1 + b1 * T + E1_cong
X2_cong <- a2 + b2 * T + E2_cong

# 5) Correlazione
corr_cong <- cor(X1_cong, X2_cong)

# 6) Affidabilità teorica di X1_cong (ad es.)
#    var( T' ) / var( X1_cong ),
#    dove T' = b1 * T (non va scordato che a1 è un shift)
#    ma in CTT spesso si assume T' come 'vera' per X1
var_true_X1 <- var(b1 * T) # var(b1*T) = b1^2 * var(T)
var_X1 <- var(X1_cong)
rel_cong <- var_true_X1 / var_X1

cat("\nItem congenerici:\n")
cat("Correlazione (X1, X2):", round(corr_cong, 3), "\n")
cat("Affidabilità teorica di X1:", round(rel_cong, 3), "\n")
```

**Risultato**: La correlazione $\mathrm{corr}(X_1, X_2)$ non coincide con $\mathrm{var}(b_1 T)/\mathrm{var}(X_1)$. Gli item congenerici “vedono” il costrutto latente con pendenze e intercette diverse, quindi non c’è motivo che la semplice correlazione tra $X_1$ e $X_2$ equivalga all’affidabilità di uno dei due.


**Conclusioni.**
  
1. **Item paralleli**: stessa media, stessa varianza vera (e d’errore), $\mathrm{corr}(X_1, X_2) =$ affidabilità.  
2. **Item $\tau$-equivalenti**: stessa “pendenza” ($b=1$), ma possibili shift di media o differenze di varianza d’errore; la correlazione tra i due item **non** equivale all’affidabilità di un singolo item.  
3. **Item congenerici**: pendenza e intercetta diverse; la correlazione tra i due item si discosta ancora di più dall’affidabilità.


### Coefficiente Alpha di Cronbach

I coefficiente Alpha di Cronbach è l'indice più utilizzato fornito dalla CTT per misurare la consistenza interna. Il coefficiente Alpha è adatto per item $\tau$-equivalenti:

$$ 
\alpha = \frac{k}{k-1} \left(1 - \frac{\sum \sigma_i^2}{\sigma_X^2}\right) ,
$$ {#eq-cronbach-application}

dove:

- $k$ è il numero di item,
- $\sigma_i^2$ è la varianza dell'item i,
- $\sigma_X^2$ è la varianza totale del test.

Una derivazione della formula del coefficiente alpha di Cronbach sarà fornita nel capitolo @sec-fa-reliability.

L’Alpha di Cronbach fornisce una stima conservativa (limite inferiore) dell'affidabilità solo quando le assunzioni del modello $\tau$-equivalente sono rispettate. In caso contrario, può sovrastimare l'affidabilità. In altre parole, per utilizzare il coefficiente alpha di Cronbach **è necessario che gli item soddisfino la condizione di $\tau$-equivalenza**.

::: {#exm-}
Per illustrare la procedura di calcolo del coefficiente $\alpha$, useremo i dati `bfi` contenuti nel pacchetto `psych`. Il dataframe `bfi` comprende 25 item di autovalutazione della personalità. Sono riportati i dati di 2800 soggetti. Ci concentreremo qui sulla sottoscala *Openness*:

- O1: *Am full of ideas*; 
- O2: *Avoid difficult reading material*; 
- O3: *Carry the conversation to a higher level*; 
- O4: *Spend time reflecting on things*; 
- O5: *Will not probe deeply into a subject*. 

Leggiamo i dati in R.

```{r}
data(bfi, package = "psych")
head(bfi[c("O1", "O2", "O3", "O4", "O5")])
```

Esaminiamo la correlazione tra gli item della sottoscale Openness.

```{r}
cor(
  bfi[c("O1", "O2", "O3", "O4", "O5")],
  use = "pairwise.complete.obs"
) |>
  round(2)
```

È necessario ricodificare due item.

```{r}
bfi$O2r <- 7 - bfi$O2
bfi$O5r <- 7 - bfi$O5
```

```{r}
cor(
  bfi[c("O1", "O2r", "O3", "O4", "O5r")],
  use = "pairwise.complete.obs"
) |>
  round(2)
```

Consideriamo la matrice di varianze e covarianze della sottoscala Openness. 

```{r}
C <- cov(
  bfi[c("O1", "O2r", "O3", "O4", "O5r")],
  use = "pairwise.complete.obs"
)
C |> round(2)
```

Calcoliamo alpha:

```{r}
p <- 5
alpha <- (p / (p - 1)) * (1 - tr(C) / sum(C))
alpha
```

Lo stesso risultato si ottiene utilizzando la funzione `alpha()`
contenuta nel pacchetto `psych`:

```{r}
psych::alpha(C)
```
:::

### Coefficiente KR-20

La formula di Kuder-Richardson-20 (KR-20) è un caso particolare del coefficiente $\alpha$ di Cronbach. Se ogni item è dicotomico, il coefficiente $\alpha$ di Cronbach diventa il KR-20. Il coefficiente Coefficiente KR-20 si calcola con la formula:

$$
\mathrm{KR}_{20} 
\;=\; \frac{k}{k - 1}
\left(
1 \;-\; \frac{\sum_{i=1}^k p_i \,\bigl(1 - p_i\bigr)}{\sigma_X^2}
\right),
$$ {#eq-kr20}

dove:

- $k$ è il numero di item nel test,
- $p_i$ è la proporzione di risposte esatte all’item $i$,
- $\sigma_{X}^{2}$ è la varianza del punteggio totale.

::: {#exm-}
Consideriamo il data-set `LSAT` contenuto nel pacchetto `ltm`.

```{r}
KR20 <- function(responses) {
  # Get number of items (N) and individuals
  n.items <- ncol(responses)
  n.persons <- nrow(responses)
  # get p_j for each item
  p <- colMeans(responses)
  # Get total scores (X)
  x <- rowSums(responses)
  # observed score variance
  var.x <- var(x) * (n.persons - 1) / n.persons
  # Apply KR-20 formula
  rel <- (n.items / (n.items - 1)) * (1 - sum(p * (1 - p)) / var.x)
  return(rel)
}
```

```{r}
data(LSAT)
head(LSAT)
```

```{r}
KR20(LSAT)
```
:::


### Coefficiente KR-21

Il coefficiente KR-21 si ottiene da KR-20 **assumendo che tutti gli item abbiano la stessa difficoltà**, ossia la stessa proporzione di risposte corrette $\bar p$. In tale condizione, la formula si semplifica a:

$$
\mathrm{KR}_{21} 
\;=\; 
\frac{k}{k - 1}
\Biggl(
1 
- 
\frac{k\,\bar{p}\,\bigl(1 - \bar{p}\bigr)}{\sigma_X^2}
\Biggr),
$$ {#eq-kr21}

dove:

- $k$ è il numero di item,
- $\bar p = \frac{1}{k}\sum_{i=1}^k p_i$ è la media delle proporzioni di risposte corrette,
- $\sigma_X^2$ è la varianza del **punteggio totale** del test (ovvero la varianza della somma di tutti gli item).

Se, invece, si vuole esprimere $\bar p$ in funzione di $\sum_{i=1}^k p_i$, la formula rimane comunque la stessa, ma si “nasconde” la somma all’interno del termine $\bar p$.

**Cosa cambia rispetto a KR-20?**

1. **KR-20 (generale)**  
   $$
   \mathrm{KR}_{20}
   \;=\;
   \frac{k}{k-1}
   \Bigl(
     1 - \frac{\sum_{i=1}^k p_i(1 - p_i)}{\sigma_X^2}
   \Bigr).
   $$

   - **Non** richiede che tutti gli item abbiano la stessa $\,p_i$.  
   - La sommatoria $\,\sum_{i=1}^k p_i(1-p_i)$ cattura la **variazione** di difficoltà tra i singoli item.

2. **KR-21 (caso speciale)**  
   $$
   \mathrm{KR}_{21}
   \;=\;
   \frac{k}{k - 1}
   \Bigl(
     1 - \frac{k\,\bar{p}\,(1 - \bar{p})}{\sigma_X^2}
   \Bigr),
   $$
   con $\bar{p} = \frac{1}{k}\sum_{i=1}^k p_i$.

   - **Assume** esplicitamente $\,p_i = \bar{p}$ (cioè tutti gli item hanno la **stessa** difficoltà).  
   - In tal caso, la sommatoria di KR-20 si riduce a $k\,\bar{p}(1-\bar{p})$.

**In sintesi**: KR-21 è una **semplificazione** di KR-20 che diventa valida solo sotto l’assunzione (spesso irrealistica) di “item ugualmente difficili”. Per questo, nella pratica, KR-21 è meno utilizzato di KR-20, in quanto meno flessibile e più restrittivo.

### Formula di Spearman-Brown

La **formula di Spearman-Brown** consente di stimare l’affidabilità di un test composto da $p$ **item paralleli**, partendo dalla stima dell’affidabilità di un singolo item ($\rho_1$):

$$
\rho_p 
= \frac{p \,\rho_1}{(p - 1)\,\rho_1 + 1}.
$$ {#eq-spearman-brown}

In questo contesto, **$\rho_1$** rappresenta la correlazione (o affidabilità) media di un item rispetto al costrutto che si intende misurare, mentre **$p$** è il numero di item totali. Una derivazione della formula del coefficiente di Spearman-Brown sarà fornita nel capitolo @sec-fa-reliability.

::: {#exm-}
Stimiamo l’affidabilità della sottoscala Openness di un questionario, assumendo che gli item siano **paralleli**. Di seguito, otteniamo la matrice di correlazione degli item e calcoliamo la correlazione media tra di essi, usandola come $\rho_1$.

```{r}
R <- cor(
  bfi[c("O1", "O2r", "O3", "O4", "O5r")],
  use = "pairwise.complete.obs"
)
round(R, 2)

# Numero di item
p <- 5

# Calcolo della correlazione media tra item (stima di rho_1)

# Sostituiamo la diagonale con NA
diag(R) <- NA
# Calcoliamo la media di tutti i valori non-NA
ro_1 <- mean(R, na.rm = TRUE)

ro_1
```

Infine, applichiamo la formula di Spearman-Brown per ottenere la stima dell’affidabilità dell’intera scala:

```{r}
rho_p <- (p * ro_1) / ((p - 1) * ro_1 + 1)
rho_p
```
:::

In sintesi, la **Spearman-Brown** combina il numero degli item ($p$) con l’affidabilità di un singolo item ($\rho_1$) per stimare la **consistenza interna globale** di una scala composta da item paralleli. 

### L’indice $\omega$ di McDonald

L’indice $\omega$ di McDonald è un metodo per stimare l’affidabilità come consistenza interna in presenza di item congenerici. Maggiori approfondimenti sul suo calcolo e utilizzo saranno forniti nel @sec-fa-reliability.

## Affidabilità delle forme alternative

L’affidabilità delle forme alternative si riferisce alla **coerenza** tra i punteggi ottenuti da versioni diverse, ma **equivalenti**, di uno stesso test. Lo scopo principale di queste forme è ridurre gli effetti di pratica e memoria, ma è essenziale assicurarsi che le diverse versioni siano effettivamente **parallele** o almeno molto simili in termini di **contenuto** e **difficoltà**.

Per valutarne l’affidabilità, si esamina la **correlazione** tra i punteggi delle varie forme. Nei termini della CTT, se le forme alternative possono essere considerate **parallele** (cioè stesse medie e varianze vere, più varianze d’errore identiche), allora la loro correlazione corrisponde alla **affidabilità**.


## Affidabilità Test-Retest

L’affidabilità test-retest valuta **la stabilità** dei punteggi ottenuti somministrando lo stesso test a **distanza di tempo** sullo stesso gruppo di persone. Questo metodo è particolarmente indicato per costrutti considerati **relativamente stabili** (ad esempio, intelligenza e tratti di personalità). Di contro, risulta **inadeguato** per costrutti che possono subire **variazioni rapide o frequenti**, poiché i mutamenti nei punteggi potrebbero riflettere reali cambiamenti del costrutto piuttosto che una mancanza di attendibilità del test.

## Affidabilità dei Punteggi Compositi

L’affidabilità di un punteggio composito si riferisce alla **stabilità** di un indice costruito combinando più sottoscale o item. Di norma, un punteggio composito risulta **più affidabile** rispetto a ciascun sottopunteggio considerato individualmente, perché l’errore specifico di ogni sottoscale/item tende a “compensarsi” quando i punteggi sono tra loro **positivamente correlati**.

### Esempio numerico

Supponiamo di avere due subtest, $X_1$ e $X_2$, ciascuno con:  

- varianza del **punteggio vero** pari a $25$,  
- varianza dell’**errore di misura** pari a $10$,  
- covarianza tra i punteggi veri dei due subtest pari a $15$.

1. **Varianza del punteggio composito $Z$** (sola parte vera). Se $Z$ è la somma dei due subtest:  
   $$
   \mathrm{Var}(Z_{\text{vero}}) 
   = \mathrm{Var}(X_1^{\text{vero}}) + \mathrm{Var}(X_2^{\text{vero}}) + 2\,\mathrm{Cov}(X_1^{\text{vero}},X_2^{\text{vero}})
   = 25 + 25 + 2 \cdot 15 = 80.
   $$

2. **Varianza totale del composito** (punteggio vero + errore). Dato che ciascun subtest ha 10 di varianza d’errore, la varianza totale di $Z$ diventa:  
   $$
   \mathrm{Var}(Z_{\text{totale}}) 
   = \underbrace{25 + 25}_{\text{parte vera}} 
   + \underbrace{10 + 10}_{\text{parte errore}}
   + \underbrace{2 \cdot 15}_{\text{covarianza vera}}
   = 35 + 35 + 2 \cdot 15 = 100.
   $$

::: {.callout-tip title="Nota" collapse="true"}

La varianza totale del punteggio composito $Z$ è data dalla somma della varianza della componente vera, della varianza della componente di errore e del doppio della covarianza tra le due variabili, in base alle proprietà della varianza di una somma di variabili aleatorie.

Se consideriamo un punteggio composito $Z$ ottenuto come somma di due subtest $X_1$ e $X_2$, la varianza di $Z$ è data dalla formula:

$$
\text{Var}(Z) = \text{Var}(X_1 + X_2) = \text{Var}(X_1) + \text{Var}(X_2) + 2\cdot\text{Cov}(X_1, X_2)
$$

Ciascun subtest $X_1$ e $X_2$ è composto da due componenti:

- La parte vera $X_1^{\text{vero}}$ e $X_2^{\text{vero}}$
- La parte di errore $X_1^{\text{errore}}$ e $X_2^{\text{errore}}$

Possiamo quindi scrivere:

$$
X_1 = X_1^{\text{vero}} + X_1^{\text{errore}}
$$

$$
X_2 = X_2^{\text{vero}} + X_2^{\text{errore}}
$$

Utilizzando la proprietà della varianza per una somma di variabili, la varianza totale di $Z$ diventa:

$$
\text{Var}(Z) = \text{Var}(X_1^{\text{vero}} + X_1^{\text{errore}} + X_2^{\text{vero}} + X_2^{\text{errore}})
$$

Espandendo il termine:

$$
\text{Var}(Z) = \text{Var}(X_1^{\text{vero}}) + \text{Var}(X_2^{\text{vero}}) + \text{Var}(X_1^{\text{errore}}) + \text{Var}(X_2^{\text{errore}})
+ 2\cdot\text{Cov}(X_1^{\text{vero}}, X_2^{\text{vero}})
$$

Dato che si assume che gli errori non siano correlati tra loro né con le parti vere ($\text{Cov}(X_1^{\text{vero}}, X_1^{\text{errore}}) = 0$ e simili), tutte le altre covarianze sono nulle.

Ora, sostituendo i valori numerici:

1. La varianza della parte vera è:

   $$
   \text{Var}(X_1^{\text{vero}}) + \text{Var}(X_2^{\text{vero}}) = 25 + 25 = 50
   $$

2. La varianza della parte di errore è:

   $$
   \text{Var}(X_1^{\text{errore}}) + \text{Var}(X_2^{\text{errore}}) = 10 + 10 = 20
   $$

3. La covarianza tra le parti vere contribuisce con:

   $$
   2\cdot\text{Cov}(X_1^{\text{vero}}, X_2^{\text{vero}}) = 2 \cdot 15 = 30
   $$

Pertanto, la varianza totale di $Z$ è:

$$
\text{Var}(Z) = 50 + 20 + 30 = 100
$$

Il termine $2\cdot\text{Cov}(X_1^{\text{vero}}, X_2^{\text{vero}})$ appare perché, quando calcoliamo la varianza di una somma, dobbiamo considerare non solo le varianze individuali, ma anche la covarianza tra le due variabili, che rappresenta la loro interdipendenza. Questo termine indica quanto i due punteggi veri tendono a variare insieme in modo sistematico.

:::

3. **Affidabilità del composito**. Il coefficiente di affidabilità corrisponde alla proporzione di varianza vera rispetto alla varianza totale:  
   $$
   \rho_Z
   = \frac{\mathrm{Var}(Z_{\text{vero}})}{\mathrm{Var}(Z_{\text{totale}})}
   = \frac{80}{100} = 0.80.
   $$

4. **Confronto con un singolo subtest**. Per un solo subtest:  
   - $\mathrm{Var}(\text{vero}) = 25$.  
   - $\mathrm{Var}(\text{totale}) = 25 + 10 = 35$.  
   $$
   \rho_{X_i}
   = \frac{25}{35} 
   \approx 0.714.
   $$

L’affidabilità del **punteggio composito** ($0.80$) risulta quindi maggiore rispetto a quella di un **singolo subtest** ($\approx 0.71$). Questa differenza è dovuta al fatto che la covarianza vera tra i due subtest (15) **aumenta** la porzione di varianza attribuibile a differenze reali, mentre la parte di errore si “diluisce” nel composito.

### Dimostrazione pratica in R

Per confermare questi calcoli, possiamo simulare dati in R:

```{r}
set.seed(123)

n <- 10000 # numero di soggetti

# Creiamo due punteggi veri correlati
# Varianza = 25 => sd = 5
# Covarianza = 15 => correlazione = 15/(5*5) = 0.60
library(MASS)
Sigma_true <- matrix(c(
  25, 15,
  15, 25
), nrow = 2)
X_true <- MASS::mvrnorm(
  n = n,
  mu = c(0, 0),
  Sigma = Sigma_true
)

# Aggiungiamo errori indipendenti (varianza 10 => sd = ~3.162)
E1 <- rnorm(n, 0, sqrt(10))
E2 <- rnorm(n, 0, sqrt(10))

# Subtest osservati (X1 e X2)
X1 <- X_true[, 1] + E1
X2 <- X_true[, 2] + E2

# Composito
Z <- X1 + X2

# Calcoliamo varianza vera stimata
var_true_X1 <- var(X_true[, 1]) # Dovrebbe ~25
var_true_X2 <- var(X_true[, 2]) # Dovrebbe ~25
cov_true <- cov(X_true[, 1], X_true[, 2]) # Dovrebbe ~15
c(var_true_X1, var_true_X2, cov_true)

# Verifichiamo varianza e affidabilità osservate
var_X1 <- var(X1)
var_X2 <- var(X2)
var_Z <- var(Z)
c(var_X1, var_X2, var_Z)

# Stima affidabilità subtest 1
reliability_X1 <- var_true_X1 / var_X1

# Stima affidabilità subtest 2
reliability_X2 <- var_true_X2 / var_X2

# Stima affidabilità punteggio composito
# var(Z_vero) = var_true_X1 + var_true_X2 + 2 * cov_true
var_true_Z <- var_true_X1 + var_true_X2 + 2 * cov_true
reliability_Z <- var_true_Z / var_Z

cat("Affidabilità subtest 1:", round(reliability_X1, 3), "\n")
cat("Affidabilità subtest 2:", round(reliability_X2, 3), "\n")
cat("Affidabilità composito :", round(reliability_Z, 3), "\n")
```

Eseguendo questo codice, troveremo valori prossimi a quelli **attesi** teoricamente: circa 0.71 per i singoli subtest e circa 0.80 per il punteggio composito.

**Conclusioni.**

- **Maggiore affidabilità**: Combinare più subtest (o item) **correlati** tende a migliorare l’affidabilità del punteggio totale, perché aumenta la porzione di varianza vera rispetto alla varianza d’errore.  
- **Vantaggio pratico**: Un punteggio composito è spesso considerato più  **rappresentativo** del costrutto di interesse, specialmente in ambito psicometrico, dove il singolo item o subtest può essere soggetto a un errore specifico più elevato.


## Affidabilità dei Punteggi Differenza

Quando si parla di “punteggio differenza”, ci si riferisce generalmente a un valore ottenuto sottraendo due misure, ad esempio i punteggi di un test somministrato **prima** e **dopo** un trattamento (punteggi “pre” e “post”). L’affidabilità di questo punteggio differenza si collega alla sua **stabilità** come indicatore, ovvero alla coerenza con cui misura la variazione tra pre e post.

Nella pratica, tali punteggi differenza spesso risultano **meno affidabili** rispetto ai punteggi originali, soprattutto quando le due misure sono fortemente correlate. Questo accade perché, all’aumentare della correlazione tra i due test, gran parte della varianza in comune “si annulla” nella differenza, lasciando i punteggi differenza più esposti alla variabilità casuale.

### 1. Formula Generale 

Se $X$ e $Y$ sono le due misure (ad es. “pre” e “post”), l’affidabilità del punteggio differenza $(X - Y)$ può essere calcolata con la formula di Lord:

$$
r_{dd} 
= 
\frac{0.5\,[r_{xx} + r_{yy}] - r_{xy}}{1 - r_{xy}},
$$ {#eq-diff-scores-lord}

dove:

- $r_{xx}$ e $r_{yy}$ sono le affidabilità (per esempio Cronbach $\alpha$ o test-retest) delle due misure $X$ e $Y$;  
- $r_{xy}$ è la correlazione tra $X$ e $Y$;  
- $r_{dd}$ è l’affidabilità del punteggio differenza $(X - Y)$.

::: {.callout-tip title="Dimostrazione" collapse="true"}

**Passo 1: Definizione dell'affidabilità di una variabile**

L'affidabilità di una variabile è definita come la proporzione di varianza vera sulla varianza totale. Se una variabile $X$ è composta da una componente vera $T_X$ e un errore $E_X$, possiamo scrivere:

$$
X = T_X + E_X
$$

L'affidabilità di $X$ è data da:

$$
r_{xx} = \frac{\text{Var}(T_X)}{\text{Var}(X)}
$$

Analogamente, per $Y$:

$$
Y = T_Y + E_Y
$$

$$
r_{yy} = \frac{\text{Var}(T_Y)}{\text{Var}(Y)}
$$

**Passo 2: Definizione del punteggio differenza**

Il punteggio differenza è definito come:

$$
D = X - Y
$$

La varianza del punteggio differenza è:

$$
\text{Var}(D) = \text{Var}(X - Y)
$$

Utilizzando la proprietà della varianza:

$$
\text{Var}(D) = \text{Var}(X) + \text{Var}(Y) - 2\text{Cov}(X, Y)
$$

Poiché la covarianza è:

$$
\text{Cov}(X, Y) = r_{xy} \cdot \sigma_X \cdot \sigma_Y
$$

si può riscrivere la varianza del punteggio differenza come:

$$
\text{Var}(D) = \sigma_X^2 + \sigma_Y^2 - 2 r_{xy} \sigma_X \sigma_Y
$$

**Passo 3: Calcolo della componente vera del punteggio differenza**

Analogamente a quanto fatto per $X$ e $Y$, scriviamo la componente vera del punteggio differenza:

$$
T_D = T_X - T_Y
$$

La sua varianza è:

$$
\text{Var}(T_D) = \text{Var}(T_X - T_Y)
$$

Utilizzando la stessa proprietà della varianza:

$$
\text{Var}(T_D) = \text{Var}(T_X) + \text{Var}(T_Y) - 2\text{Cov}(T_X, T_Y)
$$

Poiché l'affidabilità è definita come il rapporto tra la varianza vera e la varianza osservata:

$$
\text{Var}(T_X) = r_{xx} \text{Var}(X), \quad \text{Var}(T_Y) = r_{yy} \text{Var}(Y)
$$

$$
\text{Cov}(T_X, T_Y) = r_{xy} \sqrt{r_{xx} r_{yy}} \sigma_X \sigma_Y
$$

Quindi:

$$
\text{Var}(T_D) = r_{xx} \sigma_X^2 + r_{yy} \sigma_Y^2 - 2 r_{xy} \sqrt{r_{xx} r_{yy}} \sigma_X \sigma_Y
$$

**Passo 4: Calcolo dell'affidabilità del punteggio differenza**

L'affidabilità del punteggio differenza è data da:

$$
r_{dd} = \frac{\text{Var}(T_D)}{\text{Var}(D)}
$$

Sostituendo le espressioni trovate:

$$
r_{dd} = \frac{r_{xx} \sigma_X^2 + r_{yy} \sigma_Y^2 - 2 r_{xy} \sqrt{r_{xx} r_{yy}} \sigma_X \sigma_Y}{\sigma_X^2 + \sigma_Y^2 - 2 r_{xy} \sigma_X \sigma_Y}
$$

Se assumiamo che $\sigma_X = \sigma_Y$ (varianza uguale per entrambe le misure), possiamo semplificare la formula.

**Semplificazione della varianza del punteggio differenza**

Abbiamo definito la varianza del punteggio differenza come:

$$
\text{Var}(D) = \sigma_X^2 + \sigma_Y^2 - 2 r_{xy} \sigma_X \sigma_Y
$$

Sostituendo $\sigma_X = \sigma_Y = \sigma$:

$$
\text{Var}(D) = \sigma^2 + \sigma^2 - 2 r_{xy} \sigma \sigma
$$

$$
\text{Var}(D) = 2\sigma^2 (1 - r_{xy})
$$

**Semplificazione della varianza della componente vera del punteggio differenza**

Analogamente, la varianza della componente vera del punteggio differenza è:

$$
\text{Var}(T_D) = r_{xx} \sigma_X^2 + r_{yy} \sigma_Y^2 - 2 r_{xy} \sqrt{r_{xx} r_{yy}} \sigma_X \sigma_Y
$$

Sostituendo $\sigma_X = \sigma_Y = \sigma$:

$$
\text{Var}(T_D) = r_{xx} \sigma^2 + r_{yy} \sigma^2 - 2 r_{xy} \sqrt{r_{xx} r_{yy}} \sigma \sigma
$$

$$
\text{Var}(T_D) = \sigma^2 \left( r_{xx} + r_{yy} - 2 r_{xy} \sqrt{r_{xx} r_{yy}} \right)
$$

**Calcolo dell'affidabilità del punteggio differenza**

L'affidabilità del punteggio differenza è:

$$
r_{dd} = \frac{\text{Var}(T_D)}{\text{Var}(D)}
$$

Sostituendo le espressioni ottenute per $\text{Var}(T_D)$ e $\text{Var}(D)$:

$$
r_{dd} = \frac{\sigma^2 \left( r_{xx} + r_{yy} - 2 r_{xy} \sqrt{r_{xx} r_{yy}} \right)}{2\sigma^2 (1 - r_{xy})}
$$

Semplificando $\sigma^2$:

$$
r_{dd} = \frac{r_{xx} + r_{yy} - 2 r_{xy} \sqrt{r_{xx} r_{yy}}}{2(1 - r_{xy})}
$$

Questa è la formula intermedia che, riorganizzata, porta alla **formula di Lord**.

**Passo 5: Ottenimento della formula di Lord**

Riscriviamo il numeratore:

$$
r_{xx} + r_{yy} - 2 r_{xy} \sqrt{r_{xx} r_{yy}} = (r_{xx} + r_{yy}) - 2 r_{xy} \sqrt{r_{xx} r_{yy}}
$$

Fattorizziamo il numeratore estraendo un fattore $0.5$:

$$
r_{dd} = \frac{0.5 (r_{xx} + r_{yy}) - r_{xy} \sqrt{r_{xx} r_{yy}}}{1 - r_{xy}}
$$

Se $r_{xx} = r_{yy}$, possiamo riscrivere ulteriormente come:

$$
r_{dd} = \frac{0.5 (r_{xx} + r_{yy}) - r_{xy}}{1 - r_{xy}}
$$

Questa è la **formula di Lord**, che descrive l'affidabilità del punteggio differenza in termini delle affidabilità delle misure individuali e della loro correlazione.

---

**Intuizione:**  

- Se $r_{xy}$ è alto, significa che le due misure sono molto correlate e la differenza tra loro ha una varianza ridotta, quindi la sua affidabilità diminuisce.  
- Se $r_{xx}$ e $r_{yy}$ sono elevate, significa che le misure originali sono più affidabili, aumentando l'affidabilità del punteggio differenza.  
- Se $r_{xy}$ è basso, il punteggio differenza cattura una maggiore variabilità vera, aumentando $r_{dd}$.
:::


**Interpretazione:**  

- se $r_{xy}$ è molto alto, allora la parte di varianza in comune tra i due test è ampia e, di conseguenza, la differenza $(X - Y)$ finisce per catturare soprattutto la varianza dovuta all’errore di misura, perdendo stabilità (affidabilità più bassa); 
- viceversa, quando la correlazione è bassa, $X$ e $Y$ condividono meno varianza (e di conseguenza meno “errore comune”), permettendo al punteggio differenza di essere relativamente più stabile.


### Esempio in R (funzione `rdd()`)

Un modo per osservare come varia $r_{dd}$ al variare di $r_{xy}$ è creare una funzione in R che applichi la formula di Lord. Di seguito un esempio:

```{r}
# Definizione della formula
rdd <- function(rxx, ryy, rxy) {
  (0.5 * (rxx + ryy) - rxy) / (1 - rxy)
}

# Esempio: rxx = 0.9, ryy = 0.8, rxy varia da 0.01 a 0.81
rxx <- 0.9
ryy <- 0.8
corr_values <- seq(0.01, 0.81, by = 0.1)

rdd_values <- rdd(rxx, ryy, corr_values)
rdd_values
```

Stampando `rdd_values` si nota che l’affidabilità del punteggio differenza **diminuisce** man mano che aumenta la correlazione tra $X$ e $Y$, confermando la spiegazione teorica.

## Esempio Completo di Simulazione in R

Per dimostrare in modo empirico la relazione tra correlazione delle misure e affidabilità del punteggio differenza, possiamo simulare due test con caratteristiche desiderate (affidabilità, correlazione, varianza) e confrontare l’affidabilità empirica con il valore teorico previsto dalla formula di Lord.

**Passaggio 1: Impostazione dei Parametri.**

Definiamo:

- $r_{xx}$ e $r_{yy}$: affidabilità di $X$ e $Y$.  
- $r_{xy}$: correlazione vera tra i costrutti misurati da $X$ e $Y$.  
- $\mathrm{Var}(T_x)$ e $\mathrm{Var}(T_y)$: varianze delle parti “vere” di $X$ e $Y$.  
- `n`: numero di soggetti da simulare.  

```{r}
set.seed(123)

# Parametri
r_xx <- 0.9 # affidabilità X
r_yy <- 0.8 # affidabilità Y
r_xy <- 0.5 # correlazione vera tra i costrutti
n <- 100000

# Varianze delle parti vere
var_Tx <- 100
var_Ty <- 100
sd_Tx <- sqrt(var_Tx) # 10
sd_Ty <- sqrt(var_Ty) # 10

# Covarianza vera
cov_TxTy <- r_xy * sd_Tx * sd_Ty # = 50

# Matrice var-cov per (T_x, T_y)
Sigma_true <- matrix(
  c(
    var_Tx, cov_TxTy,
    cov_TxTy, var_Ty
  ),
  nrow = 2
)
```

**Passaggio 2: Calcolo delle Varianze d’Errore.**

Dalla Teoria Classica dei Test (TCT) sappiamo che:

$$
r_{xx} 
= 
\frac{\mathrm{Var}(T_x)}{\mathrm{Var}(T_x) + \mathrm{Var}(E_x)}
\quad\Longrightarrow\quad
\mathrm{Var}(E_x) = \frac{\mathrm{Var}(T_x)}{r_{xx}} - \mathrm{Var}(T_x).
$$

In modo analogo per $Y$. Queste varianze d’errore ci serviranno per generare i punteggi osservati.

```{r}
var_Ex <- var_Tx * (1 / r_xx - 1)
var_Ey <- var_Ty * (1 / r_yy - 1)

sd_Ex <- sqrt(var_Ex)
sd_Ey <- sqrt(var_Ey)

var_Ex
var_Ey
```

**Passaggio 3: Generazione dei Dati.**

1. Generiamo la parte vera $(T_x, T_y)$ con varianze e covarianza definite da `Sigma_true`.  
2. Generiamo gli errori $(E_x, E_y)$ come variabili indipendenti, con le varianze appena trovate.  
3. Sommiamo le parti vere e gli errori per ottenere i punteggi osservati $X$ e $Y$.  
4. Calcoliamo la differenza $D = X - Y$.

```{r}
# (1) Parte vera
T <- MASS::mvrnorm(
  n = n,
  mu = c(0, 0),
  Sigma = Sigma_true
)

T_x <- T[, 1]
T_y <- T[, 2]

# (2) Errori indipendenti
E_x <- rnorm(n, mean = 0, sd = sd_Ex)
E_y <- rnorm(n, mean = 0, sd = sd_Ey)

# (3) Punteggi osservati
X <- T_x + E_x
Y <- T_y + E_y

# (4) Punteggio differenza
D <- X - Y
```

**Passaggio 4: Stima dell’Affidabilità Empirica di $D$.**

In TCT, l’affidabilità è il rapporto tra la varianza vera e la varianza totale.  
Per il punteggio differenza $D$:

- parte vera: $T_d = T_x - T_y$;  
- varianza vera: $\mathrm{Var}(T_d)$;  
- varianza totale osservata: $\mathrm{Var}(D)$;  
- affidabilità: 

  $$
  r_{dd} = \frac{\mathrm{Var}(T_d)}{\mathrm{Var}(D)}.
  $$

```{r}
T_d <- T_x - T_y
var_Td <- var(T_d)
var_D <- var(D)

rdd_empirical <- var_Td / var_D
rdd_empirical
```

**Passaggio 5: Confronto con la Formula di Lord.**

Richiamiamo la formula teorica:

$$
r_{dd} 
= \frac{0.5 \,[r_{xx} + r_{yy}] - r_{xy}}{1 - r_{xy}}.
$$

E applichiamola ai nostri parametri:

```{r}
rdd_theoretical <- (0.5 * (r_xx + r_yy) - r_xy) / (1 - r_xy)

cat("Affidabilità differenza (empirica):       ", round(rdd_empirical, 3), "\n")
cat("Affidabilità differenza (formula Lord):   ", round(rdd_theoretical, 3), "\n")
```

Idealmente, i due valori coincidono (o sono molto simili) se la simulazione rispecchia bene i presupposti della formula (grande campione, corretta generazione di errori, ecc.).

In sintesi:

1. **all’aumentare di $r_{xy}$**, diminuisce in genere l’affidabilità del punteggio differenza, poiché crescono le porzioni di varianza in comune tra $X$ e $Y$, che si sottraggono a vicenda;   
2. **se $r_{xy}$ è più basso**, i punteggi di $X$ e $Y$ condividono meno varianza: la parte vera delle differenze risulta più “solida”, innalzando il potenziale di affidabilità del punteggio differenza.

Questo esempio dimostra chiaramente perché i punteggi differenza (ad esempio, nei confronti pre-post) possano spesso presentare un’affidabilità inferiore rispetto ai singoli punteggi originali, specialmente in presenza di misure molto simili o fortemente correlate.


## Scelta del Coefficiente di Affidabilità

La selezione del coefficiente di affidabilità più adeguato dipende dal tipo di test, dal contesto d’uso e dallo scopo della misurazione.

1. **Affidabilità Test-Retest**  
   - Valuta la **stabilità nel tempo** della misura.  
   - Indicata quando si vuole verificare se un costrutto permane stabile (o se varia) in assenza di fattori esterni che possano influenzarlo.

2. **Consistenza Interna** (ad esempio, Cronbach’s Alpha, KR-20)  
   - Appropriata con **una singola somministrazione** del test.  
   - Utile soprattutto quando il test è progettato per misurare **un unico costrutto** e gli item dovrebbero, teoricamente, essere omogenei tra loro.

3. **Affidabilità di Forme Alternative**  
   - Rilevante quando esistono **versioni diverse (parallele o equivalenti)** di uno stesso test, e si desidera misurare se forniscono punteggi coerenti.  
   - Aiuta a ridurre l’effetto dell’apprendimento o della familiarità con un’unica forma del test.

4. **Affidabilità Inter-Valutatori**  
   - Necessaria in presenza di **giudizi soggettivi** (ad esempio, valutazioni cliniche o osservazioni dirette).  
   - Stima l’accordo tra più valutatori, misurando in che misura le discrepanze siano dovute a differenze reali nel comportamento/situazione osservata e non a differenze di criterio tra osservatori.


### Linee Guida Generali sui Valori di Affidabilità

Sebbene le soglie di accettabilità varino in base allo scopo specifico del test e al contesto (clinico, educativo, di ricerca), le indicazioni più comuni sono:

- **≥ 0.90**: raccomandato per decisioni di grande importanza, come diagnosi cliniche o valutazioni ad alto impatto.  
- **≥ 0.80**: standard desiderabile per test di rendimento o di personalità, dove serve un livello di precisione piuttosto elevato.  
- **≥ 0.70**: spesso considerato accettabile per screening didattici o contesti in cui il rischio di errore è più tollerabile.  
- **≥ 0.60**: talvolta considerato sufficiente in contesti di ricerca di base o di gruppo, ma occorre molta cautela al di sotto di 0.70 perché la variabilità dovuta all’errore può influenzare notevolmente l’interpretazione dei punteggi.

In sintesi, la decisione su quale coefficiente adottare e su quale soglia considerare “accettabile” deve sempre tener conto:  
- della **finalità** del test (diagnostico, selettivo, esplorativo);  
- della **natura del costrutto** misurato (omogeneo o multi-dimensionale);  
- del **contesto** (clinico, organizzativo, educativo, di ricerca).  

Un’analisi delle esigenze e degli obiettivi specifici permette di scegliere con maggiore precisione quale forma di affidabilità risulti più indicata e quali valori siano ritenuti adeguati.

## Riflessioni Conclusive

La stima dell’affidabilità non costituisce un mero esercizio metodologico, bensì un **passaggio cruciale** per validare la solidità di qualsiasi misura psicologica. Scegliere l’indice più adeguato in base al contesto, al tipo di test e alla natura del costrutto consente di:

- **ridurre il rischio di interpretazioni errate**, poiché una misura non affidabile mette in dubbio i risultati e le eventuali decisioni che ne conseguono;  
- **contestualizzare correttamente i risultati**, valutando come il tipo di errore (temporale, inter-valutatore, differenze di forme, ecc.) possa incidere sull’uso finale del punteggio;  
- **guidare l’ottimizzazione del test**, rivelando quali aree dello strumento possono essere migliorate per incrementare la coerenza e la precisione della misurazione.

Integrare queste considerazioni nella fase di progettazione e valutazione di una misura consente di sviluppare strumenti più sensibili e appropriati alle finalità dell’indagine psicologica, garantendo **robustezza** e **credibilità** alle conclusioni tratte.

## Session Info

```{r}
sessionInfo()
```


