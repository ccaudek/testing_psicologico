# Il modello unifattoriale {#sec-fa-unifactor-model}

**Prerequisiti**

- Leggere il capitolo 6, *Factor Analysis and Principal Component Analysis*, del testo *Principles of psychological assessment* di @petersen2024principles. 

**Concetti e Competenze Chiave**

- Correlazione parziale
- Teoria dei due fattori
- Annullamento della tetrade

**Preparazione del Notebook**

```{r}
# Carica il file _common.R per impostazioni di pacchetti e opzioni
here::here("code", "_common.R") |> source()

# Carica pacchetti aggiuntivi
pacman::p_load(lavaan, corrplot, tidyr, kableExtra, lavaanPlot, lavaanExtra)
```

## Introduzione

L’analisi fattoriale è una tecnica statistica che permette di spiegare le correlazioni tra variabili osservate attraverso la loro dipendenza da uno o più fattori latenti. In questo modello, le $p$ variabili osservate (ad esempio, item di un questionario o indicatori comportamentali) sono considerate **condizionalmente indipendenti, dato l’insieme di $m$ fattori comuni non osservabili**. L’obiettivo principale è interpretare tali fattori come costrutti teorici latenti sottostanti alle risposte osservate.

::: {.callout-note title="Nota sulla indipendenza condizionale" collapse="true"}
In un modello fattoriale, si dice che le variabili osservate $y_1, y_2, \dots, y_p$ sono *condizionalmente indipendenti* dato il fattore latente $\xi$ quando, una volta noto il valore di $\xi$, la conoscenza di una qualunque delle variabili osservate non fornisce informazioni aggiuntive sulle altre. Formalmente, per ogni coppia $i \neq j$ si ha:

$$
P(y_i, y_j \mid \xi) = P(y_i \mid \xi) \cdot P(y_j \mid \xi).
$$

Questa proprietà implica che tutte le covarianze tra le variabili osservate sono spiegate esclusivamente dal fattore comune. In assenza di tale fattore, le variabili osservate risulterebbero tra loro incorrelate. L’indipendenza condizionale è una delle assunzioni centrali del modello fattoriale, e giustifica la possibilità di interpretare la covarianza tra item come effetto della loro dipendenza condivisa da un unico costrutto latente. Un esempio numerico è fornito nella @sec-fa-cor-parz.
:::

Ad esempio, si può utilizzare l’analisi fattoriale per spiegare le correlazioni tra le prestazioni di un gruppo di individui in diversi compiti cognitivi attraverso un singolo fattore latente come l’intelligenza generale. In questo modo, l’analisi fattoriale consente di identificare i costrutti a cui gli item si riferiscono e di quantificare quanto ciascun item contribuisca a rappresentarli.

Il modello può prevedere uno (modello unifattoriale, $m = 1$) o più fattori latenti (modello multifattoriale, $m > 1$). In questo capitolo ci concentreremo sul modello unifattoriale, che assume l’esistenza di un unico fattore comune che influenza tutte le variabili osservate.

Nel contesto dell’analisi fattoriale, le variabili latenti rappresentano **costrutti teorici non direttamente osservabili**, come abilità cognitive o tratti psicologici. Queste variabili riflettono le comunanze sottostanti a un insieme di indicatori osservabili, chiamati variabili manifeste. Le prime sono rappresentate nei diagrammi di percorso come cerchi, mentre le seconde come quadrati.

Il legame tra fattore latente e variabili manifeste è descritto tramite i **carichi fattoriali** ($\lambda$), che quantificano l’intensità dell’influenza esercitata dal fattore latente su ciascuna variabile osservata. Il valore di $\lambda$ indica la proporzione di varianza della variabile osservata che è spiegata dal fattore comune: più alto è il carico, maggiore è la rappresentatività dell’item rispetto al costrutto latente.

Dal punto di vista matematico, ciascuna misura osservabile $y$ è modellata come una combinazione lineare del fattore latente $\xi$, ponderato dal carico $\lambda$, più un termine di errore specifico $\delta$, che rappresenta la componente di varianza non spiegata dal fattore comune.

Un esempio intuitivo può chiarire il significato di questa decomposizione: immaginate di usare una bilancia imprecisa per misurare il peso corporeo. Ogni misurazione ($y$) rifletterà in parte il peso reale della persona ($\xi$), ma anche un errore casuale ($\delta$) dovuto all’inaffidabilità dello strumento.

Quando si dispone di più variabili osservabili $y$ che fanno riferimento a un medesimo costrutto latente $\xi$, diventa possibile stimare con maggiore precisione sia il punteggio latente sia la componente di errore. Questo consente di ottenere una rappresentazione più affidabile del costrutto teorico di interesse e di migliorare l’interpretazione psicometrica dei dati.

## Modello monofattoriale

Nel caso di un solo fattore comune e $p$ variabili manifeste $y_i$, il modello assume la forma:

$$
\begin{equation}
y_i = \mu_i + \lambda_{i} \xi + 1 \cdot \delta_i \qquad i = 1, \dots, p,
\end{equation}
$$ {#eq-mod-unifattoriale}

dove:

- $\mu_i$ è la media della variabile osservata $y_i$,
- $\xi$ è il fattore latente comune a tutte le variabili osservate,
- $\lambda_i$ è la saturazione fattoriale, ovvero il peso del fattore comune $\xi$ sulla variabile $y_i$,
- $\delta_i$ è il fattore specifico (o errore unico) associato a $y_i$, indipendente da $\xi$.

Si assume che:

- il fattore comune $\xi$ abbia media zero e varianza unitaria,
- i fattori specifici $\delta_i$ abbiano media zero, varianza $\psi_i$, e siano indipendenti tra loro e dal fattore comune.

In questo modello, ciascuna variabile osservata $y_i$ è influenzata da una componente condivisa ($\xi$) e da una componente specifica ($\delta_i$), che rappresenta la parte di varianza unica della variabile non spiegata dal fattore comune.

Il modello di analisi fattoriale può ricordare formalmente il modello di regressione lineare, ma esistono alcune differenze fondamentali. Innanzitutto, sia il fattore comune $\xi$ sia i fattori specifici $\delta_i$ sono variabili latenti, cioè non osservabili direttamente. Di conseguenza, tutti i termini presenti nel lato destro dell’equazione sono incogniti. Inoltre, i due modelli perseguono obiettivi differenti: la regressione lineare mira a identificare variabili indipendenti osservabili che spiegano la varianza di una variabile dipendente, mentre l’analisi fattoriale cerca di individuare una o più variabili latenti che rendano conto della covarianza tra un insieme di variabili osservate.

Per semplicità, si assume spesso che le variabili osservate siano centrate, cioè che la loro media sia pari a zero ($\mu_i = 0$). Questo equivale a considerare ogni $y_i$ come uno scarto rispetto alla propria media. Il modello unifattoriale può quindi essere riscritto nella forma:

$$
\begin{equation}
y_i - \mu_i = \lambda_i \xi + 1 \cdot \delta_i,
\end{equation}
$$ {#eq-mod-monofattoriale}

dove:

- $\lambda_i$ è la saturazione (o carico) fattoriale della variabile $y_i$ sul fattore comune $\xi$,
- $\delta_i$ rappresenta la componente specifica della variabile $y_i$, ovvero la parte di varianza non condivisa con le altre variabili.

Si assume che:

- il fattore comune $\xi$ abbia media zero e varianza unitaria ($\mathbb{E}[\xi] = 0$, $\mathrm{Var}(\xi) = 1$),
- ciascun fattore specifico $\delta_i$ abbia media zero, varianza $\psi_i$ e sia incorrelato con $\xi$ e con gli altri $\delta_j$ (per $j \ne i$).

Sotto queste assunzioni, l’interdipendenza tra le variabili osservate è interamente spiegata dalla loro dipendenza dal fattore comune. I fattori specifici, invece, rendono conto della varianza residua non condivisa.

Questa formulazione permette di derivare espressioni analitiche per quantità fondamentali come:

- la covarianza tra una variabile osservata $y_i$ e il fattore comune $\xi$,
- la varianza della variabile osservata $y_i$,
- la covarianza tra due variabili osservate $y_i$ e $y_k$.

L’obiettivo di questo capitolo è analizzare nel dettaglio tali quantità e mostrare come esse riflettano la struttura latente imposta dal modello unifattoriale.

## Correlazione parziale

Prima di introdurre formalmente il modello statistico dell’analisi fattoriale, è utile chiarire il concetto di **correlazione parziale**, centrale per comprendere la logica della separazione tra fattori comuni e specifici.

L’analisi fattoriale è spesso fatta risalire agli studi di Charles Spearman. Nel 1904, Spearman pubblicò un articolo intitolato *"General Intelligence, Objectively Determined and Measured"*, nel quale propose la Teoria dei Due Fattori. In quel lavoro, mostrò che era possibile identificare un fattore latente a partire da una matrice di correlazioni osservate, utilizzando la tecnica dell’**annullamento della tetrade** (*tetrad differences*). Questa tecnica si basa sul principio della correlazione parziale e mira a verificare se, una volta controllati gli effetti di variabili latenti (i fattori $\xi_j$), le correlazioni tra le variabili osservate $Y_i$ risultino nulle.

Per illustrare il concetto, consideriamo un esempio con tre variabili: $Y_1$, $Y_2$ e una variabile comune $F$. La correlazione semplice $r_{12}$ tra $Y_1$ e $Y_2$ può riflettere l’influenza condivisa di $F$ su entrambe. La *correlazione parziale* tra $Y_1$ e $Y_2$ al netto di $F$ misura invece la relazione diretta tra $Y_1$ e $Y_2$ una volta rimosso l’effetto lineare di $F$ da entrambe le variabili.

Per farlo, si calcolano i residui delle regressioni di $Y_1$ e $Y_2$ su $F$, ovvero le componenti ortogonali a $F$. Ad esempio, nel caso di $Y_1$:

$$
Y_1 = b_{01} + b_{11}F + E_1,
$$  {#eq-mod-reg-mult-fa}

dove $E_1$ è la parte di $Y_1$ linearmente indipendente da $F$. Ripetendo l’operazione per $Y_2$ si ottiene un residuo $E_2$, anch’esso ortogonale a $F$. La correlazione di Pearson tra $E_1$ ed $E_2$ rappresenta la correlazione parziale tra $Y_1$ e $Y_2$ dato $F$.

La stessa quantità può essere calcolata direttamente a partire dalle correlazioni semplici tra le tre variabili, tramite la formula:

$$
\begin{equation}
r_{1,2 \mid F} = \frac{r_{12} - r_{1F}r_{2F}}{\sqrt{(1-r_{1F}^2)(1-r_{2F}^2)}}.
\end{equation}
$$  {#eq-corr-parz}

Questa formula mostra che la correlazione parziale è ottenuta sottraendo al valore osservato di $r_{12}$ il prodotto delle correlazioni tra $Y_1$ e $F$ e tra $Y_2$ e $F$, e normalizzando per la varianza residua non spiegata da $F$.

### Esempio numerico {#sec-fa-cor-parz}

Supponiamo di avere una variabile $f$ generata da una distribuzione normale:

```{r}
set.seed(123)
n <- 1000
f <- rnorm(n, 24, 12)
```

Costruiamo due variabili, $y_1$ e $y_2$, come combinazioni lineari di $f$ più un errore casuale:

```{r}
y1 <- 10 + 7 * f + rnorm(n, 0, 50)
y2 <- 3  + 2 * f + rnorm(n, 0, 50)
```

Le tre variabili sono correlate; in particolare $y_1$ e $y_2$ hanno una correlazione semplice $r_{12} = 0.380$:

```{r}
Y <- cbind(y1, y2, f)
cor(Y) |>
    round(3)
```

Per calcolare la correlazione parziale tra $y_1$ e $y_2$ al netto di $f$, eseguiamo due modelli di regressione lineare:

```{r}
fm1 <- lm(y1 ~ f)
fm2 <- lm(y2 ~ f)
```

Ogni osservazione viene così scomposta in due componenti: i valori adattati $\hat{y}_i$ (dipendenti da $f$) e i residui $e_i$ (indipendenti da $f$):

```{r}
cbind(y1, y1.hat=fm1$fit, e=fm1$res, sum=fm1$fit+fm1$res) |>
    head() |>
    round(3)
```

La correlazione parziale tra $y_1$ e $y_2$ è quindi calcolabile come la correlazione tra i residui:

```{r}
cor(fm1$res, fm2$res)
```

Nel nostro esempio, la correlazione parziale tra $y_1$ e $y_2$ al netto di $f$ è $r_{12|f} = 0.028$, praticamente nulla. Questo indica che la correlazione osservata tra $y_1$ e $y_2$ ($r = 0.380$) era dovuta esclusivamente all’influenza condivisa di $f$ su entrambe.

In altre parole, una volta eliminato l’effetto di $f$, non rimane alcuna associazione lineare diretta tra $y_1$ e $y_2$. Le due variabili sono quindi **condizionalmente indipendenti** dato $f$: le componenti di $y_1$ e $y_2$ non spiegate da $f$ risultano tra loro incorrelate.

Infine, possiamo verificare che il valore ottenuto con la formula della correlazione parziale coincide con quello calcolato sui residui:

```{r}
R <- cor(Y)

(R[1, 2] - R[1, 3] * R[2, 3]) / 
  sqrt((1 - R[1, 3]^2) * (1- R[2, 3]^2)) |>
  round(3)
```

Il risultato conferma la coerenza tra il calcolo algebrico e il metodo dei residui. Questo esempio evidenzia come la correlazione parziale sia uno strumento fondamentale per isolare le relazioni dirette tra variabili, rimuovendo l’influenza di fattori comuni non osservabili.

## Principio base dell'analisi fattoriale

Oggi, l'inferenza statistica in ambito fattoriale si basa prevalentemente su metodi di stima per massima verosimiglianza, ottenuti attraverso procedure iterative. Tuttavia, nelle fasi iniziali dello sviluppo dell’analisi fattoriale, l’estrazione dei fattori si fondava su proprietà invarianti che il modello fattoriale impone alla matrice di covarianza (o di correlazione) delle variabili osservate. Tra queste proprietà, la più nota è quella dell’*annullamento della tetrade*, caratteristica distintiva dei modelli a un solo fattore.

Una *tetrade* è una combinazione lineare di quattro correlazioni tra variabili osservate. Se le correlazioni tra le variabili possono essere spiegate da un singolo fattore latente comune, allora è possibile costruire combinazioni di correlazioni la cui differenza si annulla. In altri termini, il modello a un fattore impone vincoli strutturali tali da rendere nulle alcune espressioni algebriche (le tetradi), che dipendono unicamente dalla matrice di correlazione.

L’analisi fattoriale può dunque essere formulata come un problema di ricerca di un insieme ristretto di variabili latenti ($m < p$), tali che, una volta controllato il loro effetto, tutte le correlazioni parziali tra le variabili osservate $y_i$ risultino nulle. Se l’annullamento delle correlazioni parziali è confermato, lo psicologo può concludere che esistono $m$ fattori latenti capaci di spiegare la struttura di covarianza del sistema osservato.

Per chiarire il principio, consideriamo la seguente matrice di correlazione, costruita in modo da riflettere un modello con un unico fattore latente $\xi$:

|       | $\xi$   | $y_1$ | $y_2$ | $y_3$ | $y_4$ | $y_5$ |
|-------|---------|-------|-------|-------|-------|-------|
| $\xi$ | **1.00** |       |       |       |       |       |
| $y_1$ | **0.90** | 1.00  |       |       |       |       |
| $y_2$ | **0.80** | 0.72  | 1.00  |       |       |       |
| $y_3$ | **0.70** | 0.63  | 0.56  | 1.00  |       |       |
| $y_4$ | **0.60** | 0.54  | 0.48  | 0.42  | 1.00  |       |
| $y_5$ | **0.50** | 0.45  | 0.40  | 0.35  | 0.30  | 1.00  |

In questa matrice, ogni variabile $y_i$ ha una correlazione positiva con $\xi$, e le correlazioni tra le $y_i$ sono coerenti con un modello in cui $\xi$ agisce come unica fonte comune di covarianza. Per esempio, la correlazione parziale tra $y_3$ e $y_5$ al netto di $\xi$ risulta:

$$
\begin{align}
  r_{35 \mid \xi} &= \frac{r_{35} - r_{3\xi}r_{5\xi}}{\sqrt{(1-r_{3\xi}^2)(1-r_{5\xi}^2)}} \notag \\[12pt]
  &= \frac{0.35 - 0.7 \times 0.5}{\sqrt{(1 - 0.7^2)(1 - 0.5^2)}} = 0. \notag
\end{align}
$$

Lo stesso vale per qualunque altra coppia di variabili: tutte le correlazioni parziali condizionate a $\xi$ sono nulle, cioè $r_{ij \mid \xi} = 0$ per ogni $i \ne j$.

In questa matrice, ci sono $p(p-1)/2 = 5(5-1)/2 = 10$ correlazioni tra le variabili osservate, tutte spiegate dal singolo fattore $\xi$. Questo non sorprende, poiché la matrice è stata costruita appositamente per rispettare questa proprietà.

Tuttavia, immaginiamo ora di trovarci in una situazione reale, in cui osserviamo soltanto le variabili $y_1, \dots, y_5$, ma non abbiamo accesso diretto a $\xi$. Possiamo allora porci la seguente domanda: esiste una variabile latente $\xi$ tale che, se ne controllassimo l’effetto, tutte le correlazioni parziali tra le variabili osservate risulterebbero nulle?

Se una tale variabile esiste, e riesce a spiegare completamente le interdipendenze osservate tra le $y_i$, essa assume lo status di **fattore**.

::: {#def-}
Un **fattore** è una variabile latente non osservabile che, una volta controllata, rende significativamente nulle tutte le correlazioni parziali tra le variabili manifeste.
:::

## Vincoli sulle correlazioni

Come possiamo determinare se esiste una variabile latente $\xi$ in grado di spiegare tutte le correlazioni osservate tra le variabili manifeste, rendendo nulle le loro correlazioni parziali? Possiamo partire dalla formula della correlazione parziale, già introdotta in precedenza (vedi @eq-corr-parz), e riscriverla per esprimere la correlazione parziale tra due variabili osservate $y_i$ e $y_j$ condizionata a $\xi$:

$$
\begin{align}
r_{ij \mid \xi} = \frac{r_{ij} - r_{i\xi} \, r_{j\xi}}{\sqrt{(1 - r_{i\xi}^2)(1 - r_{j\xi}^2)}}.
\end{align}
$$

Affinché la correlazione parziale $r_{ij \mid \xi}$ sia uguale a zero — cioè affinché $y_i$ e $y_j$ risultino condizionalmente indipendenti dato $\xi$ — il numeratore della frazione deve annullarsi. Questa condizione si traduce nella seguente equazione:

$$
r_{ij} = r_{i\xi} \cdot r_{j\xi}.
$$ {#eq-fa-num-cor-parz}

In altri termini, la correlazione tra ogni coppia di variabili osservate deve essere pari al prodotto delle correlazioni tra ciascuna variabile e il fattore comune $\xi$. Questa relazione vincola la struttura della matrice di correlazione osservata e costituisce una delle implicazioni fondamentali del modello fattoriale unifattoriale.

Il principio è il seguente: **se un unico fattore latente $\xi$ è in grado di spiegare tutte le correlazioni tra le variabili osservate $y_i$, allora ciascuna correlazione $r_{ij}$ deve poter essere scritta come il prodotto $r_{i\xi} \cdot r_{j\xi}$.**

Questa condizione è il cuore del modello fattoriale a un fattore: tutte le interdipendenze tra le variabili manifeste sono attribuibili alla loro relazione con una variabile latente comune. Se la struttura delle correlazioni osservate viola questa condizione, il modello a un fattore non è compatibile con i dati, e sarà necessario considerare un modello con due o più fattori.

Dal punto di vista pratico, questo principio può essere utilizzato per valutare se una matrice di correlazione è compatibile con un modello fattoriale semplice. Ad esempio, per ogni terna di variabili si possono calcolare differenze di tetradi, cioè differenze tra prodotti incrociati di correlazioni. Se tali differenze risultano sistematicamente prossime a zero, ciò suggerisce che la struttura osservata potrebbe essere spiegata da un unico fattore comune.

Nei prossimi paragrafi analizzeremo in dettaglio la procedura per stimare formalmente le relazioni tra fattori e variabili manifeste.

## Teoria dei Due Fattori

Per illustrare in modo concreto il principio dell’annullamento della tetrade, consideriamo un esempio tratto dallo studio originale di @ch1904general. In uno dei suoi primi lavori, Spearman raccolse una serie di misurazioni delle capacità intellettive su un campione di studenti, includendo sia prestazioni scolastiche sia abilità percettive.

Le sei variabili considerate furono:

- Classics: rendimento nello studio dei classici;
- French: rendimento in lingua francese;
- English: rendimento in letteratura inglese;
- Math: rendimento in matematica;
- Pitch: abilità nella discriminazione dell’altezza dei suoni;
- Music: competenza musicale.

Per semplicità, nella discussione seguente considereremo solo tre materie scolastiche (studi classici, $c$, letteratura inglese, $e$, e matematica, $m$) e la discriminazione dell’altezza dei suoni, $p$. Nel suo studio, Spearman riportò le seguenti correlazioni:

$$
\begin{array}{ccccc}
  \hline
    & y_c & y_e & y_m & y_p \\
  \hline
  y_c & 1.00 & 0.78 & 0.70 & 0.66 \\
  y_e &      & 1.00 & 0.64 & 0.54 \\
  y_m &      &      & 1.00 & 0.45 \\
  y_p &      &      &      & 1.00 \\
  \hline
\end{array}
$$

Secondo la **Teoria dei Due Fattori**, proposta da Spearman, ogni prestazione intellettiva può essere scomposta in due componenti:

- un **fattore generale** (*g*), comune a tutti i compiti cognitivi;
- un **fattore specifico** (*s*), unico per ciascun compito.

Il fattore $g$ rappresenta la componente stabile e condivisa dell’intelligenza, mentre ciascun fattore $s$ spiega la varianza residua specifica della singola prova. La domanda centrale è: esiste un’unica variabile latente in grado di spiegare le covarianze osservate tra le variabili manifeste?

Per rispondere, Spearman utilizzò il metodo dell’**annullamento della tetrade**, che si basa sulle implicazioni della correlazione parziale. Abbiamo visto in precedenza che, se una variabile latente $\xi$ è in grado di rendere nulle le correlazioni parziali tra le variabili osservate, allora:

$$
r_{ij} = r_{i\xi} \cdot r_{j\xi}.
$$

Nel contesto dei dati di Spearman, ciò significa ad esempio che la correlazione osservata tra “studi classici” ($c$) e “letteratura inglese” ($e$) deve essere uguale al prodotto delle loro correlazioni con il fattore comune $\xi$: $r_{ec} = \lambda_e \cdot \lambda_c$. Lo stesso vale per tutte le altre coppie di variabili.

Le correlazioni tra le variabili manifeste e il fattore latente sono chiamate **saturazioni fattoriali** e vengono denotate con $\lambda$. Il vincolo fondamentale del modello fattoriale è che ogni correlazione osservata può essere scomposta come prodotto di due saturazioni.

## Annullamento della tetrade

Il metodo dell’**annullamento della tetrade** si basa sull’assunto centrale del modello fattoriale unifattoriale: se un singolo fattore comune spiega tutte le covarianze tra le variabili osservate, allora la correlazione tra ciascuna coppia di variabili può essere espressa come il prodotto delle loro **saturazioni fattoriali**:

$$
r_{ij} = \lambda_i \cdot \lambda_j.
$$

Questa relazione consente di tradurre le correlazioni osservate in un sistema di equazioni non lineari, in cui le incognite sono le saturazioni fattoriali $\lambda_i$.

Ad esempio, se consideriamo tre variabili — ad esempio *Classics* ($c$), *Math* ($m$) e *English* ($e$) — e assumiamo che siano tutte influenzate dallo stesso fattore latente $\xi$, possiamo scrivere:

$$
\begin{align}
r_{cm} &= \lambda_c \cdot \lambda_m, \notag \\
r_{em} &= \lambda_e \cdot \lambda_m, \\
r_{ce} &= \lambda_c \cdot \lambda_e. \notag
\end{align}
$$

Risolvendo questo sistema di equazioni, otteniamo un'espressione per ciascuna saturazione in funzione delle sole correlazioni osservate. Ad esempio, isolando $\lambda_m$:

$$
\lambda_m = \sqrt{ \frac{r_{cm} \cdot r_{em}}{r_{ce}} }.
$$  {#eq-tetrade}

Questa equazione mostra come si possa stimare il contributo di una variabile al fattore latente partendo esclusivamente dalle correlazioni empiriche tra le variabili osservate.

### Esempio con i Dati di Spearman

Utilizzando i dati riportati da Spearman nel 1904, e in particolare le variabili *Classics*, *Math* ed *English*, abbiamo:

$$
r_{cm} = 0.70, \quad r_{em} = 0.64, \quad r_{ce} = 0.78.
$$

Sostituendo nella formula:

$$
\hat{\lambda}_{\text{Math}} = \sqrt{ \frac{0.70 \cdot 0.64}{0.78} } = \sqrt{0.5733} \approx 0.76.
$$

Questo risultato rappresenta la **saturazione stimata** della variabile *Math* nel fattore comune latente $\xi$.

Il principio dell’annullamento della tetrade fornisce quindi un ponte diretto tra le correlazioni osservate e la struttura latente sottostante, assumendo che un singolo fattore sia responsabile delle covarianze tra le variabili.


## Verifica del modello

Ripetendo il procedimento per diverse terne di variabili osservate, è possibile ottenere **stime multiple** per la stessa saturazione fattoriale. Nel caso dei dati di Spearman, ad esempio, la saturazione $\lambda_m$ relativa alla variabile *Math* può essere stimata in almeno tre modi diversi, utilizzando combinazioni alternative di variabili:

$$
\begin{align}
  \hat{\lambda}_m &= \sqrt{ \frac{0.70 \cdot 0.64}{0.78} } = 0.76, \notag \\
  \hat{\lambda}_m &= \sqrt{ \frac{0.78 \cdot 0.45}{0.66} } = 0.69, \notag \\
  \hat{\lambda}_m &= \sqrt{ \frac{0.64 \cdot 0.45}{0.54} } = 0.73. \notag
\end{align}
$$

Questa molteplicità di stime evidenzia un aspetto fondamentale del modello fattoriale unifattoriale: **se il modello è corretto**, ossia se un unico fattore comune spiega le correlazioni tra tutte le variabili, allora **tutte le stime ottenute devono essere coerenti tra loro**, entro un margine di errore accettabile. Se, al contrario, le stime risultano incoerenti o divergenti, ciò suggerisce che il modello a un solo fattore non è compatibile con i dati.

In altri termini, il modello di Spearman è **verificabile empiricamente**: è possibile confrontare le previsioni del modello (in termini di relazioni tra saturazioni e correlazioni osservate) con i dati raccolti. Sebbene Spearman non abbia formalizzato un test statistico per valutare la bontà del modello, ha introdotto un principio centrale nell’analisi fattoriale moderna: **non tutte le matrici di correlazione giustificano l’ipotesi dell’esistenza di un unico fattore latente**.

## Metodo del centroide

Una volta accettata l’ipotesi che le stime multiple siano sufficientemente simili, ci si può chiedere come ottenere un’unica stima sintetica per ciascuna saturazione.

Un primo approccio consiste semplicemente nel calcolare la **media aritmetica** delle stime ottenute:

$$
\bar{\lambda}_m = \frac{0.76 + 0.69 + 0.73}{3} \approx 0.73.
$$

Un’alternativa più robusta è rappresentata dal **metodo del centroide**, che consiste nel calcolare una media pesata dei prodotti numeratori e dei denominatori coinvolti nelle diverse stime:

$$
\hat{\lambda}_m = \sqrt{
\frac{0.70 \cdot 0.64 + 0.78 \cdot 0.45 + 0.64 \cdot 0.45}{0.78 + 0.66 + 0.54}
} = \sqrt{ \frac{1.031}{1.98} } \approx 0.73.
$$

In questo caso, entrambi i metodi producono un risultato identico. Applicando lo stesso procedimento alle altre variabili, otteniamo:

$$
\hat{\lambda}_c = 0.97, \quad \hat{\lambda}_e = 0.84, \quad \hat{\lambda}_p = 0.65,
$$

e quindi il vettore delle saturazioni fattoriali stimate è:

$$
\boldsymbol{\hat{\Lambda}}' =
(\hat{\lambda}_c, \hat{\lambda}_e, \hat{\lambda}_m, \hat{\lambda}_p) = (0.97, 0.84, 0.73, 0.65).
$$

**In sintesi**, l’esempio tratto dai dati di Spearman mostra come, a partire da una semplice matrice di correlazione tra variabili osservate, sia possibile **ricostruire una struttura latente** assumendo l’esistenza di un unico fattore comune. Il metodo dell’annullamento della tetrade fornisce sia un criterio per verificare la coerenza del modello, sia una procedura per stimare le saturazioni fattoriali.

Sebbene oggi l’analisi fattoriale si avvalga di metodi più avanzati, come la stima per massima verosimiglianza o i modelli bayesiani, il principio introdotto da Spearman conserva tutta la sua rilevanza: **la covarianza tra variabili osservate può essere interpretata come riflesso di costrutti latenti condivisi**. È proprio questo passaggio — dal livello osservabile al livello teorico — che rende l’analisi fattoriale uno strumento fondamentale per la psicometria e le scienze psicologiche.

## Introduzione a `lavaan`

Attualmente, l'analisi fattoriale viene svolta mediante software. Il pacchetto R più ampiamente utilizzato per condurre l'analisi fattoriale è `lavaan`. 

### Sintassi del modello

Al cuore del pacchetto `lavaan` si trova la "sintassi del modello". La sintassi del modello è una descrizione del modello da stimare. In questa sezione, spieghiamo brevemente gli elementi della sintassi del modello `lavaan`. 

Nell'ambiente R, una formula di regressione ha la seguente forma:

```
y ~ x1 + x2 + x3 + x4
```

In questa formula, la tilde ("~") è l'operatore di regressione. Sul lato sinistro dell'operatore, abbiamo la variabile dipendente (y), e sul lato destro abbiamo le variabili indipendenti, separate dall'operatore "+" . In `lavaan`, un modello tipico è semplicemente un insieme (o sistema) di formule di regressione, in cui alcune variabili (che iniziano con una 'f' qui sotto) possono essere latenti. Ad esempio:

```
y ~ f1 + f2 + x1 + x2
f1 ~ f2 + f3
f2 ~ f3 + x1 + x2
```

Se abbiamo variabili latenti in una qualsiasi delle formule di regressione, dobbiamo "definirle" elencando i loro indicatori (manifesti o latenti). Lo facciamo utilizzando l'operatore speciale "=~", che può essere letto come "è misurato da". Ad esempio, per definire le tre variabili latenti f1, f2 e f3, possiamo usare la sintassi seguente:

```
f1 =~ y1 + y2 + y3
f2 =~ y4 + y5 + y6
f3 =~ y7 + y8 + y9 + y10
```

Inoltre, le varianze e le covarianze sono specificate utilizzando un operatore "doppia tilde", ad esempio:

```
y1 ~~ y1 # varianza
y1 ~~ y2 # covarianza
f1 ~~ f2 # covarianza
```

E infine, le intercette per le variabili osservate e latenti sono semplici formule di regressione con solo una intercetta (esplicitamente indicato dal numero "1") come unico predittore:

```
y1 ~ 1
f1 ~ 1
```

Utilizzando questi quattro tipi di formule, è possibile descrivere una vasta gamma di modelli di variabili latenti. L'attuale insieme di tipi di formula è riassunto nella tabella sottostante.

| tipo di formula | operatore |  mnemonic |
| --------------- | --------- | --------- |
| definizione variabile latente | =~ | è misurato da |
| regressione | ~ | viene regredito su |
| (co)varianza (residuale) | ~~  |è correlato con |
| intercetta | ~ 1 | intercetta |

Una sintassi completa del modello lavaan è semplicemente una combinazione di questi tipi di formule, racchiusi tra virgolette singole. Ad esempio:

```
my_model <- ' 
  # regressions
  y1 + y2 ~ f1 + f2 + x1 + x2
  f1 ~ f2 + f3
  f2 ~ f3 + x1 + x2

  # latent variable definitions 
  f1 =~ y1 + y2 + y3 
  f2 =~ y4 + y5 + y6 
  f3 =~ y7 + y8 + y9
  
  # variances and covariances 
  y1 ~~ y1 
  y1 ~~ y2 
  f1 ~~ f2

  # intercepts 
  y1 ~ 1 
  f1 ~ 1
'
```

Per adattare il modello ai dati usiamo la seguente sintassi.

```
fit <- cfa(model = my_model, data = my_data)
```

## Un esempio concreto

Analizziamo nuovamente i dati di Spearman che abbiamo esaminato in precedenza usando `lavaan`. La matrice completa dei dati di Spearman è messa a disposizione da @kan2019extending. 

Specifichiamo il nome delle variabili manifeste

```{r}
varnames <- c(
  "Classics", "French", "English", "Math", "Pitch", "Music"
)
```

e il loro numero

```{r}
ny <- length(varnames)
```

Creiamo la matrice di correlazione:

```{r}
spearman_cor_mat <- matrix(
  c(
    1.00,  .83,  .78,  .70,  .66,  .63,
     .83, 1.00,  .67,  .67,  .65,  .57,
     .78,  .67, 1.00,  .64,  .54,  .51,
     .70,  .67,  .64, 1.00,  .45,  .51,
     .66,  .65,  .54,  .45, 1.00,  .40,
     .63,  .57,  .51,  .51,  .40, 1.00
  ),
  ny, ny,
  byrow = TRUE,
  dimnames = list(varnames, varnames)
)
spearman_cor_mat
```

Specifichiamo l'ampiezza campionaria:

```{r}
n <- 33
```

Definiamo il modello unifattoriale in `lavaan`. L'operatore `=~` si può leggere dicendo che la variabile latente a sinistra dell'operatore viene identificata dalle variabili manifeste elencate a destra dell'operatore e separate dal segno `+`. Per il caso presente, il modello dei due fattori di Spearman può essere specificato come segue.

```{r}
spearman_mod <- "
  g =~ Classics + French + English + Math + Pitch + Music
"
```

Adattiamo il modello ai dati con la funzione `cfa()`:

```{r}
fit1 <- lavaan::cfa(
  spearman_mod,
  sample.cov = spearman_cor_mat,
  sample.nobs = n,
  std.lv = TRUE
)
```

La funzione `cfa()` del pacchetto `lavaan` serve per stimare un **modello di analisi fattoriale confermativa (CFA)**, cioè un modello che verifica se un insieme di variabili osservate può essere spiegato da una o più **variabili latenti** (cioè non direttamente osservabili).

Vediamo passo passo cosa significano gli argomenti specificati:

🔹 `spearman_mod`: è il **modello CFA specificato dall’utente**. Di solito è scritto come stringa e indica quali variabili osservate sono collegate a quali fattori latenti. Per esempio, `g =~ Classics + French + English + Math + Pitch + Music` significa che tutte le variabili (`Classics`, `French`, ...) sono indicatori del **fattore latente g**.

🔹 `sample.cov = spearman_cor_mat`: qui non stiamo fornendo direttamente i dati grezzi, ma una **matrice di correlazioni** tra le variabili osservate.  Questo approccio è comune quando si lavora con dati standardizzati o quando si vuole fare una CFA direttamente sulla matrice di correlazione.

🔹 `sample.nobs = n`: è il **numero di osservazioni** (cioè i soggetti) usato per costruire la matrice di correlazioni. Serve a `lavaan` per calcolare **gli errori standard** e **gli indici di bontà di adattamento** del modello.

🔹 `std.all = TRUE`: questa opzione chiede a lavaan di standardizzare le variabili latenti nel modello. Ciò significa che:

- le variabili latenti (in questo caso il fattore g) vengono fissate con varianza = 1;
- questo approccio fornisce una scala alle variabili latenti, che altrimenti non avrebbero una metrica naturale;
- i coefficienti stimati (saturazioni fattoriali) saranno espressi in una scala più facilmente interpretabile;
- rappresenta una convenzione standard nell'analisi fattoriale confermativa.

Questo è utile perché:

- semplifica l'interpretazione dei risultati;
- consente di confrontare più facilmente l'importanza relativa delle variabili osservate nel definire il fattore;
- fornisce una scala di riferimento coerente per il fattore latente.

### Esaminare i risultati del modello con `summary()`

Una volta adattato il modello CFA, possiamo esaminarne i risultati principali usando la funzione `summary()`. Ad esempio:

```{r}
out <- summary(
  fit1, 
  fit.measures = TRUE, 
  standardized = TRUE
)
out
```

Vediamo nel dettaglio cosa fanno i singoli argomenti:

📌 `summary(fit1, ...)`: Questa funzione fornisce un **riassunto dettagliato** del modello stimato (in questo caso, `fit1`). L’output include:

- informazioni generali sul modello (tipo di stimatore, ottimizzatore, numero di osservazioni, ecc.),
- le **stime dei parametri** (es. carichi fattoriali, varianze residue, covarianze),
- e, se richiesto, anche **gli indici di bontà dell’adattamento** e le **stime standardizzate**.

📏 `fit.measures = TRUE`: Questa opzione richiede a `lavaan` di includere nella stampa una **sezione con gli indici di bontà di adattamento del modello**, tra cui:

- **Chi-quadrato** del modello e p-value associato,
- **CFI** (Comparative Fit Index),
- **TLI** (Tucker-Lewis Index),
- **RMSEA** (Root Mean Square Error of Approximation),
- **SRMR** (Standardized Root Mean Residual),  
- e altri.

Questi indici aiutano a valutare **quanto bene il modello riproduce le relazioni osservate nei dati**. Sono particolarmente utili per confrontare modelli alternativi e per verificare se il modello è coerente con la struttura teorica ipotizzata.


🔁 `standardized = TRUE`: Questo argomento dice a `lavaan` di **aggiungere le stime standardizzate dei parametri**. Verranno incluse due colonne in più nell’output:

🔹 `Std.lv` (standardizzato rispetto alla variabile latente)

- Ogni carico fattoriale è standardizzato **considerando che la variabile latente ha varianza 1**, ma **senza standardizzare le variabili osservate**.
- Questa colonna mostra **quanto una variabile osservata cambia in media per un aumento di 1 deviazione standard nel fattore latente**, mantenendo le osservate nella loro scala originale.
- Se hai usato `std.lv = TRUE` nella stima (come nel nostro esempio), allora `Estimate` e `Std.lv` saranno **identici**, perché la varianza della latente è già fissata a 1.

🔹 `Std.all` (completamente standardizzato)

- Questa colonna mostra i **carichi completamente standardizzati**, cioè **sia la variabile latente che le variabili osservate sono standardizzate (varianza = 1)**.
- I valori possono essere letti come **correlazioni** tra ogni indicatore osservato e il fattore latente.
- È la forma più comunemente riportata negli articoli scientifici, perché:
  - facilita il confronto tra variabili;
  - semplifica l’interpretazione (tutti i parametri sono su una scala comparabile);
  - consente di capire **quali indicatori sono più rappresentativi del costrutto**.

ℹ️ Nota sulla sezione **Varianze**

- Le righe con un **punto davanti al nome** (es. `.x1`) indicano **variabili osservate**: il valore rappresenta la **varianza residua**, cioè la parte non spiegata dal fattore.
- Le **variabili latenti** (es. `g`, `F1`, ecc.) non hanno il punto iniziale: il valore indicato rappresenta la loro **varianza totale**.

📋 Come semplificare l’output

Per estrarre solo la tabella delle stime:

```{r}
coef(fit1)  # solo le stime grezze (non standardizzate)
```

Oppure, per una tabella completa:

```{r}
parameterEstimates(fit1, standardized = TRUE)
```

Puoi anche **filtrare e formattare** l’output per visualizzare solo le saturazioni fattoriali:

```{r}
parameterEstimates(fit1, standardized = TRUE) |>
  dplyr::filter(op == "=~") |>
  dplyr::select(
    "Fattore latente" = lhs,
    Indicatore = rhs,
    B = est,
    SE = se,
    Z = z,
    "p-value" = pvalue,
    Beta = std.all
  )
```


#### Analisi dei residui

Per valutare quanto il modello riesce a riprodurre le correlazioni osservate, possiamo esaminare la **matrice delle correlazioni residue**:

```{r}
residuals(fit1, type = "cor")$cov
```

Possiamo visualizzare graficamente i residui, ad esempio con un **Q-Q plot**:

```{r fig.asp=1}
res1 <- residuals(fit1, type = "cor")$cov
res1[upper.tri(res1, diag = TRUE)] <- NA  # Consideriamo solo i residui non ridondanti
v1 <- as.vector(res1)
v2 <- v1[!is.na(v1)]

tibble(v2) %>%
  ggplot(aes(sample = v2)) + 
  stat_qq() + 
  stat_qq_line()
```

Questo grafico serve a verificare se le **correlazioni residue** seguono una distribuzione normale: deviazioni dalla linea indicano **coppie di variabili mal rappresentate** dal modello fattoriale.


## Diagrammi di percorso

Il pacchetto `semPlot` consente di disegnare diagrammi di percorso per vari modelli SEM. La funzione `semPaths` prende in input un oggetto creato da `lavaan` e disegna il diagramma, con diverse opzioni disponibili. Il diagramma prodotto controlla le dimensioni dei caratteri/etichette, la visualizzazione dei residui e il colore dei percorsi/coefficienti. Sono disponibili queste e molte altre opzioni di controllo. 

```{r}
semPaths(
    fit1,
    "std",
    posCol = c("black"),
    edge.label.cex = 1.2,
    whatLabels = "std", 
    edge.width = 0.3, # Imposta lo spessore delle linee 
    fade = FALSE # Disabilita il fading
)
```

Il calcolo delle saturazioni fattoriali con il metodo del centroide aveva prodotto il seguente risultato: 

- classici (Cls): 0.97
- inglese (Eng): 0.84
- matematica (Mth): 0.73
- pitch discrimination (Ptc): 0.65

Si noti la somiglianza con i valori ottenuti mediante il metodo di massima verosimiglianza riportati nella figura.


## Analisi Fattoriale Esplorativa (EFA) in `lavaan`

L'**analisi fattoriale esplorativa (EFA)** è una tecnica utilizzata per esplorare la struttura latente di un insieme di variabili osservate, **senza imporre vincoli a priori** su quali variabili siano associate a quali fattori. Anche in presenza di un solo fattore, EFA e CFA **non coincidono**, poiché nel CFA i legami tra fattori e indicatori sono **specificati dall’utente**, mentre nell’EFA sono **liberamente stimati dal modello**.

In `lavaan`, possiamo specificare un modello EFA utilizzando la sintassi `efa("nome")*` nel modello.

Specificazione del modello EFA:

```{r}
efa_model <- '
  efa("efa")*g =~ Classics + French + English + Math + Pitch + Music
'
```

In questo esempio:

- `g` è un **fattore esplorativo** (non confermativo);
- le saturazioni fattoriali saranno stimate **senza vincoli** a priori.


Adattamento del modello ai dati:

```{r}
fit2 <- lavaan::cfa(
  efa_model,
  sample.cov = spearman_cor_mat,
  sample.nobs = n,
  std.lv = TRUE
)
```

Nota: anche se usiamo la funzione `cfa()`, il modello è trattato come EFA perché abbiamo specificato `efa()` nella sintassi del modello.

Esame della soluzione ottenuta:

```{r}
summary(fit2, standardized = TRUE)
```

L’output riporterà:

- le **saturazioni fattoriali esplorative**;
- le **varianze residue**;
- le **stime standardizzate**, tra cui `Std.all`, interpretabili come **correlazioni tra fattore e variabili osservate**.

Visualizzazione del modello (diagramma di percorso):

```{r}
semPaths(
  fit2,
  "std",
  posCol = c("black"),
  edge.label.cex = 1.2,
  whatLabels = "std",
  edge.width = 0.3,
  fade = FALSE
)
```

Questo comando crea un diagramma in cui:

- le **variabili osservate** sono collegate al **fattore esplorativo**;
- le etichette sui bordi mostrano i **carichi standardizzati** (`Std.all`).

💡 Osservazione importante

Se confronti la soluzione ottenuta con il modello CFA specificato come:

```r
cfa_model <- '
  g =~ Classics + French + English + Math + Pitch + Music
'
```

potresti ottenere **valori simili**, **ma la differenza concettuale rimane**:

- nella CFA, i legami tra fattore e indicatori sono **vincolati** (nessun cross-loading);
- nell’EFA (via `efa()`), i carichi sono **liberamente stimati**: è il modello a decidere "quanto" ogni variabile è legata al fattore.


## Riflessioni Conclusive

In questo capitolo, abbiamo introdotto il metodo dell’annullamento della tetrade, che permette di stimare le saturazioni in un modello monofattoriale. Abbiamo anche illustrato come questo metodo sia, in effetti, un’applicazione del concetto di correlazione parziale.

Un aspetto fondamentale nella costruzione dei test psicologici riguarda la determinazione del numero di fattori o tratti sottostanti al set di indicatori in esame. La teoria classica dei test presuppone che un test sia monofattoriale, cioè che gli indicatori riflettano un unico tratto latente. La mancata monodimensionalità introduce difficoltà nell’applicare i principi della teoria classica ai punteggi di un test che non soddisfa tale proprietà.

L’analisi della dimensionalità di un insieme di indicatori rappresenta, quindi, una fase cruciale nel processo di costruzione di un test. Solitamente, questa valutazione viene effettuata attraverso l’analisi fattoriale. In questo capitolo, abbiamo descritto le proprietà di base del modello unifattoriale, gettando le fondamenta per una comprensione più approfondita della dimensionalità e dell'influenza di un singolo tratto latente sugli indicatori.

## Session Info

```{r}
sessionInfo()
```

