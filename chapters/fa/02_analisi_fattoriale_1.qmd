# Il modello unifattoriale {#sec-fa-unifactor-model}

**Prerequisiti**

- Leggere il capitolo 6, *Factor Analysis and Principal Component Analysis*, del testo *Principles of psychological assessment* di @petersen2024principles. 

**Concetti e Competenze Chiave**

- Correlazione parziale
- Teoria dei due fattori
- Annullamento della tetrade

**Preparazione del Notebook**

```{r}
# Carica il file _common.R per impostazioni di pacchetti e opzioni
here::here("code", "_common.R") |> source()

# Carica pacchetti aggiuntivi
pacman::p_load(lavaan, corrplot, tidyr, kableExtra, lavaanPlot, lavaanExtra)
```

## Introduzione

Lâ€™analisi fattoriale Ã¨ una tecnica statistica che permette di spiegare le correlazioni tra variabili osservate attraverso la loro dipendenza da uno o piÃ¹ fattori latenti. In questo modello, le $p$ variabili osservate (ad esempio, item di un questionario o indicatori comportamentali) sono considerate **condizionalmente indipendenti, dato lâ€™insieme di $m$ fattori comuni non osservabili**. Lâ€™obiettivo principale Ã¨ interpretare tali fattori come costrutti teorici latenti sottostanti alle risposte osservate.

::: {.callout-note title="Nota sulla indipendenza condizionale" collapse="true"}
In un modello fattoriale, si dice che le variabili osservate $y_1, y_2, \dots, y_p$ sono *condizionalmente indipendenti* dato il fattore latente $\xi$ quando, una volta noto il valore di $\xi$, la conoscenza di una qualunque delle variabili osservate non fornisce informazioni aggiuntive sulle altre. Formalmente, per ogni coppia $i \neq j$ si ha:

$$
P(y_i, y_j \mid \xi) = P(y_i \mid \xi) \cdot P(y_j \mid \xi).
$$

Questa proprietÃ  implica che tutte le covarianze tra le variabili osservate sono spiegate esclusivamente dal fattore comune. In assenza di tale fattore, le variabili osservate risulterebbero tra loro incorrelate. Lâ€™indipendenza condizionale Ã¨ una delle assunzioni centrali del modello fattoriale, e giustifica la possibilitÃ  di interpretare la covarianza tra item come effetto della loro dipendenza condivisa da un unico costrutto latente. Un esempio numerico Ã¨ fornito nella @sec-fa-cor-parz.
:::

Ad esempio, si puÃ² utilizzare lâ€™analisi fattoriale per spiegare le correlazioni tra le prestazioni di un gruppo di individui in diversi compiti cognitivi attraverso un singolo fattore latente come lâ€™intelligenza generale. In questo modo, lâ€™analisi fattoriale consente di identificare i costrutti a cui gli item si riferiscono e di quantificare quanto ciascun item contribuisca a rappresentarli.

Il modello puÃ² prevedere uno (modello unifattoriale, $m = 1$) o piÃ¹ fattori latenti (modello multifattoriale, $m > 1$). In questo capitolo ci concentreremo sul modello unifattoriale, che assume lâ€™esistenza di un unico fattore comune che influenza tutte le variabili osservate.

Nel contesto dellâ€™analisi fattoriale, le variabili latenti rappresentano **costrutti teorici non direttamente osservabili**, come abilitÃ  cognitive o tratti psicologici. Queste variabili riflettono le comunanze sottostanti a un insieme di indicatori osservabili, chiamati variabili manifeste. Le prime sono rappresentate nei diagrammi di percorso come cerchi, mentre le seconde come quadrati.

Il legame tra fattore latente e variabili manifeste Ã¨ descritto tramite i **carichi fattoriali** ($\lambda$), che quantificano lâ€™intensitÃ  dellâ€™influenza esercitata dal fattore latente su ciascuna variabile osservata. Il valore di $\lambda$ indica la proporzione di varianza della variabile osservata che Ã¨ spiegata dal fattore comune: piÃ¹ alto Ã¨ il carico, maggiore Ã¨ la rappresentativitÃ  dellâ€™item rispetto al costrutto latente.

Dal punto di vista matematico, ciascuna misura osservabile $y$ Ã¨ modellata come una combinazione lineare del fattore latente $\xi$, ponderato dal carico $\lambda$, piÃ¹ un termine di errore specifico $\delta$, che rappresenta la componente di varianza non spiegata dal fattore comune.

Un esempio intuitivo puÃ² chiarire il significato di questa decomposizione: immaginate di usare una bilancia imprecisa per misurare il peso corporeo. Ogni misurazione ($y$) rifletterÃ  in parte il peso reale della persona ($\xi$), ma anche un errore casuale ($\delta$) dovuto allâ€™inaffidabilitÃ  dello strumento.

Quando si dispone di piÃ¹ variabili osservabili $y$ che fanno riferimento a un medesimo costrutto latente $\xi$, diventa possibile stimare con maggiore precisione sia il punteggio latente sia la componente di errore. Questo consente di ottenere una rappresentazione piÃ¹ affidabile del costrutto teorico di interesse e di migliorare lâ€™interpretazione psicometrica dei dati.

## Modello monofattoriale

Nel caso di un solo fattore comune e $p$ variabili manifeste $y_i$, il modello assume la forma:

$$
\begin{equation}
y_i = \mu_i + \lambda_{i} \xi + 1 \cdot \delta_i \qquad i = 1, \dots, p,
\end{equation}
$$ {#eq-mod-unifattoriale}

dove:

- $\mu_i$ Ã¨ la media della variabile osservata $y_i$,
- $\xi$ Ã¨ il fattore latente comune a tutte le variabili osservate,
- $\lambda_i$ Ã¨ la saturazione fattoriale, ovvero il peso del fattore comune $\xi$ sulla variabile $y_i$,
- $\delta_i$ Ã¨ il fattore specifico (o errore unico) associato a $y_i$, indipendente da $\xi$.

Si assume che:

- il fattore comune $\xi$ abbia media zero e varianza unitaria,
- i fattori specifici $\delta_i$ abbiano media zero, varianza $\psi_i$, e siano indipendenti tra loro e dal fattore comune.

In questo modello, ciascuna variabile osservata $y_i$ Ã¨ influenzata da una componente condivisa ($\xi$) e da una componente specifica ($\delta_i$), che rappresenta la parte di varianza unica della variabile non spiegata dal fattore comune.

Il modello di analisi fattoriale puÃ² ricordare formalmente il modello di regressione lineare, ma esistono alcune differenze fondamentali. Innanzitutto, sia il fattore comune $\xi$ sia i fattori specifici $\delta_i$ sono variabili latenti, cioÃ¨ non osservabili direttamente. Di conseguenza, tutti i termini presenti nel lato destro dellâ€™equazione sono incogniti. Inoltre, i due modelli perseguono obiettivi differenti: la regressione lineare mira a identificare variabili indipendenti osservabili che spiegano la varianza di una variabile dipendente, mentre lâ€™analisi fattoriale cerca di individuare una o piÃ¹ variabili latenti che rendano conto della covarianza tra un insieme di variabili osservate.

Per semplicitÃ , si assume spesso che le variabili osservate siano centrate, cioÃ¨ che la loro media sia pari a zero ($\mu_i = 0$). Questo equivale a considerare ogni $y_i$ come uno scarto rispetto alla propria media. Il modello unifattoriale puÃ² quindi essere riscritto nella forma:

$$
\begin{equation}
y_i - \mu_i = \lambda_i \xi + 1 \cdot \delta_i,
\end{equation}
$$ {#eq-mod-monofattoriale}

dove:

- $\lambda_i$ Ã¨ la saturazione (o carico) fattoriale della variabile $y_i$ sul fattore comune $\xi$,
- $\delta_i$ rappresenta la componente specifica della variabile $y_i$, ovvero la parte di varianza non condivisa con le altre variabili.

Si assume che:

- il fattore comune $\xi$ abbia media zero e varianza unitaria ($\mathbb{E}[\xi] = 0$, $\mathrm{Var}(\xi) = 1$),
- ciascun fattore specifico $\delta_i$ abbia media zero, varianza $\psi_i$ e sia incorrelato con $\xi$ e con gli altri $\delta_j$ (per $j \ne i$).

Sotto queste assunzioni, lâ€™interdipendenza tra le variabili osservate Ã¨ interamente spiegata dalla loro dipendenza dal fattore comune. I fattori specifici, invece, rendono conto della varianza residua non condivisa.

Questa formulazione permette di derivare espressioni analitiche per quantitÃ  fondamentali come:

- la covarianza tra una variabile osservata $y_i$ e il fattore comune $\xi$,
- la varianza della variabile osservata $y_i$,
- la covarianza tra due variabili osservate $y_i$ e $y_k$.

Lâ€™obiettivo di questo capitolo Ã¨ analizzare nel dettaglio tali quantitÃ  e mostrare come esse riflettano la struttura latente imposta dal modello unifattoriale.

## Correlazione parziale

Prima di introdurre formalmente il modello statistico dellâ€™analisi fattoriale, Ã¨ utile chiarire il concetto di **correlazione parziale**, centrale per comprendere la logica della separazione tra fattori comuni e specifici.

Lâ€™analisi fattoriale Ã¨ spesso fatta risalire agli studi di Charles Spearman. Nel 1904, Spearman pubblicÃ² un articolo intitolato *"General Intelligence, Objectively Determined and Measured"*, nel quale propose la Teoria dei Due Fattori. In quel lavoro, mostrÃ² che era possibile identificare un fattore latente a partire da una matrice di correlazioni osservate, utilizzando la tecnica dellâ€™**annullamento della tetrade** (*tetrad differences*). Questa tecnica si basa sul principio della correlazione parziale e mira a verificare se, una volta controllati gli effetti di variabili latenti (i fattori $\xi_j$), le correlazioni tra le variabili osservate $Y_i$ risultino nulle.

Per illustrare il concetto, consideriamo un esempio con tre variabili: $Y_1$, $Y_2$ e una variabile comune $F$. La correlazione semplice $r_{12}$ tra $Y_1$ e $Y_2$ puÃ² riflettere lâ€™influenza condivisa di $F$ su entrambe. La *correlazione parziale* tra $Y_1$ e $Y_2$ al netto di $F$ misura invece la relazione diretta tra $Y_1$ e $Y_2$ una volta rimosso lâ€™effetto lineare di $F$ da entrambe le variabili.

Per farlo, si calcolano i residui delle regressioni di $Y_1$ e $Y_2$ su $F$, ovvero le componenti ortogonali a $F$. Ad esempio, nel caso di $Y_1$:

$$
Y_1 = b_{01} + b_{11}F + E_1,
$$  {#eq-mod-reg-mult-fa}

dove $E_1$ Ã¨ la parte di $Y_1$ linearmente indipendente da $F$. Ripetendo lâ€™operazione per $Y_2$ si ottiene un residuo $E_2$, anchâ€™esso ortogonale a $F$. La correlazione di Pearson tra $E_1$ ed $E_2$ rappresenta la correlazione parziale tra $Y_1$ e $Y_2$ dato $F$.

La stessa quantitÃ  puÃ² essere calcolata direttamente a partire dalle correlazioni semplici tra le tre variabili, tramite la formula:

$$
\begin{equation}
r_{1,2 \mid F} = \frac{r_{12} - r_{1F}r_{2F}}{\sqrt{(1-r_{1F}^2)(1-r_{2F}^2)}}.
\end{equation}
$$  {#eq-corr-parz}

Questa formula mostra che la correlazione parziale Ã¨ ottenuta sottraendo al valore osservato di $r_{12}$ il prodotto delle correlazioni tra $Y_1$ e $F$ e tra $Y_2$ e $F$, e normalizzando per la varianza residua non spiegata da $F$.

### Esempio numerico {#sec-fa-cor-parz}

Supponiamo di avere una variabile $f$ generata da una distribuzione normale:

```{r}
set.seed(123)
n <- 1000
f <- rnorm(n, 24, 12)
```

Costruiamo due variabili, $y_1$ e $y_2$, come combinazioni lineari di $f$ piÃ¹ un errore casuale:

```{r}
y1 <- 10 + 7 * f + rnorm(n, 0, 50)
y2 <- 3  + 2 * f + rnorm(n, 0, 50)
```

Le tre variabili sono correlate; in particolare $y_1$ e $y_2$ hanno una correlazione semplice $r_{12} = 0.380$:

```{r}
Y <- cbind(y1, y2, f)
cor(Y) |>
    round(3)
```

Per calcolare la correlazione parziale tra $y_1$ e $y_2$ al netto di $f$, eseguiamo due modelli di regressione lineare:

```{r}
fm1 <- lm(y1 ~ f)
fm2 <- lm(y2 ~ f)
```

Ogni osservazione viene cosÃ¬ scomposta in due componenti: i valori adattati $\hat{y}_i$ (dipendenti da $f$) e i residui $e_i$ (indipendenti da $f$):

```{r}
cbind(y1, y1.hat=fm1$fit, e=fm1$res, sum=fm1$fit+fm1$res) |>
    head() |>
    round(3)
```

La correlazione parziale tra $y_1$ e $y_2$ Ã¨ quindi calcolabile come la correlazione tra i residui:

```{r}
cor(fm1$res, fm2$res)
```

Nel nostro esempio, la correlazione parziale tra $y_1$ e $y_2$ al netto di $f$ Ã¨ $r_{12|f} = 0.028$, praticamente nulla. Questo indica che la correlazione osservata tra $y_1$ e $y_2$ ($r = 0.380$) era dovuta esclusivamente allâ€™influenza condivisa di $f$ su entrambe.

In altre parole, una volta eliminato lâ€™effetto di $f$, non rimane alcuna associazione lineare diretta tra $y_1$ e $y_2$. Le due variabili sono quindi **condizionalmente indipendenti** dato $f$: le componenti di $y_1$ e $y_2$ non spiegate da $f$ risultano tra loro incorrelate.

Infine, possiamo verificare che il valore ottenuto con la formula della correlazione parziale coincide con quello calcolato sui residui:

```{r}
R <- cor(Y)

(R[1, 2] - R[1, 3] * R[2, 3]) / 
  sqrt((1 - R[1, 3]^2) * (1- R[2, 3]^2)) |>
  round(3)
```

Il risultato conferma la coerenza tra il calcolo algebrico e il metodo dei residui. Questo esempio evidenzia come la correlazione parziale sia uno strumento fondamentale per isolare le relazioni dirette tra variabili, rimuovendo lâ€™influenza di fattori comuni non osservabili.

## Principio base dell'analisi fattoriale

Oggi, l'inferenza statistica in ambito fattoriale si basa prevalentemente su metodi di stima per massima verosimiglianza, ottenuti attraverso procedure iterative. Tuttavia, nelle fasi iniziali dello sviluppo dellâ€™analisi fattoriale, lâ€™estrazione dei fattori si fondava su proprietÃ  invarianti che il modello fattoriale impone alla matrice di covarianza (o di correlazione) delle variabili osservate. Tra queste proprietÃ , la piÃ¹ nota Ã¨ quella dellâ€™*annullamento della tetrade*, caratteristica distintiva dei modelli a un solo fattore.

Una *tetrade* Ã¨ una combinazione lineare di quattro correlazioni tra variabili osservate. Se le correlazioni tra le variabili possono essere spiegate da un singolo fattore latente comune, allora Ã¨ possibile costruire combinazioni di correlazioni la cui differenza si annulla. In altri termini, il modello a un fattore impone vincoli strutturali tali da rendere nulle alcune espressioni algebriche (le tetradi), che dipendono unicamente dalla matrice di correlazione.

Lâ€™analisi fattoriale puÃ² dunque essere formulata come un problema di ricerca di un insieme ristretto di variabili latenti ($m < p$), tali che, una volta controllato il loro effetto, tutte le correlazioni parziali tra le variabili osservate $y_i$ risultino nulle. Se lâ€™annullamento delle correlazioni parziali Ã¨ confermato, lo psicologo puÃ² concludere che esistono $m$ fattori latenti capaci di spiegare la struttura di covarianza del sistema osservato.

Per chiarire il principio, consideriamo la seguente matrice di correlazione, costruita in modo da riflettere un modello con un unico fattore latente $\xi$:

|       | $\xi$   | $y_1$ | $y_2$ | $y_3$ | $y_4$ | $y_5$ |
|-------|---------|-------|-------|-------|-------|-------|
| $\xi$ | **1.00** |       |       |       |       |       |
| $y_1$ | **0.90** | 1.00  |       |       |       |       |
| $y_2$ | **0.80** | 0.72  | 1.00  |       |       |       |
| $y_3$ | **0.70** | 0.63  | 0.56  | 1.00  |       |       |
| $y_4$ | **0.60** | 0.54  | 0.48  | 0.42  | 1.00  |       |
| $y_5$ | **0.50** | 0.45  | 0.40  | 0.35  | 0.30  | 1.00  |

In questa matrice, ogni variabile $y_i$ ha una correlazione positiva con $\xi$, e le correlazioni tra le $y_i$ sono coerenti con un modello in cui $\xi$ agisce come unica fonte comune di covarianza. Per esempio, la correlazione parziale tra $y_3$ e $y_5$ al netto di $\xi$ risulta:

$$
\begin{align}
  r_{35 \mid \xi} &= \frac{r_{35} - r_{3\xi}r_{5\xi}}{\sqrt{(1-r_{3\xi}^2)(1-r_{5\xi}^2)}} \notag \\[12pt]
  &= \frac{0.35 - 0.7 \times 0.5}{\sqrt{(1 - 0.7^2)(1 - 0.5^2)}} = 0. \notag
\end{align}
$$

Lo stesso vale per qualunque altra coppia di variabili: tutte le correlazioni parziali condizionate a $\xi$ sono nulle, cioÃ¨ $r_{ij \mid \xi} = 0$ per ogni $i \ne j$.

In questa matrice, ci sono $p(p-1)/2 = 5(5-1)/2 = 10$ correlazioni tra le variabili osservate, tutte spiegate dal singolo fattore $\xi$. Questo non sorprende, poichÃ© la matrice Ã¨ stata costruita appositamente per rispettare questa proprietÃ .

Tuttavia, immaginiamo ora di trovarci in una situazione reale, in cui osserviamo soltanto le variabili $y_1, \dots, y_5$, ma non abbiamo accesso diretto a $\xi$. Possiamo allora porci la seguente domanda: esiste una variabile latente $\xi$ tale che, se ne controllassimo lâ€™effetto, tutte le correlazioni parziali tra le variabili osservate risulterebbero nulle?

Se una tale variabile esiste, e riesce a spiegare completamente le interdipendenze osservate tra le $y_i$, essa assume lo status di **fattore**.

::: {#def-}
Un **fattore** Ã¨ una variabile latente non osservabile che, una volta controllata, rende significativamente nulle tutte le correlazioni parziali tra le variabili manifeste.
:::

## Vincoli sulle correlazioni

Come possiamo determinare se esiste una variabile latente $\xi$ in grado di spiegare tutte le correlazioni osservate tra le variabili manifeste, rendendo nulle le loro correlazioni parziali? Possiamo partire dalla formula della correlazione parziale, giÃ  introdotta in precedenza (vedi @eq-corr-parz), e riscriverla per esprimere la correlazione parziale tra due variabili osservate $y_i$ e $y_j$ condizionata a $\xi$:

$$
\begin{align}
r_{ij \mid \xi} = \frac{r_{ij} - r_{i\xi} \, r_{j\xi}}{\sqrt{(1 - r_{i\xi}^2)(1 - r_{j\xi}^2)}}.
\end{align}
$$

AffinchÃ© la correlazione parziale $r_{ij \mid \xi}$ sia uguale a zero â€” cioÃ¨ affinchÃ© $y_i$ e $y_j$ risultino condizionalmente indipendenti dato $\xi$ â€” il numeratore della frazione deve annullarsi. Questa condizione si traduce nella seguente equazione:

$$
r_{ij} = r_{i\xi} \cdot r_{j\xi}.
$$ {#eq-fa-num-cor-parz}

In altri termini, la correlazione tra ogni coppia di variabili osservate deve essere pari al prodotto delle correlazioni tra ciascuna variabile e il fattore comune $\xi$. Questa relazione vincola la struttura della matrice di correlazione osservata e costituisce una delle implicazioni fondamentali del modello fattoriale unifattoriale.

Il principio Ã¨ il seguente: **se un unico fattore latente $\xi$ Ã¨ in grado di spiegare tutte le correlazioni tra le variabili osservate $y_i$, allora ciascuna correlazione $r_{ij}$ deve poter essere scritta come il prodotto $r_{i\xi} \cdot r_{j\xi}$.**

Questa condizione Ã¨ il cuore del modello fattoriale a un fattore: tutte le interdipendenze tra le variabili manifeste sono attribuibili alla loro relazione con una variabile latente comune. Se la struttura delle correlazioni osservate viola questa condizione, il modello a un fattore non Ã¨ compatibile con i dati, e sarÃ  necessario considerare un modello con due o piÃ¹ fattori.

Dal punto di vista pratico, questo principio puÃ² essere utilizzato per valutare se una matrice di correlazione Ã¨ compatibile con un modello fattoriale semplice. Ad esempio, per ogni terna di variabili si possono calcolare differenze di tetradi, cioÃ¨ differenze tra prodotti incrociati di correlazioni. Se tali differenze risultano sistematicamente prossime a zero, ciÃ² suggerisce che la struttura osservata potrebbe essere spiegata da un unico fattore comune.

Nei prossimi paragrafi analizzeremo in dettaglio la procedura per stimare formalmente le relazioni tra fattori e variabili manifeste.

## Teoria dei Due Fattori

Per illustrare in modo concreto il principio dellâ€™annullamento della tetrade, consideriamo un esempio tratto dallo studio originale di @ch1904general. In uno dei suoi primi lavori, Spearman raccolse una serie di misurazioni delle capacitÃ  intellettive su un campione di studenti, includendo sia prestazioni scolastiche sia abilitÃ  percettive.

Le sei variabili considerate furono:

- Classics: rendimento nello studio dei classici;
- French: rendimento in lingua francese;
- English: rendimento in letteratura inglese;
- Math: rendimento in matematica;
- Pitch: abilitÃ  nella discriminazione dellâ€™altezza dei suoni;
- Music: competenza musicale.

Per semplicitÃ , nella discussione seguente considereremo solo tre materie scolastiche (studi classici, $c$, letteratura inglese, $e$, e matematica, $m$) e la discriminazione dellâ€™altezza dei suoni, $p$. Nel suo studio, Spearman riportÃ² le seguenti correlazioni:

$$
\begin{array}{ccccc}
  \hline
    & y_c & y_e & y_m & y_p \\
  \hline
  y_c & 1.00 & 0.78 & 0.70 & 0.66 \\
  y_e &      & 1.00 & 0.64 & 0.54 \\
  y_m &      &      & 1.00 & 0.45 \\
  y_p &      &      &      & 1.00 \\
  \hline
\end{array}
$$

Secondo la **Teoria dei Due Fattori**, proposta da Spearman, ogni prestazione intellettiva puÃ² essere scomposta in due componenti:

- un **fattore generale** (*g*), comune a tutti i compiti cognitivi;
- un **fattore specifico** (*s*), unico per ciascun compito.

Il fattore $g$ rappresenta la componente stabile e condivisa dellâ€™intelligenza, mentre ciascun fattore $s$ spiega la varianza residua specifica della singola prova. La domanda centrale Ã¨: esiste unâ€™unica variabile latente in grado di spiegare le covarianze osservate tra le variabili manifeste?

Per rispondere, Spearman utilizzÃ² il metodo dellâ€™**annullamento della tetrade**, che si basa sulle implicazioni della correlazione parziale. Abbiamo visto in precedenza che, se una variabile latente $\xi$ Ã¨ in grado di rendere nulle le correlazioni parziali tra le variabili osservate, allora:

$$
r_{ij} = r_{i\xi} \cdot r_{j\xi}.
$$

Nel contesto dei dati di Spearman, ciÃ² significa ad esempio che la correlazione osservata tra â€œstudi classiciâ€ ($c$) e â€œletteratura ingleseâ€ ($e$) deve essere uguale al prodotto delle loro correlazioni con il fattore comune $\xi$: $r_{ec} = \lambda_e \cdot \lambda_c$. Lo stesso vale per tutte le altre coppie di variabili.

Le correlazioni tra le variabili manifeste e il fattore latente sono chiamate **saturazioni fattoriali** e vengono denotate con $\lambda$. Il vincolo fondamentale del modello fattoriale Ã¨ che ogni correlazione osservata puÃ² essere scomposta come prodotto di due saturazioni.

## Annullamento della tetrade

Il metodo dellâ€™**annullamento della tetrade** si basa sullâ€™assunto centrale del modello fattoriale unifattoriale: se un singolo fattore comune spiega tutte le covarianze tra le variabili osservate, allora la correlazione tra ciascuna coppia di variabili puÃ² essere espressa come il prodotto delle loro **saturazioni fattoriali**:

$$
r_{ij} = \lambda_i \cdot \lambda_j.
$$

Questa relazione consente di tradurre le correlazioni osservate in un sistema di equazioni non lineari, in cui le incognite sono le saturazioni fattoriali $\lambda_i$.

Ad esempio, se consideriamo tre variabili â€” ad esempio *Classics* ($c$), *Math* ($m$) e *English* ($e$) â€” e assumiamo che siano tutte influenzate dallo stesso fattore latente $\xi$, possiamo scrivere:

$$
\begin{align}
r_{cm} &= \lambda_c \cdot \lambda_m, \notag \\
r_{em} &= \lambda_e \cdot \lambda_m, \\
r_{ce} &= \lambda_c \cdot \lambda_e. \notag
\end{align}
$$

Risolvendo questo sistema di equazioni, otteniamo un'espressione per ciascuna saturazione in funzione delle sole correlazioni osservate. Ad esempio, isolando $\lambda_m$:

$$
\lambda_m = \sqrt{ \frac{r_{cm} \cdot r_{em}}{r_{ce}} }.
$$  {#eq-tetrade}

Questa equazione mostra come si possa stimare il contributo di una variabile al fattore latente partendo esclusivamente dalle correlazioni empiriche tra le variabili osservate.

### Esempio con i Dati di Spearman

Utilizzando i dati riportati da Spearman nel 1904, e in particolare le variabili *Classics*, *Math* ed *English*, abbiamo:

$$
r_{cm} = 0.70, \quad r_{em} = 0.64, \quad r_{ce} = 0.78.
$$

Sostituendo nella formula:

$$
\hat{\lambda}_{\text{Math}} = \sqrt{ \frac{0.70 \cdot 0.64}{0.78} } = \sqrt{0.5733} \approx 0.76.
$$

Questo risultato rappresenta la **saturazione stimata** della variabile *Math* nel fattore comune latente $\xi$.

Il principio dellâ€™annullamento della tetrade fornisce quindi un ponte diretto tra le correlazioni osservate e la struttura latente sottostante, assumendo che un singolo fattore sia responsabile delle covarianze tra le variabili.


## Verifica del modello

Ripetendo il procedimento per diverse terne di variabili osservate, Ã¨ possibile ottenere **stime multiple** per la stessa saturazione fattoriale. Nel caso dei dati di Spearman, ad esempio, la saturazione $\lambda_m$ relativa alla variabile *Math* puÃ² essere stimata in almeno tre modi diversi, utilizzando combinazioni alternative di variabili:

$$
\begin{align}
  \hat{\lambda}_m &= \sqrt{ \frac{0.70 \cdot 0.64}{0.78} } = 0.76, \notag \\
  \hat{\lambda}_m &= \sqrt{ \frac{0.78 \cdot 0.45}{0.66} } = 0.69, \notag \\
  \hat{\lambda}_m &= \sqrt{ \frac{0.64 \cdot 0.45}{0.54} } = 0.73. \notag
\end{align}
$$

Questa molteplicitÃ  di stime evidenzia un aspetto fondamentale del modello fattoriale unifattoriale: **se il modello Ã¨ corretto**, ossia se un unico fattore comune spiega le correlazioni tra tutte le variabili, allora **tutte le stime ottenute devono essere coerenti tra loro**, entro un margine di errore accettabile. Se, al contrario, le stime risultano incoerenti o divergenti, ciÃ² suggerisce che il modello a un solo fattore non Ã¨ compatibile con i dati.

In altri termini, il modello di Spearman Ã¨ **verificabile empiricamente**: Ã¨ possibile confrontare le previsioni del modello (in termini di relazioni tra saturazioni e correlazioni osservate) con i dati raccolti. Sebbene Spearman non abbia formalizzato un test statistico per valutare la bontÃ  del modello, ha introdotto un principio centrale nellâ€™analisi fattoriale moderna: **non tutte le matrici di correlazione giustificano lâ€™ipotesi dellâ€™esistenza di un unico fattore latente**.

## Metodo del centroide

Una volta accettata lâ€™ipotesi che le stime multiple siano sufficientemente simili, ci si puÃ² chiedere come ottenere unâ€™unica stima sintetica per ciascuna saturazione.

Un primo approccio consiste semplicemente nel calcolare la **media aritmetica** delle stime ottenute:

$$
\bar{\lambda}_m = \frac{0.76 + 0.69 + 0.73}{3} \approx 0.73.
$$

Unâ€™alternativa piÃ¹ robusta Ã¨ rappresentata dal **metodo del centroide**, che consiste nel calcolare una media pesata dei prodotti numeratori e dei denominatori coinvolti nelle diverse stime:

$$
\hat{\lambda}_m = \sqrt{
\frac{0.70 \cdot 0.64 + 0.78 \cdot 0.45 + 0.64 \cdot 0.45}{0.78 + 0.66 + 0.54}
} = \sqrt{ \frac{1.031}{1.98} } \approx 0.73.
$$

In questo caso, entrambi i metodi producono un risultato identico. Applicando lo stesso procedimento alle altre variabili, otteniamo:

$$
\hat{\lambda}_c = 0.97, \quad \hat{\lambda}_e = 0.84, \quad \hat{\lambda}_p = 0.65,
$$

e quindi il vettore delle saturazioni fattoriali stimate Ã¨:

$$
\boldsymbol{\hat{\Lambda}}' =
(\hat{\lambda}_c, \hat{\lambda}_e, \hat{\lambda}_m, \hat{\lambda}_p) = (0.97, 0.84, 0.73, 0.65).
$$

**In sintesi**, lâ€™esempio tratto dai dati di Spearman mostra come, a partire da una semplice matrice di correlazione tra variabili osservate, sia possibile **ricostruire una struttura latente** assumendo lâ€™esistenza di un unico fattore comune. Il metodo dellâ€™annullamento della tetrade fornisce sia un criterio per verificare la coerenza del modello, sia una procedura per stimare le saturazioni fattoriali.

Sebbene oggi lâ€™analisi fattoriale si avvalga di metodi piÃ¹ avanzati, come la stima per massima verosimiglianza o i modelli bayesiani, il principio introdotto da Spearman conserva tutta la sua rilevanza: **la covarianza tra variabili osservate puÃ² essere interpretata come riflesso di costrutti latenti condivisi**. Ãˆ proprio questo passaggio â€” dal livello osservabile al livello teorico â€” che rende lâ€™analisi fattoriale uno strumento fondamentale per la psicometria e le scienze psicologiche.

## Introduzione a `lavaan`

Attualmente, l'analisi fattoriale viene svolta mediante software. Il pacchetto R piÃ¹ ampiamente utilizzato per condurre l'analisi fattoriale Ã¨ `lavaan`. 

### Sintassi del modello

Al cuore del pacchetto `lavaan` si trova la "sintassi del modello". La sintassi del modello Ã¨ una descrizione del modello da stimare. In questa sezione, spieghiamo brevemente gli elementi della sintassi del modello `lavaan`. 

Nell'ambiente R, una formula di regressione ha la seguente forma:

```
y ~ x1 + x2 + x3 + x4
```

In questa formula, la tilde ("~") Ã¨ l'operatore di regressione. Sul lato sinistro dell'operatore, abbiamo la variabile dipendente (y), e sul lato destro abbiamo le variabili indipendenti, separate dall'operatore "+" . In `lavaan`, un modello tipico Ã¨ semplicemente un insieme (o sistema) di formule di regressione, in cui alcune variabili (che iniziano con una 'f' qui sotto) possono essere latenti. Ad esempio:

```
y ~ f1 + f2 + x1 + x2
f1 ~ f2 + f3
f2 ~ f3 + x1 + x2
```

Se abbiamo variabili latenti in una qualsiasi delle formule di regressione, dobbiamo "definirle" elencando i loro indicatori (manifesti o latenti). Lo facciamo utilizzando l'operatore speciale "=~", che puÃ² essere letto come "Ã¨ misurato da". Ad esempio, per definire le tre variabili latenti f1, f2 e f3, possiamo usare la sintassi seguente:

```
f1 =~ y1 + y2 + y3
f2 =~ y4 + y5 + y6
f3 =~ y7 + y8 + y9 + y10
```

Inoltre, le varianze e le covarianze sono specificate utilizzando un operatore "doppia tilde", ad esempio:

```
y1 ~~ y1 # varianza
y1 ~~ y2 # covarianza
f1 ~~ f2 # covarianza
```

E infine, le intercette per le variabili osservate e latenti sono semplici formule di regressione con solo una intercetta (esplicitamente indicato dal numero "1") come unico predittore:

```
y1 ~ 1
f1 ~ 1
```

Utilizzando questi quattro tipi di formule, Ã¨ possibile descrivere una vasta gamma di modelli di variabili latenti. L'attuale insieme di tipi di formula Ã¨ riassunto nella tabella sottostante.

| tipo di formula | operatore |  mnemonic |
| --------------- | --------- | --------- |
| definizione variabile latente | =~ | Ã¨ misurato da |
| regressione | ~ | viene regredito su |
| (co)varianza (residuale) | ~~  |Ã¨ correlato con |
| intercetta | ~ 1 | intercetta |

Una sintassi completa del modello lavaan Ã¨ semplicemente una combinazione di questi tipi di formule, racchiusi tra virgolette singole. Ad esempio:

```
my_model <- ' 
  # regressions
  y1 + y2 ~ f1 + f2 + x1 + x2
  f1 ~ f2 + f3
  f2 ~ f3 + x1 + x2

  # latent variable definitions 
  f1 =~ y1 + y2 + y3 
  f2 =~ y4 + y5 + y6 
  f3 =~ y7 + y8 + y9
  
  # variances and covariances 
  y1 ~~ y1 
  y1 ~~ y2 
  f1 ~~ f2

  # intercepts 
  y1 ~ 1 
  f1 ~ 1
'
```

Per adattare il modello ai dati usiamo la seguente sintassi.

```
fit <- cfa(model = my_model, data = my_data)
```

## Un esempio concreto

Analizziamo nuovamente i dati di Spearman che abbiamo esaminato in precedenza usando `lavaan`. La matrice completa dei dati di Spearman Ã¨ messa a disposizione da @kan2019extending. 

Specifichiamo il nome delle variabili manifeste

```{r}
varnames <- c(
  "Classics", "French", "English", "Math", "Pitch", "Music"
)
```

e il loro numero

```{r}
ny <- length(varnames)
```

Creiamo la matrice di correlazione:

```{r}
spearman_cor_mat <- matrix(
  c(
    1.00,  .83,  .78,  .70,  .66,  .63,
     .83, 1.00,  .67,  .67,  .65,  .57,
     .78,  .67, 1.00,  .64,  .54,  .51,
     .70,  .67,  .64, 1.00,  .45,  .51,
     .66,  .65,  .54,  .45, 1.00,  .40,
     .63,  .57,  .51,  .51,  .40, 1.00
  ),
  ny, ny,
  byrow = TRUE,
  dimnames = list(varnames, varnames)
)
spearman_cor_mat
```

Specifichiamo l'ampiezza campionaria:

```{r}
n <- 33
```

Definiamo il modello unifattoriale in `lavaan`. L'operatore `=~` si puÃ² leggere dicendo che la variabile latente a sinistra dell'operatore viene identificata dalle variabili manifeste elencate a destra dell'operatore e separate dal segno `+`. Per il caso presente, il modello dei due fattori di Spearman puÃ² essere specificato come segue.

```{r}
spearman_mod <- "
  g =~ Classics + French + English + Math + Pitch + Music
"
```

Adattiamo il modello ai dati con la funzione `cfa()`:

```{r}
fit1 <- lavaan::cfa(
  spearman_mod,
  sample.cov = spearman_cor_mat,
  sample.nobs = n,
  std.lv = TRUE
)
```

La funzione `cfa()` del pacchetto `lavaan` serve per stimare un **modello di analisi fattoriale confermativa (CFA)**, cioÃ¨ un modello che verifica se un insieme di variabili osservate puÃ² essere spiegato da una o piÃ¹ **variabili latenti** (cioÃ¨ non direttamente osservabili).

Vediamo passo passo cosa significano gli argomenti specificati:

ðŸ”¹ `spearman_mod`: Ã¨ il **modello CFA specificato dallâ€™utente**. Di solito Ã¨ scritto come stringa e indica quali variabili osservate sono collegate a quali fattori latenti. Per esempio, `g =~ Classics + French + English + Math + Pitch + Music` significa che tutte le variabili (`Classics`, `French`, ...) sono indicatori del **fattore latente g**.

ðŸ”¹ `sample.cov = spearman_cor_mat`: qui non stiamo fornendo direttamente i dati grezzi, ma una **matrice di correlazioni** tra le variabili osservate.  Questo approccio Ã¨ comune quando si lavora con dati standardizzati o quando si vuole fare una CFA direttamente sulla matrice di correlazione.

ðŸ”¹ `sample.nobs = n`: Ã¨ il **numero di osservazioni** (cioÃ¨ i soggetti) usato per costruire la matrice di correlazioni. Serve a `lavaan` per calcolare **gli errori standard** e **gli indici di bontÃ  di adattamento** del modello.

ðŸ”¹ `std.all = TRUE`: questa opzione chiede a lavaan di standardizzare le variabili latenti nel modello. CiÃ² significa che:

- le variabili latenti (in questo caso il fattore g) vengono fissate con varianza = 1;
- questo approccio fornisce una scala alle variabili latenti, che altrimenti non avrebbero una metrica naturale;
- i coefficienti stimati (saturazioni fattoriali) saranno espressi in una scala piÃ¹ facilmente interpretabile;
- rappresenta una convenzione standard nell'analisi fattoriale confermativa.

Questo Ã¨ utile perchÃ©:

- semplifica l'interpretazione dei risultati;
- consente di confrontare piÃ¹ facilmente l'importanza relativa delle variabili osservate nel definire il fattore;
- fornisce una scala di riferimento coerente per il fattore latente.

### Esaminare i risultati del modello con `summary()`

Una volta adattato il modello CFA, possiamo esaminarne i risultati principali usando la funzione `summary()`. Ad esempio:

```{r}
out <- summary(
  fit1, 
  fit.measures = TRUE, 
  standardized = TRUE
)
out
```

Vediamo nel dettaglio cosa fanno i singoli argomenti:

ðŸ“Œ `summary(fit1, ...)`: Questa funzione fornisce un **riassunto dettagliato** del modello stimato (in questo caso, `fit1`). Lâ€™output include:

- informazioni generali sul modello (tipo di stimatore, ottimizzatore, numero di osservazioni, ecc.),
- le **stime dei parametri** (es. carichi fattoriali, varianze residue, covarianze),
- e, se richiesto, anche **gli indici di bontÃ  dellâ€™adattamento** e le **stime standardizzate**.

ðŸ“ `fit.measures = TRUE`: Questa opzione richiede a `lavaan` di includere nella stampa una **sezione con gli indici di bontÃ  di adattamento del modello**, tra cui:

- **Chi-quadrato** del modello e p-value associato,
- **CFI** (Comparative Fit Index),
- **TLI** (Tucker-Lewis Index),
- **RMSEA** (Root Mean Square Error of Approximation),
- **SRMR** (Standardized Root Mean Residual),  
- e altri.

Questi indici aiutano a valutare **quanto bene il modello riproduce le relazioni osservate nei dati**. Sono particolarmente utili per confrontare modelli alternativi e per verificare se il modello Ã¨ coerente con la struttura teorica ipotizzata.


ðŸ” `standardized = TRUE`: Questo argomento dice a `lavaan` di **aggiungere le stime standardizzate dei parametri**. Verranno incluse due colonne in piÃ¹ nellâ€™output:

ðŸ”¹ `Std.lv` (standardizzato rispetto alla variabile latente)

- Ogni carico fattoriale Ã¨ standardizzato **considerando che la variabile latente ha varianza 1**, ma **senza standardizzare le variabili osservate**.
- Questa colonna mostra **quanto una variabile osservata cambia in media per un aumento di 1 deviazione standard nel fattore latente**, mantenendo le osservate nella loro scala originale.
- Se hai usato `std.lv = TRUE` nella stima (come nel nostro esempio), allora `Estimate` e `Std.lv` saranno **identici**, perchÃ© la varianza della latente Ã¨ giÃ  fissata a 1.

ðŸ”¹ `Std.all` (completamente standardizzato)

- Questa colonna mostra i **carichi completamente standardizzati**, cioÃ¨ **sia la variabile latente che le variabili osservate sono standardizzate (varianza = 1)**.
- I valori possono essere letti come **correlazioni** tra ogni indicatore osservato e il fattore latente.
- Ãˆ la forma piÃ¹ comunemente riportata negli articoli scientifici, perchÃ©:
  - facilita il confronto tra variabili;
  - semplifica lâ€™interpretazione (tutti i parametri sono su una scala comparabile);
  - consente di capire **quali indicatori sono piÃ¹ rappresentativi del costrutto**.

â„¹ï¸ Nota sulla sezione **Varianze**

- Le righe con un **punto davanti al nome** (es. `.x1`) indicano **variabili osservate**: il valore rappresenta la **varianza residua**, cioÃ¨ la parte non spiegata dal fattore.
- Le **variabili latenti** (es. `g`, `F1`, ecc.) non hanno il punto iniziale: il valore indicato rappresenta la loro **varianza totale**.

ðŸ“‹ Come semplificare lâ€™output

Per estrarre solo la tabella delle stime:

```{r}
coef(fit1)  # solo le stime grezze (non standardizzate)
```

Oppure, per una tabella completa:

```{r}
parameterEstimates(fit1, standardized = TRUE)
```

Puoi anche **filtrare e formattare** lâ€™output per visualizzare solo le saturazioni fattoriali:

```{r}
parameterEstimates(fit1, standardized = TRUE) |>
  dplyr::filter(op == "=~") |>
  dplyr::select(
    "Fattore latente" = lhs,
    Indicatore = rhs,
    B = est,
    SE = se,
    Z = z,
    "p-value" = pvalue,
    Beta = std.all
  )
```


#### Analisi dei residui

Per valutare quanto il modello riesce a riprodurre le correlazioni osservate, possiamo esaminare la **matrice delle correlazioni residue**:

```{r}
residuals(fit1, type = "cor")$cov
```

Possiamo visualizzare graficamente i residui, ad esempio con un **Q-Q plot**:

```{r fig.asp=1}
res1 <- residuals(fit1, type = "cor")$cov
res1[upper.tri(res1, diag = TRUE)] <- NA  # Consideriamo solo i residui non ridondanti
v1 <- as.vector(res1)
v2 <- v1[!is.na(v1)]

tibble(v2) %>%
  ggplot(aes(sample = v2)) + 
  stat_qq() + 
  stat_qq_line()
```

Questo grafico serve a verificare se le **correlazioni residue** seguono una distribuzione normale: deviazioni dalla linea indicano **coppie di variabili mal rappresentate** dal modello fattoriale.


## Diagrammi di percorso

Il pacchetto `semPlot` consente di disegnare diagrammi di percorso per vari modelli SEM. La funzione `semPaths` prende in input un oggetto creato da `lavaan` e disegna il diagramma, con diverse opzioni disponibili. Il diagramma prodotto controlla le dimensioni dei caratteri/etichette, la visualizzazione dei residui e il colore dei percorsi/coefficienti. Sono disponibili queste e molte altre opzioni di controllo. 

```{r}
semPaths(
    fit1,
    "std",
    posCol = c("black"),
    edge.label.cex = 1.2,
    whatLabels = "std", 
    edge.width = 0.3, # Imposta lo spessore delle linee 
    fade = FALSE # Disabilita il fading
)
```

Il calcolo delle saturazioni fattoriali con il metodo del centroide aveva prodotto il seguente risultato: 

- classici (Cls): 0.97
- inglese (Eng): 0.84
- matematica (Mth): 0.73
- pitch discrimination (Ptc): 0.65

Si noti la somiglianza con i valori ottenuti mediante il metodo di massima verosimiglianza riportati nella figura.


## Analisi Fattoriale Esplorativa (EFA) in `lavaan`

L'**analisi fattoriale esplorativa (EFA)** Ã¨ una tecnica utilizzata per esplorare la struttura latente di un insieme di variabili osservate, **senza imporre vincoli a priori** su quali variabili siano associate a quali fattori. Anche in presenza di un solo fattore, EFA e CFA **non coincidono**, poichÃ© nel CFA i legami tra fattori e indicatori sono **specificati dallâ€™utente**, mentre nellâ€™EFA sono **liberamente stimati dal modello**.

In `lavaan`, possiamo specificare un modello EFA utilizzando la sintassi `efa("nome")*` nel modello.

Specificazione del modello EFA:

```{r}
efa_model <- '
  efa("efa")*g =~ Classics + French + English + Math + Pitch + Music
'
```

In questo esempio:

- `g` Ã¨ un **fattore esplorativo** (non confermativo);
- le saturazioni fattoriali saranno stimate **senza vincoli** a priori.


Adattamento del modello ai dati:

```{r}
fit2 <- lavaan::cfa(
  efa_model,
  sample.cov = spearman_cor_mat,
  sample.nobs = n,
  std.lv = TRUE
)
```

Nota: anche se usiamo la funzione `cfa()`, il modello Ã¨ trattato come EFA perchÃ© abbiamo specificato `efa()` nella sintassi del modello.

Esame della soluzione ottenuta:

```{r}
summary(fit2, standardized = TRUE)
```

Lâ€™output riporterÃ :

- le **saturazioni fattoriali esplorative**;
- le **varianze residue**;
- le **stime standardizzate**, tra cui `Std.all`, interpretabili come **correlazioni tra fattore e variabili osservate**.

Visualizzazione del modello (diagramma di percorso):

```{r}
semPaths(
  fit2,
  "std",
  posCol = c("black"),
  edge.label.cex = 1.2,
  whatLabels = "std",
  edge.width = 0.3,
  fade = FALSE
)
```

Questo comando crea un diagramma in cui:

- le **variabili osservate** sono collegate al **fattore esplorativo**;
- le etichette sui bordi mostrano i **carichi standardizzati** (`Std.all`).

ðŸ’¡ Osservazione importante

Se confronti la soluzione ottenuta con il modello CFA specificato come:

```r
cfa_model <- '
  g =~ Classics + French + English + Math + Pitch + Music
'
```

potresti ottenere **valori simili**, **ma la differenza concettuale rimane**:

- nella CFA, i legami tra fattore e indicatori sono **vincolati** (nessun cross-loading);
- nellâ€™EFA (via `efa()`), i carichi sono **liberamente stimati**: Ã¨ il modello a decidere "quanto" ogni variabile Ã¨ legata al fattore.


## Riflessioni Conclusive

In questo capitolo, abbiamo introdotto il metodo dellâ€™annullamento della tetrade, che permette di stimare le saturazioni in un modello monofattoriale. Abbiamo anche illustrato come questo metodo sia, in effetti, unâ€™applicazione del concetto di correlazione parziale.

Un aspetto fondamentale nella costruzione dei test psicologici riguarda la determinazione del numero di fattori o tratti sottostanti al set di indicatori in esame. La teoria classica dei test presuppone che un test sia monofattoriale, cioÃ¨ che gli indicatori riflettano un unico tratto latente. La mancata monodimensionalitÃ  introduce difficoltÃ  nellâ€™applicare i principi della teoria classica ai punteggi di un test che non soddisfa tale proprietÃ .

Lâ€™analisi della dimensionalitÃ  di un insieme di indicatori rappresenta, quindi, una fase cruciale nel processo di costruzione di un test. Solitamente, questa valutazione viene effettuata attraverso lâ€™analisi fattoriale. In questo capitolo, abbiamo descritto le proprietÃ  di base del modello unifattoriale, gettando le fondamenta per una comprensione piÃ¹ approfondita della dimensionalitÃ  e dell'influenza di un singolo tratto latente sugli indicatori.

## Session Info

```{r}
sessionInfo()
```

