# Il modello multifattoriale {#sec-fa-multifactor-model}

::: callout-important
## In questo capitolo imparerai a

- calcolare e interpretare la correlazione parziale;
- capire la teoria dei due fattori;
- applicare e comprendere il metodo dell'annullamento della tetrade.
:::

::: callout-tip
## Prerequisiti

- Leggere il capitolo 6, *Factor Analysis and Principal Component Analysis*, del testo *Principles of psychological assessment* di @petersen2024principles. 
:::

::: callout-caution
## Preparazione del Notebook

```{r}
here::here("code", "_common.R") |> 
  source()

if (!requireNamespace("pacman")) install.packages("pacman")
pacman::p_load(lavaan, semPlot, corrplot, tidyr, tidySEM, kableExtra)
```
:::

## Fattori ortogonali e teoria multifattoriale

La *teoria dei due fattori* di Spearman, secondo cui la prestazione in
compiti cognitivi sarebbe spiegata da un fattore generale (*g*) comune a
tutte le variabili e da un fattore specifico per ognuna di esse, ha
influenzato gli studi sull’intelligenza per diversi anni. Tuttavia, con
il passare del tempo, è emersa la necessità di spiegare in modo più
articolato la covariazione tra più variabili osservabili. A questo scopo
Thurstone (1945) propose la *teoria multifattoriale*, in base alla quale
la covariazione tra le variabili manifeste (*test*, *misure*) non può
essere riconducibile a un singolo fattore generale, ma deve essere
spiegata dall’azione congiunta di *diversi fattori* comuni (ognuno
relativo ad alcuni soltanto dei test).

### Dal modello a due fattori al modello multifattoriale

Nel modello multifattoriale di Thurstone, si assume che ogni variabile
manifesta $Y_i$ dipenda da:

1. **Un insieme di $m$ fattori comuni** ($\xi_1,\dots,\xi_m$), che
   spiegano la correlazione tra variabili diverse. Questi fattori sono
   detti *comuni* perché intervengono in più variabili manifeste.
2. **Un fattore specifico** (o *fattore unico*, indicato con
   $\delta_i$) proprio di ogni variabile manifesta. Questo fattore
   spiega la parte di varianza *non* condivisa con le altre variabili e
   viene spesso trattato come termine di errore o rumore statistico.

In presenza di $p$ variabili manifeste $Y_1,\dots,Y_p$, l’ipotesi
fondamentale è che vi siano molti meno fattori comuni ($m$) rispetto
al numero di variabili ($p$), così da avere un modello *parsimonioso*:
poche variabili latenti riescono a spiegare un gran numero di variabili
osservate.

### Notazione

- Le **variabili manifeste** $Y$ sono indicizzate da $i=1,\dots,p$.
- Le **variabili latenti a fattore comune** ($\xi$) sono indicizzate
  da $j=1,\dots,m$.
- I **fattori specifici** ($\delta$) sono indicizzati da
  $i=1,\dots,p$. Ciascun $\delta_i$ agisce *soltanto* su
  $Y_i$.
- Le **saturazioni fattoriali** $\lambda_{ij}$ sono i parametri che
  quantificano l’importanza del fattore $\xi_j$ nella composizione
  della variabile osservabile $Y_i$.

Inoltre, $\mu_i$ è la media della $i$-esima variabile manifesta
$Y_i$. Per semplicità, si assume che i fattori latenti abbiano media
zero, il che permette di separare nettamente la componente sistematica
($\xi_j$) dalla media $\mu_i$.

### Equazioni del modello multifattoriale

1. **Caso senza fattori comuni** (solo fattori specifici):

   $$
   \begin{cases}
     Y_{1k} = \mu_1 + \delta_{1k}, \\
     \vdots \\
     Y_{ik} = \mu_i + \delta_{ik}, \\
     \vdots \\
     Y_{pk} = \mu_p + \delta_{pk}.
   \end{cases}
   $$

   In questo scenario, ogni variabile $Y_i$ dipende solo dalla propria
   media $\mu_i$ e dal fattore specifico $\delta_i$. Poiché
   $\delta_i$ non è condiviso tra più variabili, le $Y_i$ risulterebbero
   incorrelate.

2. **Caso con $m$ fattori comuni** (oltre ai fattori specifici):

   $$
   \begin{cases}
     Y_1 - \mu_1 &= \lambda_{11}\,\xi_1 + \dots + \lambda_{1k}\,\xi_k + \dots + \lambda_{1m}\,\xi_m + \delta_1, \\
     \vdots & \\
     Y_i - \mu_i &= \lambda_{i1}\,\xi_1 + \dots + \lambda_{ik}\,\xi_k + \dots + \lambda_{im}\,\xi_m + \delta_i, \\
     \vdots & \\
     Y_p - \mu_p &= \lambda_{p1}\,\xi_1 + \dots + \lambda_{pk}\,\xi_k + \dots + \lambda_{pm}\,\xi_m + \delta_p.
   \end{cases}
   $$

   Ogni variabile manifesta $Y_i$ viene dunque vista come **combinazione
   lineare** di tutti e soli i fattori comuni $\xi_j$ e di un fattore
   specifico $\delta_i$ che la riguarda esclusivamente.

Riassumendo, nel modello multifattoriale:

- $\xi_j$ (con $j=1,\dots,m$) è la $j$-esima variabile latente a
  fattore comune;
- $\lambda_{ij}$ è il *peso fattoriale* o *saturazione* che misura
  quanto il fattore $\xi_j$ contribuisce a definire la variabile
  osservabile $Y_i$;
- $\delta_i$ è il fattore specifico, esclusivo della variabile $Y_i$.

### Assunzioni del modello multifattoriale

Per semplificare l’identificazione e l’interpretazione del modello,
vengono poste alcune assunzioni chiave:

1. **Media e varianza dei fattori comuni**:

   - $\mathbb{E}(\xi_j)=0$ per ogni $j=1,\dots,m$.  
     (*Non avendo unità di misura proprie, si impone una media nulla per
     rendere il modello più semplice da stimare.*)

   - $\mathbb{V}(\xi_j)=1$.  
     (*Analogamente, si normalizza la varianza dei fattori comuni a 1.*)

2. **Incorrelazione tra i fattori comuni**:  
   $$
   \text{Cov}(\xi_j, \xi_h)=0 \quad \text{per} \ j \neq h.
   $$  
   Se i fattori sono incorrelati, si parla di *fattori ortogonali*.
   Questa è un’ipotesi tipica nelle prime formulazioni, ma **può essere
   rilassata** nei modelli a fattori *obliqui*, dove si ammette la
   possibilità che i fattori comuni siano correlati tra loro.

3. **Incorrelazione tra i fattori specifici**:  
   $$
   \text{Cov}(\delta_i,\delta_k)=0 \quad \text{per} \ i \neq k,
   $$  
   con $\mathbb{E}(\delta_i) = 0$ e $\mathbb{V}(\delta_i) = \psi_{ii}$.
   La quantità $\psi_{ii}$ è la *varianza specifica* (o *unicità*) di
   $Y_i$.

4. **Incorrelazione tra fattori comuni e fattori specifici**:  
   $$
   \text{Cov}(\xi_j, \delta_i)=0
   \quad \text{per ogni} \ i=1,\dots,p \ \text{e} \ j=1,\dots,m.
   $$

In sintesi, il modello multifattoriale di Thurstone propone che la
relazione tra le variabili osservate sia spiegabile da un numero
relativamente piccolo di fattori latenti *ortogonali* (per via
dell’ipotesi di incorrelazione), ognuno dei quali influenza *solo alcune*
delle variabili. Ciascuna variabile riceve inoltre un contributo unico
da un fattore specifico. Grazie a queste ipotesi, si ottiene una
struttura lineare parsimoniosa, utile sia in ambito di ricerca (per
evidenziare eventuali dimensioni latenti comuni) sia in contesti
applicativi (ad esempio nella costruzione e validazione di test
psicologici).

---

**Nota**: Nella pratica, i fattori estratti con metodi come l’Analisi
dei Fattori (*Factor Analysis*) possono anche essere obliqui (cioè
correlati tra loro) qualora l’ipotesi di ortogonalità non sia realistica
o non sia empiricamente supportata dai dati. L’ortogonalità non è dunque
un dogma, bensì un’opzione che semplifica l’interpretazione ma che
potrebbe non essere sempre adeguata a rappresentare la realtà
psicologica sottostante.


## Interpretazione dei parametri del modello

### Covarianza tra variabili e fattori

Supponiamo che le variabili manifeste $Y_i$ abbiano media nulla
$\bigl(\mathbb{E}(Y_i)=0\bigr)$. In questo caso, la covarianza tra una
variabile $Y_i$ e un fattore comune $\xi_j$ coincide esattamente con
la corrispondente saturazione fattoriale $\lambda_{ij}$. Mostriamo
questo risultato nel dettaglio:

$$
\begin{aligned}
  \text{Cov}(Y_i, \xi_j) &= \mathbb{E}(Y_i \,\xi_j)\\
  &= \mathbb{E}\Bigl[\bigl(\lambda_{i1} \,\xi_1 + \dots + \lambda_{im} \,\xi_m + \delta_i\bigr)\,\xi_j \Bigr]\\
  &= \lambda_{i1}\,\underbrace{\mathbb{E}(\xi_1\,\xi_j)}_{=0}
     + \dots 
     + \lambda_{ij}\,\underbrace{\mathbb{E}(\xi_j^2)}_{=1}
     + \dots 
     + \lambda_{im}\,\underbrace{\mathbb{E}(\xi_m\,\xi_j)}_{=0}
     + \underbrace{\mathbb{E}(\delta_i\,\xi_j)}_{=0} \\
  &= \lambda_{ij}.
\end{aligned}
$$

- La prima e l’ultima eguaglianza seguono dall’ipotesi che i fattori
  comuni $\xi_j$ abbiano media zero e siano incorrelati tra loro
  ($\text{Cov}(\xi_j,\xi_h) = 0$ per $j \neq h$) e con i fattori
  specifici $\delta_i$.  
- Inoltre, poiché $\mathbb{V}(\xi_j)=1$, si ha
  $\mathbb{E}(\xi_j^2)=1$.  

In sintesi, **le saturazioni fattoriali
$\lambda_{ij}$ misurano la covarianza tra la variabile manifesta
$Y_i$ e il fattore latente $\xi_j$**, a patto che $\mathbb{E}(Y_i)=0$.

#### Saturazioni e correlazioni nel caso di variabili standardizzate

Se le variabili $Y_i$ sono ulteriormente *standardizzate*, ossia
$\mathbb{V}(Y_i)=1$, allora la saturazione fattoriale $\lambda_{ij}$
diventa la *correlazione* tra $Y_i$ e $\xi_j$. In tal caso,
scriveremo

$$
r_{ij} \;=\; \lambda_{ij}.
$$

Ciò fornisce un’interpretazione ancora più immediata delle saturazioni
fattoriali: in presenza di fattori latenti standardizzati, ciascuna
$\lambda_{ij}$ riflette quanto il fattore $\xi_j$ è correlato con
la variabile osservabile $Y_i$.


### Espressione fattoriale della varianza

Nel modello multifattoriale, analogamente a quanto avviene nel modello
monofattoriale, la varianza di ciascuna variabile manifesta $Y_i$ può
essere scomposta in due componenti:

1. Una **componente comune**, detta *comunalità*, che riflette la porzione
   di varianza di $Y_i$ spiegata dai fattori comuni.
2. Una **componente specifica**, detta *unicità*, che rappresenta la
   porzione di varianza di $Y_i$ non spiegata dai fattori comuni ed è
   attribuibile al fattore specifico $\delta_i$.

Supponendo che $\mathbb{E}(Y_i)=0$ per ogni $i$, la varianza della
variabile $Y_i$ è:

$$
\mathbb{V}(Y_i) 
= \mathbb{E}\Bigl[\bigl(\lambda_{i1}\,\xi_1 + \dots + \lambda_{im}\,\xi_m + \delta_i\bigr)^2\Bigr].
$$

#### Sviluppo del polinomio

Sviluppando il **quadrato** del termine tra parentesi, ricordiamo che il
quadrato di una somma include:

1. La somma dei quadrati di tutti i termini.
2. Il doppio prodotto di ogni termine con ciascuno degli altri termini
   successivi.

In formula:

$$
(a + b + c)^2 \;=\; a^2 + b^2 + c^2 \;+\; 2ab + 2ac + 2bc.
$$

Applicando lo stesso principio al nostro caso:

$$
\Bigl(\lambda_{i1}\,\xi_1 + \dots + \lambda_{im}\,\xi_m + \delta_i\Bigr)^2
= \sum_{j=1}^m \lambda_{ij}^2\,\xi_j^2 \;+\; \delta_i^2
  \;+\; \text{(termini di doppio prodotto)}.
$$

#### Valore atteso: contributo dei singoli fattori e delle loro interazioni

Per calcolare $\mathbb{V}(Y_i)$, prendiamo il **valore atteso** di
questo polinomio:

- **Termini al quadrato dei fattori comuni**:  
  Poiché $\mathbb{V}(\xi_j) = 1$ e $\mathbb{E}(\xi_j^2) = 1$, il
  contributo di ciascun fattore comune $\xi_j$ è
  $\lambda_{ij}^2$.
- **Termine al quadrato del fattore specifico**:  
  $\mathbb{E}(\delta_i^2) = \psi_{ii}$, dove $\psi_{ii}$ è la
  varianza specifica della variabile $Y_i$.
- **Termini di doppio prodotto**:  
  Grazie all’ipotesi di ortogonalità, la covarianza tra fattori comuni
  diversi è nulla ($\mathbb{E}(\xi_j\,\xi_h) = 0$ per $j \neq h$), e
  la covarianza tra fattori comuni e specifici è anch’essa nulla
  ($\mathbb{E}(\delta_i\,\xi_j) = 0$). Di conseguenza, tutti i doppi
  prodotti si annullano e non contribuiscono alla varianza totale.

#### Risultato finale

In conclusione, la varianza di $Y_i$ si ottiene sommando i contributi
di tutti i fattori comuni e del fattore specifico:

$$
\mathbb{V}(Y_i) 
= \lambda_{i1}^2 + \lambda_{i2}^2 + \dots + \lambda_{im}^2 + \psi_{ii}
= \sum_{j=1}^m \lambda_{ij}^2 + \psi_{ii}.
$$

- **Comunalità** $\,h_i^2 = \sum_{j=1}^m \lambda_{ij}^2$: misura la
  quota di varianza di $Y_i$ spiegata dai fattori comuni.
- **Unicità** $\,\psi_{ii}$: rappresenta la varianza *non* spiegata
  dai fattori comuni, associata esclusivamente al fattore specifico
  $\delta_i$.

In sintesi, per ogni variabile $\,Y_i$, la **somma** di comunalità e
unicità deve coincidere con la sua varianza totale, a conferma che
l’analisi fattoriale scompone ciascuna variabile in una parte “comune”
e una parte “specifica”.


### Espressione fattoriale della covarianza

A titolo di esempio, consideriamo il caso di $p=5$ variabili osservate
e $m=2$ fattori ortogonali. Assumiamo inoltre che le variabili
manifeste siano state *centrate* (ossia abbiano media nulla), così da
poter omettere i termini costanti. In queste condizioni, il modello
multifattoriale si scrive:

$$
\begin{cases}
  Y_1 = \lambda_{11}\,\xi_1 + \lambda_{12}\,\xi_2 + \delta_1, \\
  Y_2 = \lambda_{21}\,\xi_1 + \lambda_{22}\,\xi_2 + \delta_2, \\
  Y_3 = \lambda_{31}\,\xi_1 + \lambda_{32}\,\xi_2 + \delta_3, \\
  Y_4 = \lambda_{41}\,\xi_1 + \lambda_{42}\,\xi_2 + \delta_4, \\
  Y_5 = \lambda_{51}\,\xi_1 + \lambda_{52}\,\xi_2 + \delta_5.
\end{cases}
$$

Ricordiamo che:

- $\xi_1$ e $\xi_2$ sono i due fattori comuni, con
  $\mathbb{E}(\xi_j) = 0$, $\mathbb{V}(\xi_j)=1$ e
  $\mathrm{Cov}(\xi_1,\xi_2)=0$.
- $\delta_i$ è il fattore specifico associato a $Y_i$, con
  $\mathbb{E}(\delta_i)=0$ e
  $\mathrm{Cov}(\delta_i,\delta_k)=0$ per $i \neq k$.
- I fattori comuni sono incorrelati con i fattori specifici
  ($\mathrm{Cov}(\xi_j,\delta_i)=0$).

#### Calcolo esplicito di una covarianza

Mostriamo, nello specifico, come si ottiene la covarianza
$\mathrm{Cov}(Y_1,Y_2)$. Supponendo $\mathbb{E}(Y_1)=\mathbb{E}(Y_2)=0$,

$$
\begin{aligned}
  \mathrm{Cov}(Y_1, Y_2) 
    &= \mathbb{E}\bigl(Y_1\,Y_2\bigr) \\
    &= \mathbb{E}\Bigl[
       (\lambda_{11} \,\xi_1 + \lambda_{12} \,\xi_2 + \delta_1)
       \,(\lambda_{21} \,\xi_1 + \lambda_{22} \,\xi_2 + \delta_2)
       \Bigr].
\end{aligned}
$$

Sviluppando il prodotto e facendo uso delle ipotesi di ortogonalità, si
ottiene:

1. $\lambda_{11}\,\lambda_{21}\,\mathbb{E}(\xi_1^2)$. Qui, siccome
   $\mathrm{Var}(\xi_1)=1$, abbiamo $\mathbb{E}(\xi_1^2)=1$.
2. $\lambda_{11}\,\lambda_{22}\,\mathbb{E}(\xi_1\,\xi_2)=0$ perché
   $\mathrm{Cov}(\xi_1,\xi_2)=0$.
3. $\lambda_{11}\,\mathbb{E}(\xi_1\,\delta_2)=0$ perché i fattori
   comuni e i fattori specifici sono incorrelati.
4. $\lambda_{12}\,\lambda_{21}\,\mathbb{E}(\xi_2\,\xi_1)=0$ per la
   stessa ragione del punto 2.
5. $\lambda_{12}\,\lambda_{22}\,\mathbb{E}(\xi_2^2)$. Analogamente,
   $\mathrm{Var}(\xi_2)=1$, quindi $\mathbb{E}(\xi_2^2)=1$.
6. $\lambda_{12}\,\mathbb{E}(\xi_2\,\delta_2)=0$ (incorrelazione tra
   fattori).
7. $\lambda_{21}\,\mathbb{E}(\xi_1\,\delta_1)=0$ (stessa ragione).
8. $\lambda_{22}\,\mathbb{E}(\xi_2\,\delta_1)=0$ (stessa ragione).
9. $\mathbb{E}(\delta_1\,\delta_2)=0$ (i fattori specifici sono
   incorrelati tra loro).

Con tutti i termini di doppio prodotto che si annullano, rimane solo la
somma dei termini che includono $\xi_1^2$ e $\xi_2^2$:

$$
\mathrm{Cov}(Y_1, Y_2) 
= \lambda_{11}\,\lambda_{21} \,+\, \lambda_{12}\,\lambda_{22}.
$$

#### Interpretazione

In generale, la covarianza tra due variabili manifeste $Y_\ell$ e
$Y_m$ in un modello multifattoriale con $m$ fattori ortogonali si
può interpretare come **la somma dei prodotti tra le saturazioni nei
fattori comuni condivisi**:

$$
\mathrm{Cov}(Y_\ell, Y_m) 
= \sum_{j=1}^m \lambda_{\ell j}\,\lambda_{mj}.
$$

Questo vuol dire che due variabili $Y_\ell$ e $Y_m$ risultano
correlate soltanto nella misura in cui condividono (in senso letterale)
gli stessi fattori comuni. Se in una determinata posizione $j$ una
variabile ha saturazione prossima a zero, il contributo di quel fattore
alla covarianza tra le due variabili sarà molto basso (o nullo).

---

Questo esempio illustra in modo concreto come, nel modello
multifattoriale, la *covarianza* osservata tra due variabili sia
l’effetto cumulativo dei **prodotti delle loro saturazioni** nei fattori
comuni. La struttura di queste saturazioni diventa quindi fondamentale
per interpretare quali fattori latenti spiegano la correlazione tra le
varie misure osservate.

::: {#exm-} 
Consideriamo, a titolo di esempio, i dati presentati da @brown2015confirmatory relativi a un campione di 250 pazienti che hanno completato un programma di psicoterapia. Su ciascun soggetto sono state raccolte otto misure di personalità, corrispondenti ad altrettante scale:

- **N1**: anxiety  
- **N2**: hostility  
- **N3**: depression  
- **N4**: self-consciousness  
- **E1**: warmth  
- **E2**: gregariousness  
- **E3**: assertiveness  
- **E4**: positive emotions  

Le prime quattro scale riflettono diverse sfaccettature del
**neuroticismo** (N), mentre le ultime quattro sono riferite
all’**estroversione** (E). Di seguito vengono mostrate le deviazioni
standard di ciascuna scala, oltre alla matrice di correlazioni osservate
(qui denominata `psychot_cor_mat`) e alla dimensione campionaria
($n=250$).

```{r}
varnames <- c("N1", "N2", "N3", "N4", "E1", "E2", "E3", "E4")
sds <- '5.7  5.6  6.4  5.7  6.0  6.2  5.7  5.6'

cors <- '
    1.000
    0.767  1.000 
    0.731  0.709  1.000 
    0.778  0.738  0.762  1.000 
    -0.351  -0.302  -0.356  -0.318  1.000 
    -0.316  -0.280  -0.300  -0.267  0.675  1.000 
    -0.296  -0.289  -0.297  -0.296  0.634  0.651  1.000 
    -0.282  -0.254  -0.292  -0.245  0.534  0.593  0.566  1.000
'

psychot_cor_mat <- getCov(cors, names = varnames)
n_obs <- 250
```


### Analisi fattoriale esplorativa

Applichiamo un’**analisi fattoriale esplorativa** (EFA) con metodo di
stima a **massima verosimiglianza** e ipotizziamo la presenza di
**due fattori comuni incorrelati**. In R, il comando utilizzato è:

```{r}
fit_efa <- factanal(
  covmat = psychot_cor_mat,
  factors = 2,
  rotation = "varimax",
  n.obs = n_obs
)
```

Dalle saturazioni fattoriali (ottenute tramite `fit_efa$loadings`)
emerge la presenza di **due fattori**:

1. Il **primo fattore** satura principalmente sulle scale di
   **neuroticismo** (N1, N2, N3, N4).  
2. Il **secondo fattore** satura principalmente sulle scale di
   **estroversione** (E1, E2, E3, E4).

### Covarianze (o correlazioni) riprodotte dal modello

Una volta stimati i fattori e le relative saturazioni
$\boldsymbol{\Lambda}$, possiamo confrontare la matrice di
correlazioni osservate con quella **riprodotta** dal modello fattoriale.

#### Esempio di correlazione riprodotta

Per illustrare il concetto, consideriamo la correlazione riprodotta tra
N1 (prima variabile) e N2 (seconda variabile), che secondo il modello a
**due fattori incorrelati** si ottiene sommando i prodotti delle
saturazioni sui singoli fattori:

$$
r_{12}^{(\text{modello})} 
= \lambda_{11}\,\lambda_{21} + \lambda_{12}\,\lambda_{22},
$$

dove:

- $\lambda_{1j}$ è la saturazione della variabile $Y_1$ sul fattore
  $j$.
- $\lambda_{2j}$ è la saturazione della variabile $Y_2$ sul fattore
  $j$.
  
Nell’esempio in R, otteniamo le saturazioni fattoriali:
  
```{r}
lambda <- fit_efa$loadings 
lambda
```


L’espressione

```{r}
lambda[1, 1] * lambda[2, 1] + lambda[1, 2] * lambda[2, 2]
```

restituisce un valore molto simile alla correlazione empirica 0.767,
confermando la bontà dell’adattamento del modello per questa coppia di
variabili.

#### Intera matrice di correlazioni riprodotte

L’intera **matrice di correlazioni riprodotte** dal modello è data da:

$$
\mathbf{R}_{\text{riprodotta}}
=
\boldsymbol{\Lambda}\,\boldsymbol{\Lambda}^{\mathsf{T}}
+
\boldsymbol{\Psi},
$$

dove $\boldsymbol{\Lambda}$ è la matrice delle saturazioni fattoriali
(e righe corrispondono alle variabili, colonne ai fattori) e
$\boldsymbol{\Psi}$ è la matrice diagonale contenente le varianze
specifiche (unicità) stimate. In R, possiamo calcolare questa matrice
con:

```{r}
Rr <- lambda %*% t(lambda) + diag(fit_efa$uniq)
Rr |> round(2)
```

Confrontando `Rr` con la matrice di correlazioni osservate
(`psychot_cor_mat`), otteniamo l’**errore di riproduzione**, ovvero la
differenza tra i valori osservati e quelli teorici previsti dal modello:

```{r}
psychot_cor_mat - Rr |> round(3)
```

Idealmente, se il modello fattoriale si adatta bene ai dati, tale
differenza risulterà piccola per tutte le coppie di variabili
(osservate).

**In sintesi**, questo esempio dimostra come un modello con **due fattori
incorrelati** sia in grado di spiegare le correlazioni tra le otto scale
di personalità. Il primo fattore, caricato dalle scale di neuroticismo,
e il secondo, caricato da quelle di estroversione, confermano così la
struttura a due dimensioni attesa.
:::


## Fattori obliqui

Nel modello fattoriale, può accadere che i **fattori comuni** non siano ortogonali, ma **correlati tra loro**. In tal caso si parla di **fattori obliqui**. Anche in questa situazione, è possibile esprimere:

- la **covarianza teorica** tra una variabile manifesta $Y_i$ e un fattore comune $\xi_j$,
- la **covarianza teorica** tra due variabili manifeste,
- la **comunalità** di ciascuna variabile manifesta,

ma le formule diventano più complesse rispetto al caso ortogonale, perché occorre tener conto anche delle **covarianze tra i fattori comuni**.

### Covarianza teorica tra variabili manifeste e fattori comuni

Nel modello multifattoriale con $m$ fattori comuni, ciascuna variabile manifesta è modellata come:

$$
Y_i = \lambda_{i1} \xi_1 + \dots + \lambda_{im} \xi_m + \delta_i .
$$ {#eq-modello-obliquo}

Vogliamo calcolare la **covarianza teorica** tra la variabile $Y_i$ e un fattore comune $\xi_j$. Sfruttando la linearità del valore atteso, abbiamo:

$$
\begin{aligned}
\mathrm{Cov}(Y_i, \xi_j)
&= \mathbb{E}(Y_i \cdot \xi_j) \\
&= \mathbb{E}\left[
\left(\sum_{h=1}^{m} \lambda_{ih} \xi_h + \delta_i \right) \xi_j
\right] \\
&= \sum_{h=1}^{m} \lambda_{ih} \cdot \mathrm{Cov}(\xi_h, \xi_j)
+ \underbrace{\mathrm{Cov}(\delta_i, \xi_j)}_{=0} \\
&= \sum_{h=1}^{m} \lambda_{ih} \cdot \phi_{hj},
\end{aligned}
$$

dove $\phi_{hj} = \mathrm{Cov}(\xi_h, \xi_j)$ è l’elemento $(h,j)$ della matrice di covarianze tra i fattori comuni, denotata con $\boldsymbol{\Phi}$.

**Esempio con 3 fattori comuni.** 

La covarianza tra $Y_1$ e il primo fattore $\xi_1$ è:

$$
\mathrm{Cov}(Y_1, \xi_1) = \lambda_{11} + \lambda_{12} \cdot \phi_{21} + \lambda_{13} \cdot \phi_{31}.
$$


### Varianza teorica di una variabile manifesta

Partendo dall'@eq-modello-obliquo, vogliamo calcolare la **varianza teorica** di $Y_i$. Sviluppiamo il quadrato:

$$
\mathrm{Var}(Y_i) = \mathbb{E}\left[Y_i^2\right] = \mathbb{E}\left[
\left(\sum_{j=1}^m \lambda_{ij} \xi_j + \delta_i \right)^2
\right].
$$

Sviluppando il quadrato e distribuendo il valore atteso:

$$
\mathrm{Var}(Y_i) =
\sum_{j=1}^m \lambda_{ij}^2 + 
2 \sum_{j<k} \lambda_{ij} \lambda_{ik} \cdot \phi_{jk} + 
\psi_{ii},
$$

dove $\psi_{ii} = \mathrm{Var}(\delta_i)$ è l’unicità.

**Esempio con tre fattori.** La varianza teorica di $Y_1$ sarà:

$$
\begin{aligned}
\mathrm{Var}(Y_1) =\; & \lambda_{11}^2 + \lambda_{12}^2 + \lambda_{13}^2 + \\
& 2 \lambda_{11} \lambda_{12} \phi_{12} +
  2 \lambda_{11} \lambda_{13} \phi_{13} +
  2 \lambda_{12} \lambda_{13} \phi_{23} + \\
& \psi_{11}.
\end{aligned}
$$


### Covarianza teorica tra due variabili manifeste

Nel caso di due variabili $Y_1$ e $Y_2$ spiegate da due fattori obliqui $\xi_1$ e $\xi_2$, abbiamo:

$$
\begin{aligned}
\mathrm{Cov}(Y_1, Y_2) =\; &
\lambda_{11} \lambda_{21} + 
\lambda_{12} \lambda_{22} + \\
& \lambda_{11} \lambda_{22} \cdot \phi_{12} +
  \lambda_{12} \lambda_{21} \cdot \phi_{12}.
\end{aligned}
$$

Raccogliendo $\phi_{12}$:

$$
\mathrm{Cov}(Y_1, Y_2) = 
\lambda_{11} \lambda_{21} +
\lambda_{12} \lambda_{22} +
\phi_{12} (\lambda_{11} \lambda_{22} + \lambda_{12} \lambda_{21}).
$$


### Forma matriciale del modello

Nel caso generale, con $p$ variabili e $m$ fattori obliqui, la **matrice di covarianze teoriche** è:

$$
\boldsymbol{\Sigma} =
\boldsymbol{\Lambda} \boldsymbol{\Phi} \boldsymbol{\Lambda}^\mathsf{T} +
\boldsymbol{\Psi},
$$

dove:

- $\boldsymbol{\Lambda}$: matrice $p \times m$ delle saturazioni fattoriali;
- $\boldsymbol{\Phi}$: matrice $m \times m$ di covarianze tra i fattori comuni (non più diagonale);
- $\boldsymbol{\Psi}$: matrice diagonale $p \times p$ delle unicità.


## Applicazione con R: modello con fattori obliqui

Torniamo ai dati di personalità esaminati in precedenza. Applichiamo ora un’analisi fattoriale con **rotazione obliqua (oblimin)**, che consente ai due fattori di essere correlati:

```{r}
n_obs <- 250

efa_result <- fa(
  psychot_cor_mat,
  nfactors = 2,
  n.obs = n_obs,
  rotate = "oblimin"
)
```

Visualizziamo la struttura del modello:

```{r}
fa.diagram(efa_result)
```

### Saturazioni fattoriali e parametri del modello

```{r}
lambda <- matrix(efa_result$loadings[, 1:2], nrow = 8, ncol = 2)

rownames(lambda) <- c("N1", "N2", "N3", "N4", "E1", "E2", "E3", "E4")
colnames(lambda) <- c("Factor1", "Factor2")
lambda
```

Matrice di intercorrelazioni tra i fattori:

```{r}
Phi <- efa_result$Phi
Phi
```

Unicità:

```{r}
Psi <- diag(efa_result$uniquenesses)
round(Psi, 2)
```

### Matrice delle correlazioni riprodotte

Costruiamo la matrice di correlazioni riprodotte dal modello obliquo:

```{r}
R_hat <- lambda %*% Phi %*% t(lambda) + Psi
round(R_hat, 2)
```

Differenza con la matrice osservata:

```{r}
round(psychot_cor_mat - R_hat, 2)
```

### Correlazione riprodotta tra due variabili

Per esempio, la correlazione tra N1 e N2 predetta dal modello è:

```{r}
lambda[1,1] * lambda[2,1] +
lambda[1,2] * lambda[2,2] +
lambda[1,1] * lambda[2,2] * Phi[1,2] +
lambda[1,2] * lambda[2,1] * Phi[1,2]
```

Questo valore dovrebbe essere molto vicino al valore osservato:

```{r}
psychot_cor_mat[1, 2]
```

**In sintesi**, nel modello a **fattori obliqui**, la struttura delle correlazioni tra variabili manifeste dipende **non solo dalle saturazioni**, ma anche dalla **correlazione tra i fattori comuni**. Questo tipo di modello è più flessibile e spesso più realistico in psicologia, dove i costrutti latenti tendono a essere interdipendenti.

## Riflessioni Conclusive

In questo capitolo abbiamo esaminato il modello di analisi fattoriale comune, distinguendo tra l’ipotesi di **fattori ortogonali** e quella più generale di **fattori obliqui**. Abbiamo visto che, mentre i modelli ortogonali permettono una formulazione più semplice e interpretazioni più immediate, i modelli obliqui risultano più flessibili e realistici, poiché ammettono correlazioni tra i fattori latenti. Abbiamo inoltre analizzato come le covarianze tra le variabili osservate possano essere espresse in funzione delle saturazioni fattoriali, delle intercorrelazioni tra i fattori e delle unicità specifiche. Infine, abbiamo illustrato come queste relazioni teoriche si traducono concretamente nell’applicazione empirica dell’analisi fattoriale esplorativa, mediante l’uso del software R. Comprendere la struttura fattoriale sottostante a un insieme di variabili psicologiche osservate consente di sintetizzare l’informazione in modo più parsimonioso, rivelando le dimensioni latenti che organizzano il comportamento osservato.

## Session Info

```{r}
sessionInfo()
```

