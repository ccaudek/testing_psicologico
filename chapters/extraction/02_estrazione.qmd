# L'estrazione dei fattori {#sec-extraction-factor-extraction}

::: callout-important
## In questo capitolo imparerai:

- Come implementare il **metodo delle componenti principali** per l'estrazione delle saturazioni fattoriali utilizzando l'algebra matriciale.  
- Come implementare il **metodo dei fattori principali** e comprenderne i fondamenti teorici.  
- Il funzionamento del **metodo dei fattori principali iterato**, con un focus sul processo di convergenza.  
- Le caratteristiche principali del **metodo di massima verosimiglianza** e il suo utilizzo nell'analisi fattoriale.  
:::

::: callout-tip
## Prerequisiti

- Leggere il capitolo *Factor Analysis and Principal Component Analysis* del testo di @petersen2024principles.
:::

::: callout-caution
## Preparazione del Notebook

```{r}
here::here("code", "_common.R") |> 
  source()

# Load packages
if (!requireNamespace("pacman")) install.packages("pacman")
pacman::p_load(lavaan, psych, GPArotation)
```
:::

## Introduzione

L’**analisi fattoriale** è una tecnica statistica multivariata utilizzata per **identificare strutture latenti**, ovvero **fattori non osservabili**, che spiegano le **correlazioni tra variabili osservate**.

In altre parole, quando abbiamo molte variabili (es. item di un questionario), l’analisi fattoriale cerca di scoprire **quali gruppi di item misurano lo stesso costrutto psicologico sottostante**, riducendo così la complessità dei dati.

Questa tecnica è particolarmente utile nelle scienze sociali e in psicologia, dove **costrutti astratti** come *intelligenza*, *ansia* o *autostima* non possono essere misurati direttamente, ma solo attraverso più item. L’analisi fattoriale aiuta a verificare se questi item misurano effettivamente **un numero limitato di costrutti** sottostanti, rendendo i dati più interpretabili.


### Il modello statistico dell’analisi fattoriale

Il modello matematico alla base dell’analisi fattoriale si esprime così:

$$
\boldsymbol{\Sigma} = \boldsymbol{\Lambda} \boldsymbol{\Phi} \boldsymbol{\Lambda}^{\mathsf{T}} + \boldsymbol{\Psi},
$$

dove:

- $\boldsymbol{\Sigma}$ è la **matrice delle covarianze osservate** tra le variabili;
- $\boldsymbol{\Lambda}$ è la **matrice dei carichi fattoriali**: ciascun elemento rappresenta l’intensità della relazione tra una variabile osservata e un fattore latente;
- $\boldsymbol{\Phi}$ è la **matrice delle correlazioni tra i fattori** (è l’identità se i fattori sono ortogonali, cioè non correlati);
- $\boldsymbol{\Psi}$ è una **matrice diagonale** contenente le **unicità** (la porzione di varianza di ciascuna variabile che non è spiegata dai fattori comuni).

Questo modello riflette l’idea che **ogni variabile osservata sia influenzata da uno o più fattori comuni**, più una componente specifica e casuale (l’unicità).


## Estrazione dei Fattori: Panoramica dei Metodi

L’**estrazione dei fattori** consiste nel **stimare i parametri** del modello (soprattutto $\boldsymbol{\Lambda}$), sulla base della matrice di correlazioni o covarianze. I diversi metodi si distinguono per:

- le **assunzioni statistiche** (es. normalità dei dati);
- il tipo di **informazione utilizzata** (es. varianza totale o varianza comune);
- la possibilità di **testare l’adattamento del modello ai dati**.

| Metodo                         | Tiene conto della specificità? | Richiede normalità? | Permette test di bontà del modello? |
|-------------------------------|-------------------------------|----------------------|-------------------------------------|
| Componenti principali (PCA)   | ❌ No                         | ❌ No               | ❌ No                              |
| Fattori principali            | ✅ Sì                         | ❌ No               | ❌ No                              |
| Fattori principali iterato    | ✅ Sì (con aggiornamenti)     | ❌ No               | ❌ No                              |
| Massima verosimiglianza (ML)  | ✅ Sì                         | ✅ Sì               | ✅ Sì                              |

Vediamo ora in dettaglio ciascun metodo.


## Metodo delle Componenti Principali (PCA)

> ❗ **Importante:** Sebbene sia molto diffuso, il metodo delle componenti principali (*Principal Component Analysis*, PCA) **non è un vero metodo fattoriale**. Non fa distinzione tra **varianza comune** (quella condivisa tra variabili) e **varianza specifica** (quella unica di ogni variabile), e **non assume l’esistenza di fattori latenti**. Per questo motivo, in psicometria, viene usato **per riduzione della dimensionalità**, non per identificare costrutti teorici.

### Obiettivo

La PCA costruisce un numero ridotto di **componenti principali**:

- sono **combinazioni lineari** delle variabili originali;
- sono **ortogonali** (cioè non correlate tra loro);
- spiegano progressivamente la **massima varianza possibile** nei dati.

### Fondamento teorico: il teorema spettrale

La PCA si basa sul **teorema spettrale**, che dice che ogni **matrice simmetrica** (come la matrice di correlazione $\mathbf{R}$) può essere scomposta come:

$$
\mathbf{R} = \mathbf{C} \mathbf{D} \mathbf{C}^{\mathsf{T}},
$$

dove:

- $\mathbf{C}$ è la matrice i cui **vettori colonna sono gli autovettori** (direzioni principali) di $\mathbf{R}$;
- $\mathbf{D}$ è una **matrice diagonale** con gli **autovalori** (quantità di varianza spiegata da ciascuna direzione);
- $\mathbf{C}^{\mathsf{T}}$ è la trasposta di $\mathbf{C}$.

Questa è la **scomposizione spettrale** della matrice $\mathbf{R}$.

### Costruzione delle saturazioni (carichi)

Vogliamo una matrice $\hat{\boldsymbol{\Lambda}}$ che approssimi la matrice $\mathbf{R}$:

$$
\mathbf{R} \approx \hat{\boldsymbol{\Lambda}} \hat{\boldsymbol{\Lambda}}^\mathsf{T}.
$$

Poiché $\mathbf{D}$ è diagonale, possiamo scriverla come:

$$
\mathbf{D} = \mathbf{D}^{1/2} \cdot \mathbf{D}^{1/2},
$$

dove $\mathbf{D}^{1/2}$ ha sulla diagonale le **radici quadrate degli autovalori**. Allora:

$$
\mathbf{R} = \mathbf{C} \mathbf{D}^{1/2} \cdot \mathbf{D}^{1/2} \mathbf{C}^{\mathsf{T}}.
$$

Definiamo:

$$
\hat{\boldsymbol{\Lambda}} = \mathbf{C} \mathbf{D}^{1/2},
$$

e otteniamo:

$$
\hat{\boldsymbol{\Lambda}} \hat{\boldsymbol{\Lambda}}^\mathsf{T} = \mathbf{R}.
$$

> 💡 Le **saturazioni** si ottengono moltiplicando **ogni autovettore per la radice quadrata dell’autovalore corrispondente**. Questo consente di ricostruire esattamente la matrice $\mathbf{R}$.

Ogni elemento $l_{ij}$ di $\hat{\boldsymbol{\Lambda}}$ indica **quanto la variabile $i$ contribuisce alla componente $j$**.

Quando si selezionano solo i primi $k$ autovalori e autovettori (cioè quelli che spiegano più varianza), si ottiene una **rappresentazione semplificata** dei dati, utile per la riduzione della dimensionalità.

### Interpretazione

- Gli **autovalori** indicano quanta varianza è spiegata da ciascuna componente.
- Le **componenti principali** sono nuove variabili non osservate, che sintetizzano l’informazione contenuta nelle variabili originali.

### Limiti della PCA come analisi fattoriale

- Non separa **varianza comune** da **varianza specifica**.
- Non assume **fattori latenti**.
- Non consente di valutare l’adattamento del modello ai dati.

### Quando usarla

La PCA è utile quando:

- si vuole **ridurre il numero di variabili** mantenendo la massima varianza;
- si desidera costruire **indici sintetici** (es. punteggi compositi);
- si vuole **esplorare la struttura dei dati** in modo preliminare.

Non è invece adatta quando l’obiettivo è **identificare costrutti latenti teorici**.


## Metodo dei Fattori Principali

Il **metodo dei fattori principali** (*principal factor method*) si differenzia dalla PCA perché considera solo la **varianza comune** tra le variabili, escludendo la varianza specifica e l’errore.

### Procedura

1. Si stima la **comunalità iniziale** di ogni variabile (cioè la quota di varianza spiegata da fattori comuni).
2. Si **sostituiscono le varianze totali sulla diagonale** della matrice $\mathbf{R}$ con le comunalità stimate.
3. Si esegue la **decomposizione spettrale** sulla nuova matrice per ottenere autovettori e autovalori.
4. Si costruisce la matrice dei **carichi fattoriali**.

### Stima delle comunalità iniziali

Può essere fatta, ad esempio:

- prendendo il **massimo quadrato della correlazione** della variabile con le altre;
- oppure il **$R^2$** da una regressione multipla della variabile sulle altre.

### Vantaggi

- Tiene conto della **specificità** delle variabili.
- È più **coerente con il modello fattoriale classico**.

### Limiti

- I risultati dipendono fortemente dalla **stima iniziale delle comunalità**.
- Non permette test di bontà dell’adattamento del modello.


## Metodo dei Fattori Principali Iterato

Questo metodo **affina** il precedente aggiornando iterativamente le comunalità:

1. si calcolano carichi fattoriali e comunalità iniziali;
2. si sostituiscono le nuove comunalità nella diagonale;
3. si ripete la procedura finché i valori **convergono**.

### Vantaggi

- Fornisce **stime più stabili** delle comunalità.
- Migliora la qualità della rappresentazione se la struttura è forte.

### Limiti

- Può generare **soluzioni improprie** (es. comunalità > 1: *problemi di Heywood*).
- Non offre criteri interni per la scelta del numero di fattori.


## Metodo della Massima Verosimiglianza (ML)

Il metodo di **massima verosimiglianza** assume che i dati provengano da una **distribuzione normale multivariata**. Si basa sulla **stima dei parametri** che rendono massimamente probabile l’osservazione dei dati dati i parametri.

### Caratteristiche

- Permette di stimare **carichi**, **unicità** e **correlazioni tra fattori**.
- Fornisce un **test statistico di bontà dell’adattamento** (test chi-quadro).
- Permette **confronti tra modelli alternativi** (usando AIC, BIC, etc.).

### Vantaggi

- È il più coerente con un’interpretazione psicometrica.
- Consente **analisi inferenziali** e confronti tra ipotesi.

### Limiti

- Sensibile alle **violazioni della normalità**.
- Richiede **campioni sufficientemente numerosi**.
- Può non convergere in presenza di dati problematici.


> 🎓 *Suggerimento:*  
> Se l’obiettivo è **identificare costrutti psicologici latenti**, scegliete metodi coerenti con il modello fattoriale, come la **massima verosimiglianza**.  
> Se invece volete solo **ridurre le dimensioni** dei dati per scopi descrittivi o pratici, allora la **PCA può essere sufficiente**.


## Esempio in R: Confronto tra Metodi di Estrazione

Per illustrare i principali metodi di estrazione dei fattori, useremo un semplice esempio tratto da Rencher (2010). Una ragazza ha valutato 7 persone su 5 tratti personali:

- **K** = Kind (Gentile)  
- **I** = Intelligent (Intelligente)  
- **H** = Happy (Felice)  
- **L** = Likeable (Simpatica)  
- **J** = Just (Giusta)

La matrice di correlazione tra i tratti è la seguente:

```{r}
R <- matrix(c(
  1.000, .296, .881, .995, .545,
  .296, 1.000, -.022, .326, .837,
  .881, -.022, 1.000, .867, .130,
  .995, .326, .867, 1.000, .544,
  .545, .837, .130, .544, 1.000
),
ncol = 5, byrow = TRUE,
dimnames = list(c("K", "I", "H", "L", "J"),
                c("K", "I", "H", "L", "J")))
R
```


### Metodo delle Componenti Principali (PCA)

**1. Calcolo degli autovalori e autovettori.**

```{r}
e <- eigen(R)
e$values       # varianza spiegata da ciascuna componente
e$vectors      # coefficienti delle combinazioni lineari
```

- Gli **autovalori** indicano quanta varianza spiega ciascuna componente.
- Gli **autovettori** sono le "direzioni" lungo cui le componenti combinano le variabili.

**2. Verifica della decomposizione spettrale.**

```{r}
round(e$vectors %*% diag(e$values) %*% t(e$vectors), 3)
```

Questa moltiplicazione ricostruisce la matrice di correlazione originale:  
$\mathbf{R} = \mathbf{C} \mathbf{D} \mathbf{C}^{\mathsf{T}}$

**3. Varianza spiegata dai primi 2 fattori.**

```{r}
sum(e$values[1:2]) / sum(e$values)
```

**Interpretazione**: Se i primi due autovalori spiegano, ad esempio, il 96% della varianza totale, possiamo **ridurre da 5 a 2 dimensioni** con perdita minima di informazione.

**4. Calcolo delle saturazioni fattoriali (matrice $\hat{\Lambda}$).**

```{r}
L <- cbind(
  e$vectors[, 1] * sqrt(e$values[1]),
  e$vectors[, 2] * sqrt(e$values[2])
)
round(L, 3)
```

- Ogni colonna rappresenta una componente.
- Ogni riga rappresenta una variabile.
- Gli elementi indicano **quanto una variabile satura su una componente**.

**5. Matrice riprodotta e residui.**

```{r}
R_hat <- round(L %*% t(L), 3)
residui <- round(R - R_hat, 3)
residui
```

Se i **residui** (cioè la differenza tra $\mathbf{R}$ e $\hat{\Lambda} \hat{\Lambda}^\mathsf{T}$) sono piccoli, la soluzione a 2 fattori è soddisfacente.


### Metodo dei Fattori Principali

Il metodo dei fattori principali (o **principal factor method**) mira a estrarre **solo la varianza comune** tra le variabili, escludendo la varianza specifica (quella unica di ciascuna variabile).

**Stima iniziale delle comunalità.**

Per avere una stima iniziale delle comunalità, qui si propone di prendere, per ciascuna variabile, il **massimo valore di correlazione** in valore assoluto, **escludendo la diagonale principale** della matrice delle correlazioni (che rappresenta la correlazione di ogni variabile con sé stessa, pari a 1). In R si può fare così:

```{r}
# Copiamo la matrice di correlazione
R_no_diag <- R

# Mettiamo 0 sulla diagonale (anziché 1)
diag(R_no_diag) <- 0

# Calcoliamo il massimo (in valore assoluto) per ogni riga
h.hat <- apply(abs(R_no_diag), 1, max)

# Arrotondiamo a 3 cifre decimali
round(h.hat, 3)
```

Questa stima della comunalità (per ogni variabile) è volutamente molto semplificata e potrebbe risultare meno accurata di altre procedure (ad esempio, **correlazione multipla al quadrato**). Tuttavia, si usa spesso a scopo didattico per illustrare l’algoritmo.

**Matrice ridotta.**

Creiamo la **matrice ridotta** sostituendo la diagonale di $R$ (che vale 1, in quanto matrice di correlazione) con i valori stimati di comunalità:

```{r}
R1 <- R
diag(R1) <- h.hat
R1
```

**Decomposizione della matrice ridotta.**

Calcoliamo gli autovalori e gli autovettori della matrice ridotta:

```{r}
ee <- eigen(R1)
round(ee$values, 3)  # autovalori
```

**Saturazioni fattoriali.**

Infine, estraiamo le **saturazioni fattoriali** per i fattori desiderati (ad esempio, i primi due). Le saturazioni ($\mathbf{L}$) indicano la relazione tra le variabili originali e i fattori latenti, **considerando solo la varianza comune**:

```{r}
L <- ee$vectors[, 1:2] %*% sqrt(diag(ee$values[1:2]))
round(L, 3)
```

**Conclusione**: questa procedura illustra il metodo dei fattori principali e mostra come escludere correttamente la diagonale principale dal calcolo delle comunalità iniziali, utilizzando il massimo (in valore assoluto) delle correlazioni di ogni variabile con le altre.


### Metodo dei Fattori Principali Iterato

Questo metodo **aggiorna iterativamente** le stime delle comunalità finché le saturazioni non cambiano più (convergenza).

**Esecuzione in R con il pacchetto `psych`.**

```{r}
pa <- psych::fa(R, nfactors = 2, rotate = "none", fm = "pa")
pa
```

- `fm = "pa"`: specifica il metodo dei fattori principali.
- L'output include:
  - saturazioni fattoriali;
  - unicità ($1 - h^2$);
  - varianza spiegata da ciascun fattore.

⚠️ Se una **unicità > 1** o negativa → **soluzione impropria** (*caso di Heywood*).


## Metodo di Massima Verosimiglianza (ML)

Questo è il metodo più coerente con l’analisi fattoriale teorica: **assume normalità multivariata** e consente **test formali di adattamento**.

**Esecuzione con `factanal()`.**

```{r}
ml <- factanal(
  covmat = R,
  factors = 2,
  rotation = "none",
  n.obs = 225  # necessario per il test chi-quadro
)
ml
```

- `loadings`: **saturazioni fattoriali**
- `uniquenesses`: varianza specifica di ciascuna variabile
- `test statistic`: **test chi-quadro** per valutare se i fattori estratti spiegano sufficientemente la correlazione tra le variabili

Il **p-value** indica se il modello a 2 fattori è adeguato:  

- p alto → il modello **spiega bene** i dati (non c’è differenza significativa con la matrice osservata).  
- p basso → il modello è **insufficiente** (i residui sono troppo grandi).

Se vogliamo usare `lavaan` dobbiamo introdurre una rotazione obliqua:

```{r}
ml2 <- factanal(
  covmat = R,
  factors = 2,
  rotation = "oblimin",
  n.obs = 225  
)
ml2
```

Replichiamo ora i risultati con `lavaan`:

```{r}
fit <- efa(sample.cov = R, 
           sample.nobs = 225,
           nfactors = 1:2,
           rotation = "geomin",
           rotation.args = list(geomin.epsilon = 0.01, rstarts = 1))

summary(fit, nd = 3L, cutoff = 0.2, dot.cutoff = 0.05)
fitMeasures(fit, fit.measures = "all")
```

In sintesi:

| Metodo           | Obiettivo principale                  | Include specificità? | Test del modello? | Quando usarlo |
|------------------|----------------------------------------|----------------------|-------------------|----------------|
| **PCA**          | Ridurre dimensionalità                | ❌ No               | ❌ No             | Sintesi descrittiva |
| **Fattori principali** | Isolare la varianza comune        | ✅ Sì               | ❌ No             | Analisi esplorativa |
| **Fattori iterato**    | Raffinare le comunalità            | ✅ Sì               | ❌ No             | Soluzioni più stabili |
| **ML**           | Testare modello fattoriale            | ✅ Sì               | ✅ Sì             | Verifica ipotesi psicologiche |

🎓 *Suggerimento:*  

- se lo scopo è **identificare costrutti teorici** (es. "l’autostima ha due dimensioni?"), si preferisce il metodo **ML**.  
- se invece lo scopo è solo quello di **riassumendo dati** (es. da un questionario), anche la **PCA può andare bene**.

## Riflessioni Conclusive

L’analisi fattoriale non è una procedura automatica, ma un processo di modellizzazione che richiede **scelte motivate** e **valutazioni critiche** a ogni passaggio. I metodi di estrazione dei fattori, pur essendo matematicamente diversi, riflettono **concezioni differenti del ruolo della varianza** nelle variabili osservate e, quindi, **diverse filosofie di ricerca**.

Ad esempio, la **PCA** tende a trattare le variabili come manifestazioni dirette della varianza totale, rendendola utile per scopi pratici come la riduzione della dimensionalità, ma meno adatta per inferenze teoriche su costrutti latenti. Al contrario, i metodi che stimano la **varianza comune** (come i fattori principali o la massima verosimiglianza) assumono che ci siano **cause sottostanti e non osservabili** che generano le covarianze tra le variabili.

Ma oltre agli aspetti tecnici, è utile considerare alcune **domande chiave** che dovrebbero guidare la scelta del metodo:

- Qual è il mio obiettivo? *Sintesi descrittiva*, *conferma di ipotesi teoriche*, *preparazione a un’analisi fattoriale confermativa*?
- I dati rispettano le assunzioni richieste (normalità, dimensione del campione, struttura semplice)?
- Quanto voglio spingermi nell’**interpretazione psicologica dei fattori**?  
- Quanto è affidabile la mia **stima della varianza specifica o dell’errore di misura**?

Inoltre, è importante non trascurare che l’analisi fattoriale, per quanto potente, è **sensibile a molte scelte analitiche**: dal numero di fattori estratti, al metodo di rotazione, fino alle decisioni su quali variabili includere o escludere. Ogni decisione ha impatto sull’**interpretabilità**, sulla **stabilità** delle soluzioni e sulla **replicabilità** dei risultati.

> *Un buon analista fattoriale non cerca solo di “ottenere carichi elevati”, ma si interroga su cosa quei carichi rappresentano, se sono coerenti con la teoria, e se possono essere replicati in un altro campione.*

Infine, non dimentichiamo che l’analisi fattoriale **non esaurisce l’indagine sulla struttura latente** dei dati. È spesso il primo passo di un percorso più ampio che può includere:

- **analisi fattoriale confermativa (CFA)**;
- **modelli strutturali (SEM)**;
- **analisi di validità di costrutto**.

In sintesi, padroneggiare i diversi metodi di estrazione non significa solo saperli applicare, ma anche **comprendere le implicazioni epistemologiche e psicometriche** delle scelte fatte. Il vero valore dell’analisi fattoriale non sta solo nella sintesi dei dati, ma nella sua capacità di **collegare numeri e teoria**, variabili osservate e costrutti latenti, **statistica e psicologia**.


## Informazioni sull'Ambiente di Sviluppo {.unnumbered}

```{r}
sessionInfo()
```

