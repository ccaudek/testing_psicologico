# L'estrazione dei fattori {#sec-extraction-factor-extraction}

::: callout-important
## In questo capitolo imparerai:

- Come implementare il **metodo delle componenti principali** per l'estrazione delle saturazioni fattoriali utilizzando l'algebra matriciale.  
- Come implementare il **metodo dei fattori principali** e comprenderne i fondamenti teorici.  
- Il funzionamento del **metodo dei fattori principali iterato**, con un focus sul processo di convergenza.  
- Le caratteristiche principali del **metodo di massima verosimiglianza** e il suo utilizzo nell'analisi fattoriale.  
:::

::: callout-tip
## Prerequisiti

- Leggere il capitolo *Factor Analysis and Principal Component Analysis* del testo di @petersen2024principles.
:::

::: callout-caution
## Preparazione del Notebook

```{r}
here::here("code", "_common.R") |> 
  source()

# Load packages
if (!requireNamespace("pacman")) install.packages("pacman")
pacman::p_load(lavaan, psych)
```
:::

## Introduzione

L‚Äô**analisi fattoriale** √® una tecnica statistica multivariata utilizzata per **identificare strutture latenti**, ovvero **fattori non osservabili**, che spiegano le **correlazioni tra variabili osservate**.

In altre parole, quando abbiamo molte variabili (es. item di un questionario), l‚Äôanalisi fattoriale cerca di scoprire **quali gruppi di item misurano lo stesso costrutto psicologico sottostante**, riducendo cos√¨ la complessit√† dei dati.

Questa tecnica √® particolarmente utile nelle scienze sociali e in psicologia, dove **costrutti astratti** come *intelligenza*, *ansia* o *autostima* non possono essere misurati direttamente, ma solo attraverso pi√π item. L‚Äôanalisi fattoriale aiuta a verificare se questi item misurano effettivamente **un numero limitato di costrutti** sottostanti, rendendo i dati pi√π interpretabili.


### Il modello statistico dell‚Äôanalisi fattoriale

Il modello matematico alla base dell‚Äôanalisi fattoriale si esprime cos√¨:

$$
\boldsymbol{\Sigma} = \boldsymbol{\Lambda} \boldsymbol{\Phi} \boldsymbol{\Lambda}^{\mathsf{T}} + \boldsymbol{\Psi},
$$

dove:

- $\boldsymbol{\Sigma}$ √® la **matrice delle covarianze osservate** tra le variabili;
- $\boldsymbol{\Lambda}$ √® la **matrice dei carichi fattoriali**: ciascun elemento rappresenta l‚Äôintensit√† della relazione tra una variabile osservata e un fattore latente;
- $\boldsymbol{\Phi}$ √® la **matrice delle correlazioni tra i fattori** (√® l‚Äôidentit√† se i fattori sono ortogonali, cio√® non correlati);
- $\boldsymbol{\Psi}$ √® una **matrice diagonale** contenente le **unicit√†** (la porzione di varianza di ciascuna variabile che non √® spiegata dai fattori comuni).

Questo modello riflette l‚Äôidea che **ogni variabile osservata sia influenzata da uno o pi√π fattori comuni**, pi√π una componente specifica e casuale (l‚Äôunicit√†).


## Estrazione dei Fattori: Panoramica dei Metodi

L‚Äô**estrazione dei fattori** consiste nel **stimare i parametri** del modello (soprattutto $\boldsymbol{\Lambda}$), sulla base della matrice di correlazioni o covarianze. I diversi metodi si distinguono per:

- le **assunzioni statistiche** (es. normalit√† dei dati);
- il tipo di **informazione utilizzata** (es. varianza totale o varianza comune);
- la possibilit√† di **testare l‚Äôadattamento del modello ai dati**.

| Metodo                         | Tiene conto della specificit√†? | Richiede normalit√†? | Permette test di bont√† del modello? |
|-------------------------------|-------------------------------|----------------------|-------------------------------------|
| Componenti principali (PCA)   | ‚ùå No                         | ‚ùå No               | ‚ùå No                              |
| Fattori principali            | ‚úÖ S√¨                         | ‚ùå No               | ‚ùå No                              |
| Fattori principali iterato    | ‚úÖ S√¨ (con aggiornamenti)     | ‚ùå No               | ‚ùå No                              |
| Massima verosimiglianza (ML)  | ‚úÖ S√¨                         | ‚úÖ S√¨               | ‚úÖ S√¨                              |

Vediamo ora in dettaglio ciascun metodo.


## Metodo delle Componenti Principali (PCA)

> ‚ùó **Importante:** Sebbene sia molto diffuso, il metodo delle componenti principali (*Principal Component Analysis*, PCA) **non √® un vero metodo fattoriale**. Non fa distinzione tra **varianza comune** (quella condivisa tra variabili) e **varianza specifica** (quella unica di ogni variabile), e **non assume l‚Äôesistenza di fattori latenti**. Per questo motivo, in psicometria, viene usato **per riduzione della dimensionalit√†**, non per identificare costrutti teorici.

### Obiettivo

La PCA costruisce un numero ridotto di **componenti principali**:

- sono **combinazioni lineari** delle variabili originali;
- sono **ortogonali** (cio√® non correlate tra loro);
- spiegano progressivamente la **massima varianza possibile** nei dati.

### Fondamento teorico: il teorema spettrale

La PCA si basa sul **teorema spettrale**, che dice che ogni **matrice simmetrica** (come la matrice di correlazione $\mathbf{R}$) pu√≤ essere scomposta come:

$$
\mathbf{R} = \mathbf{C} \mathbf{D} \mathbf{C}^{\mathsf{T}},
$$

dove:

- $\mathbf{C}$ √® la matrice i cui **vettori colonna sono gli autovettori** (direzioni principali) di $\mathbf{R}$;
- $\mathbf{D}$ √® una **matrice diagonale** con gli **autovalori** (quantit√† di varianza spiegata da ciascuna direzione);
- $\mathbf{C}^{\mathsf{T}}$ √® la trasposta di $\mathbf{C}$.

Questa √® la **scomposizione spettrale** della matrice $\mathbf{R}$.

### Costruzione delle saturazioni (carichi)

Vogliamo una matrice $\hat{\boldsymbol{\Lambda}}$ che approssimi la matrice $\mathbf{R}$:

$$
\mathbf{R} \approx \hat{\boldsymbol{\Lambda}} \hat{\boldsymbol{\Lambda}}^\mathsf{T}.
$$

Poich√© $\mathbf{D}$ √® diagonale, possiamo scriverla come:

$$
\mathbf{D} = \mathbf{D}^{1/2} \cdot \mathbf{D}^{1/2},
$$

dove $\mathbf{D}^{1/2}$ ha sulla diagonale le **radici quadrate degli autovalori**. Allora:

$$
\mathbf{R} = \mathbf{C} \mathbf{D}^{1/2} \cdot \mathbf{D}^{1/2} \mathbf{C}^{\mathsf{T}}.
$$

Definiamo:

$$
\hat{\boldsymbol{\Lambda}} = \mathbf{C} \mathbf{D}^{1/2},
$$

e otteniamo:

$$
\hat{\boldsymbol{\Lambda}} \hat{\boldsymbol{\Lambda}}^\mathsf{T} = \mathbf{R}.
$$

> üí° Le **saturazioni** si ottengono moltiplicando **ogni autovettore per la radice quadrata dell‚Äôautovalore corrispondente**. Questo consente di ricostruire esattamente la matrice $\mathbf{R}$.

Ogni elemento $l_{ij}$ di $\hat{\boldsymbol{\Lambda}}$ indica **quanto la variabile $i$ contribuisce alla componente $j$**.

Quando si selezionano solo i primi $k$ autovalori e autovettori (cio√® quelli che spiegano pi√π varianza), si ottiene una **rappresentazione semplificata** dei dati, utile per la riduzione della dimensionalit√†.

### Interpretazione

- Gli **autovalori** indicano quanta varianza √® spiegata da ciascuna componente.
- Le **componenti principali** sono nuove variabili non osservate, che sintetizzano l‚Äôinformazione contenuta nelle variabili originali.

### Limiti della PCA come analisi fattoriale

- Non separa **varianza comune** da **varianza specifica**.
- Non assume **fattori latenti**.
- Non consente di valutare l‚Äôadattamento del modello ai dati.

### Quando usarla

La PCA √® utile quando:

- si vuole **ridurre il numero di variabili** mantenendo la massima varianza;
- si desidera costruire **indici sintetici** (es. punteggi compositi);
- si vuole **esplorare la struttura dei dati** in modo preliminare.

Non √® invece adatta quando l‚Äôobiettivo √® **identificare costrutti latenti teorici**.


## Metodo dei Fattori Principali

Il **metodo dei fattori principali** (*principal factor method*) si differenzia dalla PCA perch√© considera solo la **varianza comune** tra le variabili, escludendo la varianza specifica e l‚Äôerrore.

### Procedura

1. Si stima la **comunalit√† iniziale** di ogni variabile (cio√® la quota di varianza spiegata da fattori comuni).
2. Si **sostituiscono le varianze totali sulla diagonale** della matrice $\mathbf{R}$ con le comunalit√† stimate.
3. Si esegue la **decomposizione spettrale** sulla nuova matrice per ottenere autovettori e autovalori.
4. Si costruisce la matrice dei **carichi fattoriali**.

### Stima delle comunalit√† iniziali

Pu√≤ essere fatta, ad esempio:

- prendendo il **massimo quadrato della correlazione** della variabile con le altre;
- oppure il **$R^2$** da una regressione multipla della variabile sulle altre.

### Vantaggi

- Tiene conto della **specificit√†** delle variabili.
- √à pi√π **coerente con il modello fattoriale classico**.

### Limiti

- I risultati dipendono fortemente dalla **stima iniziale delle comunalit√†**.
- Non permette test di bont√† dell‚Äôadattamento del modello.


## Metodo dei Fattori Principali Iterato

Questo metodo **affina** il precedente aggiornando iterativamente le comunalit√†:

1. si calcolano carichi fattoriali e comunalit√† iniziali;
2. si sostituiscono le nuove comunalit√† nella diagonale;
3. si ripete la procedura finch√© i valori **convergono**.

### Vantaggi

- Fornisce **stime pi√π stabili** delle comunalit√†.
- Migliora la qualit√† della rappresentazione se la struttura √® forte.

### Limiti

- Pu√≤ generare **soluzioni improprie** (es. comunalit√† > 1: *problemi di Heywood*).
- Non offre criteri interni per la scelta del numero di fattori.


## Metodo della Massima Verosimiglianza (ML)

Il metodo di **massima verosimiglianza** assume che i dati provengano da una **distribuzione normale multivariata**. Si basa sulla **stima dei parametri** che rendono massimamente probabile l‚Äôosservazione dei dati dati i parametri.

### Caratteristiche

- Permette di stimare **carichi**, **unicit√†** e **correlazioni tra fattori**.
- Fornisce un **test statistico di bont√† dell‚Äôadattamento** (test chi-quadro).
- Permette **confronti tra modelli alternativi** (usando AIC, BIC, etc.).

### Vantaggi

- √à il pi√π coerente con un‚Äôinterpretazione psicometrica.
- Consente **analisi inferenziali** e confronti tra ipotesi.

### Limiti

- Sensibile alle **violazioni della normalit√†**.
- Richiede **campioni sufficientemente numerosi**.
- Pu√≤ non convergere in presenza di dati problematici.


> üéì *Suggerimento didattico:*  
> Se l‚Äôobiettivo √® **identificare costrutti psicologici latenti**, scegliete metodi coerenti con il modello fattoriale, come la **massima verosimiglianza**.  
> Se invece volete solo **ridurre le dimensioni** dei dati per scopi descrittivi o pratici, allora la **PCA pu√≤ essere sufficiente**.


## Esempio pratico in R: Confronto tra metodi di estrazione

Per illustrare i principali metodi di estrazione dei fattori, useremo un semplice esempio tratto da Rencher (2010). Una ragazza ha valutato 7 persone su 5 tratti personali:

- **K** = Kind (Gentile)  
- **I** = Intelligent (Intelligente)  
- **H** = Happy (Felice)  
- **L** = Likeable (Simpatica)  
- **J** = Just (Giusta)

La matrice di correlazione tra i tratti √® la seguente:

```{r}
R <- matrix(c(
  1.000, .296, .881, .995, .545,
  .296, 1.000, -.022, .326, .837,
  .881, -.022, 1.000, .867, .130,
  .995, .326, .867, 1.000, .544,
  .545, .837, .130, .544, 1.000
),
ncol = 5, byrow = TRUE,
dimnames = list(c("K", "I", "H", "L", "J"),
                c("K", "I", "H", "L", "J")))
R
```


## Metodo delle Componenti Principali (PCA)

**1. Calcolo degli autovalori e autovettori.**

```{r}
e <- eigen(R)
e$values       # varianza spiegata da ciascuna componente
e$vectors      # coefficienti delle combinazioni lineari
```

- Gli **autovalori** indicano quanta varianza spiega ciascuna componente.
- Gli **autovettori** sono le "direzioni" lungo cui le componenti combinano le variabili.

**2. Verifica della decomposizione spettrale.**

```{r}
round(e$vectors %*% diag(e$values) %*% t(e$vectors), 3)
```

Questa moltiplicazione ricostruisce la matrice di correlazione originale:  
$\mathbf{R} = \mathbf{C} \mathbf{D} \mathbf{C}^{\mathsf{T}}$

**3. Varianza spiegata dai primi 2 fattori.**

```{r}
sum(e$values[1:2]) / sum(e$values)
```

**Interpretazione**: Se i primi due autovalori spiegano, ad esempio, il 96% della varianza totale, possiamo **ridurre da 5 a 2 dimensioni** con perdita minima di informazione.

**4. Calcolo delle saturazioni fattoriali (matrice $\hat{\Lambda}$).**

```{r}
L <- cbind(
  e$vectors[, 1] * sqrt(e$values[1]),
  e$vectors[, 2] * sqrt(e$values[2])
)
round(L, 3)
```

- Ogni colonna rappresenta una componente.
- Ogni riga rappresenta una variabile.
- Gli elementi indicano **quanto una variabile satura su una componente**.

**5. Matrice riprodotta e residui.**

```{r}
R_hat <- round(L %*% t(L), 3)
residui <- round(R - R_hat, 3)
residui
```

Se i **residui** (cio√® la differenza tra $\mathbf{R}$ e $\hat{\Lambda} \hat{\Lambda}^\mathsf{T}$) sono piccoli, la soluzione a 2 fattori √® soddisfacente.


## Metodo dei Fattori Principali

Il metodo dei fattori principali estrae **solo la varianza comune**, escludendo la varianza specifica.

**1. Stima iniziale delle comunalit√†.**

Una stima semplice: **il valore massimo della correlazione per ogni variabile** (approssimazione):

```{r}
h.hat <- apply(abs(R), 1, max)
round(h.hat, 3)
```

**2. Matrice ridotta.**

Sostituiamo le varianze sulla diagonale con le comunalit√† stimate:

```{r}
R1 <- R
diag(R1) <- h.hat
R1
```

**3. Decomposizione della matrice ridotta.**

```{r}
ee <- eigen(R1)
round(ee$values, 3)  # autovalori
```

**4. Saturazioni fattoriali.**

```{r}
L <- ee$vectors[, 1:2] %*% sqrt(diag(ee$values[1:2]))
round(L, 3)
```

Le saturazioni qui rappresentano la **relazione tra variabili e fattori latenti**, tenendo conto solo della **varianza comune**.


## Metodo dei Fattori Principali Iterato

Questo metodo **aggiorna iterativamente** le stime delle comunalit√† finch√© le saturazioni non cambiano pi√π (convergenza).

**Esecuzione in R con il pacchetto `psych`.**

```{r}
pa <- psych::fa(R, nfactors = 2, rotate = "none", fm = "pa")
pa
```

- `fm = "pa"`: specifica il metodo dei fattori principali.
- L'output include:
  - saturazioni fattoriali;
  - unicit√† ($1 - h^2$);
  - varianza spiegata da ciascun fattore.

‚ö†Ô∏è Se una **unicit√† > 1** o negativa ‚Üí **soluzione impropria** (*caso di Heywood*).


## Metodo di Massima Verosimiglianza (ML)

Questo √® il metodo pi√π coerente con l‚Äôanalisi fattoriale teorica: **assume normalit√† multivariata** e consente **test formali di adattamento**.

**Esecuzione con `factanal()`.**

```{r}
ml <- factanal(
  covmat = R,
  factors = 2,
  rotation = "none",
  n.obs = 225  # necessario per il test chi-quadro
)
ml
```

- `loadings`: **saturazioni fattoriali**
- `uniquenesses`: varianza specifica di ciascuna variabile
- `test statistic`: **test chi-quadro** per valutare se i fattori estratti spiegano sufficientemente la correlazione tra le variabili

Il **p-value** indica se il modello a 2 fattori √® adeguato:  

- p alto ‚Üí il modello **spiega bene** i dati (non c‚Äô√® differenza significativa con la matrice osservata).  
- p basso ‚Üí il modello √® **insufficiente** (i residui sono troppo grandi).

In sintesi:

| Metodo           | Obiettivo principale                  | Include specificit√†? | Test del modello? | Quando usarlo |
|------------------|----------------------------------------|----------------------|-------------------|----------------|
| **PCA**          | Ridurre dimensionalit√†                | ‚ùå No               | ‚ùå No             | Sintesi descrittiva |
| **Fattori principali** | Isolare la varianza comune        | ‚úÖ S√¨               | ‚ùå No             | Analisi esplorativa |
| **Fattori iterato**    | Raffinare le comunalit√†            | ‚úÖ S√¨               | ‚ùå No             | Soluzioni pi√π stabili |
| **ML**           | Testare modello fattoriale            | ‚úÖ S√¨               | ‚úÖ S√¨             | Verifica ipotesi psicologiche |

üéì *Suggerimento:*  

- se lo scopo √® **identificare costrutti teorici** (es. "l‚Äôautostima ha due dimensioni?"), si preferisce il metodo **ML**.  
- se invece lo scopo √® solo quello di **riassumendo dati** (es. da un questionario), anche la **PCA pu√≤ andare bene**.

## Riflessioni Conclusive

L‚Äôanalisi fattoriale non √® una procedura automatica, ma un processo di modellizzazione che richiede **scelte motivate** e **valutazioni critiche** a ogni passaggio. I metodi di estrazione dei fattori, pur essendo matematicamente diversi, riflettono **concezioni differenti del ruolo della varianza** nelle variabili osservate e, quindi, **diverse filosofie di ricerca**.

Ad esempio, la **PCA** tende a trattare le variabili come manifestazioni dirette della varianza totale, rendendola utile per scopi pratici come la riduzione della dimensionalit√†, ma meno adatta per inferenze teoriche su costrutti latenti. Al contrario, i metodi che stimano la **varianza comune** (come i fattori principali o la massima verosimiglianza) assumono che ci siano **cause sottostanti e non osservabili** che generano le covarianze tra le variabili.

Ma oltre agli aspetti tecnici, √® utile considerare alcune **domande chiave** che dovrebbero guidare la scelta del metodo:

- Qual √® il mio obiettivo? *Sintesi descrittiva*, *conferma di ipotesi teoriche*, *preparazione a un‚Äôanalisi fattoriale confermativa*?
- I dati rispettano le assunzioni richieste (normalit√†, dimensione del campione, struttura semplice)?
- Quanto voglio spingermi nell‚Äô**interpretazione psicologica dei fattori**?  
- Quanto √® affidabile la mia **stima della varianza specifica o dell‚Äôerrore di misura**?

Inoltre, √® importante non trascurare che l‚Äôanalisi fattoriale, per quanto potente, √® **sensibile a molte scelte analitiche**: dal numero di fattori estratti, al metodo di rotazione, fino alle decisioni su quali variabili includere o escludere. Ogni decisione ha impatto sull‚Äô**interpretabilit√†**, sulla **stabilit√†** delle soluzioni e sulla **replicabilit√†** dei risultati.

> *Un buon analista fattoriale non cerca solo di ‚Äúottenere carichi elevati‚Äù, ma si interroga su cosa quei carichi rappresentano, se sono coerenti con la teoria, e se possono essere replicati in un altro campione.*

Infine, non dimentichiamo che l‚Äôanalisi fattoriale **non esaurisce l‚Äôindagine sulla struttura latente** dei dati. √à spesso il primo passo di un percorso pi√π ampio che pu√≤ includere:

- **analisi fattoriale confermativa (CFA)**;
- **modelli strutturali (SEM)**;
- **analisi di validit√† di costrutto**.

In sintesi, padroneggiare i diversi metodi di estrazione non significa solo saperli applicare, ma anche **comprendere le implicazioni epistemologiche e psicometriche** delle scelte fatte. Il vero valore dell‚Äôanalisi fattoriale non sta solo nella sintesi dei dati, ma nella sua capacit√† di **collegare numeri e teoria**, variabili osservate e costrutti latenti, **statistica e psicologia**.



## Informazioni sull'Ambiente di Sviluppo {.unnumbered}

```{r}
sessionInfo()
```

