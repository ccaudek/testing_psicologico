{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Predizione\n",
        "jupyter: ir\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Prerequisiti**\n",
        "\n",
        "- Leggere il capitolo 9 *Prediction* del testo di @petersen2024principles.\n",
        "\n",
        "**Concetti e Competenze Chiave**\n",
        "\n",
        "**Preparazione del Notebook**\n"
      ],
      "id": "fc71aff1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "here::here(\"code\", \"_common.R\") |> source()\n",
        "\n",
        "# Load packages\n",
        "if (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\n",
        "pacman::p_load(\n",
        "    petersenlab, magrittr, viridis, pROC, ROCR, rms, ResourceSelection,\n",
        "    PredictABEL, gridExtra, grid, ggpubr, msir, car, ggrepel, MOTE,\n",
        "    tinytex\n",
        ")"
      ],
      "id": "bf0998a2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduzione\n",
        "\n",
        "Le predizioni possono essere di vario tipo. Alcune riguardano dati categoriali, mentre altre si basano su dati continui. Per i dati categoriali, possiamo valutare le predizioni utilizzando una tabella 2x2, nota come matrice di confusione, o con modelli di regressione logistica. Invece, per i dati continui, possiamo utilizzare la regressione multipla o varianti come il modello a equazioni strutturali o i modelli misti.\n",
        "\n",
        "Consideriamo un esempio pratico di predizione della probabilità di contrarre l'HIV, utilizzando le seguenti informazioni [@petersen2024principles]:\n",
        "\n",
        "- **Tasso di base dell'HIV (P(HIV))**: 0.3% (0.003). Questa è la probabilità che una persona nella popolazione generale abbia l'HIV.\n",
        "- **Sensibilità del test (P(Test+ \\mid HIV))**: 95% (0.95). Questa è la probabilità che il test risulti positivo se la persona ha effettivamente l'HIV.\n",
        "- **Specificità del test (P(Test- \\mid ¬HIV))**: 99.28% (0.9928). Questa è la probabilità che il test risulti negativo se la persona non ha l'HIV.\n",
        "\n",
        "## Calcolo della probabilità di HIV dato un test positivo\n",
        "\n",
        "Per calcolare la probabilità di avere l'HIV dato un test positivo (P(HIV \\mid Test+)), utilizziamo il teorema di Bayes:\n",
        "\n",
        "$$\n",
        "P(HIV \\mid Test+) = \\frac{P(Test+ \\mid HIV) \\times P(HIV)}{P(Test+)}.\n",
        "$$\n",
        "\n",
        "Abbiamo bisogno di calcolare il denominatore, ovvero la probabilità complessiva di ottenere un test positivo (P(Test+)). Questo valore include sia i veri positivi che i falsi positivi:\n",
        "\n",
        "$$\n",
        "P(Test+) = P(Test+ \\mid HIV) \\times P(HIV) + P(Test+ \\mid \\neg HIV) \\times P(\\neg HIV),\n",
        "$$\n",
        "\n",
        "dove:\n",
        "\n",
        "- $P(Test+ \\mid \\neg HIV) = 1 - P(Test- \\mid \\neg HIV) = 1 - 0.9928 = 0.0072$ (tasso di falsi positivi),\n",
        "- $P(\\neg HIV) = 1 - P(HIV) = 1 - 0.003 = 0.997$.\n",
        "\n",
        "Calcoliamo $P(Test+)$:\n",
        "\n",
        "$$\n",
        "P(Test+) = (0.95 \\times 0.003) + (0.0072 \\times 0.997) \\approx 0.010027.\n",
        "$$\n",
        "\n",
        "Ora possiamo calcolare $P(HIV \\mid Test+)$:\n",
        "\n",
        "$$\n",
        "P(HIV \\mid Test+) = \\frac{0.95 \\times 0.003}{0.010027} \\approx 0.2844 \\text{ o 28.44\\%}.\n",
        "$$\n",
        "\n",
        "Quindi, se il test risulta positivo, la probabilità di avere l'HIV è circa il 28.44%.\n",
        "\n",
        "## Calcolo della probabilità di un secondo test positivo\n",
        "\n",
        "Dopo un primo test positivo, la probabilità di avere l'HIV è aumentata al 28.44%. Ora calcoleremo la probabilità che un secondo test risulti positivo e la conseguente probabilità di avere l'HIV dopo due test positivi consecutivi.\n",
        "\n",
        "Per calcolare $P(\\text{Secondo Test+})$, consideriamo due scenari:\n",
        "\n",
        "1. La persona ha effettivamente l'HIV:\n",
        "   - Probabilità: $P(HIV \\mid Test+) = 0.2844$.\n",
        "   - Probabilità di un test positivo: $P(\\text{Test+} \\mid HIV) = 0.95$ (sensibilità del test).\n",
        "\n",
        "2. La persona non ha l'HIV:\n",
        "   - Probabilità: $P(\\neg HIV \\mid Test+) = 1 - P(HIV \\mid Test+) = 0.7156$.\n",
        "   - Probabilità di un test positivo: $P(\\text{Test+} \\mid \\neg HIV) = 0.0072$ (tasso di falsi positivi).\n",
        "\n",
        "Utilizziamo la formula della probabilità totale:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "P(\\text{Secondo Test+}) &= P(\\text{Test+} \\mid HIV) \\times P(HIV \\mid Test+) + \\\\\n",
        "&\\quad P(\\text{Test+} \\mid \\neg HIV) \\times P(\\neg HIV \\mid Test+).\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Sostituendo i valori:\n",
        "\n",
        "$$\n",
        "P(\\text{Secondo Test+}) = (0.95 \\times 0.2844) + (0.0072 \\times 0.7156) \\approx 0.2753.\n",
        "$$\n",
        "\n",
        "Applichiamo nuovamente il teorema di Bayes per calcolare la probabilità di avere l'HIV dopo un secondo test positivo:\n",
        "\n",
        "$$\n",
        "P(HIV \\mid \\text{Secondo Test+}) = \\frac{P(\\text{Secondo Test+} \\mid HIV) \\times P(HIV \\mid Test+)}{P(\\text{Secondo Test+})}.\n",
        "$$\n",
        "\n",
        "Sostituendo i valori:\n",
        "\n",
        "$$\n",
        "P(HIV \\mid \\text{Secondo Test+}) = \\frac{0.95 \\times 0.2844}{0.2753} \\approx 0.981.\n",
        "$$\n",
        "\n",
        "In conclusione, dopo un secondo test positivo, la probabilità di avere l'HIV aumenta notevolmente, passando dal 28.44% al 98.1%. Questo drastico aumento dimostra l'importanza di:\n",
        "\n",
        "1. considerare il tasso di base (prevalenza) nella popolazione;\n",
        "2. aggiornare progressivamente le probabilità con nuove evidenze;\n",
        "3. interpretare i risultati di test diagnostici multipli in modo bayesiano.\n",
        "\n",
        "L'analisi evidenzia come l'accumulo di evidenze attraverso test ripetuti, in linea con i principi del teorema di Bayes, possa portare a una stima molto più accurata della probabilità di avere una condizione medica, riducendo sostanzialmente l'incertezza iniziale.\n",
        "\n",
        "## Accuratezza delle Predizioni\n",
        "\n",
        "Per valutare l'accuratezza delle predizioni, @petersen2024principles considera un esempio adattato da Meehl & Rosen (1955). L'esercito americano esegue un test sui candidati per escludere quelli che hanno basse probabilità di superare l'addestramento di base. Per analizzare l'accuratezza di queste predizioni, possiamo utilizzare una *matrice di confusione*.\n",
        "\n",
        " |  Decision (Prediction)  |  Actual Adjustment (Poor)  |  Actual Adjustment (Good)  |  Total Predicted  |  Selection Ratio  | \n",
        " | ----------------------- | -------------------------- | --------------------------- | ----------------- | ----------------- | \n",
        " |  **Reject**              |  TP = 86 (.043)            |  FP = 422 (.211)             |  508              |  SR = 0.254       | \n",
        " |  **Retain**              |  FN = 14 (.007)            |  TN = 1,478 (.739)           |  1,492            |  1 - SR = 0.746   | \n",
        " |  **Total Actual**        |  100                       |  1,900                       |  N = 2,000        |                   | \n",
        " |  **Base Rate**           |  BR = 0.05                 |  1 - BR = 0.95               |                   |                   | \n",
        "\n",
        "Una *matrice di confusione* è una tabella che mette a confronto le predizioni fatte da un modello con gli esiti reali. Quando si tratta di una predizione dicotomica (ad esempio, sì/no o positivo/negativo), la matrice di confusione è organizzata in una tabella 2x2 che descrive le seguenti combinazioni.\n",
        "\n",
        "- **Vero positivo (TP)**: La predizione indica che la persona possiede la caratteristica, e ciò risulta corretto perché la persona effettivamente la possiede.\n",
        "- **Vero negativo (TN)**: La predizione indica che la persona non possiede la caratteristica, e ciò risulta corretto perché la persona effettivamente non la possiede.\n",
        "- **Falso positivo (FP)**: La predizione indica che la persona possiede la caratteristica, ma in realtà la persona non la possiede.\n",
        "- **Falso negativo (FN)**: La predizione indica che la persona non possiede la caratteristica, ma in realtà la persona la possiede.\n",
        "\n",
        "Questi quattro risultati sono alla base dell'analisi dell'accuratezza di un modello predittivo. Il termine \"vero\" indica una predizione corretta, mentre \"falso\" rappresenta un errore. \"Positivo\" e \"negativo\" si riferiscono rispettivamente al fatto che la predizione indichi la presenza o l'assenza di una determinata caratteristica.\n",
        "\n",
        "### Calcolo dei Tassi Marginali\n",
        "\n",
        "Con le informazioni presenti nella matrice di confusione, possiamo calcolare i *tassi marginali*, ovvero le probabilità globali che una persona presenti una certa caratteristica o sia classificata in un determinato modo. \n",
        "\n",
        "1. **Tasso di base (BR)**: Questo rappresenta la probabilità marginale che una persona abbia la caratteristica di interesse. Ad esempio, se 100 persone su 2.000 mostrano un cattivo adattamento, il tasso di base è:\n",
        "\n",
        "   $$\n",
        "   BR = \\frac{FN + TP}{N} = \\frac{100}{2000} = 0.05\n",
        "   $$\n",
        "\n",
        "   Ciò significa che il 5% dei candidati ha un cattivo adattamento.\n",
        "\n",
        "2. **Rapporto di selezione (SR)**: Questo indica la probabilità marginale che una persona venga esclusa dal test. Se 508 persone vengono escluse, il rapporto di selezione è:\n",
        "\n",
        "   $$\n",
        "   SR = \\frac{TP + FP}{N} = \\frac{508}{2000} = 0.254\n",
        "   $$\n",
        "\n",
        "   Il 25.4% dei candidati è stato escluso.\n",
        "\n",
        "Il *rapporto di selezione* può dipendere dal punteggio di cutoff del test o da fattori esterni, come il numero di candidati che possono essere trattati o inclusi nel programma.\n",
        "\n",
        "### Percentuale di Accuratezza\n",
        "\n",
        "La *percentuale di accuratezza* è un indicatore generale di quante predizioni sono corrette. Si calcola dividendo il numero di predizioni corrette (TP + TN) per il numero totale di predizioni (N), e moltiplicando per 100:\n",
        "\n",
        "$$\n",
        "\\text{Percentuale di accuratezza} = 100 \\times \\frac{TP + TN}{N}.\n",
        "$$\n",
        "\n",
        "Nel nostro esempio, il 78% delle predizioni è corretto, il che indica che il modello ha una buona accuratezza complessiva.\n",
        "\n",
        "### Accuratezza per Caso\n",
        "\n",
        "Sebbene il 78% di accuratezza possa sembrare un buon risultato, è essenziale confrontare questo valore con quello che si otterrebbe semplicemente per puro caso. Questo confronto ci aiuta a capire se il modello apporta un reale valore aggiunto rispetto a una previsione casuale.\n",
        "\n",
        "Ad esempio, consideriamo un tasso di base (BR) del 5% e un rapporto di selezione (SR) del 25,4%. La probabilità di ottenere un vero positivo per caso è data da:\n",
        "\n",
        "$$\n",
        "P(TP) = BR \\times SR = 0.05 \\times 0.254 = 0.0127.\n",
        "$$\n",
        "\n",
        "Qui, il tasso di base BR = 0,05 rappresenta la probabilità che un individuo appartenga al gruppo con la caratteristica di interesse, mentre il rapporto di selezione SR = 0,254 indica la probabilità che l’individuo venga classificato come positivo. Moltiplicando queste due probabilità marginali, otteniamo la probabilità congiunta di ottenere un vero positivo per caso, pari a 0,0127.\n",
        "\n",
        "Analogamente, la probabilità di ottenere un vero negativo per caso è data da:\n",
        "\n",
        "$$\n",
        "P(TN) = (1 - BR) \\times (1 - SR) = 0.95 \\times 0.746 = 0.7087.\n",
        "$$\n",
        "\n",
        "Pertanto, la *percentuale di accuratezza per caso*, ossia l’accuratezza attesa basandosi esclusivamente sulle probabilità marginali (BR e SR) senza informazioni specifiche del modello, è:\n",
        "\n",
        "$$\n",
        "P(\\text{Accuratezza per caso}) = P(TP) + P(TN) = 0.0127 + 0.7087 = 0.7214.\n",
        "$$\n",
        "\n",
        "In questo esempio, il 72,14% di accuratezza potrebbe essere raggiunto anche senza l’uso del modello, semplicemente per caso. Dato che il nostro modello raggiunge un’accuratezza del 78%, il reale incremento di accuratezza rispetto al caso è solo del 6%. Questo evidenzia l'importanza di confrontare l’accuratezza del modello con quella ottenibile per puro caso per valutare il suo valore aggiunto.\n",
        "\n",
        "Confrontando le aspettative casuali con l’accuratezza effettiva del modello, possiamo quindi misurare il reale beneficio del modello. Se l'accuratezza effettiva supera di poco quella ottenibile per caso, significa che il modello offre un miglioramento limitato rispetto a una semplice scelta casuale.\n",
        "\n",
        "### Predire dal Tasso di Base\n",
        "\n",
        "Chiediamoci ora cosa accadrebbe se facessimo una previsione basata solo sulla probabilità generale degli esiti (tasso di base), senza considerare alcun modello predittivo.\n",
        "\n",
        "Con un tasso di base basso (BR = 0.05) di un insufficiente adattamento alla vita militare, possiamo massimizzare l'accuratezza complessiva scegliendo di non “escludere” nessuno. Questo equivale a un *rapporto di selezione* (SR) pari a zero, indicando che non classifichiamo alcun caso come \"Reject\". In questo scenario, tutti i casi sarebbero previsti come \"Retain\", aumentando l'accuratezza totale ma rinunciando completamente alla possibilità di identificare casi positivi.\n",
        "\n",
        "Se applichiamo questa logica alla matrice di confusione dei dati:\n",
        "\n",
        "|  Decision (Prediction)  |  Actual Adjustment (Poor)  |  Actual Adjustment (Good)  |  Total Predicted  |\n",
        "| ----------------------- | -------------------------- | --------------------------- | ----------------- |\n",
        "|  **Reject**              |  TP = 0                    |  FP = 0                      |  0                |\n",
        "|  **Retain**              |  FN = 100                  |  TN = 1,900                  |  2,000            |\n",
        "|  **Total Actual**        |  100                       |  1,900                       |  N = 2,000        |\n",
        "\n",
        "- Con **SR = 0** (nessun caso viene classificato come \"Reject\"), otterremo:\n",
        "  - **TN** (Vero Negativo) = 1,900 (tutti i casi corretti di \"Retain\")\n",
        "  - **FN** (Falso Negativo) = 100 (tutti i casi di \"Poor\" classificati erroneamente come \"Retain\").\n",
        "\n",
        "L'accuratezza complessiva in questo caso sarà quindi:\n",
        "\n",
        "$$\n",
        "P(\\text{Accuratezza}) = \\frac{\\text{TP} + \\text{TN}}{N} = \\frac{0 + 1,900}{2,000} = 0.95\n",
        "$$\n",
        "\n",
        "In sintesi:\n",
        "\n",
        "- \"Predire dal Tasso di Base\" implica usare l'esito prevalente per ogni predizione senza cercare di individuare i casi positivi;\n",
        "- nei dati forniti, impostando SR = 0 otteniamo un’accuratezza del 95%, che è superiore all’accuratezza del 78% del modello originale;\n",
        "- questo approccio aumenta l'accuratezza complessiva ma non offre alcuna informazione discriminativa.\n",
        "\n",
        "In conclusione, optare per l'esito più probabile in ogni predizione (cioè, predire sempre in base al tasso di base) può portare a un’elevata accuratezza, come osservato da Meehl e Rosen (1955), soprattutto quando il tasso di base è molto basso o molto alto. Questo effetto ci mostra quanto sia importante confrontare l'accuratezza del nostro modello con l'accuratezza che potremmo ottenere (1) per puro caso e (2) utilizzando solo il tasso di base. Questo confronto è cruciale, poiché l’accuratezza di un modello può cambiare notevolmente a seconda del contesto in cui viene applicato. Infatti, in alcuni contesti, dove il tasso di base si discosta molto dal 50%, l’uso del modello può addirittura ridurre la sua capacità di predizione accurata.\n",
        "\n",
        "Inoltre, è importante considerare che la *percentuale di accuratezza* tratta tutti i tipi di errore allo stesso modo, senza fare distinzioni. Tuttavia, nella pratica, non tutti gli errori hanno lo stesso peso o importanza. Il costo di un falso positivo può essere molto diverso da quello di un falso negativo, e questa differenza può variare a seconda del contesto specifico, come verrà discusso nel prossimo paragrafo.\n",
        "\n",
        "### Diversi Tipi di Errori e i loro Costi\n",
        "\n",
        "In un processo di classificazione, non tutti gli errori hanno lo stesso costo. Esistono due tipi principali di errori: i *falsi positivi* e i *falsi negativi*, ciascuno con implicazioni diverse che dipendono dal contesto della predizione.\n",
        "\n",
        "Spesso, l’accuratezza complessiva può essere aumentata affidandosi semplicemente al *tasso di base*, ma in molte situazioni può essere preferibile utilizzare uno strumento di screening, anche a costo di una minore *accuratezza complessiva*, se ciò consente di minimizzare errori specifici che hanno costi elevati. Ad esempio:\n",
        "\n",
        "1. **Screening medico**: Consideriamo uno strumento di screening per l’HIV. I *falsi positivi* (classificare erroneamente una persona come a rischio) comportano costi come la necessità di test di conferma e, talvolta, ansia temporanea per l’individuo. Tuttavia, un *falso negativo* (non identificare una persona effettivamente a rischio) ha costi molto più alti, poiché potrebbe portare a un mancato intervento precoce, con conseguenze gravi per la salute. In questo caso, i costi associati ai falsi negativi superano di gran lunga quelli dei falsi positivi, rendendo lo screening preferibile nonostante una diminuzione dell’accuratezza complessiva.\n",
        "\n",
        "2. **Selezione del personale in situazioni di rischio**: La CIA, ad esempio, ha utilizzato strumenti di selezione per identificare potenziali spie durante periodi di guerra. Un *falso positivo* in questo contesto (considerare erroneamente una persona come una spia) potrebbe risultare nell’esclusione di un candidato innocente. Un *falso negativo* (assumere una persona che è effettivamente una spia) comporta rischi molto più gravi, rendendo cruciale l’identificazione corretta delle spie, anche a costo di più *falsi positivi*.\n",
        "\n",
        "Il modo in cui i costi degli errori vengono valutati dipende fortemente dal contesto. Alcuni potenziali costi dei *falsi positivi* includono trattamenti medici non necessari o il rischio di incarcerare una persona innocente. Al contrario, i *falsi negativi* possono portare al rilascio di una persona pericolosa, alla mancata individuazione di una malattia grave, o al mancato riconoscimento di un rischio imminente.\n",
        "\n",
        "### Importanza del Rapporto di Selezione e del Tasso di Base\n",
        "\n",
        "Il costo degli errori può variare a seconda di come si imposta il *rapporto di selezione* (cioè, quanto rigorosamente si applica il criterio per accettare o escludere un individuo). La scelta di un rapporto di selezione meno restrittivo o più restrittivo influisce sulla probabilità di incorrere in *falsi positivi* e *falsi negativi* e può dipendere dal contesto e dai costi associati agli errori.\n",
        "\n",
        "- **Criterio meno rigido**: Se escludere candidati è costoso, ad esempio quando si ha la necessità di assumere molte persone, potrebbe essere più utile un criterio di selezione permissivo, che accetta anche persone con un rischio potenziale. \n",
        "- **Criterio più rigido**: In contesti in cui non è necessario accettare molti individui, si può adottare un criterio di selezione più rigido per ridurre i rischi, scartando un numero maggiore di candidati sospetti.\n",
        "\n",
        "Quando il *rapporto di selezione* differisce dal *tasso di base* degli esiti negativi effettivi, inevitabilmente si generano errori:\n",
        "\n",
        "- Se, ad esempio, il rapporto di selezione prevede di escludere il 25% dei candidati, ma solo il 5% risulta effettivamente “non idoneo,” il risultato sarà un numero elevato di *falsi positivi*.\n",
        "- D’altro canto, se si esclude solo l’1% dei candidati mentre il tasso di non idoneità è del 5%, si finirà per includere molti *falsi negativi*.\n",
        "\n",
        "### Predizioni e Affidabilità in Condizioni di Basso Tasso di Base\n",
        "\n",
        "Fare predizioni accurate diventa particolarmente complesso quando il tasso di base è basso, come nel caso di eventi rari (ad esempio, il suicidio). In questi casi, il numero di casi positivi reali è molto ridotto, rendendo difficile identificare correttamente i pochi eventi positivi senza generare numerosi falsi positivi o falsi negativi.\n",
        "\n",
        "Questa difficoltà può essere compresa in relazione alla teoria classica dei test, che definisce l'affidabilità come il rapporto tra la varianza del punteggio vero e la varianza del punteggio osservato. Con un tasso di base molto basso, la varianza del punteggio vero è ridotta, il che abbassa l'affidabilità della misura e rende più complessa una predizione accurata.\n",
        "\n",
        "### Sensibilità, Specificità, PPV e NPV\n",
        "\n",
        "Come abbiamo visto, la *percentuale di accuratezza* da sola non è sufficiente per valutare l’efficacia di un modello, poiché è molto influenzata dai *tassi di base*. Ad esempio, se il tasso di base è basso, potremmo ottenere un’alta percentuale di accuratezza semplicemente affermando che nessuno ha la condizione; se è alto, affermando che tutti ce l’hanno. Perciò, è essenziale considerare altre metriche di accuratezza, come *sensibilità* (SN), *specificità* (SP), *valore predittivo positivo* (PPV) e *valore predittivo negativo* (NPV).\n",
        "\n",
        "Queste metriche, che si possono calcolare dalla *matrice di confusione*, ci aiutano a valutare se il modello è efficace nel rilevare la condizione senza includere erroneamente i casi negativi. Analizziamole in dettaglio:\n",
        "\n",
        "- **Sensibilità** (SN): indica la capacità del test di identificare correttamente i veri positivi, cioè le persone con la condizione. Si calcola come la proporzione di veri positivi ($\\text{TP}$) rispetto al totale di persone con la condizione ($\\text{TP} + \\text{FN}$):\n",
        "  \n",
        "  $$\n",
        "  \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{86}{86 + 14} = 0.86\n",
        "  $$\n",
        "\n",
        "- **Specificità** (SP): misura la capacità del test di identificare correttamente i veri negativi, ossia le persone senza la condizione. Si calcola come la proporzione di veri negativi ($\\text{TN}$) rispetto al totale di persone senza la condizione ($\\text{TN} + \\text{FP}$):\n",
        "  \n",
        "  $$\n",
        "  \\frac{\\text{TN}}{\\text{TN} + \\text{FP}} = \\frac{1,478}{1,478 + 422} = 0.78\n",
        "  $$\n",
        "\n",
        "- **Valore Predittivo Positivo** (PPV): indica la probabilità che una persona classificata come positiva abbia effettivamente la condizione. Si calcola come la proporzione di veri positivi ($\\text{TP}$) sul totale dei positivi stimati ($\\text{TP} + \\text{FP}$):\n",
        "  \n",
        "  $$\n",
        "  \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{86}{86 + 422} = 0.17\n",
        "  $$\n",
        "\n",
        "- **Valore Predittivo Negativo** (NPV): rappresenta la probabilità che una persona classificata come negativa non abbia effettivamente la condizione. Si calcola come la proporzione di veri negativi ($\\text{TN}$) sul totale dei negativi stimati ($\\text{TN} + \\text{FN}$):\n",
        "  \n",
        "  $$\n",
        "  \\frac{\\text{TN}}{\\text{TN} + \\text{FN}} = \\frac{1,478}{1,478 + 14} = 0.99\n",
        "  $$\n",
        "\n",
        "Ogni misura è espressa come una proporzione, variando da 0 a 1, dove valori più alti indicano una maggiore accuratezza per ciascun aspetto specifico. Usando queste metriche otteniamo un quadro dettagliato dell’efficacia dello strumento a un determinato cutoff.\n",
        "\n",
        "### Interpretazione delle Metriche\n",
        "\n",
        "In questo caso, il nostro strumento mostra:\n",
        "- **Alta sensibilità** (0,86): è efficace nel rilevare chi ha la condizione.\n",
        "- **Bassa specificità** (0,78): classifica erroneamente come positivi molti casi che non hanno la condizione.\n",
        "- **Basso PPV** (0,17): la maggior parte dei casi classificati come positivi sono in realtà negativi, indicando una frequenza elevata di falsi positivi.\n",
        "- **Alto NPV** (0,99): quasi tutti i casi classificati come negativi non hanno la condizione.\n",
        "\n",
        "Quindi, pur avendo una buona capacità di rilevare i positivi (alta sensibilità), il modello è meno efficace nel limitare i falsi positivi (basso PPV). Questo potrebbe essere accettabile se l’obiettivo è identificare tutti i potenziali casi positivi, anche a costo di includere molti falsi positivi, ma potrebbe non essere ideale se il costo degli errori di falsa positività è elevato.\n",
        "\n",
        "## Alcune Stime di Accuratezza Dipendono dal Cutoff\n",
        "\n",
        "Sensibilità, specificità, PPV e NPV variano in base al cutoff (ovvero, la soglia) per la classificazione. Consideriamo il seguente esempio. Degli alieni visitano la Terra e sviluppano un test per determinare se una bacca è commestibile o non commestibile.\n"
      ],
      "id": "59f355c7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "sampleSize <- 1000\n",
        "\n",
        "edibleScores <- rnorm(sampleSize, 50, 15)\n",
        "inedibleScores <- rnorm(sampleSize, 100, 15)\n",
        "\n",
        "edibleData <- data.frame(score = c(edibleScores, inedibleScores), type = c(rep(\"edible\", sampleSize), rep(\"inedible\", sampleSize)))\n",
        "\n",
        "cutoff <- 75\n",
        "\n",
        "hist_edible <- density(edibleScores, from = 0, to = 150) %$%\n",
        "    data.frame(x = x, y = y) %>%\n",
        "    mutate(area = x >= cutoff)\n",
        "\n",
        "hist_edible$type[hist_edible$area == TRUE] <- \"edible_FP\"\n",
        "hist_edible$type[hist_edible$area == FALSE] <- \"edible_TN\"\n",
        "\n",
        "hist_inedible <- density(inedibleScores, from = 0, to = 150) %$%\n",
        "    data.frame(x = x, y = y) %>%\n",
        "    mutate(area = x < cutoff)\n",
        "\n",
        "hist_inedible$type[hist_inedible$area == TRUE] <- \"inedible_FN\"\n",
        "hist_inedible$type[hist_inedible$area == FALSE] <- \"inedible_TP\"\n",
        "\n",
        "density_data <- bind_rows(hist_edible, hist_inedible)\n",
        "\n",
        "density_data$type <- factor(density_data$type, levels = c(\"edible_TN\", \"inedible_TP\", \"edible_FP\", \"inedible_FN\"))"
      ],
      "id": "e7e36de6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La Figura successiva mostra le distribuzioni dei punteggi in base al tipo di bacca. Si può notare come ci sono due distribuzioni distinte, ma con una certa sovrapposizione. Pertanto, qualsiasi cutoff selezionato comporterà almeno alcune classificazioni errate. L'entità della sovrapposizione delle distribuzioni riflette la quantità di errore di misurazione dello strumento rispetto alla caratteristica di interesse.\n"
      ],
      "id": "b0863327"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "ggplot(data = edibleData, aes(x = score, ymin = 0, fill = type)) +\n",
        "    geom_density(alpha = .5) +\n",
        "    scale_fill_manual(name = \"Tipo di Bacca\", values = c(viridis(2)[1], viridis(2)[2])) +\n",
        "    scale_y_continuous(name = \"Frequenza\") "
      ],
      "id": "1beb3349",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La Figura successiva mostra le distribuzioni dei punteggi in base al tipo di bacca con un cutoff. La linea rossa indica il cutoff: il livello al di sopra del quale le bacche vengono classificate come non commestibili. Ci sono errori su entrambi i lati del cutoff. Sotto il cutoff, ci sono dei falsi negativi (blu): bacche non commestibili erroneamente classificate come commestibili. Sopra il cutoff, ci sono dei falsi positivi (verde): bacche commestibili erroneamente classificate come non commestibili. I costi dei falsi negativi potrebbero includere malattia o morte derivanti dal consumo di bacche non commestibili, mentre i costi dei falsi positivi potrebbero includere maggiore tempo per trovare cibo, insufficienza di cibo e fame.\n"
      ],
      "id": "7b62ebd9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "ggplot(data = density_data, aes(x = x, ymin = 0, ymax = y, fill = type)) +\n",
        "    geom_ribbon(alpha = 1) +\n",
        "    scale_fill_manual(\n",
        "        name = \"Tipo di Bacca\",\n",
        "        values = c(viridis(4)[4], viridis(4)[1], viridis(4)[3], viridis(4)[2]),\n",
        "        breaks = c(\"edible_TN\", \"inedible_TP\", \"edible_FP\", \"inedible_FN\"),\n",
        "        labels = c(\"Commestibile: TN\", \"Non Commestibile: TP\", \"Commestibile: FP\", \"Non Commestibile: FN\")\n",
        "    ) +\n",
        "    geom_line(aes(y = y)) +\n",
        "    geom_vline(xintercept = cutoff, color = \"red\", linewidth = 2) +\n",
        "    scale_x_continuous(name = \"Punteggio\") +\n",
        "    scale_y_continuous(name = \"Frequenza\") "
      ],
      "id": "26740e5b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A seconda dei nostri obiettivi di valutazione, potremmo voler usare un diverso rapporto di selezione modificando il cutoff. La Figura  mostra le distribuzioni dei punteggi quando si aumenta il cutoff. Ora ci sono più falsi negativi (blu) e meno falsi positivi (verde). Se alziamo il cutoff per essere più conservativi, il numero di falsi negativi aumenta, mentre il numero di falsi positivi diminuisce. Di conseguenza, aumentando il cutoff, la sensibilità e il valore predittivo negativo (NPV) diminuiscono, mentre la specificità e il valore predittivo positivo (PPV) aumentano. Un cutoff più alto potrebbe essere ottimale se i costi dei falsi positivi sono considerati superiori a quelli dei falsi negativi. Ad esempio, se gli alieni non possono rischiare di mangiare bacche non commestibili perché sono fatali, e ci sono abbastanza bacche commestibili per nutrire la colonia aliena.\n"
      ],
      "id": "b4f7b756"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "# Raise the cutoff\n",
        "cutoff <- 85\n",
        "\n",
        "ggplot(data = density_data, aes(x = x, ymin = 0, ymax = y, fill = type)) +\n",
        "    geom_ribbon(alpha = 1) +\n",
        "    scale_fill_manual(\n",
        "        name = \"Tipo di Bacca\",\n",
        "        values = c(viridis(4)[4], viridis(4)[1], viridis(4)[3], viridis(4)[2]),\n",
        "        breaks = c(\"edible_TN\", \"inedible_TP\", \"edible_FP\", \"inedible_FN\"),\n",
        "        labels = c(\"Commestibile: TN\", \"Non Commestibile: TP\", \"Commestibile: FP\", \"Non Commestibile: FN\")\n",
        "    ) +\n",
        "    geom_line(aes(y = y)) +\n",
        "    geom_vline(xintercept = cutoff, color = \"red\", linewidth = 2) +\n",
        "    scale_x_continuous(name = \"Punteggio\") +\n",
        "    scale_y_continuous(name = \"Frequenza\") +\n",
        "    theme(\n",
        "        axis.text.y = element_blank(),\n",
        "        axis.ticks.y = element_blank()\n",
        "    )"
      ],
      "id": "eda2632c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In alternativa, possiamo abbassare il cutoff per essere più liberali. La Figura seguente mostra le distribuzioni dei punteggi quando abbassiamo il cutoff. Ora ci sono meno falsi negativi (blu) e più falsi positivi (verde). Abbassando il cutoff, la sensibilità e il NPV aumentano, mentre la specificità e il PPV diminuiscono. Un cutoff più basso potrebbe essere ottimale se i costi dei falsi negativi sono considerati superiori a quelli dei falsi positivi. Ad esempio, se gli alieni non possono rischiare di perdere bacche commestibili perché sono scarse, e mangiare bacche non commestibili comporta solo disagi temporanei.\n"
      ],
      "id": "e8dfbdbb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "# Lower the cutoff\n",
        "cutoff <- 65\n",
        "\n",
        "ggplot(data = density_data, aes(x = x, ymin = 0, ymax = y, fill = type)) +\n",
        "    geom_ribbon(alpha = 1) +\n",
        "    scale_fill_manual(\n",
        "        name = \"Tipo di Bacca\",\n",
        "        values = c(viridis(4)[4], viridis(4)[1], viridis(4)[3], viridis(4)[2]),\n",
        "        breaks = c(\"edible_TN\", \"inedible_TP\", \"edible_FP\", \"inedible_FN\"),\n",
        "        labels = c(\"Commestibile: TN\", \"Non Commestibile: TP\", \"Commestibile: FP\", \"Non Commestibile: FN\")\n",
        "    ) +\n",
        "    geom_line(aes(y = y)) +\n",
        "    geom_vline(xintercept = cutoff, color = \"red\", linewidth = 2) +\n",
        "    scale_x_continuous(name = \"Punteggio\") +\n",
        "    scale_y_continuous(name = \"Frequenza\") +\n",
        "    theme(\n",
        "        axis.text.y = element_blank(),\n",
        "        axis.ticks.y = element_blank()\n",
        "    )"
      ],
      "id": "e923a1b3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In sintesi, sensibilità e specificità variano in base al cutoff utilizzato per la classificazione. Se aumentiamo il cutoff, la specificità e il PPV aumentano, mentre la sensibilità e il NPV diminuiscono. Se abbassiamo il cutoff, la sensibilità e il NPV aumentano, mentre la specificità e il PPV diminuiscono. Pertanto, il cutoff ottimale dipende dai costi associati ai falsi negativi e ai falsi positivi. Se i falsi negativi sono più costosi, dovremmo impostare un cutoff basso; se i falsi positivi sono più costosi, dovremmo impostare un cutoff alto.\n",
        "\n",
        "## Teoria della Detezione del Segnale\n",
        "\n",
        "La *teoria della detezione del segnale* (Signal Detection Theory, SDT) è una teoria probabilistica utilizzata per il rilevamento di uno stimolo (segnale) all'interno di un insieme di stimoli che include anche stimoli non target (rumore). La SDT è nata durante lo sviluppo del radar (**RA**dio **D**etection **A**nd **R**anging) e del sonar (**SO**und **N**avigation **A**nd **R**anging) durante la Seconda Guerra Mondiale, basandosi su ricerche in ambito sensoriale-percettivo. Il settore militare desiderava determinare quali oggetti rilevati da radar/sonar fossero effettivamente aerei o sottomarini nemici, e quali fossero solo rumore (ad esempio, oggetti diversi nell'ambiente).\n",
        "\n",
        "La SDT ha permesso di valutare il numero di errori commessi dagli operatori (cioè, quanto fossero precisi) e di scomporre tali errori in diverse categorie. La teoria distingue tra *sensibilità* e *bias*. Nella SDT, la *sensibilità* (o discriminabilità) è la capacità di un test di distinguere tra uno stimolo target e stimoli non target, ossia quanto bene il test riesca a rilevare il segnale tra i rumori. Il *bias* rappresenta invece la tendenza del test a sovrastimare o sottostimare la probabilità di un evento target rispetto al tasso reale di occorrenza di tale evento.\n",
        "\n",
        "Alcuni operatori radar/sonar non erano molto sensibili alla differenza tra segnale e rumore, a causa di fattori come l'età o la capacità di distinguere sottili variazioni di segnale. Gli individui con bassa sensibilità, che quindi non riuscivano a distinguere efficacemente tra segnale e rumore, venivano esclusi, poiché la sensibilità era considerata una competenza che difficilmente si può insegnare. Altri operatori, pur avendo una buona sensibilità, mostravano bias sistematici o scarsa *calibrazione*, cioè commettevano errori sistematici nel giudicare i segnali, ad esempio sovra-rifiutando o sotto-rifiutando il target. \n",
        "\n",
        "Sovra-rifiutare significa produrre molti *falsi negativi* (cioè, giudicare un segnale sicuro quando in realtà non lo è), mentre sotto-rifiutare genera molti *falsi positivi* (cioè, giudicare un segnale come pericoloso quando in realtà non lo è). Un operatore con buona sensibilità ma bias sistematico veniva considerato più facile da addestrare rispetto a chi aveva una bassa sensibilità. Gli operatori radar e sonar venivano quindi selezionati in base alla loro sensibilità nel distinguere tra segnale e rumore, e poi addestrati per migliorare la calibrazione e ridurre il bias sistematico, evitando così di sovra- o sotto-rifiutare gli stimoli.\n",
        "\n",
        "Anche se la SDT è stata sviluppata inizialmente durante la Seconda Guerra Mondiale, oggi ha un ruolo importante in molti ambiti della scienza e della medicina. Un esempio in medicina è il rilevamento di tumori nella radiologia. La SDT è fondamentale anche in psicologia, specialmente nella psicologia cognitiva. Ad esempio, ricerche sulla percezione sociale hanno dimostrato che gli uomini tendono a mostrare una scarsa sensibilità nel distinguere le manifestazioni di interesse sessuale nelle donne, confondendo la cordialità con l'interesse sessuale. Inoltre, gli uomini tendono ad avere un bias sistematico, sovrastimando l'interesse sessuale delle donne nei loro confronti, mostrando così una soglia troppo bassa nel giudicare tali segnali.\n",
        "\n",
        "Le metriche SDT di sensibilità includono [$d'$](#dPrimeSDT) (d-prime), [$A$](#aSDT) (o $A'$), e l'area sotto la curva ROC (Receiver Operating Characteristic). Le metriche di bias includono [$\\beta$](#betaSDT), [$c$](#cSDT) e [$b$](#bSDT).\n",
        "\n",
        "### Curva ROC (Receiver Operating Characteristic)\n",
        "\n",
        "L'asse delle x della curva ROC rappresenta il tasso di falsi allarmi o tasso di falsi positivi ($1 -$ specificità). L'asse delle y rappresenta il tasso di successi o tasso di veri positivi (sensibilità). La curva ROC traccia la combinazione tra sensibilità e specificità per ogni possibile valore di cutoff.\n",
        "\n",
        "Iniziamo con un cutoff pari a zero (in alto a destra sulla curva ROC). In questo caso, la sensibilità è pari a 1.0 e la specificità è 0, e il punto corrispondente viene tracciato sulla curva. Con un cutoff di zero, il test decide di agire su ogni stimolo (quindi il test è estremamente liberale). Aumentiamo progressivamente il cutoff e tracciamo la sensibilità e la specificità a ogni valore di cutoff. All'aumentare del cutoff, la sensibilità diminuisce e la specificità aumenta.\n",
        "\n",
        "Terminiamo con il valore di cutoff più alto possibile, dove la sensibilità è pari a 0 e la specificità è 1.0 (in altre parole, il test non agisce mai; quindi è il massimo della conservatività). Ogni punto sulla curva ROC corrisponde a una coppia di tasso di successi (sensibilità) e tasso di falsi allarmi (falsi positivi) risultante da uno specifico valore di cutoff. Dopodiché, possiamo collegare i punti con delle linee o curve per ottenere la curva ROC.\n",
        "\n",
        "La Figura seguente mostra un esempio empirico di curva ROC, dove le linee connettono i tassi di successi e di falsi allarmi.\n"
      ],
      "id": "4ddbb333"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "plot(roc(aSAH$outcome, aSAH$s100b), legacy.axes = TRUE, print.auc = TRUE)"
      ],
      "id": "eb3afc0c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creiamo ora una curva ROC lisciata, dove viene tracciata una curva continua e adattata per connettere i tassi di successi e di falsi allarmi.\n"
      ],
      "id": "2a358b53"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "plot(roc(aSAH$outcome, aSAH$s100b, smooth = TRUE), legacy.axes = TRUE, print.auc = TRUE)"
      ],
      "id": "5a241eed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Area Sotto la Curva ROC (AUC)\n",
        "\n",
        "Le metodologie ROC possono essere utilizzate per confrontare e calcolare il potere discriminativo degli strumenti di misurazione, senza essere influenzati da fattori come il *selection ratio*, il *base rate* e i costi e benefici associati. L'analisi ROC fornisce un indice quantitativo di quanto bene uno strumento possa prevedere un segnale di interesse o discriminare tra segnali diversi. Questo approccio ci aiuta a capire con quale frequenza la nostra valutazione è corretta.\n",
        "\n",
        "Se scegliamo casualmente due osservazioni, e una è corretta mentre l'altra è sbagliata, la precisione sarebbe del 50%, ma questo rifletterebbe una risposta casuale, quindi inutile. L'area geometrica sotto la curva ROC riflette l'accuratezza discriminativa della misura. Questo indice è noto come AUC (Area Under the Curve) della curva ROC. AUC quantifica il potere discriminativo di un test. Più precisamente, AUC rappresenta la probabilità che, selezionando casualmente un target e un non-target, il test classifichi correttamente il target come tale. I valori dell'AUC variano da 0.0 a 1.0, dove 0.5 rappresenta la precisione casuale, come indicato dalla linea diagonale nella curva ROC. Un test è utile nella misura in cui la sua curva ROC si trova sopra la linea diagonale, indicando che la sua accuratezza discriminativa è superiore al caso.\n"
      ],
      "id": "4b47c2de"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "plot(roc(aSAH$outcome, aSAH$s100b, smooth = TRUE), legacy.axes = TRUE, print.auc = TRUE, auc.polygon = TRUE)"
      ],
      "id": "8ee1f6f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "# Simulazione dei dati per AUC\n",
        "simulateDataFromAUC <- function(auc, n) {\n",
        "    t <- sqrt(log(1 / (1 - auc)**2))\n",
        "    z <- t - ((2.515517 + 0.802853 * t + 0.0103328 * t**2) / (1 + 1.432788 * t + 0.189269 * t**2 + 0.001308 * t**3))\n",
        "    d <- z * sqrt(2)\n",
        "\n",
        "    x <- c(rnorm(n / 2, mean = 0), rnorm(n / 2, mean = d))\n",
        "    y <- c(rep(0, n / 2), rep(1, n / 2))\n",
        "\n",
        "    data <- data.frame(x = x, y = y)\n",
        "\n",
        "    return(data)\n",
        "}\n",
        "\n",
        "set.seed(52242)\n",
        "\n",
        "auc60 <- simulateDataFromAUC(.60, 50000)\n",
        "auc70 <- simulateDataFromAUC(.70, 50000)\n",
        "auc80 <- simulateDataFromAUC(.80, 50000)\n",
        "auc90 <- simulateDataFromAUC(.90, 50000)\n",
        "auc95 <- simulateDataFromAUC(.95, 50000)\n",
        "auc99 <- simulateDataFromAUC(.99, 50000)\n",
        "\n",
        "plot(roc(y ~ x, auc60, smooth = TRUE), legacy.axes = TRUE, print.auc = TRUE, print.auc.x = .52, print.auc.y = .61, print.auc.pattern = \"%.2f\")\n",
        "plot(roc(y ~ x, auc70, smooth = TRUE), legacy.axes = TRUE, print.auc = TRUE, print.auc.x = .6, print.auc.y = .67, print.auc.pattern = \"%.2f\", add = TRUE)\n",
        "plot(roc(y ~ x, auc80, smooth = TRUE), legacy.axes = TRUE, print.auc = TRUE, print.auc.x = .695, print.auc.y = .735, print.auc.pattern = \"%.2f\", add = TRUE)\n",
        "plot(roc(y ~ x, auc90, smooth = TRUE), legacy.axes = TRUE, print.auc = TRUE, print.auc.x = .805, print.auc.y = .815, print.auc.pattern = \"%.2f\", add = TRUE)\n",
        "plot(roc(y ~ x, auc95, smooth = TRUE), legacy.axes = TRUE, print.auc = TRUE, print.auc.x = .875, print.auc.y = .865, print.auc.pattern = \"%.2f\", add = TRUE)\n",
        "plot(roc(y ~ x, auc99, smooth = TRUE), legacy.axes = TRUE, print.auc = TRUE, print.auc.x = .94, print.auc.y = .94, print.auc.pattern = \"%.2f\", add = TRUE)"
      ],
      "id": "ef6693c4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ad esempio, se l'AUC è pari a 0.75, ciò significa che il punteggio complessivo di un individuo che possiede la caratteristica in questione sarà più alto nel 75% dei casi rispetto a quello di un individuo che non la possiede. In termini più semplici, l'AUC fornisce la probabilità che lo strumento classifichi correttamente, se scegliamo casualmente un esito positivo e uno negativo.\n",
        "\n",
        "L'AUC è un indice più robusto rispetto alla percentuale di accuratezza perché la percentuale di accuratezza può essere influenzata da fattori come il base rate. L'AUC misura quanto un test è migliore del caso nel discriminare tra esiti diversi. Inoltre, è utile come indicatore generale di accuratezza discriminativa, poiché mostra quanto un test sia accurato su tutti i possibili cutoff.\n",
        "\n",
        "Anche se conoscere l'accuratezza del test a ogni cutoff può essere utile per selezionare il cutoff ottimale, nella realtà non siamo interessati a tutti i possibili cutoff, poiché non tutti gli errori hanno lo stesso costo.\n",
        "\n",
        "Iniziare {#gettingStarted-prediction}\n",
        "\n",
        "Caricare le Librerie {#loadLibraries-prediction}\n",
        "\n",
        "Preparare i Dati {#prepareData-prediction}\n",
        "Caricamento dei Dati {#loadData-prediction}\n",
        "\n",
        "Il dataset aSAH del pacchetto pROC contiene i punteggi dei test (s100b) e gli esiti clinici (outcome) di pazienti.\n"
      ],
      "id": "a041fe7a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "data(aSAH)\n",
        "mydataSDT <- aSAH"
      ],
      "id": "4c82f0b5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Per garantire la riproducibilità, imposto il seed qui sotto. L'utilizzo dello stesso seed garantirà gli stessi risultati ogni volta. Non c'è nulla di speciale in questo seed specifico.\n"
      ],
      "id": "443b76cb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "set.seed(52242)\n",
        "\n",
        "mydataSDT$testScore <- mydataSDT$s100b\n",
        "mydataSDT <- mydataSDT %>%\n",
        "    mutate(testScoreSimple = ntile(testScore, 10))\n",
        "\n",
        "mydataSDT$predictedProbability <-\n",
        "    (mydataSDT$s100b - min(mydataSDT$s100b, na.rm = TRUE)) /\n",
        "        (max(mydataSDT$s100b, na.rm = TRUE) - min(mydataSDT$s100b, na.rm = TRUE))\n",
        "mydataSDT$continuousOutcome <- mydataSDT$testScore +\n",
        "    rnorm(nrow(mydataSDT), mean = 0.20, sd = 0.20)\n",
        "mydataSDT$disorder <- NA\n",
        "mydataSDT$disorder[mydataSDT$outcome == \"Good\"] <- 0\n",
        "mydataSDT$disorder[mydataSDT$outcome == \"Poor\"] <- 1"
      ],
      "id": "e4165ed9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La curva ROC (Receiver Operating Characteristic) mostra la combinazione tra il tasso di successo (o sensibilità) e il tasso di falsi allarmi ($1 - \\text{specificità}$) per ogni possibile soglia di cutoff. La curva dimostra come, all'aumentare della soglia (diventando più conservativa), la sensibilità diminuisce e la specificità aumenta, e viceversa.\n",
        "\n",
        "Le curve ROC possono essere generate utilizzando il pacchetto pROC, e gli esempi mostrano che la misura ha un'accuratezza moderata—è più accurata del caso, ma c'è margine di miglioramento.\n",
        "\n",
        "Curva ROC Empirica {#empiricalROC}\n",
        "Il codice per generare una curva ROC empirica è mostrato qui sotto, e il grafico è visibile in Figura @ref(fig\n"
      ],
      "id": "4d240cf0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "rocCurve <- roc(data = mydataSDT, response = disorder, predictor = testScore, smooth = FALSE)"
      ],
      "id": "de61740a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "plot(rocCurve, legacy.axes = TRUE, print.auc = TRUE)"
      ],
      "id": "e4c8a734",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una curva ROC empirica con i cutoff sovrapposti è mostrata \n"
      ],
      "id": "9a0caa82"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "pred <- prediction(\n",
        "    na.omit(mydataSDT[, c(\"testScoreSimple\", \"disorder\")])$testScoreSimple,\n",
        "    na.omit(mydataSDT[, c(\"testScoreSimple\", \"disorder\")])$disorder\n",
        ")\n",
        "perf <- performance(pred, \"tpr\", \"fpr\")\n",
        "plot(perf, print.cutoffs.at = 1:11, text.adj = c(1, -1), ylim = c(0, 1.05))\n",
        "abline(coef = c(0, 1))"
      ],
      "id": "15882391",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Curva ROC Liscia si ottiene nel modo seguente.\n"
      ],
      "id": "b22b2791"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "rocCurveSmooth <- roc(data = mydataSDT, response = disorder, predictor = testScore, smooth = TRUE)\n",
        "plot(rocCurveSmooth, legacy.axes = TRUE, print.auc = TRUE)"
      ],
      "id": "51ddc815",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Statistica di Youden J {#youdenJ}\n",
        "La soglia della statistica di Youden J è il punto in cui il test ha la massima combinazione (somma) di sensibilità e specificità: $\\text{max}(\\text{sensitivity} + \\text{specificity} - 1)$.\n"
      ],
      "id": "d79b519e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "youdenJ <- coords(rocCurve, x = \"best\", best.method = \"youden\")\n",
        "youdenJthreshold <- youdenJ$threshold\n",
        "youdenJspecificity <- youdenJ$specificity\n",
        "youdenJsensitivity <- youdenJ$sensitivity\n",
        "\n",
        "youdenJ"
      ],
      "id": "f13b69ce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Per questo test, la soglia ottimale secondo la statistica di Youden J è $r youdenJthreshold$, con una sensibilità di $r youdenJsensitivity$ e una specificità di $r youdenJspecificity$.\n",
        "\n",
        "Punto più vicino alla parte in alto a sinistra della curva ROC {#topLeftROC}\n",
        "Il punto più vicino alla parte superiore sinistra della curva ROC, dove sensibilità e specificità sono perfette, è dato da: $\\text{min}[(1 - \\text{sensitivity})^2 + (1 - \\text{specificity})^2]$.\n"
      ],
      "id": "cc296c31"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "closestTopLeft <- coords(rocCurve, x = \"best\", best.method = \"closest.topleft\")\n",
        "closestTopLeftthreshold <- closestTopLeft$threshold\n",
        "closestTopLeftspecificity <- closestTopLeft$specificity\n",
        "closestTopLeftsensitivity <- closestTopLeft$sensitivity\n",
        "\n",
        "closestTopLeft"
      ],
      "id": "eaa5315a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Per questo test, la combinazione di sensibilità e specificità è ottimale alla soglia di $r closestTopLeftthreshold$, con una sensibilità di $r closestTopLeftsensitivity$ e una specificità di $r closestTopLeftspecificity$.\n",
        "\n",
        "## Accuratezza della Predizione attraverso i Cutoff {#predictionAccuracy}\n",
        "\n",
        "Esistono due dimensioni principali dell'accuratezza: (1) la *discriminazione* (ad esempio, [sensibilità](#sensitivity), [specificità](#specificity), [area sotto la curva ROC](#auc)) e (2) la *calibrazione*. Alcuni indici generali di accuratezza combinano la discriminazione e la calibrazione.\n",
        "\n",
        "Il pacchetto [`petersenlab`](https://github.com/DevPsyLab/petersenlab) include la funzione `accuracyOverall()`, che stima l'accuratezza della predizione su tutti i cutoff.\\index{petersenlab package}\\index{cutoff}\n",
        "\n",
        "Ecco un esempio di codice che utilizza questa funzione:\n"
      ],
      "id": "2f040495"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "accuracyOverall(\n",
        "    predicted = mydataSDT$testScore,\n",
        "    actual = mydataSDT$disorder\n",
        ") %>%\n",
        "    t() %>%\n",
        "    round(., 2)\n",
        "\n",
        "accuracyOverall(\n",
        "    predicted = mydataSDT$testScore,\n",
        "    actual = mydataSDT$disorder,\n",
        "    dropUndefined = TRUE\n",
        ") %>%\n",
        "    t() %>%\n",
        "    round(., 2)"
      ],
      "id": "1936d091",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In questo esempio, la funzione `accuracyOverall()` calcola l'accuratezza complessiva della predizione su tutta la gamma di cutoff disponibili, fornendo una sintesi del grado di accuratezza globale del modello.\n",
        "\n",
        "## Calibrazione {#calibration}\n",
        "\n",
        "Quando si tratta di un risultato categorico, la calibrazione è il grado in cui una stima probabilistica di un evento riflette la reale probabilità sottostante di quell'evento.\\index{calibration}  \n",
        "Quando si tratta di un risultato continuo, la calibrazione indica quanto i valori previsti siano vicini ai valori effettivi osservati.\\index{calibration}  \n",
        "L'importanza di esaminare la calibrazione, oltre alla *discriminazione*, è descritta da Lindhiem (2020).\\index{calibration}\\index{discrimination}\n",
        "\n",
        "La calibrazione è diventata centrale nella valutazione dell'accuratezza delle previsioni meteorologiche.\\index{calibration}  \n",
        "Ad esempio, nei giorni in cui un meteorologo prevede il 60% di possibilità di pioggia, dovrebbe effettivamente piovere circa il 60% delle volte.  \n",
        "Grazie ai progressi nella comprensione scientifica dei sistemi meteorologici, le previsioni della pioggia sono diventate più accurate.  \n",
        "Le previsioni della National Weather Service, per esempio, sono ben calibrate.  \n",
        "Tuttavia, le previsioni di pioggia fatte da meteorologi televisivi locali possono essere esagerate per aumentare l'audience (Silver, 2012).  \n",
        "Curiosamente, alcune previsioni di pioggia di The Weather Channel risultano miscalibrate in certe condizioni (Bickel, 2008).  \n",
        "Ad esempio, nei giorni in cui viene prevista una probabilità di pioggia del 20%, la probabilità reale è in realtà intorno al 5%.  \n",
        "Questa miscalibrazione è deliberata, poiché le persone tendono a essere più arrabbiate se viene detto loro che non pioverà e invece piove (falsi negativi) rispetto al contrario (falsi positivi).\\index{calibration}\\index{false negative}\\index{false positive}  \n",
        "Come osserva Silver (2012), \"Se piove quando non dovrebbe, le persone si arrabbiano con il meteorologo, mentre una giornata inaspettatamente soleggiata è vista come un bonus fortuito.\"\n",
        "\n",
        "La calibrazione non è importante solo per le previsioni meteorologiche, ma anche per la valutazione psicologica.  \n",
        "Esistono diversi metodi per esaminare la calibrazione, come i *Brier Scores*, il test di Hosmer-Lemeshow, lo *z* di Spiegelhalter e la differenza media tra i valori previsti e osservati a diversi intervalli di soglie, rappresentata graficamente tramite un *calibration plot*.\n",
        "\n",
        "### Calibration Plot {#calibrationPlot}\n",
        "\n",
        "Un *calibration plot* può aiutare a individuare la miscalibrazione.  \n",
        "Questo grafico rappresenta la probabilità prevista di un evento sull'asse x e la probabilità effettiva osservata sull'asse y.  \n",
        "Le previsioni sono suddivise in gruppi (spesso 10).  \n",
        "La linea diagonale rappresenta previsioni perfettamente calibrate.  \n",
        "Quando le previsioni si discostano da questa linea, indicano miscalibrazione.  \n",
        "Esistono quattro pattern generali di miscalibrazione: *overextremity*, *underextremity*, *overprediction*, e *underprediction*.\n",
        "\n",
        "- *Overextremity* si verifica quando le probabilità previste sono troppo vicine agli estremi (zero o uno).  \n",
        "- *Underextremity* accade quando le probabilità previste sono troppo lontane dagli estremi.  \n",
        "- *Overprediction* si ha quando le probabilità previste sono costantemente maggiori di quelle osservate.  \n",
        "- *Underprediction* si ha quando le probabilità previste sono costantemente minori di quelle osservate.\n",
        "\n",
        "Per generare un *calibration plot*, possiamo utilizzare il pacchetto `PredictABEL`. Di seguito è riportato un esempio di codice in R per creare questo grafico:\n"
      ],
      "id": "83b161b5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "data(aSAH)\n",
        "mydataSDT <- aSAH\n",
        "\n",
        "set.seed(52242)\n",
        "\n",
        "mydataSDT$testScore <- mydataSDT$s100b\n",
        "mydataSDT <- mydataSDT %>%\n",
        "    mutate(testScoreSimple = ntile(testScore, 10))\n",
        "\n",
        "mydataSDT$predictedProbability <-\n",
        "    (mydataSDT$s100b - min(mydataSDT$s100b, na.rm = TRUE)) /\n",
        "        (max(mydataSDT$s100b, na.rm = TRUE) - min(mydataSDT$s100b, na.rm = TRUE))\n",
        "mydataSDT$continuousOutcome <- mydataSDT$testScore +\n",
        "    rnorm(nrow(mydataSDT), mean = 0.20, sd = 0.20)\n",
        "mydataSDT$disorder <- NA\n",
        "mydataSDT$disorder[mydataSDT$outcome == \"Good\"] <- 0\n",
        "mydataSDT$disorder[mydataSDT$outcome == \"Poor\"] <- 1\n",
        "\n",
        "colNumberOutcome <- which(names(mydataSDT) == \"disorder\")\n",
        "myDataNoMissing <- na.omit(mydataSDT)"
      ],
      "id": "c7640e91",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "plotCalibration(\n",
        "    data = na.omit(myDataNoMissing),\n",
        "    cOutcome = colNumberOutcome,\n",
        "    predRisk = myDataNoMissing$predictedProbability,\n",
        "    groups = 10\n",
        ")"
      ],
      "id": "13fecdb1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Questo *calibration plot* mostra che le probabilità previste erano consistentemente inferiori a quelle effettive osservate, suggerendo un problema di *underprediction*. Per correggere questa miscalibrazione, le probabilità previste dovrebbero essere aumentate affinché risultino coerenti con quelle osservate.\n",
        "\n",
        "#### Brier Scores {#brierScores}\n",
        "\n",
        "#### Brier Scores {#brierScores}\n",
        "\n",
        "I punteggi di Brier (*Brier scores*) offrono una misura di accuratezza per le previsioni probabilistiche, valutando quanto le probabilità previste si avvicinino ai risultati reali. Questo punteggio è calcolato come la media dei quadrati delle differenze tra le probabilità previste e i risultati osservati, permettendo di valutare la qualità delle previsioni considerando sia errori di sovrastima che di sottostima.\n",
        "\n",
        "Un punteggio di Brier basso indica una migliore calibrazione delle previsioni, con un valore di 0 che rappresenta una previsione perfettamente calibrata (cioè, le probabilità previste coincidono esattamente con i risultati osservati). Valori più vicini a 1 indicano invece una calibrazione peggiore, suggerendo che le previsioni si discostano ampiamente dai risultati effettivi.\n",
        "\n",
        "Il pacchetto `rms` di R può essere utilizzato per calcolare i punteggi di Brier, come mostrato nell'esempio seguente:\n"
      ],
      "id": "7cf8c4d2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "val.prob(\n",
        "    mydataSDT$predictedProbability,\n",
        "    mydataSDT$disorder,\n",
        "    pl = FALSE\n",
        ")[\"Brier\"]"
      ],
      "id": "3ee5d649",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In sintesi, il punteggio di Brier rappresenta una misura sintetica della precisione e della calibrazione di un modello predittivo: quanto più è basso, tanto maggiore è la capacità del modello di riflettere accuratamente le probabilità reali.\n",
        "\n",
        "## Matrice di Confusione \n",
        "\n",
        "Una matrice di confusione (detta anche tabella 2x2 di accuratezza, tabella di cross-tabulation o tabella di contingenza) è una matrice utilizzata per i dati categoriali che mostra l'esito predetto su una dimensione e l'esito reale (la verità) sull'altra. Se le predizioni e gli esiti sono dicotomici, la matrice di confusione è una matrice 2x2 con due righe e due colonne che rappresentano le quattro possibili combinazioni previste-reali (i cosiddetti outcome decisionali). In questo caso, la matrice di confusione fornisce un conteggio tabellare dei casi corretti (true positives e true negatives) rispetto agli errori (false positives e false negatives).\n",
        "\n",
        "Conteggio Assoluto {#confusionMatrix-number}\n",
        "\n",
        "È possibile visualizzare i dati di una matrice di confusione utilizzando il seguente codice:\n"
      ],
      "id": "3bbbc366"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "cutoff <- 0.205\n",
        "\n",
        "mydataSDT$diagnosis <- NA\n",
        "mydataSDT$diagnosis[mydataSDT$testScore < cutoff] <- 0\n",
        "mydataSDT$diagnosis[mydataSDT$testScore >= cutoff] <- 1\n",
        "\n",
        "mydataSDT$diagnosisFactor <- factor(\n",
        "    mydataSDT$diagnosis,\n",
        "    levels = c(1, 0),\n",
        "    labels = c(\"Decision: Diagnosis\", \"Decision: No Diagnosis\")\n",
        ")\n",
        "\n",
        "mydataSDT$disorderFactor <- factor(\n",
        "    mydataSDT$disorder,\n",
        "    levels = c(1, 0),\n",
        "    labels = c(\"Truth: Disorder\", \"Truth: No Disorder\")\n",
        ")\n",
        "\n",
        "table(mydataSDT$diagnosisFactor, mydataSDT$disorderFactor)"
      ],
      "id": "faf045e5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conteggio con Margini Aggiunti {#confusionMatrix-numberMargins}\n",
        "\n",
        "Per aggiungere i margini ai conteggi assoluti:\n"
      ],
      "id": "17c5ce16"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "addmargins(table(mydataSDT$diagnosisFactor, mydataSDT$disorderFactor))"
      ],
      "id": "c0a4c07f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Per calcolare le proporzioni:\n"
      ],
      "id": "94ebeeca"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "prop.table(table(mydataSDT$diagnosisFactor, mydataSDT$disorderFactor))"
      ],
      "id": "7f34f588",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aggiungi i margini alle proporzioni:\n"
      ],
      "id": "395eb3e5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "addmargins(prop.table(table(mydataSDT$diagnosisFactor, mydataSDT$disorderFactor)))"
      ],
      "id": "ad699709",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### True Positives (TP) {#truePositive}\n",
        "\n",
        "I *true positives* (TP) sono i casi in cui una classificazione positiva (ad esempio, la presenza di un disturbo) è corretta, ovvero il test indica che la classificazione è presente, e lo è davvero. Più alto è il numero di *true positives* rispetto alla dimensione del campione, maggiore è l'accuratezza. La formula per calcolare i *true positives* è:\n",
        "\n",
        "$$\n",
        "TP = BR \\times SR \\times N\n",
        "$$\n",
        "\n",
        "Ecco come calcolare i true positives in R:\n"
      ],
      "id": "ff36aa05"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "TPvalue <- length(which(\n",
        "    mydataSDT$diagnosis == 1 & mydataSDT$disorder == 1\n",
        "))\n",
        "\n",
        "TPvalue"
      ],
      "id": "35145ecf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### True Negatives (TN) {#trueNegative}\n",
        "\n",
        "I *true negatives* (TN) sono i casi in cui una classificazione negativa (assenza di disturbo) è corretta, ovvero il test indica che la classificazione non è presente e questa effettivamente non è presente. La formula per calcolare i *true negatives* è:\n",
        "\n",
        "$$\n",
        "TN = (1 - BR) \\times (1 - SR) \\times N\n",
        "$$\n",
        "\n",
        "Per calcolare i true negatives in R:\n"
      ],
      "id": "5b8dd784"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "TNvalue <- length(which(\n",
        "    mydataSDT$diagnosis == 0 & mydataSDT$disorder == 0\n",
        "))\n",
        "\n",
        "TNvalue"
      ],
      "id": "4ca65b39",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### False Positives (FP) {#falsePositive}\n",
        "\n",
        "I *false positives* (FP) sono i casi in cui una classificazione positiva è errata, ovvero il test indica la presenza di un disturbo che in realtà non è presente. Valori più bassi di *false positives* riflettono una maggiore accuratezza. La formula per calcolare i *false positives* è:\n",
        "\n",
        "$$\n",
        "FP = (1 - BR) \\times SR \\times N\n",
        "$$\n",
        "\n",
        "Ecco come calcolare i false positives in R:\n"
      ],
      "id": "3e6a5cdb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "FPvalue <- length(which(\n",
        "    mydataSDT$diagnosis == 1 & mydataSDT$disorder == 0\n",
        "))\n",
        "\n",
        "FPvalue"
      ],
      "id": "33595065",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### False Negatives (FN) {#falseNegative}\n",
        "\n",
        "I *false negatives* (FN) sono i casi in cui una classificazione negativa è errata, ovvero il test indica l'assenza di un disturbo che in realtà è presente. Valori più bassi di *false negatives* riflettono una maggiore accuratezza. La formula per calcolare i *false negatives* è:\n",
        "\n",
        "$$\n",
        "FN = BR \\times (1 - SR) \\times N\n",
        "$$\n",
        "\n",
        "Per calcolare i false negatives in R:\n"
      ],
      "id": "9d36f5c1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "FNvalue <- length(which(\n",
        "    mydataSDT$diagnosis == 0 & mydataSDT$disorder == 1\n",
        "))\n",
        "\n",
        "FNvalue"
      ],
      "id": "91caef00",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dimensione del Campione (*N*) {#sampleSize-prediction}\n",
        "\n",
        "La dimensione del campione può essere calcolata sommando il numero di TP, TN, FP e FN:\n"
      ],
      "id": "be40ca6a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "sampleSize <- function(TP, TN, FP, FN) {\n",
        "    value <- TP + TN + FP + FN\n",
        "\n",
        "    return(value)\n",
        "}\n",
        "\n",
        "sampleSize(\n",
        "    TP = TPvalue,\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue,\n",
        "    FN = FNvalue\n",
        ")"
      ],
      "id": "7bd842b0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Selection Ratio (SR) {#selectionRatio}\n",
        "\n",
        "The selection ratio (SR) is the marginal probability of selection, independent of other things: $P(R_i)$.\\index{selection ratio}\\index{probability!marginal}\n",
        "In clinical psychology, the selection ratio is the proportion of people who test positive for the disorder, as in Equation \\@ref(eq:selectionRatio):\\index{selection ratio}\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\text{SR} &= P(R_i) \\\\\n",
        "  &= \\frac{\\text{TP} + \\text{FP}}{N}\n",
        "\\end{aligned}\n",
        "(\\#eq:selectionRatio)\n",
        "$$\n"
      ],
      "id": "3769c48b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "selectionRatio <- function(TP, TN, FP, FN){\n",
        "  N <- TP + TN + FP + FN\n",
        "  value <- (TP + FP)/N\n",
        "  \n",
        "  return(value)\n",
        "}\n",
        "\n",
        "selectionRatio(\n",
        "    TP = TPvalue,\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue,\n",
        "    FN = FNvalue\n",
        ")\n",
        "\n",
        "selectionRatioValue <- selectionRatio(TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue)"
      ],
      "id": "41b3e345",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Base Rate (BR) {#pretestProbability}\n",
        "\n",
        "The [base rate](#baseRate) (BR) of a classification is its [marginal probability](#baseRate), independent of other things: $P(C_i)$.\\index{base rate}\\index{probability!marginal}\n",
        "In clinical psychology, the base rate of a disorder is its prevalence in the population, as in Equation \\@ref(eq:baseRate).\\index{base rate}\\index{probability!marginal}\n",
        "Without additional information, the [base rate](#baseRate) is used as the initial *pretest probability*.\\index{base rate}\\index{probability!marginal}\\index{probability!pretest}\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\text{BR} &= P(C_i) \\\\\n",
        "  &= \\frac{\\text{TP} + \\text{FN}}{N}\n",
        "\\end{aligned}\n",
        "(\\#eq:baseRate)\n",
        "$$\n"
      ],
      "id": "f8808dd3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "baseRate <- function(TP, TN, FP, FN){\n",
        "  N <- TP + TN + FP + FN\n",
        "  value <- (TP + FN)/N\n",
        "  \n",
        "  return(value)\n",
        "}\n",
        "\n",
        "baseRate(\n",
        "  TP = TPvalue,\n",
        "  TN = TNvalue,\n",
        "  FP = FPvalue,\n",
        "  FN = FNvalue)\n",
        "\n",
        "baseRateValue <- baseRate(TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue)"
      ],
      "id": "f40d8de1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pretest Odds {#pretestOdds}\n",
        "\n",
        "The pretest odds of a classification can be estimated using the pretest probability (i.e., [base rate](#baseRate)).\\index{odds!pretest}\\index{probability!pretest}\n",
        "To convert a probability to odds, divide the probability by one minus that probability, as in Equation \\@ref(eq:pretestOdds).\\index{probability}\\index{odds}\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\text{pretest odds} &= \\frac{\\text{pretest probability}}{1 - \\text{pretest probability}} \\\\\n",
        "\\end{aligned}\n",
        "(\\#eq:pretestOdds)\n",
        "$$\n"
      ],
      "id": "4ae2ae5f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "pretestOdds <- function(TP, TN, FP, FN, pretestProb = NULL){\n",
        "  if(!is.null(pretestProb)){\n",
        "    pretestProbability <- pretestProb\n",
        "  } else {\n",
        "    N <- TP + TN + FP + FN\n",
        "    pretestProbability <- (TP + FN)/N\n",
        "  }\n",
        "  \n",
        "  value <- pretestProbability / (1 - pretestProbability)\n",
        "  \n",
        "  return(value)\n",
        "}\n",
        "\n",
        "pretestOdds(\n",
        "  TP = TPvalue,\n",
        "  TN = TNvalue,\n",
        "  FP = FPvalue,\n",
        "  FN = FNvalue)\n",
        "\n",
        "pretestOdds(pretestProb = baseRate(\n",
        "  TP = TPvalue,\n",
        "  TN = TNvalue,\n",
        "  FP = FPvalue,\n",
        "  FN = FNvalue))\n",
        "\n",
        "pretestOddsValue <- pretestOdds(TP = TPvalue,\n",
        "                                TN = TNvalue,\n",
        "                                FP = FPvalue,\n",
        "                                FN = FNvalue)"
      ],
      "id": "e5123edd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Percent Accuracy {#percentAccuracy}\n",
        "\n",
        "Percent Accuracy is also called overall accuracy.\\index{percent accuracy}\n",
        "Higher values reflect greater accuracy.\\index{percent accuracy}\n",
        "The formula for percent accuracy is in Equation \\@ref(eq:percentAccuracy).\\index{percent accuracy}\n",
        "Percent accuracy has several problems.\\index{percent accuracy}\n",
        "First, it treats all errors ([FP](#falsePositive) and [FN](#falseNegative)) as equally important.\\index{percent accuracy}\\index{false positive}\\index{false negative}\n",
        "However, in practice, it is rarely the case that [false positives](#falsePositive) and [false negatives](#falseNegative) are equally important.\\index{percent accuracy}\\index{false positive}\\index{false negative}\n",
        "Second, percent accuracy can be misleading because it is highly influenced by [base rates](#baseRate).\\index{percent accuracy}\\index{base rate}\n",
        "You can have a high percent accuracy by predicting from the [base rate](#baseRate) and saying that no one has the characteristic (if the [base rate](#baseRate) is low) or that everyone has the characteristic (if the [base rate](#baseRate) is high).\\index{percent accuracy}\\index{base rate}\\index{base rate!predicting from}\\index{selection ratio}\n",
        "Thus, it is also important to consider other aspects of accuracy.\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{Percent Accuracy} = 100\\% \\times \\frac{\\text{TP} + \\text{TN}}{N}\n",
        "(\\#eq:percentAccuracy)\n",
        "\\end{equation}\n"
      ],
      "id": "fc78b7e6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "vscode": {
          "languageId": "r"
        }
      },
      "source": [
        "percentAccuracy <- function(TP, TN, FP, FN){\n",
        "  N <- TP + TN + FP + FN\n",
        "  value <- 100 * ((TP + TN)/N)\n",
        "  \n",
        "  return(value)\n",
        "}\n",
        "\n",
        "percentAccuracy(\n",
        "  TP = TPvalue,\n",
        "  TN = TNvalue,\n",
        "  FP = FPvalue,\n",
        "  FN = FNvalue)\n",
        "\n",
        "percentAccuracyValue <- percentAccuracy(TP = TPvalue,\n",
        "                                        TN = TNvalue,\n",
        "                                        FP = FPvalue,\n",
        "                                        FN = FNvalue)"
      ],
      "id": "ca83695c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Percent Accuracy by Chance {#percentAccuracyByChance}\n",
        "\n",
        "The formula for calculating percent accuracy by chance is in Equation \\@ref(eq:PercentAccuracyByChance).\\index{percent accuracy!by chance}\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\text{Percent Accuracy by Chance} &= 100\\% \\times [P(\\text{TP}) + P(\\text{TN})] \\\\\n",
        "  &= 100\\% \\times \\{(\\text{BR} \\times {\\text{SR}}) + [(1 - \\text{BR}) \\times (1 - \\text{SR})]\\}\n",
        "\\end{aligned}\n",
        "(\\#eq:PercentAccuracyByChance)\n",
        "$$\n"
      ],
      "id": "1bbe238c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "percentAccuracyByChance <- function(TP, TN, FP, FN){\n",
        "  N <- TP + TN + FP + FN\n",
        "  BR <- (TP + FN)/N\n",
        "  SR <- (TP + FP)/N\n",
        "  value <- 100 * ((BR * SR) + ((1 - BR) * (1 - SR)))\n",
        "  \n",
        "  return(value)\n",
        "}"
      ],
      "id": "5fd6f277",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "percentAccuracyByChance(\n",
        "  TP = TPvalue,\n",
        "  TN = TNvalue,\n",
        "  FP = FPvalue,\n",
        "  FN = FNvalue)"
      ],
      "id": "6578d5f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "percentAccuracyByChanceValue <- percentAccuracyByChance(TP = TPvalue,\n",
        "                                                        TN = TNvalue,\n",
        "                                                        FP = FPvalue,\n",
        "                                                        FN = FNvalue)"
      ],
      "id": "dda4e785",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Percent Accuracy Predicting from the Base Rate {#percentAccuracyPredictingFromBaseRate}\n",
        "\n",
        "[*Predicting from the base rate*](#predictingFromBaseRate) is going with the most likely outcome in every prediction.\\index{base rate!predicting from}\n",
        "It is also called \"betting from the base rate\".\\index{base rate!predicting from}\n",
        "If the [base rate](#baseRate) is less than .50, it would involve predicting that the condition is absent for every case.\\index{base rate!predicting from}\\index{selection ratio}\n",
        "If the [base rate](#baseRate) is .50 or above, it would involve predicting that the condition is present for every case.\\index{base rate!predicting from}\\index{selection ratio}\n",
        "[Predicting from the base rate](#predictingFromBaseRate) is a special case of [percent accuracy by chance](#percentAccuracyByChance) when the [selection ratio](#selectionRatio) is set to either one (if the [base rate](#baseRate) $\\geq$ .5) or zero (if the [base rate](#baseRate) < .5).\\index{base rate!predicting from}\\index{selection ratio}\\index{percent accuracy!by chance}\\index{base rate}\n"
      ],
      "id": "c4810352"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "percentAccuracyPredictingFromBaseRate <- function(TP, TN, FP, FN){\n",
        "  N <- TP + TN + FP + FN\n",
        "  BR <- (TP + FN)/N\n",
        "  \n",
        "  ifelse(BR >= .5, SR <- 1, NA)\n",
        "  ifelse(BR < .5, SR <- 0, NA)\n",
        "\n",
        "  value <- 100 * ((BR * SR) + ((1 - BR) * (1 - SR)))\n",
        "  \n",
        "  return(value)\n",
        "}"
      ],
      "id": "e36478f9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "percentAccuracyPredictingFromBaseRate(\n",
        "  TP = TPvalue,\n",
        "  TN = TNvalue,\n",
        "  FP = FPvalue,\n",
        "  FN = FNvalue)"
      ],
      "id": "a82c32eb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "percentAccuracyPredictingFromBaseRateValue <- percentAccuracyPredictingFromBaseRate(TP = TPvalue,\n",
        "                                                                                    TN = TNvalue,\n",
        "                                                                                    FP = FPvalue,\n",
        "                                                                                    FN = FNvalue)"
      ],
      "id": "1e6ef37e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Relative Improvement Over Chance (RIOC) {#relativeImprovementOverChance}\n",
        "\n",
        "Relative improvement over chance (RIOC) is a prediction's improvement over chance as a proportion of the maximum possible improvement over chance, as described by @Farrington1989.\\index{relative improvement!over chance}\n",
        "Higher values reflect greater accuracy.\\index{relative improvement!over chance}\n",
        "The formula for calculating RIOC is in Equation \\@ref(eq:relativeImprovementOverChance).\\index{relative improvement!over chance}\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\text{relative improvement over chance (RIOC)} &= \\frac{\\text{total correct} - \\text{chance correct}}{\\text{maximum correct} - \\text{chance correct}} \\\\\n",
        "\\end{aligned}\n",
        "(\\#eq:relativeImprovementOverChance)\n",
        "$$\n"
      ],
      "id": "1dcf1b87"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "relativeImprovementOverChance <- function(TP, TN, FP, FN){\n",
        "  N <- TP + TN + FP + FN\n",
        "  actualYes <- TP + FN\n",
        "  predictedYes <- TP + FP\n",
        "  value <- ((N * (TP + TN)) - (actualYes * predictedYes + (N - predictedYes) * (N - actualYes))) / ((N * (actualYes + N - predictedYes)) - (actualYes * predictedYes + (N - predictedYes) * (N - actualYes)))\n",
        "  \n",
        "  return(value)\n",
        "}"
      ],
      "id": "153de134",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "relativeImprovementOverChance(\n",
        "  TP = TPvalue,\n",
        "  TN = TNvalue,\n",
        "  FP = FPvalue,\n",
        "  FN = FNvalue)"
      ],
      "id": "8a240a14",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "relativeImprovementOverChanceValue <- relativeImprovementOverChance(TP = TPvalue,\n",
        "                                                                    TN = TNvalue,\n",
        "                                                                    FP = FPvalue,\n",
        "                                                                    FN = FNvalue)"
      ],
      "id": "3571c3bc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Relative Improvement Over Predicting from the Base Rate {#relativeImprovementOverPredictingFromBaseRate}\n",
        "\n",
        "Relative improvement over [predicting from the base rate](#predictingFromBaseRate) is a prediction's improvement over [predicting from the base rate](#predictingFromBaseRate) as a proportion of the maximum possible improvement over [predicting from the base rate](#predictingFromBaseRate).\\index{relative improvement!over-predicting from the base rate}\n",
        "Higher values reflect greater accuracy.\\index{relative improvement!over-predicting from the base rate}\n",
        "The formula for calculating relative improvement over predicting from the base rate is in Equation \\@ref(eq:relativeImprovementOverPredictingFromBaseRate).\\index{relative improvement!over-predicting from the base rate}\n",
        "\n",
        "$$\n",
        "\\scriptsize\n",
        "\\begin{aligned}\n",
        "  \\text{relative improvement over predicting from base rate} &= \\frac{\\text{total correct} - \\text{correct by predicting from base rate}}{\\text{maximum correct} - \\text{correct by predicting from base rate}} \\\\\n",
        "\\end{aligned}\n",
        "(\\#eq:relativeImprovementOverPredictingFromBaseRate)\n",
        "$$\n"
      ],
      "id": "f9057e1e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "relativeImprovementOverPredictingFromBaseRate <- function(TP, TN, FP, FN){\n",
        "  N <- TP + TN + FP + FN\n",
        "  BR <- (TP + FN)/N\n",
        "\n",
        "  ifelse(BR >= .5, SR <- 1, NA)\n",
        "  ifelse(BR < .5, SR <- 0, NA)\n",
        "  \n",
        "  actualYes <- TP + FN\n",
        "  predictedYes <- SR * N\n",
        "  value <- ((N * (TP + TN)) - (actualYes * predictedYes + (N - predictedYes) * (N - actualYes))) / ((N * (actualYes + N - predictedYes)) - (actualYes * predictedYes + (N - predictedYes) * (N - actualYes)))\n",
        "  \n",
        "  return(value)\n",
        "}"
      ],
      "id": "b8b2582b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "relativeImprovementOverPredictingFromBaseRate(\n",
        "  TP = TPvalue,\n",
        "  TN = TNvalue,\n",
        "  FP = FPvalue,\n",
        "  FN = FNvalue)"
      ],
      "id": "b2d2df5b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "relativeImprovementOverPredictingFromBaseRateValue <- relativeImprovementOverPredictingFromBaseRate(\n",
        "  TP = TPvalue,\n",
        "  TN = TNvalue,\n",
        "  FP = FPvalue,\n",
        "  FN = FNvalue)"
      ],
      "id": "60ff33ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sensitivity (SN) {#sensitivity}\n",
        "\n",
        "Sensitivity (SN) is also called true positive rate (TPR), hit rate (HR), or recall.\\index{sensitivity}\\index{recall!zzzzz@\\igobble \\mid seealso{sensitivity}}\n",
        "Sensitivity is the [conditional probability](#conditionalProbability) of a positive test given that the person has the condition: $P(R \\mid C)$.\\index{sensitivity}\\index{probability!conditional}\n",
        "Higher values reflect greater accuracy.\\index{sensitivity}\n",
        "The formula for calculating sensitivity is in Equation \\@ref(eq:sensitivity).\\index{sensitivity}\n",
        "As described in Section \\@ref(accuracyCutoff) and as depicted in Figure \\@ref(fig:sensitivitySpecificity), as the cutoff increases (becomes more conservative), sensitivity decreases.\\index{sensitivity}\\index{cutoff}\n",
        "As the cutoff decreases, sensitivity increases.\\index{sensitivity}\\index{cutoff}\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\text{sensitivity (SN)} &= P(R \\mid C) \\\\\n",
        "  &= \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{\\text{TP}}{N \\times \\text{BR}} = 1 - \\text{FNR}\n",
        "\\end{aligned}\n",
        "(\\#eq:sensitivity)\n",
        "$$\n"
      ],
      "id": "7df8fb5a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sensitivity <- function(TP, TN, FP, FN){\n",
        "  value <- TP/(TP + FN)\n",
        "  \n",
        "  return(value)\n",
        "}"
      ],
      "id": "2964571a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sensitivity(\n",
        "  TP = TPvalue,\n",
        "  FN = FNvalue)"
      ],
      "id": "8e00b6a1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sensitivityValue <- sensitivity(TP = TPvalue, FN = FNvalue)"
      ],
      "id": "a27c585e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below I compute sensitivity and [specificity](#specificity) at every possible cutoff.\\index{sensitivity}\\index{specificity}\\index{cutoff}\n"
      ],
      "id": "b8a47ab6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "possibleCutoffs <- unique(na.omit(mydataSDT$testScore))\n",
        "possibleCutoffs <- possibleCutoffs[order(possibleCutoffs)]\n",
        "possibleCutoffs <- c(\n",
        "  possibleCutoffs,\n",
        "  max(possibleCutoffs, na.rm = TRUE) + 0.01)\n",
        "\n",
        "specificity <- function(TP, TN, FP, FN){\n",
        "  value <- TN/(TN + FP)\n",
        "  \n",
        "  return(value)\n",
        "}\n",
        "\n",
        "accuracyVariables <- c(\"cutoff\", \"TP\", \"TN\", \"FP\", \"FN\")\n",
        "\n",
        "accuracyStats <- data.frame(matrix(\n",
        "  nrow = length(possibleCutoffs),\n",
        "  ncol = length(accuracyVariables)))\n",
        "\n",
        "names(accuracyStats) <- accuracyVariables\n",
        "\n",
        "for(i in 1:length(possibleCutoffs)){\n",
        "  newCutoff <- possibleCutoffs[i]\n",
        "  \n",
        "  mydataSDT$diagnosis <- NA\n",
        "  mydataSDT$diagnosis[mydataSDT$testScore < newCutoff] <- 0\n",
        "  mydataSDT$diagnosis[mydataSDT$testScore >= newCutoff] <- 1\n",
        "  \n",
        "  accuracyStats[i, \"cutoff\"] <- newCutoff\n",
        "  accuracyStats[i, \"TP\"] <- length(which(\n",
        "    mydataSDT$diagnosis == 1 & mydataSDT$disorder == 1))\n",
        "  accuracyStats[i, \"TN\"] <- length(which(\n",
        "    mydataSDT$diagnosis == 0 & mydataSDT$disorder == 0))\n",
        "  accuracyStats[i, \"FP\"] <- length(which(\n",
        "    mydataSDT$diagnosis == 1 & mydataSDT$disorder == 0))\n",
        "  accuracyStats[i, \"FN\"] <- length(which(\n",
        "    mydataSDT$diagnosis == 0 & mydataSDT$disorder == 1))\n",
        "}\n",
        "\n",
        "accuracyStats$sensitivity <- accuracyStats$TPrate <- sensitivity(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "\n",
        "accuracyStats$specificity <- accuracyStats$TNrate <- specificity(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "\n",
        "sensitivitySpecificityData <- pivot_longer(\n",
        "  accuracyStats,\n",
        "  cols = all_of(c(\"sensitivity\",\"specificity\")))"
      ],
      "id": "3d1400fa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ggplot(\n",
        "  sensitivitySpecificityData,\n",
        "  aes(\n",
        "    x = cutoff,\n",
        "    y = value,\n",
        "    color = name)) +\n",
        "  geom_line(linewidth = 2) +\n",
        "  scale_x_continuous(name = \"cutoff (liberal to conservative)\") +\n",
        "  scale_color_viridis_d(name = \"\") +\n",
        "  theme_bw()"
      ],
      "id": "8597c1dd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Specificity (SP) {#specificity}\n",
        "\n",
        "Specificity (SP) is also called true negative rate (TNR) or selectivity.\\index{specificity}\\index{true negative rate!zzzzz@\\igobble \\mid seealso{specificity}}\\index{selectivity!zzzzz@\\igobble \\mid seealso{specificity}}\n",
        "Specificity is the [conditional probability](#conditionalProbability) of a negative test given that the person does not have the condition: $P(\\text{not } R \\mid \\text{not } C)$.\\index{specificity}\\index{probability!conditional}\n",
        "Higher values reflect greater accuracy.\\index{specificity}\n",
        "The formula for calculating specificity is in Equation \\@ref(eq:specificity).\n",
        "As described in Section \\@ref(accuracyCutoff) and as depicted in Figure \\@ref(fig:sensitivitySpecificity), as the cutoff increases (becomes more conservative), specificity increases.\\index{specificity}\\index{cutoff}\n",
        "As the cutoff decreases, specificity decreases.\\index{specificity}\\index{cutoff}\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\text{specificity (SP)} &= P(\\text{not } R \\mid \\text{not } C) \\\\\n",
        "  &= \\frac{\\text{TN}}{\\text{TN} + \\text{FP}} = \\frac{\\text{TN}}{N (1 - \\text{BR})} = 1 - \\text{FPR}\n",
        "\\end{aligned}\n",
        "(\\#eq:specificity)\n",
        "$$\n"
      ],
      "id": "acf14a17"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "specificity <- function(TP, TN, FP, FN){\n",
        "  value <- TN/(TN + FP)\n",
        "  \n",
        "  return(value)\n",
        "}"
      ],
      "id": "e831011a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "specificity(TN = TNvalue, FP = FPvalue)"
      ],
      "id": "5cf2c1b7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "specificityValue <- specificity(\n",
        "  TN = TNvalue,\n",
        "  FP = FPvalue)"
      ],
      "id": "da8a24b8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### False Negative Rate (FNR) {#falseNegativeRate}\n",
        "\n",
        "The false negative rate (FNR) is also called the miss rate.\\index{false negative!rate}\\index{miss rate!zzzzz@\\igobble \\mid seealso{false negative rate}}\n",
        "The false negative rate is the [conditional probability](#conditionalProbability) of a negative test given that the person has the condition: $P(\\text{not } R \\mid C)$.\\index{false negative!rate}\\index{probability!conditional}\n",
        "Lower values reflect greater accuracy.\\index{false negative!rate}\n",
        "The formula for calculating false negative rate is in Equation \\@ref(eq:falseNegativeRate).\\index{false negative!rate}\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\text{false negative rate (FNR)} &= P(\\text{not } R \\mid C) \\\\\n",
        "  &= \\frac{\\text{FN}}{\\text{FN} + \\text{TP}} = \\frac{\\text{FN}}{N \\times \\text{BR}} = 1 - \\text{TPR}\n",
        "\\end{aligned}\n",
        "(\\#eq:falseNegativeRate)\n",
        "$$\n"
      ],
      "id": "4a342f35"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "falseNegativeRate <- function(TP, TN, FP, FN){\n",
        "  value <- FN/(FN + TP)\n",
        "  \n",
        "  return(value)\n",
        "}"
      ],
      "id": "374f113e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "falseNegativeRate(\n",
        "  TP = TPvalue,\n",
        "  FN = FNvalue)"
      ],
      "id": "abcacb86",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "falseNegativeRate(TP = TPvalue, FN = FNvalue)"
      ],
      "id": "9eb4205e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### False Positive Rate (FPR) {#falsePositiveRate}\n",
        "\n",
        "The false positive rate (FPR) is also called the false alarm rate (FAR) or fall-out.\\index{false positive!rate}\\index{fall-out!zzzzz@\\igobble \\mid seealso{false positive rate}}\n",
        "The false positive rate is the [conditional probability](#conditionalProbability) of a positive test given that the person does not have the condition: $P(R \\mid \\text{not } C)$.\\index{false positive!rate}\\index{probability!conditional}\n",
        "Lower values reflect greater accuracy.\\index{false positive!rate}\n",
        "The formula for calculating false positive rate is in Equation \\@ref(eq:falsePositiveRate).\\index{false positive!rate}\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\text{false positive rate (FPR)} &= P(R \\mid \\text{not } C) \\\\\n",
        "  &= \\frac{\\text{FP}}{\\text{FP} + \\text{TN}} = \\frac{\\text{FP}}{N (1 - \\text{BR})} = 1 - \\text{TNR}\n",
        "\\end{aligned}\n",
        "(\\#eq:falsePositiveRate)\n",
        "$$\n"
      ],
      "id": "718c43c2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "falsePositiveRate <- function(TP, TN, FP, FN){\n",
        "  value <- FP/(FP + TN)\n",
        "  \n",
        "  return(value)\n",
        "}"
      ],
      "id": "2d1e4d08",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "falsePositiveRate(\n",
        "  TN = TNvalue,\n",
        "  FP = FPvalue)"
      ],
      "id": "7bcc5d80",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "falsePositiveRateValue <- falsePositiveRate(TN = TNvalue, FP = FPvalue)"
      ],
      "id": "aff07f73",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Positive Predictive Value (PPV) {#ppv}\n",
        "\n",
        "The positive predictive value (PPV) is also called the positive predictive power (PPP) or precision.\\index{positive predictive value}\n",
        "Many people confuse [sensitivity](#sensitivity) ($P(R \\mid C)$) with its inverse [conditional probability](#conditionalProbability), PPV ($P(C \\mid R)$).\\index{positive predictive value}\\index{sensitivity}\\index{probability!conditional}\\index{confusion of the inverse}\\index{probability!inverse conditional}\n",
        "PPV is the [conditional probability](#conditionalProbability) of having the condition given a positive test: $P(C \\mid R)$.\\index{positive predictive value}\\index{probability!conditional}\n",
        "Higher values reflect greater accuracy.\\index{positive predictive value}\n",
        "The formula for calculating positive predictive value is in Equation \\@ref(eq:positivePredictiveValue).\\index{positive predictive value}\n",
        "\n",
        "PPV can be low even when [sensitivity](#sensitivity) is high because it depends not only on [sensitivity](#sensitivity), but also on [specificity](#specificity) and the [base rate](#baseRate).\\index{positive predictive value}\\index{sensitivity}\\index{specificity}\\index{base rate}\n",
        "Because PPV depends on the [base rate](#baseRate), PPV is not an intrinsic property of a measure.\\index{positive predictive value}\\index{base rate}\n",
        "The same measure will have a different PPV in different contexts with different [base rates](#baseRate) [@Treat2023].\\index{positive predictive value}\\index{base rate}\n",
        "As described in Section \\@ref(accuracyCutoff) and as depicted in Figure \\@ref(fig:ppvNPVbaseRate), as the [base rate](#baseRate) increases, PPV increases.\\index{positive predictive value}\\index{base rate}\n",
        "As the [base rate](#baseRate) decreases, PPV decreases.\\index{positive predictive value}\\index{base rate}\n",
        "PPV also differs as a function of the cutoff.\\index{positive predictive value}\\index{cutoff}\n",
        "As described in Section \\@ref(accuracyCutoff) and as depicted in Figure \\@ref(fig:ppvNPVcutoff), as the cutoff increases (becomes more conservative), PPV increases.\\index{positive predictive value}\\index{cutoff}\n",
        "As the cutoff decreases (becomes more liberal), PPV decreases.\\index{positive predictive value}\\index{cutoff}\n",
        "\n",
        "$$\n",
        "\\small\n",
        "\\begin{aligned}\n",
        "  \\text{positive predictive value (PPV)} &= P(C \\mid R) \\\\\n",
        "  &= \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{\\text{TP}}{N \\times \\text{SR}}\\\\\n",
        "  &= \\frac{\\text{sensitivity} \\times {\\text{BR}}}{\\text{sensitivity} \\times {\\text{BR}} + [(1 - \\text{specificity}) \\times (1 - \\text{BR})]}\n",
        "\\end{aligned}\n",
        "(\\#eq:positivePredictiveValue)\n",
        "$$\n"
      ],
      "id": "d9470155"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "positivePredictiveValue <- function(TP, TN, FP, FN, BR = NULL, SN, SP){\n",
        "  if(is.null(BR)){\n",
        "    value <- TP/(TP + FP)\n",
        "  } else{\n",
        "    value <- (SN * BR)/(SN * BR + (1 - SP) * (1 - BR))\n",
        "  }\n",
        "  \n",
        "  return(value)\n",
        "}"
      ],
      "id": "9e91aae4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "positivePredictiveValue(\n",
        "  TP = TPvalue,\n",
        "  FP = FPvalue)\n",
        "\n",
        "positivePredictiveValue(\n",
        "  BR = baseRate(\n",
        "    TP = TPvalue,\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue,\n",
        "    FN = FNvalue),\n",
        "  SN = sensitivity(\n",
        "    TP = TPvalue,\n",
        "    FN = FNvalue),\n",
        "  SP = specificity(\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue))"
      ],
      "id": "238fbe47",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "positivePredictivevalueValue <- positivePredictiveValue(TP = TPvalue, FP = FPvalue)"
      ],
      "id": "cb436c62",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below I compute PPV and [NPV](#npv) at every possible [base rate](#baseRate) given the [sensitivity](#sensitivity) and [specificity](#specificity) at the current cutoff.\\index{positive predictive value}\\index{negative predictive value}\\index{sensitivity}\\index{specificity}\\index{cutoff}\n"
      ],
      "id": "adddd1b4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "negativePredictiveValue <- function(TP, TN, FP, FN, BR = NULL, SN, SP){\n",
        "  if(is.null(BR)){\n",
        "    value <- TN/(TN + FN)\n",
        "  } else{\n",
        "    value <- (SP * (1 - BR))/(SP * (1 - BR) + (1 - SN) * BR)\n",
        "  }\n",
        "  \n",
        "  return(value)\n",
        "}\n",
        "\n",
        "ppvNPVbaseRateData <- data.frame(\n",
        "  BR = seq(from = 0, to = 1, by = .01),\n",
        "  SN = sensitivity(\n",
        "    TP = TPvalue,\n",
        "    FN = FNvalue),\n",
        "  SP = specificity(\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue))\n",
        "\n",
        "ppvNPVbaseRateData$positivePredictiveValue <- positivePredictiveValue(\n",
        "  BR = ppvNPVbaseRateData$BR,\n",
        "  SN = ppvNPVbaseRateData$SN,\n",
        "  SP = ppvNPVbaseRateData$SP)\n",
        "\n",
        "ppvNPVbaseRateData$negativePredictiveValue <- negativePredictiveValue(\n",
        "  BR = ppvNPVbaseRateData$BR,\n",
        "  SN = ppvNPVbaseRateData$SN,\n",
        "  SP = ppvNPVbaseRateData$SP)\n",
        "\n",
        "ppvNPVbaseRateData_long <- pivot_longer(\n",
        "  ppvNPVbaseRateData,\n",
        "  cols = all_of(c(\n",
        "    \"positivePredictiveValue\",\"negativePredictiveValue\")))"
      ],
      "id": "d0a70993",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ggplot(ppvNPVbaseRateData_long, aes(x = BR, y = value, color = name)) +\n",
        "  geom_line(linewidth = 2) +\n",
        "  scale_x_continuous(name = \"base rate\") +\n",
        "  scale_y_continuous(name = \"predictive value\") +\n",
        "  scale_color_viridis_d(name = \"\",\n",
        "                        breaks = c(\"negativePredictiveValue\",\"positivePredictiveValue\"),\n",
        "                        labels = c(\"Negative Predictive Value\",\"Positive Predictive Value\")) +\n",
        "  theme_bw()"
      ],
      "id": "951bd7a3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below I compute PPV and [NPV](#npv) at every possible cutoff.\\index{positive predictive value}\\index{negative predictive value}\\index{cutoff}\n"
      ],
      "id": "65fde13f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats$positivePredictiveValue <- positivePredictiveValue(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "\n",
        "accuracyStats$negativePredictiveValue <- negativePredictiveValue(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "\n",
        "ppvNPVcutoffData <- pivot_longer(\n",
        "  accuracyStats,\n",
        "  cols = all_of(c(\n",
        "    \"positivePredictiveValue\",\"negativePredictiveValue\")))"
      ],
      "id": "7893a073",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ggplot(ppvNPVcutoffData, aes(x = cutoff, y = value, color = name)) +\n",
        "  geom_line(linewidth = 2) +\n",
        "  scale_x_continuous(name = \"cutoff (liberal to conservative)\", limits = c(0.05,2.09)) +\n",
        "  scale_y_continuous(name = \"predictive value\") +\n",
        "  scale_color_viridis_d(name = \"\", breaks = c(\"negativePredictiveValue\",\"positivePredictiveValue\"), labels = c(\"Negative Predictive Value\",\"Positive Predictive Value\")) +\n",
        "  theme_bw()"
      ],
      "id": "78b26447",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Negative Predictive Value (NPV) {#npv}\n",
        "\n",
        "The negative predictive value (NPV) is also called the negative predictive power (NPP).\\index{negative predictive value}\n",
        "Many people confuse [specificity](#specificity) ($P(\\text{not } R \\mid \\text{not } C)$) with its inverse [conditional probability](#conditionalProbability), NPV ($P(\\text{not } C \\mid  \\text{not } R)$).\\index{negative predictive value}\\index{specificity}\\index{probability!conditional}\\index{confusion of the inverse}\\index{probability!inverse conditional}\n",
        "NPV is the [conditional probability](#conditionalProbability) of not having the condition given a negative test: $P(\\text{not } C \\mid  \\text{not } R)$.\\index{negative predictive value}\\index{probability!conditional}\n",
        "Higher values reflect greater accuracy.\\index{negative predictive value}\n",
        "The formula for calculating negative predictive value is in Equation \\@ref(eq:negativePredictiveValue).\\index{negative predictive value}\n",
        "\n",
        "NPV can be low even when [specificity](#specificity) is high because it depends not only on [specificity](#specificity), but also on [sensitivity](#sensitivity) and the [base rate](#baseRate).\\index{negative predictive value}\\index{specificity}\\index{sensitivity}\\index{base rate}\n",
        "Because NPV depends on the [base rate](#baseRate), NPV is not an intrinsic property of a measure.\\index{negative predictive value}\\index{base rate}\n",
        "The same measure will have a different NPV in different contexts with different [base rates](#baseRate) [@Treat2023].\\index{negative predictive value}\\index{base rate}\n",
        "As described in Section \\@ref(accuracyCutoff) and as depicted in Figure \\@ref(fig:ppvNPVbaseRate), as the [base rate](#baseRate) increases, NPV decreases.\\index{negative predictive value}\\index{base rate}\n",
        "As the [base rate](#baseRate) decreases, NPV increases.\\index{negative predictive value}\\index{base rate}\n",
        "NPV also differs as a function of the cutoff.\\index{negative predictive value}\\index{cutoff}\n",
        "As described in Section \\@ref(accuracyCutoff) and as depicted in Figure \\@ref(fig:ppvNPVcutoff), as the cutoff increases (becomes more conservative), NPV decreases.\\index{negative predictive value}\\index{cutoff}\n",
        "As the cutoff decreases (becomes more liberal), NPV decreases.\\index{negative predictive value}\\index{cutoff}\n",
        "\n",
        "$$\n",
        "\\small\n",
        "\\begin{aligned}\n",
        "  \\text{negative predictive value (NPV)} &= P(\\text{not } C \\mid \\text{not } R) \\\\\n",
        "  &= \\frac{\\text{TN}}{\\text{TN} + \\text{FN}} = \\frac{\\text{TN}}{N(\\text{1 - SR})}\\\\\n",
        "  &= \\frac{\\text{specificity} \\times (1-{\\text{BR}})}{\\text{specificity} \\times (1-{\\text{BR}}) + [(1 - \\text{sensitivity}) \\times \\text{BR})]}\n",
        "\\end{aligned}\n",
        "(\\#eq:negativePredictiveValue)\n",
        "$$\n"
      ],
      "id": "0123903e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "negativePredictiveValue <- function(TP, TN, FP, FN, BR = NULL, SN, SP){\n",
        "  if(is.null(BR)){\n",
        "    value <- TN/(TN + FN)\n",
        "  } else{\n",
        "    value <- (SP * (1 - BR))/(SP * (1 - BR) + (1 - SN) * BR)\n",
        "  }\n",
        "  \n",
        "  return(value)\n",
        "}"
      ],
      "id": "15d4ac96",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "negativePredictiveValue(\n",
        "  TN = TNvalue,\n",
        "  FN = FNvalue)\n",
        "\n",
        "negativePredictiveValue(\n",
        "  BR = baseRate(\n",
        "    TP = TPvalue,\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue,\n",
        "    FN = FNvalue),\n",
        "  SN = sensitivity(\n",
        "    TP = TPvalue,\n",
        "    FN = FNvalue),\n",
        "  SP = specificity(\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue))"
      ],
      "id": "7361f146",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "negativePredictiveValueValue <- negativePredictiveValue(TN = TNvalue,\n",
        "                                                        FN = FNvalue)"
      ],
      "id": "704d0103",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### False Discovery Rate (FDR) {#falseDiscoveryRate}\n",
        "\n",
        "Many people confuse the false positive rate ($P(R \\mid \\text{not } C)$) with its inverse [conditional probability](#conditionalProbability), the false discovery rate ($P(\\text{not } C \\mid  R)$).\\index{false discovery rate}\\index{false positive!rate}\\index{probability!conditional}\\index{confusion of the inverse}\\index{probability!inverse conditional}\n",
        "The false discovery rate (FDR) is the [conditional probability](#conditionalProbability) of not having the condition given a positive test: $P(\\text{not } C \\mid  R)$.\\index{false discovery rate}\\index{probability!conditional}\n",
        "Lower values reflect greater accuracy.\\index{false discovery rate}\n",
        "The formula for calculating false discovery rate is in Equation \\@ref(eq:falseDiscoveryRate).\\index{false discovery rate}\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\text{false discovery rate (FDR)} &= P(\\text{not } C \\mid R) \\\\\n",
        "  &= \\frac{\\text{FP}}{\\text{FP} + \\text{TP}} = 1 - \\text{PPV}\n",
        "\\end{aligned}\n",
        "(\\#eq:falseDiscoveryRate)\n",
        "$$\n"
      ],
      "id": "63158fde"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "falseDiscoveryRate <- function(TP, TN, FP, FN){\n",
        "  value <- FP/(FP + TP)\n",
        "  \n",
        "  return(value)\n",
        "}"
      ],
      "id": "d0809ca4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "falseDiscoveryRate(\n",
        "  TP = TPvalue,\n",
        "  FP = FPvalue)"
      ],
      "id": "cda2d2e0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "falseDiscoveryRateValue <- falseDiscoveryRate(TP = TPvalue, FP = FPvalue)"
      ],
      "id": "16d72046",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### False Omission Rate (FOR) {#falseOmissionRate}\n",
        "\n",
        "Many people confuse the false negative rate ($P(\\text{not } R \\mid C)$) with its inverse [conditional probability](#conditionalProbability), the false omission rate ($P(C \\mid \\text{not } R)$).\\index{false omission rate}\\index{false negative!rate}\\index{probability!conditional}\\index{confusion of the inverse}\\index{probability!inverse conditional}\n",
        "The false omission rate (FOR) is the conditional probability of having the condition given a negative test: $P(C \\mid \\text{not } R)$.\\index{false omission rate}\\index{probability!conditional}\n",
        "Lower values reflect greater accuracy.\\index{false omission rate}\n",
        "The formula for calculating false omission rate is in Equation \\@ref(eq:falseOmissionRate).\\index{false omission rate}\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\text{false omission rate (FOR)} &= P(C \\mid \\text{not } R) \\\\\n",
        "  &= \\frac{\\text{FN}}{\\text{FN} + \\text{TN}} = 1 - \\text{NPV}\n",
        "\\end{aligned}\n",
        "(\\#eq:falseOmissionRate)\n",
        "$$\n"
      ],
      "id": "ec466ae2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "falseOmissionRate <- function(TP, TN, FP, FN){\n",
        "  value <- FN/(FN + TN)\n",
        "  \n",
        "  return(value)\n",
        "}"
      ],
      "id": "63d242bf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "falseOmissionRate(\n",
        "  TN = TNvalue,\n",
        "  FN = FNvalue)"
      ],
      "id": "53a64f51",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "falseOmissionRateValue <- falseOmissionRate(TN = TNvalue, FN = FNvalue)"
      ],
      "id": "3645c43c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Youden's J Statistic {#youdenJ-example}\n",
        "\n",
        "Youden's J statistic is also called Youden's Index or informedness.\n",
        "Youden's J statistic is the sum of [sensitivity](#sensitivity) and [specificity](#specificity) (and subtracting one).\\index{Youden's J statistic}\\index{sensitivity}\\index{specificity}\n",
        "Higher values reflect greater accuracy.\\index{Youden's J statistic}\n",
        "The formula for calculating Youden's J statistic is in Equation \\@ref(eq:youdenIndex).\\index{Youden's J statistic}\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\text{Youden's J statistic} &= \\text{sensitivity} + \\text{specificity} - 1\n",
        "\\end{aligned}\n",
        "(\\#eq:youdenIndex)\n",
        "$$\n"
      ],
      "id": "cdb221cf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "youdenJ <- function(TP, TN, FP, FN){\n",
        "  SN <- TP/(TP + FN)\n",
        "  SP <- TN/(TN + FP)\n",
        "  value <- SN + SP - 1\n",
        "  \n",
        "  return(value)\n",
        "}"
      ],
      "id": "2d8143e0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "youdenJ(\n",
        "  TP = TPvalue,\n",
        "  TN = TNvalue,\n",
        "  FP = FPvalue,\n",
        "  FN = FNvalue)"
      ],
      "id": "826014fa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "youdenJValue <- youdenJ(TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue)"
      ],
      "id": "50751faf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Balanced Accuracy {#balancedAccuracy}\n",
        "\n",
        "Balanced accuracy is the average of [sensitivity](#sensitivity) and [specificity](#specificity).\\index{balanced accuracy}\n",
        "Higher values reflect greater accuracy.\\index{balanced accuracy}\n",
        "The formula for calculating balanced accuracy is in Equation \\@ref(eq:balancedAccuracy).\\index{balanced accuracy}\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\text{balanced accuracy} &= \\frac{\\text{sensitivity} + \\text{specificity}}{2}\n",
        "\\end{aligned}\n",
        "(\\#eq:balancedAccuracy)\n",
        "$$\n"
      ],
      "id": "9bbb05d9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "balancedAccuracy <- function(TP, TN, FP, FN){\n",
        "  SN <- TP/(TP + FN)\n",
        "  SP <- TN/(TN + FP)\n",
        "  value <- (SN + SP) / 2\n",
        "  \n",
        "  return(value)\n",
        "}"
      ],
      "id": "2d48b721",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "balancedAccuracy(\n",
        "  TP = TPvalue,\n",
        "  TN = TNvalue,\n",
        "  FP = FPvalue,\n",
        "  FN = FNvalue)"
      ],
      "id": "ba854ad0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "balancedAccuracyValue <- balancedAccuracy(TP = TPvalue,\n",
        "                                          TN = TNvalue,\n",
        "                                          FP = FPvalue,\n",
        "                                          FN = FNvalue)"
      ],
      "id": "f06b7670",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### F-Score {#fScore}\n",
        "\n",
        "The F-score combines [precision](#ppv) ([positive predictive value](#ppv)) and [recall](#sensitivity) ([sensitivity](#sensitivity)), where $\\beta$ indicates how many times more important [sensitivity](#sensitivity) is than the [positive predictive value](#ppv).\\index{F-score}\\index{positive predictive value}\\index{sensitivity}\n",
        "If [sensitivity](#sensitivity) and the [positive predictive value](#ppv) are equally important, $\\beta = 1$, and the F-score is called the $F_1$ score.\\index{F-score}\\index{positive predictive value}\\index{sensitivity}\n",
        "Higher values reflect greater accuracy.\\index{F-score}\n",
        "The formula for calculating the F-score is in Equation \\@ref(eq:FScore).\\index{F-score}\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  F_\\beta &= (1 + \\beta^2) \\cdot \\frac{\\text{positive predictive value} \\cdot \\text{sensitivity}}{(\\beta^2 \\cdot \\text{positive predictive value}) + \\text{sensitivity}} \\\\\n",
        "  &= \\frac{(1 + \\beta^2) \\cdot \\text{TP}}{(1 + \\beta^2) \\cdot \\text{TP} + \\beta^2 \\cdot \\text{FN} + \\text{FP}}\n",
        "\\end{aligned}\n",
        "(\\#eq:FScore)\n",
        "$$\n",
        "\n",
        "The $F_1$ score is the harmonic mean of [sensitivity](#sensitivity) and [positive predictive value](#ppv).\\index{F-score}\n",
        "The formula for calculating the $F_1$ score is in Equation \\@ref(eq:F1Score).\\index{F-score}\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  F_1 &= \\frac{2 \\cdot \\text{positive predictive value} \\cdot \\text{sensitivity}}{(\\text{positive predictive value}) + \\text{sensitivity}} \\\\\n",
        "  &= \\frac{2 \\cdot \\text{TP}}{2 \\cdot \\text{TP} + \\text{FN} + \\text{FP}}\n",
        "\\end{aligned}\n",
        "(\\#eq:F1Score)\n",
        "$$\n"
      ],
      "id": "d6919a0d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fScore <- function(TP, TN, FP, FN, beta = 1){\n",
        "  value <- ((1 + beta^2) * TP) / ((1 + beta^2) * TP + beta^2 * FN + FP)\n",
        "  \n",
        "  return(value)\n",
        "}"
      ],
      "id": "f3e8a155",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fScore(\n",
        "  TP = TPvalue,\n",
        "  FP = FPvalue,\n",
        "  FN = FNvalue)\n",
        "\n",
        "fScore(\n",
        "  TP = TPvalue,\n",
        "  FP = FPvalue,\n",
        "  FN = FNvalue,\n",
        "  beta = 2)\n",
        "\n",
        "fScore(\n",
        "  TP = TPvalue,\n",
        "  FP = FPvalue,\n",
        "  FN = FNvalue,\n",
        "  beta = 0.5)"
      ],
      "id": "6fa2992f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "f1ScoreValue <- fScore(TP = TPvalue, FP = FPvalue, FN = FNvalue)"
      ],
      "id": "42ba8353",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Matthews Correlation Coefficient (MCC) {#matthewsCorrelationCoefficient}\n",
        "\n",
        "The Matthews correlation coefficient (MCC) is also called the phi coefficient.\\index{Matthews correlation coefficient}\n",
        "It is a correlation coefficient between predicted and observed values from a binary classification.\\index{Matthews correlation coefficient}\n",
        "Higher values reflect greater accuracy.\\index{Matthews correlation coefficient}\n",
        "The formula for calculating the MCC is in Equation \\@ref(eq:matthewsCorrelationCoefficient).\\index{Matthews correlation coefficient}\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\text{MCC} &= \\frac{\\text{TP} \\times \\text{TN} - \\text{FP} \\times \\text{FN}}{\\sqrt{(\\text{TP} + \\text{FP})(\\text{TP} + \\text{FN})(\\text{TN} + \\text{FP})(\\text{TN} + \\text{FN})}}\n",
        "\\end{aligned}\n",
        "(\\#eq:matthewsCorrelationCoefficient)\n",
        "$$\n"
      ],
      "id": "850e841f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mcc <- function(TP, TN, FP, FN){\n",
        "  TP <- as.double(TP)\n",
        "  TN <- as.double(TN)\n",
        "  FP <- as.double(FP)\n",
        "  FN <- as.double(FN)\n",
        "  value <- ((TP * TN) - (FP * FN)) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
        "  \n",
        "  return(value)\n",
        "}"
      ],
      "id": "f7fb106a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mcc(\n",
        "  TP = TPvalue,\n",
        "  TN = TNvalue,\n",
        "  FP = FPvalue,\n",
        "  FN = FNvalue)"
      ],
      "id": "2df06184",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mccValue <- mcc(TP = TPvalue,\n",
        "                TN = TNvalue,\n",
        "                FP = FPvalue,\n",
        "                FN = FNvalue)"
      ],
      "id": "7af9ed44",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Diagnostic Odds Ratio {#diagnosticOddsRatio}\n",
        "\n",
        "The diagnostic odds ratio is the odds of a positive test among people with the condition relative to the odds of a positive test among people without the condition.\\index{diagnostic odds ratio}\n",
        "Higher values reflect greater accuracy.\\index{diagnostic odds ratio}\n",
        "The formula for calculating the diagnostic odds ratio is in Equation \\@ref(eq:diagnosticOddsRatio).\\index{diagnostic odds ratio}\n",
        "If the predictor is bad, the diagnostic odds ratio could be less than one, and values can go up from there.\\index{diagnostic odds ratio}\n",
        "If the diagnostic odds ratio is greater than 2, we take the odds ratio seriously because we are twice as likely to predict accurately than inaccurately.\\index{diagnostic odds ratio}\n",
        "However, the diagnostic odds ratio ignores/hides [base rates](#baseRate).\\index{diagnostic odds ratio}\\index{base rate}\n",
        "When interpreting the diagnostic odds ratio, it is important to keep in mind the clinical significance, because otherwise it is not very meaningful.\\index{diagnostic odds ratio}\n",
        "Consider a risk factor that has a diagnostic odds ratio of 3 for tuberculosis, i.e., it puts you at 3 times as likely to develop tuberculosis.\\index{diagnostic odds ratio}\n",
        "The prevalence of tuberculosis is relatively low.\\index{diagnostic odds ratio}\n",
        "Assuming the prevalence of tuberculosis is less than 1/10th of 1%, your risk of developing tuberculosis is still very low even if the risk factor (with a diagnostic odds ratio of 3) is present.\\index{diagnostic odds ratio}\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\text{diagnostic odds ratio} &= \\frac{\\text{TP} \\times \\text{TN}}{\\text{FP} \\times \\text{FN}} \\\\\n",
        "  &= \\frac{\\text{sensitivity} \\times \\text{specificity}}{(1 - \\text{sensitivity}) \\times (1 - \\text{specificity})} \\\\\n",
        "  &= \\frac{\\text{PPV} \\times \\text{NPV}}{(1 - \\text{PPV}) \\times (1 - \\text{NPV})} \\\\\n",
        "  &= \\frac{\\text{LR+}}{\\text{LR}-}\n",
        "\\end{aligned}\n",
        "(\\#eq:diagnosticOddsRatio)\n",
        "$$\n"
      ],
      "id": "f8014177"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "diagnosticOddsRatio <- function(TP, TN, FP, FN){\n",
        "  value <- (TP * TN) / (FP * FN)\n",
        "  \n",
        "  return(value)\n",
        "}"
      ],
      "id": "03aecb58",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "diagnosticOddsRatio(\n",
        "  TP = TPvalue,\n",
        "  TN = TNvalue,\n",
        "  FP = FPvalue,\n",
        "  FN = FNvalue)"
      ],
      "id": "5a3e3d77",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "diagnosticOddsRatioValue <- diagnosticOddsRatio(TP = TPvalue,\n",
        "                                                TN = TNvalue,\n",
        "                                                FP = FPvalue,\n",
        "                                                FN = FNvalue)"
      ],
      "id": "61a89b0f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Diagnostic Likelihood Ratio {#diagnosticLikelihoodRatio}\n",
        "\n",
        "A likelihood ratio is the ratio of two probabilities.\\index{diagnostic likelihood ratio}\n",
        "It can be used to compare the likelihood of two possibilities.\\index{diagnostic likelihood ratio}\n",
        "The diagnostic likelihood ratio is an index of the predictive validity of an instrument: it is the ratio of the probability that a test result is correct to the probability that the test result is incorrect.\\index{diagnostic likelihood ratio}\n",
        "The diagnostic likelihood ratio is also called the risk ratio.\\index{diagnostic likelihood ratio}\n",
        "There are two types of diagnostic likelihood ratios: the [positive likelihood ratio](#positiveLikelihoodRatio) and the [negative likelihood ratio](#negativeLikelihoodRatio).\\index{diagnostic likelihood ratio}\\index{positive likelihood ratio}\\index{negative likelihood ratio}\n",
        "\n",
        "#### Positive Likelihood Ratio (LR+) {#positiveLikelihoodRatio}\n",
        "\n",
        "The positive likelihood ratio (LR+) compares the [true positive rate](#sensitivity) to the [false positive rate](#falsePositiveRate).\\index{positive likelihood ratio}\\index{sensitivity}\\index{false positive!rate}\n",
        "Positive likelihood ratio values range from 1 to infinity.\\index{positive likelihood ratio}\n",
        "Higher values reflect greater accuracy, because it indicates the degree to which a [true positive](#truePositive) is more likely than a [false positive](#falsePositive).\\index{positive likelihood ratio}\\index{true positive}\\index{false positive}\n",
        "The formula for calculating the positive likelihood ratio is in Equation \\@ref(eq:positiveLikelihoodRatio).\\index{positive likelihood ratio}\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\text{positive likelihood ratio (LR+)} &= \\frac{\\text{TPR}}{\\text{FPR}} \\\\\n",
        "  &= \\frac{P(R \\mid C)}{P(R \\mid \\text{not } C)} \\\\\n",
        "  &= \\frac{P(R \\mid C)}{1 - P(\\text{not } R \\mid \\text{not } C)} \\\\\n",
        "  &= \\frac{\\text{sensitivity}}{1 - \\text{specificity}}\n",
        "\\end{aligned}\n",
        "(\\#eq:positiveLikelihoodRatio)\n",
        "$$\n"
      ],
      "id": "93ea05dd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "positiveLikelihoodRatio <- function(TP, TN, FP, FN){\n",
        "  SN <- TP/(TP + FN)\n",
        "  SP <- TN/(TN + FP)\n",
        "  \n",
        "  value <- SN/(1 - SP)\n",
        "  \n",
        "  return(value)\n",
        "}"
      ],
      "id": "fac831a8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "positiveLikelihoodRatio(\n",
        "  TP = TPvalue,\n",
        "  TN = TNvalue,\n",
        "  FP = FPvalue,\n",
        "  FN = FNvalue)"
      ],
      "id": "869e1726",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "positiveLikelihoodRatioValue <- positiveLikelihoodRatio(TP = TPvalue,\n",
        "                                                        TN = TNvalue,\n",
        "                                                        FP = FPvalue,\n",
        "                                                        FN = FNvalue)"
      ],
      "id": "c9f4b245",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Negative Likelihood Ratio (LR−) {#negativeLikelihoodRatio}\n",
        "\n",
        "The negative likelihood ratio (LR−) compares the [false negative rate](#falseNegativeRate) to the [true negative rate](#specificity).\\index{negative likelihood ratio}\\index{false negative!rate}\\index{specificity}\n",
        "Negative likelihood ratio values range from 0 to 1.\\index{negative likelihood ratio}\n",
        "Smaller values reflect greater accuracy, because it indicates that a [false negative](#falseNegative) is less likely than a [true negative](#trueNegative).\\index{negative likelihood ratio}\\index{false negative}\\index{true negative}\n",
        "The formula for calculating the negative likelihood ratio is in Equation \\@ref(eq:negativeLikelihoodRatio).\\index{negative likelihood ratio}\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\text{negative likelihood ratio } (\\text{LR}-) &= \\frac{\\text{FNR}}{\\text{TNR}} \\\\\n",
        "  &= \\frac{P(\\text{not } R \\mid C)}{P(\\text{not } R \\mid \\text{not } C)} \\\\\n",
        "  &= \\frac{1 - P(R \\mid C)}{P(\\text{not } R \\mid \\text{not } C)} \\\\\n",
        "  &= \\frac{1 - \\text{sensitivity}}{\\text{specificity}}\n",
        "\\end{aligned}\n",
        "(\\#eq:negativeLikelihoodRatio)\n",
        "$$\n"
      ],
      "id": "74a66054"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "negativeLikelihoodRatio <- function(TP, TN, FP, FN){\n",
        "  SN <- TP/(TP + FN)\n",
        "  SP <- TN/(TN + FP)\n",
        "  \n",
        "  value <- (1 - SN)/SP\n",
        "  \n",
        "  return(value)\n",
        "}"
      ],
      "id": "38fb964e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "negativeLikelihoodRatio(\n",
        "  TP = TPvalue,\n",
        "  TN = TNvalue,\n",
        "  FP = FPvalue,\n",
        "  FN = FNvalue)"
      ],
      "id": "4d31a1ed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "negativeLikelihoodRatioValue <- negativeLikelihoodRatio(TP = TPvalue,\n",
        "                                                        TN = TNvalue,\n",
        "                                                        FP = FPvalue,\n",
        "                                                        FN = FNvalue)"
      ],
      "id": "2412cb38",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Posttest Odds {#posttestOdds}\n",
        "\n",
        "As presented in Equation \\@ref(eq:bayes5), the posttest (or posterior) odds are equal to the [pretest odds](#pretestOdds) multiplied by the [likelihood ratio](#diagnosticLikelihoodRatio).\\index{odds!posttest}\\index{diagnostic likelihood ratio}\n",
        "The posttest odds and [posttest probability](#posttestProbability) can be useful to calculate when the [pretest probability](#baseRate) is different from the [pretest probability](#baseRate) (or prevalence) of the classification.\\index{odds!posttest}\\index{probability!posttest}\\index{probability!pretest}\\index{base rate}\n",
        "For instance, you might use a different [pretest probability](#baseRate) if a test result is already known and you want to know the updated [posttest probability](#posttestProbability) after conducting a second test.\\index{odds!posttest}\\index{probability!posttest}\\index{probability!pretest}\n",
        "The formula for calculating posttest odds is in Equation \\@ref(eq:posttestOdds).\\index{odds!posttest}\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\text{posttest odds} &= \\text{pretest odds} \\times \\text{likelihood ratio} \\\\\n",
        "\\end{aligned}\n",
        "(\\#eq:posttestOdds)\n",
        "$$\n",
        "\n",
        "For calculating the posttest odds of a [true positive](#truePositive) compared to a [false positive](#falsePositive), we use the [positive likelihood ratio](#positiveLikelihoodRatio), described later.\\index{odds!posttest}\\index{positive likelihood ratio}\\index{true positive}\\index{false positive}\n",
        "We would use the [negative likelihood ratio](#negativeLikelihoodRatio) if we wanted to calculate the posttest odds of a [false negative](#falseNegative) compared to a [true negative](#trueNegative).\\index{odds!posttest}\\index{positive likelihood ratio}\\index{false negative}\\index{true negative}\n"
      ],
      "id": "9bb74fbb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "posttestOdds <- function(TP, TN, FP, FN, pretestProb = NULL, SN = NULL, SP = NULL, likelihoodRatio = NULL){\n",
        "  if(!is.null(pretestProb) & !is.null(SN) & !is.null(SP)){\n",
        "    pretestProbability <- pretestProb\n",
        "    pretestOdds <- pretestProbability / (1 - pretestProbability)\n",
        "    \n",
        "    likelihoodRatio <- SN/(1 - SP)\n",
        "  } else if(!is.null(pretestProb) & !is.null(likelihoodRatio)){\n",
        "    pretestProbability <- pretestProb\n",
        "    pretestOdds <- pretestProbability / (1 - pretestProbability)\n",
        "    \n",
        "    likelihoodRatio <- likelihoodRatio\n",
        "  } else {\n",
        "    N <- TP + TN + FP + FN\n",
        "    pretestProbability <- (TP + FN)/N\n",
        "    pretestOdds <- pretestProbability / (1 - pretestProbability)\n",
        "    \n",
        "    SN <- TP/(TP + FN)\n",
        "    SP <- TN/(TN + FP)\n",
        "    likelihoodRatio <- SN/(1 - SP)\n",
        "  }\n",
        "  \n",
        "  value <- pretestOdds * likelihoodRatio\n",
        "  \n",
        "  return(value)\n",
        "}"
      ],
      "id": "592d91f8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "posttestOdds(\n",
        "  TP = TPvalue,\n",
        "  TN = TNvalue,\n",
        "  FP = FPvalue,\n",
        "  FN = FNvalue)\n",
        "\n",
        "posttestOdds(\n",
        "  pretestProb = baseRate(\n",
        "    TP = TPvalue,\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue,\n",
        "    FN = FNvalue),\n",
        "  SN = sensitivity(\n",
        "    TP = TPvalue,\n",
        "    FN = FNvalue),\n",
        "  SP = specificity(\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue))\n",
        "\n",
        "posttestOdds(\n",
        "  pretestProb = baseRate(\n",
        "    TP = TPvalue,\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue,\n",
        "    FN = FNvalue),\n",
        "  likelihoodRatio = positiveLikelihoodRatio(\n",
        "    TP = TPvalue,\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue,\n",
        "    FN = FNvalue))"
      ],
      "id": "d0b05e0e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "posttestOddsValue <- posttestOdds(TP = TPvalue,\n",
        "                                  TN = TNvalue,\n",
        "                                  FP = FPvalue,\n",
        "                                  FN = FNvalue)"
      ],
      "id": "d513b66c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Posttest Probability {#posttestProbability}\n",
        "\n",
        "The posttest probability is the probability of having the disorder given a test result.\\index{probability!posttest}\n",
        "When the [base rate](#baseRate) is used as the [pretest probability](#baseRate), the posttest probability given a positive test is equal to [positive predictive value](#ppv).\\index{probability!posttest}\\index{base rate}\\index{probability!pretest}\\index{positive predictive value}\n",
        "To convert odds to a probability, divide the odds by one plus the odds, as is in Equation \\@ref(eq:posttestProbability).\\index{probability!posttest}\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\text{posttest probability} &= \\frac{\\text{posttest odds}}{1 + \\text{posttest odds}}\n",
        "\\end{aligned}\n",
        "(\\#eq:posttestProbability)\n",
        "$$\n"
      ],
      "id": "7968f257"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "posttestProbability <- function(TP, TN, FP, FN, pretestProb = NULL, SN = NULL, SP = NULL, likelihoodRatio = NULL){\n",
        "  if(!is.null(pretestProb) & !is.null(SN) & !is.null(SP)){\n",
        "    pretestProbability <- pretestProb\n",
        "    pretestOdds <- pretestProbability / (1 - pretestProbability)\n",
        "    \n",
        "    likelihoodRatio <- SN/(1 - SP)\n",
        "  } else if(!is.null(pretestProb) & !is.null(likelihoodRatio)){\n",
        "    pretestProbability <- pretestProb\n",
        "    pretestOdds <- pretestProbability / (1 - pretestProbability)\n",
        "    \n",
        "    likelihoodRatio <- likelihoodRatio\n",
        "  } else {\n",
        "    N <- TP + TN + FP + FN\n",
        "    pretestProbability <- (TP + FN)/N\n",
        "    pretestOdds <- pretestProbability / (1 - pretestProbability)\n",
        "    \n",
        "    SN <- TP/(TP + FN)\n",
        "    SP <- TN/(TN + FP)\n",
        "    likelihoodRatio <- SN/(1 - SP)\n",
        "  }\n",
        "  \n",
        "  posttestOdds <- pretestOdds * likelihoodRatio\n",
        "  value <- posttestOdds / (1 + posttestOdds)\n",
        "  \n",
        "  return(value)\n",
        "}"
      ],
      "id": "bc698b73",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "posttestProbability(\n",
        "  TP = TPvalue,\n",
        "  TN = TNvalue,\n",
        "  FP = FPvalue,\n",
        "  FN = FNvalue)\n",
        "\n",
        "posttestProbability(\n",
        "  pretestProb = baseRate(\n",
        "    TP = TPvalue,\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue,\n",
        "    FN = FNvalue),\n",
        "  SN = sensitivity(\n",
        "    TP = TPvalue,\n",
        "    FN = FNvalue),\n",
        "  SP = specificity(\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue))\n",
        "\n",
        "posttestProbability(\n",
        "  pretestProb = baseRate(\n",
        "    TP = TPvalue,\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue,\n",
        "    FN = FNvalue),\n",
        "  likelihoodRatio = positiveLikelihoodRatio(\n",
        "    TP = TPvalue,\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue,\n",
        "    FN = FNvalue))"
      ],
      "id": "77a482b4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "posttestProbabilityValue <- posttestProbability(TP = TPvalue,\n",
        "                                                TN = TNvalue,\n",
        "                                                FP = FPvalue,\n",
        "                                                FN = FNvalue)"
      ],
      "id": "d495e0a4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consider the following example: Assume the [base rate](#baseRate) of the condition is .03%.\\index{probability!posttest}\\index{base rate}\n",
        "We have two tests.\\index{probability!posttest}\n",
        "Test A has a [sensitivity](#sensitivity) of .95 and a [specificity](#specificity) of .80.\\index{probability!posttest}\\index{sensitivity}\\index{specificity}\n",
        "Test B has a [sensitivity](#sensitivity) of .70 and a [specificity](#specificity) of .90.\\index{probability!posttest}\\index{sensitivity}\\index{specificity}\n",
        "What is the probability of having the condition if a person has a positive test on Test A?\\index{probability!posttest}\n",
        "Assuming the errors of the two tests are independent, what is the probability of having the condition if the person has a positive test on Test B after having a positive test on Test A?\\index{probability!posttest}\n"
      ],
      "id": "9da7ad26"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "probGivenTestA <- posttestProbability(\n",
        "  pretestProb = .003,\n",
        "  SN = .95,\n",
        "  SP = .80)\n",
        "\n",
        "probGivenTestAthenB <- posttestProbability(\n",
        "  pretestProb = probGivenTestA,\n",
        "  SN = .70,\n",
        "  SP = .90)\n",
        "\n",
        "probGivenTestA\n",
        "probGivenTestAthenB"
      ],
      "id": "b8c5c549",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The probability of having the condition if a person has a positive test on Test A is $`r apa(probGivenTestA * 100, decimals = 1)`$%.\\index{probability!posttest}\n",
        "The probability of having the condition if the person has a positive test on Test B after having a positive test on Test A is $`r apa(probGivenTestAthenB * 100, decimals = 1)`$%.\\index{probability!posttest}\n",
        "\n",
        "### Probability Nomogram {#nomogram}\n",
        "\n",
        "The [`petersenlab`](https://github.com/DevPsyLab/petersenlab) package [@R-petersenlab] contains the `nomogrammer()` function that creates a probability nomogram plot, adapted from\\index{petersenlab package}\\index{probability!nomogram} https://github.com/achekroud/nomogrammer.\n",
        "In Figure \\@ref(fig:nomogramPlot), the probability nomogram is generated using the number of [true positives](#truePositive), [true negatives](#trueNegative), [false positives](#falsePositive), and [false negatives](#falseNegative) at a given cutoff.\\index{petersenlab package}\\index{probability!nomogram}\\index{true positive}\\index{true negative}\\index{false positive}\\index{false negative}\\index{cutoff}\n"
      ],
      "id": "d5e7c672"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "nomogrammer(\n",
        "  TP = TPvalue,\n",
        "  TN = TNvalue,\n",
        "  FP = FPvalue,\n",
        "  FN = FNvalue\n",
        ")"
      ],
      "id": "a86c5772",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The blue line indicates the [posterior probability](#posttestProbability) of the condition given a positive test.\\index{petersenlab package}\\index{probability!nomogram}\\index{probability!posttest}\n",
        "The pink line indicates the [posterior probability](#posttestProbability) of the condition given a negative test.\\index{petersenlab package}\\index{probability!nomogram}\\index{probability!posttest}\n",
        "One can also generate the probability nomogram from the [base rate](#pretestProbability) and the [sensitivity](#sensitivity) and [specificity](#specificity) of the test at a given cutoff:\\index{petersenlab package}\\index{probability!nomogram}\\index{base rate}\\index{sensitivity}\\index{specificity}\\index{cutoff}\n"
      ],
      "id": "59d25c94"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "nomogrammer(\n",
        "  pretestProb = baseRate(TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue),\n",
        "  SN = sensitivity(TP = TPvalue, FN = FNvalue),\n",
        "  SP = specificity(TN = TNvalue, FP = FPvalue)\n",
        ")"
      ],
      "id": "9fbf50e9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One can also generate the probability nomogram from the [base rate](#pretestProbability), [positive likelihood ratio](#positiveLikelihoodRatio), and [negative likelihood ratio](#negativeLikelihoodRatio) at a given cutoff:\\index{petersenlab package}\\index{probability!nomogram}\\index{positive likelihood ratio}\\index{negative likelihood ratio}\\index{cutoff}\n"
      ],
      "id": "f83665cd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "nomogrammer(\n",
        "  pretestProb = baseRate(TP = TPvalue, TN = TNvalue, FP = FPvalue, FN = FNvalue),\n",
        "  PLR = positiveLikelihoodRatio(\n",
        "    TP = TPvalue,\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue,\n",
        "    FN = FNvalue),\n",
        "  NLR = negativeLikelihoodRatio(\n",
        "    TP = TPvalue,\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue,\n",
        "    FN = FNvalue)\n",
        ")"
      ],
      "id": "200bd313",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### $d'$ Sensitivity from Signal Detection Theory {#dPrimeSDT}\n",
        "\n",
        "$d'$ ($d$ prime) is an index of sensitivity from [signal detection theory](#sdt), as described by @Stanislaw1999.\\index{signal detection theory!$d'$}\\index{signal detection theory!sensitivity}\n",
        "Higher values reflect greater accuracy.\\index{signal detection theory!$d'$}\n",
        "The formula for calculating $d'$ is in Equation \\@ref(eq:dPrimeSDT).\\index{signal detection theory!$d'$}\n",
        "\n",
        "\\begin{equation}\n",
        "d' = z(\\text{hit rate}) - z(\\text{false alarm rate})\n",
        "(\\#eq:dPrimeSDT)\n",
        "\\end{equation}\n"
      ],
      "id": "5ee1e4b4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dPrimeSDT <- function(TP, TN, FP, FN){\n",
        "  HR <- TP/(TP + FN)\n",
        "  FAR <- FP/(FP + TN)\n",
        "  value <- qnorm(HR) - qnorm(FAR)\n",
        "  \n",
        "  return(value)\n",
        "}"
      ],
      "id": "61967737",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dPrimeSDT(\n",
        "  TP = TPvalue,\n",
        "  TN = TNvalue,\n",
        "  FP = FPvalue,\n",
        "  FN = FNvalue)"
      ],
      "id": "796d5c4a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dPrimeValue <- dPrimeSDT(TP = TPvalue,\n",
        "                         TN = TNvalue,\n",
        "                         FP = FPvalue,\n",
        "                         FN = FNvalue)"
      ],
      "id": "40706329",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### $A$ (Non-Parametric) Sensitivity from Signal Detection Theory {#aSDT}\n",
        "\n",
        "$A$ is a non-parametric index of sensitivity from [signal detection theory](#sdt), as described by @Zhang2005.\\index{signal detection theory!$A$}\\index{signal detection theory!sensitivity}\n",
        "Higher values reflect greater accuracy.\\index{signal detection theory!$A$}\n",
        "The formula for calculating $A$ is in Equation \\@ref(eq:aSDT).\\index{signal detection theory!$A$}\n",
        "\n",
        "https://sites.google.com/a/mtu.edu/whynotaprime/ (archived at https://perma.cc/W2M2-39TJ)\n",
        "\n",
        "\\begin{equation}\n",
        "A = \n",
        "\\begin{cases}\n",
        " \\frac{3}{4} + \\frac{H - F}{4} - F(1 - H) & \\text{if } F \\leq 0.5 \\leq H ; \\\\\n",
        " \\frac{3}{4} + \\frac{H - F}{4} - \\frac{F}{4H} & \\text{if } F \\leq H \\leq 0.5 ;\\\\\n",
        " \\frac{3}{4} + \\frac{H - F}{4} - \\frac{1 - H}{4(1 - F)} & \\text{if } 0.5 \\leq F \\leq H .\n",
        "\\end{cases}\n",
        "(\\#eq:aSDT)\n",
        "\\end{equation}\n",
        "\n",
        "where $H$ is the hit rate and $F$ is the false alarm rate.\\index{signal detection theory!$A$}\\index{sensitivity}\\index{false positive!rate}\n"
      ],
      "id": "08bd2e17"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "aSDT <- function(TP, TN, FP, FN){\n",
        "  HR <- TP/(TP + FN)\n",
        "  FAR <- FP/(FP + TN)\n",
        "  \n",
        "  ifelse(FAR <= .5 & HR >= .5, value <- (3/4) + ((HR - FAR)/4) - (FAR * (1 - HR)), NA)\n",
        "  ifelse(FAR <= HR & HR <= .5, value <- (3/4) + ((HR - FAR)/4) - (FAR/(4 * HR)), NA)\n",
        "  ifelse(FAR >= .5 & FAR <= HR, value <- (3/4) + ((HR - FAR)/4) - ((1 - HR)/(4 * (1 - FAR))), NA)\n",
        "  \n",
        "  return(value)\n",
        "}"
      ],
      "id": "b95ab041",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "aSDT(\n",
        "  TP = TPvalue,\n",
        "  TN = TNvalue,\n",
        "  FP = FPvalue,\n",
        "  FN = FNvalue)"
      ],
      "id": "afd1ebb0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Avalue <- aSDT(TP = TPvalue,\n",
        "               TN = TNvalue,\n",
        "               FP = FPvalue,\n",
        "               FN = FNvalue)"
      ],
      "id": "036ce797",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### $\\beta$ Bias from Signal Detection Theory {#betaSDT}\n",
        "\n",
        "$\\beta$ is an index of bias from [signal detection theory](#sdt), as described by @Stanislaw1999.\\index{signal detection theory!$\\beta$}\\index{signal detection theory!bias}\n",
        "Smaller values reflect greater accuracy.\\index{signal detection theory!$\\beta$}\n",
        "The formula for calculating $\\beta$ is in Equation \\@ref(eq:betaSDT).\\index{signal detection theory!$\\beta$}\n",
        "\n",
        "\\begin{equation}\n",
        "\\beta = e^{\\bigg\\{\\frac{\\big[\\phi^{-1}(F)\\big]^2 - \\big[\\phi^{-1}(H)\\big]}{2}\\bigg\\}^2} \n",
        "(\\#eq:betaSDT)\n",
        "\\end{equation}\n",
        "\n",
        "where $H$ is the hit rate, $F$ is the false alarm rate, and $\\phi$ (phi) is a mathematical function that converts a *z* score to a probability by determining the portion of the normal distribution that lies to the left of the *z* score.\\index{signal detection theory!$\\beta$}\\index{sensitivity}\\index{false positive!rate}\n"
      ],
      "id": "b7dfab04"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "betaSDT <- function(TP, TN, FP, FN){\n",
        "  HR <- TP/(TP + FN)\n",
        "  FAR <- FP/(FP + TN)\n",
        "  value <- exp(qnorm(FAR)^2/2 - qnorm(HR)^2/2)\n",
        "  \n",
        "  return(value)\n",
        "}"
      ],
      "id": "3957037e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "betaSDT(\n",
        "  TP = TPvalue,\n",
        "  TN = TNvalue,\n",
        "  FP = FPvalue,\n",
        "  FN = FNvalue)"
      ],
      "id": "d481d901",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "betaValue <- betaSDT(TP = TPvalue,\n",
        "                     TN = TNvalue,\n",
        "                     FP = FPvalue,\n",
        "                     FN = FNvalue)"
      ],
      "id": "8701a40a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### $c$ Bias from Signal Detection Theory {#cSDT}\n",
        "\n",
        "$c$ is an index of bias from [signal detection theory](#sdt), as described by @Stanislaw1999.\\index{signal detection theory!$c$}\\index{signal detection theory!bias}\n",
        "Smaller values reflect greater accuracy.\\index{signal detection theory!$c$}\n",
        "The formula for calculating $c$ is in Equation \\@ref(eq:cSDT).\\index{signal detection theory!$c$}\n",
        "\n",
        "\\begin{equation}\n",
        "c = - \\frac{\\phi^{-1}(H) + \\phi^{-1}(F)}{2}\n",
        "(\\#eq:cSDT)\n",
        "\\end{equation}\n",
        "\n",
        "where $H$ is the hit rate, $F$ is the false alarm rate, and $\\phi$ (phi) is a mathematical function that converts a *z* score to a probability by determining the portion of the normal distribution that lies to the left of the *z* score.\\index{signal detection theory!$c$}\\index{sensitivity}\\index{false positive!rate}\n"
      ],
      "id": "f85eeca0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cSDT <- function(TP, TN, FP, FN){\n",
        "  HR <- TP/(TP + FN)\n",
        "  FAR <- FP/(FP + TN)\n",
        "  value <- -(qnorm(HR) + qnorm(FAR))/2\n",
        "  \n",
        "  return(value)\n",
        "}"
      ],
      "id": "cfb8fe67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cSDT(\n",
        "  TP = TPvalue,\n",
        "  TN = TNvalue,\n",
        "  FP = FPvalue,\n",
        "  FN = FNvalue)"
      ],
      "id": "d3abff56",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cValue <- cSDT(TP = TPvalue,\n",
        "               TN = TNvalue,\n",
        "               FP = FPvalue,\n",
        "               FN = FNvalue)"
      ],
      "id": "1464b619",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### $b$ (Non-Parametric) Bias from Signal Detection Theory {#bSDT}\n",
        "\n",
        "$b$ is a non-parametric index of bias from [signal detection theory](#sdt), as described by @Zhang2005.\\index{signal detection theory!$b$}\\index{signal detection theory!bias}\n",
        "Smaller values reflect greater accuracy.\\index{signal detection theory!$b$}\n",
        "The formula for calculating $b$ is in Equation \\@ref(eq:bSDT).\\index{signal detection theory!$b$}\n",
        "\n",
        "\\begin{equation}\n",
        "b = \n",
        "\\begin{cases}\n",
        " \\frac{5 - 4H}{1 + 4F} & \\text{if } F \\leq 0.5 \\leq H ; \\\\\n",
        " \\frac{H^2 + H}{H^2 + F} & \\text{if } F \\leq H \\leq 0.5 ;\\\\\n",
        " \\frac{(1 - F)^2 + (1 - H)}{(1 - F)^2 + (1 - F)} & \\text{if } 0.5 \\leq F \\leq H .\n",
        "\\end{cases}\n",
        "(\\#eq:bSDT)\n",
        "\\end{equation}\n"
      ],
      "id": "22bc2ffe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "bSDT <- function(TP, TN, FP, FN){\n",
        "  HR <- TP/(TP + FN)\n",
        "  FAR <- FP/(FP + TN)\n",
        "  \n",
        "  ifelse(FAR <= .5 & HR >= .5, value <-(5 - (4 * HR))/(1 + (4 * FAR)), NA)\n",
        "  ifelse(FAR <= HR & HR <= .5, value <- (HR^2 + HR)/(HR^2 + FAR), NA)\n",
        "  ifelse(FAR >= .5 & FAR <= HR, value <- ((1 - FAR)^2 + (1 - HR))/((1 - FAR)^2 + (1 - FAR)), NA)\n",
        "  \n",
        "  return(value)\n",
        "}"
      ],
      "id": "26f75344",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "bSDT(\n",
        "  TP = TPvalue,\n",
        "  TN = TNvalue,\n",
        "  FP = FPvalue,\n",
        "  FN = FNvalue)"
      ],
      "id": "b6ca9b42",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "bValue <- bSDT(TP = TPvalue,\n",
        "               TN = TNvalue,\n",
        "               FP = FPvalue,\n",
        "               FN = FNvalue)"
      ],
      "id": "3439ca75",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mean Difference between Predicted Versus Observed Values (Miscalibration) {#miscalibration}\n",
        "\n",
        "The mean difference between predicted values versus observed values at a given cutoff is an index of [miscalibration](#calibration) of predictions at that cutoff.\\index{calibration}\n",
        "It is called \"calibration-in-the-small\" (as opposed to calibration-in-the-large, which spans all cutoffs).\\index{calibration}\\index{calibration!in the small}\\index{calibration!in the large}\n",
        "Values closer to zero reflect greater accuracy.\\index{calibration}\n",
        "Values above zero indicate that the predicted values are, on average, greater than the observed values.\\index{calibration}\n",
        "Values below zero indicate that the observed values are, on average, greater than the predicted values.\\index{calibration}\n"
      ],
      "id": "e8da5c9a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "miscalibration <- function(predicted, actual, cutoff, bins = 10){\n",
        "  data <- data.frame(na.omit(cbind(predicted, actual)))\n",
        "  \n",
        "  calibrationTable <- mutate(\n",
        "    data,\n",
        "    bin = cut_number(\n",
        "      predicted,\n",
        "      n = 10)) %>%\n",
        "    group_by(bin) %>%\n",
        "    summarise(\n",
        "      n = length(predicted),\n",
        "      meanPredicted = mean(predicted, na.rm = TRUE),\n",
        "      meanObserved = mean(actual, na.rm = TRUE),\n",
        "      .groups = \"drop\")\n",
        "  \n",
        "  calibrationTable$cutoffMin <- as.numeric(str_replace_all(str_split(\n",
        "    calibrationTable$bin,\n",
        "    pattern = \",\",\n",
        "    simplify = TRUE)[,1],\n",
        "    \"[^[:alnum:]\\\\-\\\\.]\", \"\"))\n",
        "  calibrationTable$cutoffMax <- as.numeric(str_replace_all(str_split(\n",
        "    calibrationTable$bin,\n",
        "    pattern = \",\",\n",
        "    simplify = TRUE)[,2],\n",
        "    \"[^[:alnum:]\\\\-\\\\.]\", \"\"))\n",
        "\n",
        "  calibrationTable$inRange <- with(\n",
        "    calibrationTable,\n",
        "    cutoff >= cutoffMin & cutoff <= cutoffMax)\n",
        "  \n",
        "  if(length(which(calibrationTable$inRange == TRUE)) > 0){\n",
        "    nearestCutoff <- calibrationTable$bin[min(which(\n",
        "      calibrationTable$inRange == TRUE))]\n",
        "    calibrationAtNearestCutoff <- calibrationTable[which(\n",
        "      calibrationTable$bin == nearestCutoff),]\n",
        "    calibrationAtNearestCutoff <- as.data.frame(calibrationTable[max(which(\n",
        "      calibrationTable$inRange == TRUE)),])\n",
        "    \n",
        "    meanPredicted <- calibrationAtNearestCutoff[, \"meanPredicted\"]\n",
        "    meanObserved <- calibrationAtNearestCutoff[, \"meanObserved\"]\n",
        "    differenceBetweenPredictedAndObserved <- meanPredicted - meanObserved\n",
        "  } else{\n",
        "    differenceBetweenPredictedAndObserved <- NA\n",
        "  }\n",
        "  \n",
        "  return(differenceBetweenPredictedAndObserved)\n",
        "}"
      ],
      "id": "9be919df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "miscalibration(\n",
        "  predicted = mydataSDT$predictedProbability,\n",
        "  actual = mydataSDT$disorder,\n",
        "  cutoff = cutoff)"
      ],
      "id": "12c6d72b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimal Cutoff Specification {#optimalCutoff}\n",
        "\n",
        "There are two ways to improve diagnostic performance [@Swets2000].\\index{cutoff!optimal}\n",
        "One way is to increase the diagnostic accuracy of the assessment.\\index{cutoff!optimal}\\index{discrimination}\n",
        "The second way is to increase the utility of the diagnostic decisions that are made, based on where we set the cutoff.\\index{cutoff!optimal}\n",
        "The optimal cutoff depends on the differential costs of [false positives](#falsePositive) versus [false negatives](#falseNegative), as applied in [decision theory](#decisionTheory).\\index{cutoff!optimal}\\index{decision theory}\\index{false positive}\\index{false negative}\\index{prediction!prediction error!costs of}\n",
        "When differential costs of [false positives](#falsePositive) versus [false negatives](#falseNegative) cannot be specified, an alternative approach to specifying the optimal cutoff is to use [information theory](#informationTheory).\\index{cutoff!optimal}\\index{information!theory}\n",
        "\n",
        "### Decision Theory {#decisionTheory}\n",
        "\n",
        "According to the decision theory approach to picking the optimal cutoff, the optimal cutoff depends on the value/importance placed on each of the four decision outcomes [([true positives](#truePositive), [true negatives](#trueNegative), [false positives](#falsePositive), [false negatives](#falseNegative)); @Treat2023].\\index{cutoff!optimal}\\index{decision theory}\n",
        "Utility is the relative value placed on a specific decision-making outcome (i.e., user-perceived benefit or cost): utilities typically range between zero and one, where a value of zero represents the least desired outcome, and a value of one indicates the most desired outcome.\\index{cutoff!optimal}\\index{decision theory}\\index{true positive}\\index{true negative}\\index{false positive}\\index{false negative}\n",
        "According to the decision theory approach, the optimal cutoff is the cutoff with the highest overall utility.\\index{cutoff!optimal}\\index{decision theory}\\index{overall utility}\n",
        "\n",
        "#### Overall utility of a specific cutoff value {#overallUtilityCutoff}\n",
        "\n",
        "The overall utility of a specific cutoff value is a utilities-weighted sum of the probabilities of the four decision-making outcomes ([hits](#truePositive), [misses](#falseNegative), [correct rejections](#trueNegative), [false alarms](#falsePositive)).\\index{cutoff!optimal}\\index{decision theory}\\index{true positive}\\index{true negative}\\index{false positive}\\index{false negative}\\index{overall utility}\n",
        "That is, overall utility is the sum of the product of the probability of a particular outcome (TP, TN, FP, FN; e.g., $\\text{BR} \\times \\text{TP rate}$) and the utility of that outcome (e.g., how much we value [TPs](#truePositive) relative to other outcomes).\\index{cutoff!optimal}\\index{decision theory}\\index{true positive}\\index{true negative}\\index{false positive}\\index{false negative}\\index{overall utility}\n",
        "Higher values reflect greater utility, so you would pick the cutoff with the highest overall utility.\\index{cutoff!optimal}\\index{decision theory}\\index{overall utility}\n",
        "The formula for calculating overall utility is in Equation \\@ref(eq:overallUtility):\\index{cutoff!optimal}\\index{decision theory}\\index{overall utility}\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  U_\\text{overall} = \\ & (\\text{BR})(\\text{HR})(U_\\text{H}) \\\\\n",
        "  &+ (\\text{BR})(1 - \\text{HR})(U_\\text{M}) \\\\\n",
        "  &+ (1 - \\text{BR})(\\text{FAR})(U_\\text{FA}) \\\\\n",
        "  &+ (1 - \\text{BR})(1 - \\text{FAR})(U_\\text{CR})\n",
        "\\end{aligned}\n",
        "(\\#eq:overallUtility)\n",
        "$$\n",
        "\n",
        "where $\\text{BR} = \\text{base rate}$, $\\text{HR} = \\text{hit rate (true positive rate)}$, $\\text{FAR} = \\text{false alarm rate (false positive rate)}$, $U_\\text{H} = \\text{utility of hits (true positives)}$, $U_\\text{M} = \\text{utility of misses (false negatives)}$, $U_\\text{FA} = \\text{utility of false alarms (false positives)}$, $U_\\text{CR} = \\text{utility of correct rejections (true negatives)}$.\\index{cutoff!optimal}\\index{decision theory}\\index{true positive}\\index{true negative}\\index{false positive}\\index{false negative}\\index{base rate}\\index{sensitivity}\\index{false positive!rate}\\index{overall utility}\n"
      ],
      "id": "43dd1b94"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Uoverall <- function(BR, HR, FAR, UH, UM, UCR, UFA){\n",
        "  (BR*HR*UH) + (BR*(1 - HR)*UM) + ((1 - BR)*FAR*UFA) + ((1 - BR)*(1 - FAR)*(UCR))\n",
        "}"
      ],
      "id": "a0fb1fe4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Uoverall(\n",
        "  BR = baseRate(\n",
        "    TP = TPvalue,\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue,\n",
        "    FN = FNvalue),\n",
        "  HR = sensitivity(\n",
        "    TP = TPvalue,\n",
        "    FN = FNvalue),\n",
        "  FAR = falsePositiveRate(\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue),\n",
        "  UH = 1,\n",
        "  UM = 0,\n",
        "  UCR = 0.75,\n",
        "  UFA = 0.25)\n",
        "\n",
        "Uoverall(\n",
        "  BR = baseRate(\n",
        "    TP = TPvalue,\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue,\n",
        "    FN = FNvalue),\n",
        "  HR = sensitivity(\n",
        "    TP = TPvalue,\n",
        "    FN = FNvalue),\n",
        "  FAR = falsePositiveRate(\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue),\n",
        "  UH = 1,\n",
        "  UM = 0,\n",
        "  UCR = 1,\n",
        "  UFA = 0)\n",
        "\n",
        "Uoverall(\n",
        "  BR = baseRate(\n",
        "    TP = TPvalue,\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue,\n",
        "    FN = FNvalue),\n",
        "  HR = sensitivity(\n",
        "    TP = TPvalue,\n",
        "    FN = FNvalue),\n",
        "  FAR = falsePositiveRate(\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue),\n",
        "  UH = 0.75,\n",
        "  UM = 0.25,\n",
        "  UCR = 1,\n",
        "  UFA = 0)"
      ],
      "id": "4ef874f7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Utility ratio {#utilityRatio}\n",
        "\n",
        "The utility ratio is the user-perceived relative importance of decisions about negative versus positive cases.\\index{cutoff!optimal}\\index{decision theory}\\index{true positive}\\index{true negative}\\index{false positive}\\index{false negative}\n",
        "If the utility ratio value is one, it indicates that identifying negative cases and positive cases is equally important.\\index{cutoff!optimal}\\index{decision theory}\\index{true positive}\\index{true negative}\\index{false positive}\\index{false negative}\n",
        "Values above one indicate greater relative importance of identifying negative cases than positive cases.\\index{cutoff!optimal}\\index{decision theory}\\index{true positive}\\index{true negative}\\index{false positive}\\index{false negative}\n",
        "Values below one indicate greater relative importance of identifying positive cases than negative cases.\\index{cutoff!optimal}\\index{decision theory}\\index{true positive}\\index{true negative}\\index{false positive}\\index{false negative}\n",
        "Values of one indicate that you are maximizing percent accuracy.\\index{cutoff!optimal}\\index{decision theory}\\index{percent accuracy}\n",
        "The formula for calculating the utility ratio is in Equation \\@ref(eq:utilityRatio):\\index{cutoff!optimal}\\index{decision theory}\\index{true positive}\\index{true negative}\\index{false positive}\\index{false negative}\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{Utility Ratio} = \\frac{U_\\text{CR} - U_\\text{FA}}{U_\\text{H} - U_\\text{M}}\n",
        "(\\#eq:utilityRatio)\n",
        "\\end{equation}\n",
        "\n",
        "where $U_\\text{H} = \\text{utility of hits (true positives)}$, $U_\\text{M} = \\text{utility of misses (false negatives)}$, $U_\\text{FA} = \\text{utility of false alarms (false positives)}$, $U_\\text{CR} = \\text{utility of correct rejections (true negatives)}$.\\index{cutoff!optimal}\\index{decision theory}\\index{true positive}\\index{true negative}\\index{false positive}\\index{false negative}\n"
      ],
      "id": "369b42b1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "utilityRatio <- function(UH, UM, UCR, UFA){\n",
        "  (UCR - UFA) / (UH - UM)\n",
        "}"
      ],
      "id": "feb22551",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "utilityRatio(UH = 1, UM = 0, UCR = 0.75, UFA = 0.25)\n",
        "utilityRatio(UH = 1, UM = 0, UCR = 1, UFA = 0)\n",
        "utilityRatio(UH = 0.75, UM = 0.25, UCR = 1, UFA = 0)"
      ],
      "id": "c49f1d1d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Decision theory](#decisionTheory) has key advantages, because it identifies the cutoff that would help you best achieve the goals/purpose of the assessment.\\index{cutoff!optimal}\\index{decision theory}\n",
        "However, it can be challenging to specify the relative costs of errors.\\index{cutoff!optimal}\\index{decision theory}\\index{false positive}\\index{false negative}\\index{prediction!prediction error!costs of}\n",
        "If you cannot decide values for outcomes (relative importance between [FP](#falsePositive) and [FN](#falseNegative)), you can use [information theory](#informationTheory) to identify the optimal cutoff.\\index{cutoff!optimal}\\index{decision theory}\\index{false positive}\\index{false negative}\\index{prediction!prediction error!costs of}\\index{information!theory}\n",
        "\n",
        "### Information Theory {#informationTheory}\n",
        "\n",
        "When the user does not differentially weigh the value/importance of the four decision-making outcomes ([hits](#truePositive), [misses](#falseNegative), [correct rejections](#trueNegative), [false alarms](#falsePositive)), the information theory approach can be useful for specifying the optimal cutoff.\\index{cutoff!optimal}\\index{information!theory}\\index{false positive}\\index{false negative}\\index{true positive}\\index{true negative}\\index{prediction!prediction error!costs of}\n",
        "According to the information theory approach, the optimal cutoff is the cutoff that provides the greatest information gain [@Treat2023].\\index{cutoff!optimal}\\index{information!theory}\\index{information!gain}\n",
        "\n",
        "#### Information Gain {#informationGain}\n",
        "\n",
        "Information gain ($I_\\text{gain}$) is the reduction of uncertainty about the true classification of a case that results from administering an assessment or prediction measure [@Treat2023].\\index{cutoff!optimal}\\index{information!theory}\\index{information!gain}\n",
        "Greater values reflect greater reduction of uncertainty, so the optimal cutoff can be specified as the cutoff with the highest information gain.\\index{cutoff!optimal}\\index{information!theory}\\index{information!gain}\n",
        "\n",
        "##### Formula from @Treat2023 {#IgainTreat2023}\n",
        "\n",
        "The formula from @Treat2023 for calculating information gain is in Equation \\@ref(eq:informationGain1):\\index{cutoff!optimal}\\index{information!theory}\\index{information!gain}\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  I_\\text{gain} = \\ & (\\text{BR})(\\text{HR})\\bigg[\\log_2\\bigg(\\frac{\\text{HR}}{G}\\bigg)\\bigg] \\\\\n",
        "  &+ (\\text{BR})(1 - \\text{HR})\\bigg[\\log_2\\bigg(\\frac{1 - \\text{HR}}{1 - G}\\bigg)\\bigg] \\\\\n",
        "  &+ (1 - \\text{BR})(\\text{FAR})\\bigg[\\log_2\\bigg(\\frac{\\text{FAR}}{G}\\bigg)\\bigg] \\\\\n",
        "  &+ (1 - \\text{BR})(1 - \\text{FAR})\\bigg[\\log_2\\bigg(\\frac{1 - \\text{FAR}}{1 - G}\\bigg)\\bigg]\n",
        "\\end{aligned}\n",
        "(\\#eq:informationGain1)\n",
        "$$\n",
        "\n",
        "where $\\text{BR} =$ [base rate](#baseRate), $\\text{HR} =$ [hit rate](#sensitivity) ([true positive rate](#sensitivity)), $\\text{FAR} =$ [false alarm rate](#falsePositiveRate) ([false positive rate](#falsePositiveRate)), $G =$ [selection ratio](#selectionRatio) $= \\text{BR} (\\text{HR}) + (1 - \\text{BR}) (\\text{FAR})$, as reported in @Somoza1989 (see below).\\index{cutoff!optimal}\\index{information!theory}\\index{base rate}\\index{sensitivity}\\index{false positive!rate}\\index{selection ratio}\\index{information!gain}\n"
      ],
      "id": "0cdd3823"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Igain <- function(BR, HR, FAR){\n",
        "  G <- BR*(HR) + (1 - BR)*(FAR)\n",
        "  \n",
        "  (BR*HR*log2(HR/G)) + \n",
        "    (BR*(1 - HR)*(log2((1 - HR)/(1 - G)))) + \n",
        "    ((1 - BR)*FAR*(log2(FAR/G))) + \n",
        "    ((1 - BR)*(1 - FAR)*(log2((1 - FAR)/(1 - G))))\n",
        "}"
      ],
      "id": "13c45285",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Igain(\n",
        "  BR = baseRate(\n",
        "    TP = TPvalue,\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue,\n",
        "    FN = FNvalue),\n",
        "  HR = sensitivity(\n",
        "    TP = TPvalue,\n",
        "    FN = FNvalue),\n",
        "  FAR = falsePositiveRate(\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue))"
      ],
      "id": "5792f388",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Alternative formula from @Metz1973 {#IgainMetz1973}\n",
        "\n",
        "The alternative formula from @Metz1973 for calculating information gain is in Equation \\@ref(eq:informationGain2):\\index{cutoff!optimal}\\index{information!theory}\\index{information!gain}\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  I_\\text{gain} = \\ & p(S \\mid s) \\cdot p(s) \\cdot log_2\\Bigg\\{\\frac{p(S \\mid s)}{p(S \\mid s) \\cdot p(s) + p(S \\mid n)[1 - p(s)]}\\Bigg\\} \\\\\n",
        "  &+ p(S \\mid n)[1 - p(s)] \\times log_2\\Bigg\\{\\frac{p(S \\mid n)}{p(S \\mid s) \\cdot p(s) + p(S \\mid n)[1 - p(s)]}\\Bigg\\} \\\\\n",
        "  &+ [1 - p(S \\mid s)] \\cdot p(s) \\times log_2\\Bigg\\{\\frac{1 - p(S \\mid s)}{1 - p(S \\mid s) \\cdot p(s) - p(S \\mid n)[1 - p(s)]}\\Bigg\\} \\\\\n",
        "  &+ [1 - p(S \\mid n)][1 - p(s)] \\times log_2\\Bigg\\{\\frac{1 - p(S \\mid n)}{1 - p(S \\mid s) \\cdot p(s) - p(S \\mid n)[1 - p(s)]}\\Bigg\\}\n",
        "\\end{aligned}\n",
        "(\\#eq:informationGain2)\n",
        "$$\n",
        "\n",
        "where $p(S \\mid s) =$ [sensitivity](#sensitivity) ([hit rate](#sensitivity) or [true positive rate](#sensitivity)); i.e., the [conditional probability](#conditionalProbability) of deciding a signal is present ($S$) when the signal is in fact present($s$), $p(S \\mid n) =$ [false positive rate](#falsePositiveRate) ([false alarm rate](#falsePositiveRate)); i.e., the [conditional probability](#conditionalProbability) of deciding a signal is present ($S$) when the signal is in fact absent ($n$), $p(s) =$ [base rate](#baseRate), i.e., the probability that the signal is in fact present ($s$).\\index{cutoff!optimal}\\index{information!theory}\\index{base rate}\\index{sensitivity}\\index{false positive!rate}\\index{information!gain}\n"
      ],
      "id": "3f20a3d8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Igain2 <- function(BR, HR, FAR){\n",
        "  HR * BR * log2(HR / ((HR * BR) + (FAR * (1 - BR)))) +\n",
        "    FAR * (1 - BR) * log2(FAR / ((HR * BR) + (FAR * (1 - BR)))) +\n",
        "    (1 - HR) * BR * log2((1 - HR) / (1 - (HR * BR) - (FAR * (1 - BR)))) +\n",
        "    (1 - FAR) * (1 - BR) * log2((1 - FAR) / (1 - (HR * BR) - (FAR * (1 - BR))))\n",
        "}"
      ],
      "id": "a01ea481",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Igain2(\n",
        "  BR = baseRate(\n",
        "    TP = TPvalue,\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue,\n",
        "    FN = FNvalue),\n",
        "  HR = sensitivity(\n",
        "    TP = TPvalue,\n",
        "    FN = FNvalue),\n",
        "  FAR = falsePositiveRate(\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue))"
      ],
      "id": "855d60c3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Alternative formula from @Somoza1989 {#IgainSomoza1989}\n",
        "\n",
        "The alternative formula from @Somoza1989 for calculating information gain is in Equation \\@ref(eq:informationGain3):\\index{cutoff!optimal}\\index{information!theory}\\index{information!gain}\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  I_\\text{gain} = \\ & [(\\text{TPR})(\\text{Pr})] \\times \\log_2(\\text{TPR}/G) \\\\\n",
        "  &+ [(\\text{FPR})(1 - \\text{Pr})] \\times \\log_2(\\text{FPR}/G) \\\\\n",
        "  &+ [(1 - \\text{TPR})(\\text{Pr})] \\times \\log_2\\bigg(\\frac{1 - \\text{TPR}}{1 - G}\\bigg) \\\\\n",
        "  &+ [(1 - \\text{FPR})(1 - \\text{Pr})] \\times \\log_2\\bigg(\\frac{1 - \\text{FPR}}{1 - G}\\bigg)\n",
        "\\end{aligned}\n",
        "(\\#eq:informationGain3)\n",
        "$$\n",
        "\n",
        "where $\\text{TP} =$ [true positive rate](#sensitivity) ([hit rate](#sensitivity)), $\\text{Pr} =$ [prevalence](#baseRate) ([base rate](#baseRate)), $\\text{FP} =$ [false positive rate](#falsePositiveRate) ([false alarm rate](#falsePositiveRate)), $G = \\text{Pr} (\\text{TP}) + (1 - \\text{Pr}) (\\text{FP}) =$ [selection ratio](#selectionRatio)\\index{cutoff!optimal}\\index{information!theory}\\index{base rate}\\index{sensitivity}\\index{false positive!rate}\\index{selection ratio}\\index{information!gain}\n"
      ],
      "id": "6ad94acd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Igain3 <- function(BR, HR, FAR){\n",
        "  G <- BR*(HR) + (1 - BR)*(FAR)\n",
        "  \n",
        "  ((HR)*(BR))*log2((HR/G)) + \n",
        "    ((FAR)*(1-BR))*log2((FAR/G)) + \n",
        "    ((1-HR)*(BR))*log2((1-HR)/(1-G)) + \n",
        "    ((1-FAR)*(1-BR))*log2((1-FAR)/(1-G))\n",
        "}"
      ],
      "id": "27ccf459",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Igain3(\n",
        "  BR = baseRate(\n",
        "    TP = TPvalue,\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue,\n",
        "    FN = FNvalue),\n",
        "  HR = sensitivity(\n",
        "    TP = TPvalue,\n",
        "    FN = FNvalue),\n",
        "  FAR = falsePositiveRate(\n",
        "    TN = TNvalue,\n",
        "    FP = FPvalue))"
      ],
      "id": "d69b4152",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Examples {#IgainExamples}\n",
        "\n",
        "Case A from Exhibit 38.2 [@Treat2023]:\\index{cutoff!optimal}\\index{information!theory}\\index{information!gain}\n"
      ],
      "id": "d4352fd4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Igain(HR = (911/1899), FAR = (509/4757), BR = (1899/6656))"
      ],
      "id": "61abf9d9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Case B from Exhibit 38.2 [@Treat2023]:\\index{cutoff!optimal}\\index{information!theory}\\index{information!gain}\n"
      ],
      "id": "b12ae6bc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Igain(HR = (1597/3328), FAR = (356/3328), BR = (3328/6656))"
      ],
      "id": "b8b54ae9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Case C from Exhibit 38.2 [@Treat2023]:\\index{cutoff!optimal}\\index{information!theory}\\index{information!gain}\n"
      ],
      "id": "f165b9d2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Igain(HR = (2040/3328), FAR = (654/3328), BR = (3328/6656))"
      ],
      "id": "6007970a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Case B from Exhibit 38.3 [@Treat2023]:\\index{cutoff!optimal}\\index{information!theory}\\index{information!gain}\n"
      ],
      "id": "3a4956e0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Igain(HR = (1164/1899), FAR = (935/4757), BR = (1899/6656))"
      ],
      "id": "3a8d87d9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Effect of Base Rate {#baseRateEffectsIgain}\n",
        "\n",
        "Information gain depends on the [base rate](#baseRate) [@Treat2023], as depicted in Figure \\@ref(fig:informationGainBaseRate).\\index{cutoff!optimal}\\index{information!theory}\\index{base rate}\\index{information!gain}\n",
        "The maximum reduction of uncertainty (i.e., greatest information) occurs when the [base rate](#baseRate) is 0.5.\\index{cutoff!optimal}\\index{information!theory}\\index{base rate}\\index{information!gain}\n",
        "A [base rate](#baseRate) tells us little a priori about the condition if the probability of the condition is 50/50, so the measure can provide more benefit.\\index{cutoff!optimal}\\index{information!theory}\\index{base rate}\\index{information!gain}\n",
        "If the [base rate](#baseRate) is 0.3 or 0.7, we can do better than [going with the base rate](#predictingFromBaseRate).\\index{cutoff!optimal}\\index{information!theory}\\index{base rate}\\index{base rate!predicting from}\\index{information!gain}\n",
        "If the [base rate](#baseRate) is 0.9 or 0.1, it is difficult for our measure to do better than [going with the base rate](#predictingFromBaseRate).\\index{cutoff!optimal}\\index{information!theory}\\index{base rate}\\index{base rate!predicting from}\\index{information!gain}\n",
        "If the [base rate](#baseRate) is 0.05 of 0.95 (or more extreme), it is likely that our measure will do almost nothing in terms of information gain.\\index{cutoff!optimal}\\index{information!theory}\\index{base rate}\\index{information!gain}\n"
      ],
      "id": "a77886a8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "possibleCutoffs <- unique(na.omit(mydataSDT$testScore))\n",
        "possibleCutoffs <- possibleCutoffs[order(possibleCutoffs)]\n",
        "possibleCutoffs <- c(possibleCutoffs, max(possibleCutoffs, na.rm = TRUE) + 0.01)\n",
        "\n",
        "baseRates <- c(.05, .1, .3, .5, .7, .9, .95)\n",
        "\n",
        "possibleCutoffsBaseRates <- expand_grid(baseRate = baseRates, cutoff = possibleCutoffs)\n",
        "\n",
        "informationGainVars <- c(\"cutoff\",\"TP\",\"TN\",\"FP\",\"FN\")\n",
        "\n",
        "informationGainStats <- data.frame(matrix(nrow = length(possibleCutoffs), ncol = length(informationGainVars)))\n",
        "names(informationGainStats) <- informationGainVars\n",
        "\n",
        "for(i in 1:length(possibleCutoffs)){\n",
        "  cutoff <- possibleCutoffs[i]\n",
        "  \n",
        "  mydataSDT$diagnosis <- NA\n",
        "  mydataSDT$diagnosis[mydataSDT$testScore < cutoff] <- 0\n",
        "  mydataSDT$diagnosis[mydataSDT$testScore >= cutoff] <- 1\n",
        "  \n",
        "  informationGainStats[i, \"cutoff\"] <- cutoff\n",
        "  informationGainStats[i, \"TP\"] <- length(which(mydataSDT$diagnosis == 1 & mydataSDT$disorder == 1))\n",
        "  informationGainStats[i, \"TN\"] <- length(which(mydataSDT$diagnosis == 0 & mydataSDT$disorder == 0))\n",
        "  informationGainStats[i, \"FP\"] <- length(which(mydataSDT$diagnosis == 1 & mydataSDT$disorder == 0))\n",
        "  informationGainStats[i, \"FN\"] <- length(which(mydataSDT$diagnosis == 0 & mydataSDT$disorder == 1))\n",
        "}\n",
        "\n",
        "informationGainStats <- full_join(possibleCutoffsBaseRates, informationGainStats, by = \"cutoff\")\n",
        "\n",
        "informationGainStats$TPrate <- sensitivity(TP = informationGainStats$TP, TN = informationGainStats$TN, FP = informationGainStats$FP, FN = informationGainStats$FN)\n",
        "informationGainStats$FPrate <- falsePositiveRate(TP = informationGainStats$TP, TN = informationGainStats$TN, FP = informationGainStats$FP, FN = informationGainStats$FN)\n",
        "\n",
        "informationGainStats$informationGain <- Igain(BR = informationGainStats$baseRate, HR = informationGainStats$TPrate, FAR = informationGainStats$FPrate)\n",
        "informationGainStats <- na.omit(informationGainStats)\n",
        "\n",
        "ggplot(\n",
        "  data = informationGainStats,\n",
        "  aes(\n",
        "    x = cutoff,\n",
        "    y = informationGain,\n",
        "    group = as.factor(baseRate),\n",
        "    color = as.factor(baseRate))) +\n",
        "  geom_line(linewidth = 2) +\n",
        "  scale_x_continuous(name = \"Cutoff\") +\n",
        "  scale_y_continuous(name = \"Information Gain\") +\n",
        "  scale_color_viridis(\n",
        "    name = \"Base Rate\",\n",
        "    discrete = TRUE,\n",
        "    option = \"H\") +\n",
        "  geom_label_repel(\n",
        "    data = informationGainStats %>% \n",
        "      filter(cutoff == 0.5), \n",
        "    aes(label = paste(\"BR = \", baseRate, sep = \"\")),\n",
        "    nudge_x = 0.10,\n",
        "    na.rm = TRUE) +\n",
        "  theme_bw() +\n",
        "  theme(legend.position = \"none\")"
      ],
      "id": "83f0cea0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Accuracy at Every Possible Cutoff {#accuracyAtEveryPossibleCutoff}\n",
        "\n",
        "### Specify utility of each outcome {#specifyUtility}\n"
      ],
      "id": "bc672221"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "utilityHits <- 1\n",
        "utilityMisses <- 0\n",
        "utilityCorrectRejections <- 0.75\n",
        "utilityFalseAlarms <- 0.25"
      ],
      "id": "116c1794",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculate Accuracy {#calculateAccuracyAtEveryPossibleCutoff}\n"
      ],
      "id": "a38d4cac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "possibleCutoffs <- unique(na.omit(mydataSDT$testScore))\n",
        "possibleCutoffs <- possibleCutoffs[order(possibleCutoffs)]\n",
        "possibleCutoffs <- c(\n",
        "  possibleCutoffs, max(possibleCutoffs, na.rm = TRUE) + 0.01)\n",
        "\n",
        "accuracyVariables <- c(\n",
        "  \"cutoff\", \"TP\", \"TN\", \"FP\", \"FN\", \"differenceBetweenPredictedAndObserved\")\n",
        "\n",
        "accuracyStats <- data.frame(\n",
        "  matrix(\n",
        "    nrow = length(possibleCutoffs),\n",
        "    ncol = length(accuracyVariables)))\n",
        "names(accuracyStats) <- accuracyVariables\n",
        "\n",
        "for(i in 1:length(possibleCutoffs)){\n",
        "  cutoff <- possibleCutoffs[i]\n",
        "  \n",
        "  mydataSDT$diagnosis <- NA\n",
        "  mydataSDT$diagnosis[mydataSDT$testScore < cutoff] <- 0\n",
        "  mydataSDT$diagnosis[mydataSDT$testScore >= cutoff] <- 1\n",
        "  \n",
        "  accuracyStats[i, \"cutoff\"] <- cutoff\n",
        "  accuracyStats[i, \"TP\"] <- length(which(\n",
        "    mydataSDT$diagnosis == 1 & mydataSDT$disorder == 1))\n",
        "  accuracyStats[i, \"TN\"] <- length(which(\n",
        "    mydataSDT$diagnosis == 0 & mydataSDT$disorder == 0))\n",
        "  accuracyStats[i, \"FP\"] <- length(which(\n",
        "    mydataSDT$diagnosis == 1 & mydataSDT$disorder == 0))\n",
        "  accuracyStats[i, \"FN\"] <- length(which(\n",
        "    mydataSDT$diagnosis == 0 & mydataSDT$disorder == 1))\n",
        "  \n",
        "  accuracyStats[i, \"differenceBetweenPredictedAndObserved\"] <- \n",
        "    miscalibration(\n",
        "      predicted = mydataSDT$testScore,\n",
        "      actual = mydataSDT$disorder,\n",
        "      cutoff = cutoff)\n",
        "}\n",
        "\n",
        "accuracyStats$N <- sampleSize(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "\n",
        "accuracyStats$selectionRatio <- selectionRatio(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "accuracyStats$baseRate <- baseRate(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "\n",
        "accuracyStats$percentAccuracy <- percentAccuracy(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "accuracyStats$percentAccuracyByChance <- percentAccuracyByChance(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "accuracyStats$relativeImprovementOverChance <- relativeImprovementOverChance(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "accuracyStats$relativeImprovementOverPredictingFromBaseRate <- \n",
        "  relativeImprovementOverPredictingFromBaseRate(\n",
        "    TP = accuracyStats$TP,\n",
        "    TN = accuracyStats$TN,\n",
        "    FP = accuracyStats$FP,\n",
        "    FN = accuracyStats$FN)\n",
        "\n",
        "accuracyStats$sensitivity <- accuracyStats$TPrate <- sensitivity(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "accuracyStats$specificity <- accuracyStats$TNrate <- specificity(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "accuracyStats$FNrate <- falseNegativeRate(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "accuracyStats$FPrate <- falsePositiveRate(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "\n",
        "accuracyStats$youdenJ <- youdenJ(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "\n",
        "accuracyStats$positivePredictiveValue <- positivePredictiveValue(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "accuracyStats$negativePredictiveValue <- negativePredictiveValue(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "accuracyStats$falseDiscoveryRate <- falseDiscoveryRate(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "accuracyStats$falseOmissionRate <- falseOmissionRate(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "\n",
        "accuracyStats$balancedAccuracy <- balancedAccuracy(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "accuracyStats$f1Score <- fScore(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "accuracyStats$mcc <- mcc(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "\n",
        "accuracyStats$diagnosticOddsRatio <- diagnosticOddsRatio(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "accuracyStats$positiveLikelihoodRatio <- positiveLikelihoodRatio(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "accuracyStats$negativeLikelihoodRatio <- negativeLikelihoodRatio(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "\n",
        "accuracyStats$dPrimeSDT <- dPrimeSDT(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "accuracyStats$betaSDT <- betaSDT(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "accuracyStats$cSDT <- cSDT(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "accuracyStats$ASDT <- aSDT(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "accuracyStats$bSDT <- bSDT(\n",
        "  TP = accuracyStats$TP,\n",
        "  TN = accuracyStats$TN,\n",
        "  FP = accuracyStats$FP,\n",
        "  FN = accuracyStats$FN)\n",
        "\n",
        "accuracyStats$overallUtility <- Uoverall(\n",
        "  BR = accuracyStats$baseRate,\n",
        "  HR = accuracyStats$TPrate,\n",
        "  FAR = accuracyStats$FPrate,\n",
        "  UH = utilityHits,\n",
        "  UM = utilityMisses,\n",
        "  UCR = utilityCorrectRejections,\n",
        "  UFA = utilityFalseAlarms)\n",
        "accuracyStats$utilityRatio <- utilityRatio(\n",
        "  UH = utilityHits,\n",
        "  UM = utilityMisses,\n",
        "  UCR = utilityCorrectRejections,\n",
        "  UFA = utilityFalseAlarms)\n",
        "accuracyStats$informationGain <- Igain(\n",
        "  BR = accuracyStats$baseRate,\n",
        "  HR = accuracyStats$TPrate,\n",
        "  FAR = accuracyStats$FPrate)\n",
        "\n",
        "#Replace NaN and INF values with NA\n",
        "is.nan.data.frame <- function(x)\n",
        "  do.call(cbind, lapply(x, is.nan))\n",
        "\n",
        "accuracyStats[is.nan.data.frame(accuracyStats)] <- NA\n",
        "accuracyStats <- do.call(\n",
        "  data.frame,\n",
        "  lapply(\n",
        "    accuracyStats,\n",
        "    function(x) replace(x, is.infinite(x), NA)))"
      ],
      "id": "7a17b5c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### All Accuracy Statistics {#allAccuracyStatistics}\n",
        "\n",
        "The [`petersenlab`](https://github.com/DevPsyLab/petersenlab) package [@R-petersenlab] contains an R function that estimates the prediction accuracy at every possible cutoff.\\index{petersenlab package}\\index{cutoff}\n"
      ],
      "id": "4a7a0078"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyAtEachCutoff(\n",
        "  predicted = mydataSDT$testScore,\n",
        "  actual = mydataSDT$disorder,\n",
        "  UH = utilityHits,\n",
        "  UM = utilityMisses,\n",
        "  UCR = utilityCorrectRejections,\n",
        "  UFA = utilityFalseAlarms)"
      ],
      "id": "2264e1ac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "paged_table(accuracyStats)"
      ],
      "id": "6a5a2737",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats %>% \n",
        "  head %>% \n",
        "  t %>% \n",
        "  round(., 2) %>% \n",
        "  kable(.,\n",
        "      format = \"latex\",\n",
        "      booktabs = TRUE) %>%\n",
        "  kable_styling(latex_options = \"scale_down\")"
      ],
      "id": "9a7103c3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats %>% \n",
        "  head %>% \n",
        "  t %>% \n",
        "  round(., 2) %>% \n",
        "  kable(., booktabs = TRUE)"
      ],
      "id": "47515b20",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Youden's J Statistic {#allAccuracyStatistics-youdenJ}\n",
        "\n",
        "#### Threshold {#allAccuracyStatistics-youdenJthreshold}\n",
        "\n",
        "Threshold at maximum combination of [sensitivity](#sensitivity) and [specificity](#specificity):\\index{Youden's J statistic}\\index{cutoff}\n",
        "\n",
        "$\\text{max}(\\text{sensitivity} + \\text{specificity})$\n"
      ],
      "id": "48880530"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "youdenIndex <- coords(roc(data = mydataSDT,\n",
        "                          response = disorder,\n",
        "                          predictor = testScore,\n",
        "                          smooth = FALSE),\n",
        "                      x = \"best\",\n",
        "                      best.method = \"youden\")[[1]]\n",
        "\n",
        "youdenIndex"
      ],
      "id": "fff7775c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Accuracy statistics at cutoff of Youden's J Statistic {#allAccuracyStatistics-youdenJaccuracy}\n"
      ],
      "id": "03a318dd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats[head(which(\n",
        "  accuracyStats$cutoff >= youdenIndex), 1),]\n",
        "accuracyStats[which(\n",
        "  accuracyStats$youdenJ == max(accuracyStats$youdenJ, na.rm = TRUE)),]"
      ],
      "id": "76a38f5e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Closest to the Top Left of the ROC Curve {#allAccuracyStatistics-topLeftROC}\n",
        "\n",
        "#### Threshold {#allAccuracyStatistics-topLeftROCthreshold}\n",
        "\n",
        "Threshold where the ROC plot is closest to the Top Left:\n"
      ],
      "id": "78589850"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "closestToTheTopLeft <- coords(roc(\n",
        "  data = mydataSDT,\n",
        "  response = disorder,\n",
        "  predictor = testScore,\n",
        "  smooth = FALSE),\n",
        "  x = \"best\",\n",
        "  best.method = \"closest.topleft\")[[1]]"
      ],
      "id": "0cdf48dd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Accuracy stats at cutoff where the ROC plot is closest to the Top Left {#allAccuracyStatistics-topLeftROCaccuracy}\n"
      ],
      "id": "81fdb77c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats[head(which(\n",
        "  accuracyStats$cutoff >= closestToTheTopLeft), 1),]"
      ],
      "id": "8d3481b3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cutoff that optimizes each of the following criteria: {#allAccuracyStatistics-cutoff}\n",
        "\n",
        "The [`petersenlab`](https://github.com/DevPsyLab/petersenlab) package [@R-petersenlab] contains an R function that identifies the cutoff that optimizes each of various accuracy estimates.\\index{petersenlab package}\n"
      ],
      "id": "dae4c762"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "optimalCutoff(\n",
        "  predicted = mydataSDT$testScore,\n",
        "  actual = mydataSDT$disorder,\n",
        "  UH = utilityHits,\n",
        "  UM = utilityMisses,\n",
        "  UCR = utilityCorrectRejections,\n",
        "  UFA = utilityFalseAlarms)"
      ],
      "id": "aaff20ba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Percent Accuracy {#allAccuracyStatistics-cutoffPercentAccuracy}\n",
        "\n",
        "\\index{percent accuracy}\\index{cutoff}\n"
      ],
      "id": "79eabe3f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats$cutoff[which(\n",
        "  accuracyStats$percentAccuracy == max(\n",
        "    accuracyStats$percentAccuracy,\n",
        "    na.rm = TRUE))]"
      ],
      "id": "15597e62",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Percent Accuracy by Chance {#allAccuracyStatistics-cutoffPercentAccuracyByChance}\n",
        "\n",
        "\\index{percent accuracy!by chance}\\index{cutoff}\n"
      ],
      "id": "9919eefd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats$cutoff[which(\n",
        "  accuracyStats$percentAccuracyByChance == max(\n",
        "    accuracyStats$percentAccuracyByChance,\n",
        "    na.rm = TRUE))]"
      ],
      "id": "12e8ee4c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Relative Improvement Over Chance (ROIC) {#allAccuracyStatistics-cutoffROIC}\n",
        "\n",
        "\\index{relative improvement!over chance}\\index{cutoff}\n"
      ],
      "id": "1f401818"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats$cutoff[which(\n",
        "  accuracyStats$relativeImprovementOverChance == max(\n",
        "    accuracyStats$relativeImprovementOverChance,\n",
        "    na.rm = TRUE))]"
      ],
      "id": "44d818c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Relative Improvement Over Predicting from the Base Rate {#allAccuracyStatistics-cutoffRelativeImprovementOverPredictingFromBaseRate}\n",
        "\n",
        "\\index{relative improvement!over-predicting from the base rate}\\index{cutoff}\n"
      ],
      "id": "334ef04d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats$cutoff[which(\n",
        "  accuracyStats$relativeImprovementOverPredictingFromBaseRate == max(\n",
        "    accuracyStats$relativeImprovementOverPredictingFromBaseRate,\n",
        "    na.rm = TRUE))]"
      ],
      "id": "fdac8d2b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Sensitivity {#allAccuracyStatistics-cutoffSensitivity}\n",
        "\n",
        "\\index{sensitivity}\\index{cutoff}\n"
      ],
      "id": "986911cf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats$cutoff[which(\n",
        "  accuracyStats$sensitivity == max(\n",
        "    accuracyStats$sensitivity,\n",
        "    na.rm = TRUE))]"
      ],
      "id": "6d80daf9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Specificity {#allAccuracyStatistics-cutoffSpecificity}\n",
        "\n",
        "\\index{specificity}\\index{cutoff}\n"
      ],
      "id": "c88d11b2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats$cutoff[which(\n",
        "  accuracyStats$specificity == max(\n",
        "    accuracyStats$specificity,\n",
        "    na.rm = TRUE))]"
      ],
      "id": "1e87349a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Positive Predictive Value {#allAccuracyStatistics-cutoffPPV}\n",
        "\n",
        "\\index{positive predictive value}\\index{cutoff}\n"
      ],
      "id": "c6916b11"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats$cutoff[which(\n",
        "  accuracyStats$positivePredictiveValue == max(\n",
        "    accuracyStats$positivePredictiveValue,\n",
        "    na.rm = TRUE))]"
      ],
      "id": "a88b65e1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Negative Predictive Value {#allAccuracyStatistics-cutoffNPV}\n",
        "\n",
        "\\index{negative predictive value}\\index{cutoff}\n"
      ],
      "id": "7503b5e5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats$cutoff[which(\n",
        "  accuracyStats$negativePredictiveValue == max(\n",
        "    accuracyStats$negativePredictiveValue,\n",
        "    na.rm = TRUE))]"
      ],
      "id": "801c3f3e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Youden's J Statistic {#allAccuracyStatistics-cutoffYoudenJ}\n",
        "\n",
        "\\index{Youden's J statistic}\\index{cutoff}\n"
      ],
      "id": "e7261a9d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats$cutoff[which(\n",
        "  accuracyStats$youdenJ == max(\n",
        "    accuracyStats$youdenJ,\n",
        "    na.rm = TRUE))]"
      ],
      "id": "fe62668e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Balanced Accuracy {#allAccuracyStatistics-cutoffBalancedAccuracy}\n",
        "\n",
        "\\index{balanced accuracy}\\index{cutoff}\n"
      ],
      "id": "25292c4a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats$cutoff[which(\n",
        "  accuracyStats$balancedAccuracy == max(\n",
        "    accuracyStats$balancedAccuracy,\n",
        "    na.rm = TRUE))]"
      ],
      "id": "844645bf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### F1 Score {#allAccuracyStatistics-cutoffF1}\n",
        "\n",
        "\\index{F-score}\\index{cutoff}\n"
      ],
      "id": "b9a5c58a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats$cutoff[which(\n",
        "  accuracyStats$f1Score == max(\n",
        "    accuracyStats$f1Score,\n",
        "    na.rm = TRUE))]"
      ],
      "id": "633d0f21",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Matthews Correlation Coefficient {#allAccuracyStatistics-cutoffMCC}\n",
        "\n",
        "\\index{Matthews correlation coefficient}\\index{cutoff}\n"
      ],
      "id": "b755ec17"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats$cutoff[which(\n",
        "  accuracyStats$mcc == max(\n",
        "    accuracyStats$mcc,\n",
        "    na.rm = TRUE))]"
      ],
      "id": "9fce8c57",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Diagnostic Odds Ratio {#allAccuracyStatistics-cutoffDOR}\n",
        "\n",
        "\\index{diagnostic odds ratio}\\index{cutoff}\n"
      ],
      "id": "ff7335fc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats$cutoff[which(\n",
        "  accuracyStats$diagnosticOddsRatio == max(\n",
        "    accuracyStats$diagnosticOddsRatio,\n",
        "    na.rm = TRUE))]"
      ],
      "id": "fa0eaa6b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Positive Likelihood Ratio {#allAccuracyStatistics-cutoffPLR}\n",
        "\n",
        "\\index{positive likelihood ratio}\\index{cutoff}\n"
      ],
      "id": "b8d627b6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats$cutoff[which(\n",
        "  accuracyStats$positiveLikelihoodRatio == max(\n",
        "    accuracyStats$positiveLikelihoodRatio,\n",
        "    na.rm = TRUE))]"
      ],
      "id": "9f8ce12d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Negative Likelihood Ratio {#allAccuracyStatistics-cutoffNLR}\n",
        "\n",
        "\\index{negative likelihood ratio}\\index{cutoff}\n"
      ],
      "id": "3039d2a0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats$cutoff[which(\n",
        "  accuracyStats$negativeLikelihoodRatio == min(\n",
        "    accuracyStats$negativeLikelihoodRatio,\n",
        "    na.rm = TRUE))]"
      ],
      "id": "6b2379d5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### $d'$ Sensitivity {#allAccuracyStatistics-cutoffDprime}\n",
        "\n",
        "\\index{signal detection theory!$d'$}\\index{cutoff}\n"
      ],
      "id": "2a51e8c6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats$cutoff[which(\n",
        "  accuracyStats$dPrimeSDT == max(\n",
        "    accuracyStats$dPrimeSDT,\n",
        "    na.rm = TRUE))]"
      ],
      "id": "84124abe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### $A$ (Non-Parametric) Sensitivity {#allAccuracyStatistics-cutoffAsensitivity}\n",
        "\n",
        "\\index{signal detection theory!$A$}\\index{cutoff}\n"
      ],
      "id": "045a9bd8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats$cutoff[which(\n",
        "  accuracyStats$ASDT == max(\n",
        "    accuracyStats$ASDT,\n",
        "    na.rm = TRUE))]"
      ],
      "id": "150d413d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### $\\beta$ Bias {#allAccuracyStatistics-cutoffBetaBias}\n",
        "\n",
        "\\index{signal detection theory!$\\beta$}\\index{cutoff}\n"
      ],
      "id": "a8516f32"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats$cutoff[which(abs(\n",
        "  accuracyStats$betaSDT) == min(abs(\n",
        "    accuracyStats$betaSDT),\n",
        "    na.rm = TRUE))]"
      ],
      "id": "a393931d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### $c$ Bias {#allAccuracyStatistics-cutoffCbias}\n",
        "\n",
        "\\index{signal detection theory!$c$}\\index{cutoff}\n"
      ],
      "id": "77b507fa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats$cutoff[which(abs(\n",
        "  accuracyStats$cSDT) == min(abs(\n",
        "    accuracyStats$cSDT),\n",
        "    na.rm = TRUE))]"
      ],
      "id": "985de17d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### $b$ (Non-Parametric) Bias {#allAccuracyStatistics-cutoffBbias}\n",
        "\n",
        "\\index{signal detection theory!$b$}\\index{cutoff}\n"
      ],
      "id": "d11e75e5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats$cutoff[which(abs(\n",
        "  accuracyStats$bSDT) == min(abs(\n",
        "    accuracyStats$bSDT),\n",
        "    na.rm = TRUE))]"
      ],
      "id": "7ebd2a50",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mean difference between predicted and observed values (Miscalibration) {#allAccuracyStatistics-cutoffMiscalibration}\n",
        "\n",
        "\\index{calibration}\\index{cutoff}\n"
      ],
      "id": "2323ab3d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats$cutoff[which(abs(\n",
        "  accuracyStats$differenceBetweenPredictedAndObserved) == min(abs(\n",
        "    accuracyStats$differenceBetweenPredictedAndObserved),\n",
        "    na.rm = TRUE))]"
      ],
      "id": "23362148",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Overall Utility {#allAccuracyStatistics-cutoffOverallUtility}\n",
        "\n",
        "\\index{overall utility}\\index{cutoff}\n"
      ],
      "id": "9abbf73b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats$cutoff[which(\n",
        "  accuracyStats$overallUtility == max(\n",
        "    accuracyStats$overallUtility,\n",
        "    na.rm = TRUE))]"
      ],
      "id": "9cac31f2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Information Gain {#allAccuracyStatistics-cutoffIgain}\n",
        "\n",
        "\\index{information!gain}\\index{cutoff}\n"
      ],
      "id": "d2c5c6e9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracyStats$cutoff[which(\n",
        "  accuracyStats$informationGain == max(\n",
        "    accuracyStats$informationGain,\n",
        "    na.rm = TRUE))]"
      ],
      "id": "5eaf5b77",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regression for Prediction of Continuous Outcomes {#regression-prediction}\n",
        "\n",
        "When predicting a continuous outcome, regression is particularly relevant (or multiple regression, when dealing with multiple predictors).\\index{multiple regression}\n",
        "Regression takes the general form in Equation \\@ref(eq:regression):\\index{multiple regression}\n",
        "\n",
        "\\begin{equation}\n",
        "y = b_0 + b_1 \\cdot x_1 + e\n",
        "(\\#eq:regression)\n",
        "\\end{equation}\n",
        "\n",
        "where $y$ is the outcome, $b_0$ is the intercept, $b_1$ is the slope of the association between the predictor ($x_1$) and outcome, and $e$ is the error term.\\index{multiple regression}\n"
      ],
      "id": "da838ac0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "regression1 <- data.frame(\n",
        "  \"y\" = c(7, 13, 29, 10),\n",
        "  \"x1\" = c(1, 2, 7, 2))\n",
        "\n",
        "regression2 <- data.frame(\n",
        "  \"y\" = c(7, 13, 29, 10),\n",
        "  \"x1\" = c(1, 2, 7, 2),\n",
        "  \"x2\" = c(3, 5, 1, 2))\n",
        "\n",
        "regression1_model <- lm(\n",
        "  y ~ x1,\n",
        "  data = regression1)\n",
        "\n",
        "regression1_intercept <- regression1_model$coefficients[[1]]\n",
        "regression1_slope <- regression1_model$coefficients[[2]]\n",
        "regression1_rsquare <- summary(regression1_model)$r.squared\n",
        "\n",
        "regression2_model <- lm(y ~ x1 + x2, data = regression2)\n",
        "regression2_intercept <- regression2_model$coefficients[[1]]\n",
        "regression2_slope1 <- regression2_model$coefficients[[2]]\n",
        "regression2_slope2 <- regression2_model$coefficients[[3]]\n",
        "regression2_rsquare <- summary(regression2_model)$r.squared"
      ],
      "id": "287ffcdd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pseudo-Prediction {#pseudoPrediction}\n",
        "\n",
        "Consider the following example where you have one predictor and one outcome, as shown in Table \\@ref(tab:regression1).\\index{multiple regression}\n"
      ],
      "id": "d139e4b2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "kable(regression1,\n",
        "      col.names = c(\"y\",\"x1\"),\n",
        "      caption = \"Example Data of Predictor (x1) and Outcome (y) Used for Regression Model.\",\n",
        "      booktabs = TRUE)"
      ],
      "id": "744b2a63",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the data, the best fitting regression model is: $y = `r apa(regression1_intercept, decimals = 2)` + `r apa(regression1_slope, decimals = 2)` \\cdot x_1$.\\index{multiple regression}\n",
        "In this example, the $R^2$ is $`r apa(regression1_rsquare, decimals = 2)`$.\\index{multiple regression}\\index{coefficient of determination}\\index{R^2@$R^2$!zzzzz@\\igobble \\mid seealso{coefficient of determination}}\n",
        "The equation is not a perfect prediction, but with a single predictor, it captures the majority of the variance in the outcome.\\index{multiple regression}\\index{coefficient of determination}\n",
        "\n",
        "Now consider the following example where you add a second predictor to the data above, as shown in Table \\@ref(tab:regression2).\\index{multiple regression}\n"
      ],
      "id": "8e85fe33"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "kable(regression2,\n",
        "      col.names = c(\"y\",\"x1\",\"x2\"),\n",
        "      caption = \"Example Data of Predictors (x1 and x2) and Outcome (y) Used for Regression Model.\",\n",
        "      booktabs = TRUE)"
      ],
      "id": "4ff327be",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the second predictor, the best fitting regression model is: $y = `r apa(regression2_intercept, decimals = 2)` + `r apa(regression2_slope1, decimals = 2)` \\cdot x_1 + `r apa(regression2_slope2, decimals = 2)` \\cdot x_2$.\\index{multiple regression}\n",
        "In this example, the $R^2$ is $`r apa(regression2_rsquare, decimals = 2)`$.\n",
        "The equation with the second predictor provides a perfect prediction of the outcome.\\index{multiple regression}\\index{coefficient of determination}\n",
        "\n",
        "Providing perfect prediction with the right set of predictors is the dream of multiple regression.\\index{multiple regression}\n",
        "So, in psychology, we often add predictors to incrementally improve prediction.\\index{multiple regression}\n",
        "Knowing how much variance would be accounted for by random chance follows Equation \\@ref(eq:predictionByChance):\\index{multiple regression}\n",
        "\n",
        "\\begin{equation}\n",
        "E(R^2) = \\frac{K}{n-1}\n",
        "(\\#eq:predictionByChance)\n",
        "\\end{equation}\n",
        "\n",
        "where $E(R^2)$ is the expected value of $R^2$ (the proportion of variance explained), $K$ is the number of predictors, and $n$ is the sample size.\\index{multiple regression}\\index{coefficient of determination}\n",
        "The formula demonstrates that the more predictors in the regression model, the more variance will be accounted for by chance.\\index{multiple regression}\\index{coefficient of determination}\n",
        "With many predictors and a small sample, you can account for a large share of the variance merely by chance—this would be an example of pseudo-prediction.\\index{multiple regression}\\index{coefficient of determination}\\index{prediction!pseudo-}\n",
        "\n",
        "As an example, consider that we have 13 predictors to predict behavior problems for 43 children.\\index{multiple regression}\\index{coefficient of determination}\\index{prediction!pseudo-}\n",
        "Assume that, with 13 predictors, we explain 38% of the variance ($R^2 = .38; r = .62$).\\index{multiple regression}\\index{coefficient of determination}\\index{prediction!pseudo-}\n",
        "Explaining more than 20–30% of the variance can be a big deal in psychology.\\index{multiple regression}\\index{coefficient of determination}\\index{prediction!pseudo-}\n",
        "We explained a lot of the variance in the outcome, but it is important to consider how much variance could have been explained by random chance: $E(R^2) = \\frac{K}{n-1} = \\frac{13}{43 - 1} = .31$.\\index{multiple regression}\\index{coefficient of determination}\\index{spurious}\\index{prediction!pseudo-}\n",
        "We expect to explain 31% of the variance, by chance, in the outcome.\\index{multiple regression}\\index{coefficient of determination}\\index{prediction!pseudo-}\n",
        "So, 82% of the variance explained was likely spurious.\\index{multiple regression}\\index{coefficient of determination}\\index{spurious}\\index{prediction!pseudo-}\n",
        "As the sample size increases, the spuriousness decreases.\\index{multiple regression}\\index{coefficient of determination}\\index{spurious}\n",
        "Adjusted $R^2$ accounts for the number of predictors in the model, based on how much would be expected to be accounted for by chance.\\index{multiple regression}\\index{adjusted $R^2$}\n",
        "But adjusted $R^2$ also has its problems.\\index{multiple regression}\\index{adjusted $R^2$}\n",
        "\n",
        "### Multicollinearity {#multiCollinearity}\n",
        "\n",
        "*Multicollinearity* occurs when two or more predictors in a regression model are highly correlated.\\index{multicollinearity}\\index{multiple regression}\n",
        "The problem is that it makes it challenging to estimate the regression coefficients accurately.\\index{multicollinearity}\\index{multiple regression}\n",
        "\n",
        "Multicollinearity in multiple regression is depicted conceptually in Figure \\@ref(fig:multipleRegressionMulticollinearity).\\index{multicollinearity}\\index{multiple regression}\n"
      ],
      "id": "81af64e6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "knitr::include_graphics(\"./Images/multipleRegressionMulticollinearity.png\")"
      ],
      "id": "6a9296bf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consider the following example where you have two predictors and one outcome, as shown in Table \\@ref(tab:regression3).\\index{multicollinearity}\\index{multiple regression}\n"
      ],
      "id": "f7918e49"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "regression3 <- data.frame(\n",
        "  \"y\" = c(9, 11, 17, 3, 21, 13),\n",
        "  \"x1\" = c(2, 3, 4, 1, 5, 3.5),\n",
        "  \"x2\" = c(4, 6, 8, 2, 10, 7))"
      ],
      "id": "019bcfb2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "kable(regression3,\n",
        "      col.names = c(\"y\",\"x1\",\"x2\"),\n",
        "      caption = \"Example Data of Predictors (x1 and x2) and Outcome (y) Used for Regression Model.\",\n",
        "      booktabs = TRUE)"
      ],
      "id": "b2867b8d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The second measure is not very good—it is exactly twice the value of the first measure.\\index{multicollinearity}\\index{multiple regression}\n",
        "This means that there are different prediction equation possibilities that are equally good—see Equations in \\@ref(eq:multicollinearity):\\index{multicollinearity}\\index{multiple regression}\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  2x_2 &= y \\\\\n",
        "  0x_1 + 2x_2 &= y \\\\\n",
        "  4x_1 &= y \\\\\n",
        "  4x_1 + 0x_2 &= y \\\\\n",
        "  2x_1 + 1x_2 &= y \\\\\n",
        "  5x_1 - 0.5x_2 &= y \\\\\n",
        "  ...\n",
        "&= y\n",
        "\\end{aligned}\n",
        "(\\#eq:multicollinearity)\n",
        "$$\n",
        "\n",
        "Then, what are the regression coefficients?\\index{multicollinearity}\\index{multiple regression}\n",
        "We do not know, and we could come up with arbitrary estimates with an enormous standard error around each estimate.\\index{multicollinearity}\\index{multiple regression}\n",
        "Any predictors that have a correlation above ~ $r = .30$ with each other could have an impact on the confidence interval of the regression coefficient.\\index{multicollinearity}\\index{multiple regression}\n",
        "As the correlations among the predictors increase, the chance of getting an arbitrary answer increases, sometimes called \"bouncing betas.\"\\index{multicollinearity}\\index{multiple regression}\n",
        "So, it is important to examine a correlation matrix of the predictors before putting them in the same regression model.\\index{multicollinearity}\\index{multiple regression}\n",
        "You can also examine indices such as variance inflation factor (VIF).\\index{multicollinearity}\\index{multiple regression}\n",
        "\n",
        "Generalized VIF (GVIF) values are estimated below using the `car` package [@R-car].\\index{multicollinearity}\n"
      ],
      "id": "fad7ac63"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "set.seed(52242)\n",
        "mydataSDT$collinearPredictor <- mydataSDT$ndka + \n",
        "rnorm(nrow(mydataSDT), sd = 20)\n",
        "\n",
        "collinearRegression_model <- lm(\n",
        "  s100b ~ ndka + gender + age + wfns + collinearPredictor,\n",
        "  data = mydataSDT)\n",
        "\n",
        "vif(collinearRegression_model)"
      ],
      "id": "4917ce4a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To address multicollinearity, you can drop a redundant predictor or you can also use [principal component analysis](#pca) or [factor analysis](#factorAnalysis) of the predictors to reduce the predictors down to a smaller number of meaningful predictors.\\index{multicollinearity}\\index{factor analysis}\\index{principal component analysis}\n",
        "For a meaningful answer in a regression framework that is precise and confident, you need a low level of intercorrelation among predictors, unless you have a very large sample size.\\index{multicollinearity}\\index{multiple regression}\n",
        "\n",
        "However, multicollinearity does not bias parameter estimates (i.e., multicollinearity does not lead to [mean error](#meanError)).\\index{multicollinearity}\\index{multiple regression}\\index{bias}\\index{mean error}\n",
        "Instead, [multicollinearity increases the uncertainty (i.e., standard errors) around the parameter estimates](https://janhove.github.io/analysis/2019/09/11/collinearity) (archived at https://perma.cc/DJ7L-TCUK), which makes it more challenging to detect an effect as statistically significant.\\index{multicollinearity}\\index{multiple regression}\\index{uncertainty}\n",
        "[Some forms of multicollinearity are ignorable](https://statisticalhorizons.com/multicollinearity/) (archived at https://perma.cc/2JV5-2QEZ), including when the multicollinearity is among (a) the control variables rather than the variables of interest, (b) the powers (e.g., quadratic term) or products (e.g., interaction term) of other variables, or (c) dummy-coded categories.\\index{multicollinearity}\\index{multiple regression}\n",
        "Ultimately, [it is important to examine the question of interest, even if that means inclusion of predictors that are inter-correlated in a regression model](https://janhove.github.io/analysis/2019/09/11/collinearity) (archived at https://perma.cc/DJ7L-TCUK).\\index{multicollinearity}\\index{multiple regression}\n",
        "However, it would not make sense to include two predictors that are perfectly correlated, because they are redundant.\\index{multicollinearity}\\index{multiple regression}\n",
        "\n",
        "## Ways to Improve Prediction Accuracy {#waysToImprovePredictionAccuracy}\n",
        "\n",
        "On the whole, experts' predictions are inaccurate.\n",
        "Experts' predictions from many different domains tend to be inaccurate, including political scientists [@Tetlock2017], physicians [@Koehler2002], clinical psychologists [@Oskamp1965], stock market traders and corporate financial officers [@Skala2008], seismologists' predictions of earthquakes [@Hough2016], economists' predictions about the economy [@Makridakis2009], lawyers [@Koehler2002], and business managers [@Russo1992].\\index{prediction!improving accuracy of}\n",
        "The most common pattern of experts' predictions is that they show overextremity, that is, their predictions have probability judgments that tend to be too extreme, as described in Section \\@ref(calibration).\\index{calibration}\n",
        "Overextremity of experts' predictions likely reflects over-confidence.\\index{calibration}\\index{over-confidence}\n",
        "The degree of confidence of a person's predictions is often not a good indicator of the accuracy of their predictions [and confidence and prediction accuracy are sometimes inversely associated; @Silver2012].\\index{over-confidence}\n",
        "Cognitive biases including the anchoring bias [@Tversky1974], the confirmation bias [@Hoch1985; @Koriat1980], and [base rate](#baseRate) neglect [@Eddy1982; @Koehler2002] could contribute to over-confidence of predictions.\\index{bias!cognitive}\\index{base rate!neglect}\\index{bias!confirmatory}\n",
        "[Poorly calibrated](#calibration) predictions are especially likely when the [base rate](#baseRate) is very low (e.g., suicide), as is often the case in clinical psychology, or when the [base rate](#baseRate) is very high [@Koehler2002].\\index{calibration}\\index{base rate}\n",
        "\n",
        "Nevertheless, there are some domains that have shown greater predictive accuracy, from which we may learn what practices may lead to greater accuracy.\\index{prediction!improving accuracy of}\n",
        "For instance, experts have shown stronger predictive accuracy in weather forecasting [@Murphy1984], horse race betting [@Johnson2001], and playing the card game of bridge [@Keren1987], but see @Koehler2002 for exceptions.\\index{prediction!improving accuracy of}\n",
        "\n",
        "Here are some potential ways to improve the accuracy (and honesty) of predictions and judgments:\\index{prediction!improving accuracy of}\n",
        "\n",
        "- Provide appropriate anchoring of your predictions to the [base rate](#baseRate) of the phenomenon you are predicting.\\index{prediction!improving accuracy of}\\index{base rate}\n",
        "To the extent that the [base rate](#baseRate) of the event you are predicting is low, more extreme evidence should be necessary to consistently and accurately predict that the event will occur.\\index{prediction!improving accuracy of}\\index{base rate}\n",
        "Applying [Bayes' theorem](#bayesTheorem) and Bayesian approaches can help you appropriately weigh [base rate](#baseRate) and evidence.\\index{prediction!improving accuracy of}\\index{base rate}\\index{Bayesian!Bayes' theorem}\n",
        "- Include multiple predictors, ideally from different measures and measurement methods.\\index{prediction!improving accuracy of}\\index{methods!multiple}\n",
        "Include the predictors with the strongest validity based on theory of the causal process and based on [criterion-related validity](#criterionValidity).\\index{prediction!improving accuracy of}\\index{validity!criterion}\n",
        "- When possible, aggregate multiple perspectives of predictions, especially predictions made independently (from different people/methods/etc.).\\index{prediction!improving accuracy of}\\index{aggregation}\\index{methods!multiple}\n",
        "The \"wisdom of the crowd\" is often more accurate than individuals' predictions, including predictions by so-called \"experts\" [@Silver2012].\\index{prediction!improving accuracy of}\n",
        "- A goal of prediction is to capture as much signal as possible and as little noise (error) as possible [@Silver2012].\\index{prediction!improving accuracy of}\n",
        "Parsimony (i.e., not having too many predictors) can help reduce the amount of error variance captured by the prediction model.\\index{prediction!improving accuracy of}\\index{parsimony}\n",
        "However, to accurately model complex systems like human behavior, the brain, etc., complex models may be necessary.\\index{prediction!improving accuracy of}\n",
        "Nevertheless, strong theory of the causal processes and dynamics may be necessary to develop accurate complex models.\\index{prediction!improving accuracy of}\n",
        "- Although incorporating theory can be helpful, provide more weight to empiricism than to theory, until our theories and measures are stronger.\\index{prediction!improving accuracy of}\\index{theory}\\index{empiricism}\n",
        "Ideally, we would use theory to design a model that mirrors the causal system, with accurate measures of each process in the system, so we could make accurate predictions.\\index{prediction!improving accuracy of}\\index{theory}\n",
        "However, as described in Section \\@ref(theoryEmpiricism), our psychological theories of the causal processes that influence outcomes are not yet very strong.\\index{prediction!improving accuracy of}\\index{theory}\n",
        "Until we have stronger theories that specify the causal process for a given outcome, and until we have accurate measures of those causal processes, [actuarial](#actuarialPrediction) approaches are likely to be most accurate, as discussed in Chapter \\@ref(actuarial).\\index{prediction!improving accuracy of}\\index{theory}\\index{empiricism}\\index{actuarial}\n",
        "At the same time, keep in mind that measures in psychology, and their resulting data, are often noisy.\\index{prediction!improving accuracy of}\n",
        "As a result, theoretically (conceptually) informed empirical approaches may lead to more accuracy than empiricism alone.\\index{prediction!improving accuracy of}\\index{theory}\\index{empiricism}\n",
        "- Use an empirically validated and cross-validated [statistical algorithm](#actuarial) to combine information from the predictors in a formalized way.\n",
        "Give each predictor appropriate weight in the statistical algorithm, according to its strength of association with the outcome.\\index{prediction!improving accuracy of}\\index{theory}\\index{empiricism}\\index{actuarial}\\index{validity!criterion}\n",
        "Use measures with strong [reliability](#reliability) and [validity](#validity) for assessing these processes to be used in the algorithm.\\index{prediction!improving accuracy of}\\index{reliability}\\index{validity}\n",
        "Cross-validation will help reduce the likelihood that your model is fitting to noise and will maximize the likelihood that the model predicts accurately when applied to new data (i.e., the model's predictions accurately generalize), as described in Section \\@ref(modelAccuracy-actuarial).\\index{prediction!improving accuracy of}\\index{cross-validation}\n",
        "- When presenting your predictions, acknowledge what you do not know.\\index{prediction!improving accuracy of}\\index{uncertainty}\n",
        "- Express your predictions in terms of probabilistic estimates and present the uncertainty in your predictions with confidence intervals [even though bolder, more extreme predictions tend to receive stronger television ratings; @Silver2012].\\index{prediction!improving accuracy of}\\index{uncertainty}\\index{confidence interval}\n",
        "- Qualify your predictions by identifying and noting counter-examples that would not be well fit by your prediction model, such as extreme cases, edge cases, and \"broken leg\" [@Meehl1957] cases.\\index{prediction!improving accuracy of}\\index{broken leg}\n",
        "- Provide clear, consistent, and timely feedback on the outcomes of the predictions to the people making the predictions [@Bolger2004].\\index{prediction!improving accuracy of}\n",
        "- Be self-critical about your predictions.\\index{prediction!improving accuracy of}\n",
        "Update your judgments based on their accuracy, rather than trying to confirm your beliefs [@Atanasov2020].\\index{prediction!improving accuracy of}\n",
        "- In addition to considering the accuracy of the prediction, consider the quality of the prediction *process*, especially when random chance is involved to a degree (such as in poker) [@Silver2012].\\index{prediction!improving accuracy of}\n",
        "- Work to identify and mitigate potential blindspots; be aware of cognitive biases, such as confirmation bias and [base rate](#baseRate) neglect.\\index{prediction!improving accuracy of}\\index{bias!cognitive}\\index{base rate!neglect}\\index{bias!confirmatory}\n",
        "- Evaluate for the possibility of [bias](#bias) in the predictions or in the tests from which the predictions are derived.\\index{bias}\n",
        "Correct for any [test bias](#bias).\\index{bias}\n",
        "\n",
        "## Conclusion {#conclusion-prediction}\n",
        "\n",
        "Human behavior is challenging to predict.\n",
        "People commonly make cognitive pseudo-prediction errors, such as the [confusion of inverse probabilities](#inverseFallacy).\\index{prediction!pseudo-}\\index{confusion of the inverse}\\index{probability!inverse conditional}\n",
        "People also tend to ignore [base rates](#baseRate) when making predictions.\\index{base rate!neglect}\n",
        "When the [base rate](#baseRate) of a behavior is very low or very high, you can be highly accurate in predicting the behavior by [predicting from the base rate](#predictingFromBaseRate).\\index{base rate}\\index{base rate!predicting from}\n",
        "Thus, you cannot judge how accurate your prediction is until you know how accurate your predictions would be by [random chance](#accuracyByChance).\\index{percent accuracy!by chance}\n",
        "Moreover, maximizing [percent accuracy](#percentAccuracy) may not be the ultimate goal because [different errors have different costs](#differentErrorsDifferentCosts).\\index{percent accuracy}\\index{prediction!prediction error!costs of}\n",
        "Though there are many types of accuracy, there are two broad types: [discrimination](#discrimination) and [calibration](#calibration)—and they are orthogonal.\\index{discrimination}\\index{calibration}\n",
        "[Discrimination](#discrimination) accuracy is frequently evaluated with the [area under the receiver operating characteristic curve](#auc), or with [sensitivity](#sensitivity) and [specificity](#specificity), or [standardized regression coefficients](#standardizedRegressionCoefficient).\\index{discrimination}\\index{receiver operating characteristic curve!area under the curve}\\index{sensitivity}\\index{specificity}\\index{standardized regression coefficient}\n",
        "[Calibration](#calibration) accuracy is frequently evaluated graphically and with various indices.\\index{calibration}\n",
        "[Sensitivity](#sensitivity) and [specificity](#specificity) [depend on the cutoff](#accuracyCutoff).\\index{sensitivity}\\index{specificity}\\index{cutoff}\n",
        "Therefore, the [optimal cutoff](#optimalCutoff) depends on the purposes of the assessment and how much one weights the various costs of the different types of errors: [false negatives](#falseNegative) and [false positives](#falsePositive).\\index{cutoff!optimal}\\index{prediction!prediction error!costs of}\\index{false negative}\\index{false positive}\n",
        "It is important to evaluate both [discrimination](#discrimination) and [calibration](#calibration) when evaluating prediction accuracy.\\index{discrimination}\\index{calibration}\n",
        "\n",
        "## Suggested Readings {#readings-prediction}\n",
        "\n",
        "@Steyerberg2010; @Meehl1955; @Treat2023; @Wiggins1973\n",
        "\n",
        "## Exercises {#exercises-prediction}\n"
      ],
      "id": "aa56c0b6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "library(\"MOTE\")"
      ],
      "id": "a0ec92cb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load Data ---------------------------------------------------------------\n",
        "\n",
        "titanic <- read_csv(here(\"Data\", \"titanic.csv\"))"
      ],
      "id": "cfa31256",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ROC Curve ---------------------------------------------------------------\n",
        "\n",
        "rocCurve_ex <- roc(data = titanic, response = survived, predictor = prediction, smooth = FALSE)\n",
        "plot(rocCurve_ex, legacy.axes = TRUE)\n",
        "coords(rocCurve_ex, x = \"best\", best.method = \"youden\") #Youden's index (max combination of sensitivity and specificity) is at threshold of 0.205, where sensitivity is 0.65 and specificity is 0.80\n",
        "coords(rocCurve_ex, x = \"best\", best.method = \"closest.topleft\") #The point closest to the top-left part of the ROC plot with perfect sensitivity or specificity: min((1 - sensitivity)^2 + (1 - specificity)^2)\n",
        "\n",
        "rocCurveSmooth_ex <- roc(data = titanic, response = survived, predictor = prediction, smooth = TRUE)\n",
        "plot(rocCurveSmooth_ex, legacy.axes = TRUE)"
      ],
      "id": "f3a682a2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Overall Prediction Accuracy ---------------------------------------------\n",
        "\n",
        "meanError_ex <- meanError(predicted = titanic$prediction, actual = titanic$survived)\n",
        "meanAbsoluteError_ex <- meanAbsoluteError(predicted = titanic$prediction, actual = titanic$survived)\n",
        "meanSquaredError_ex <- meanSquaredError(predicted = titanic$prediction, actual = titanic$survived)\n",
        "rootMeanSquaredError_ex <- rootMeanSquaredError(predicted = titanic$prediction, actual = titanic$survived)\n",
        "meanPercentageError_ex <- meanPercentageError(predicted = titanic$prediction, actual = titanic$survived, dropUndefined = TRUE)\n",
        "meanAbsolutePercentageError_ex <- meanAbsolutePercentageError(predicted = titanic$prediction, actual = titanic$survived, dropUndefined = TRUE)\n",
        "symmetricMeanAbsolutePercentageError_ex <- symmetricMeanAbsolutePercentageError(predicted = titanic$prediction, actual = titanic$survived)\n",
        "meanAbsoluteScaledError_ex <- meanAbsoluteScaledError(predicted = titanic$prediction, actual = titanic$survived)\n",
        "rootMeanSquaredLogError_ex <- rootMeanSquaredLogError(predicted = titanic$prediction, actual = titanic$survived, dropUndefined = TRUE)\n",
        "\n",
        "summary(lm(survived ~ prediction, data = titanic))\n",
        "rsquared_ex <- summary(lm(survived ~ prediction, data = titanic))$r.squared\n",
        "rsquaredAdj_ex <- summary(lm(survived ~ prediction, data = titanic))$adj.r.squared\n",
        "predictiveRsquaredValue_ex <- predictiveRSquared(predicted = titanic$prediction, actual = titanic$survived)"
      ],
      "id": "ddcae2e6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Discrimination: Area under the ROC curve (AUC)\n",
        "rocCurve_ex$auc\n",
        "\n",
        "auc_ex <- rocCurve_ex$auc"
      ],
      "id": "f2162e61",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Calibration Plot (see here: https://perma.cc/6J3J-69G7)\n",
        "val.prob(titanic$prediction, titanic$survived)"
      ],
      "id": "8766661c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "g1_ex <- mutate(titanic, bin = cut_number(prediction, 10)) %>%\n",
        "  # Bin prediction into 10ths\n",
        "  group_by(bin) %>%\n",
        "  mutate(n = length(na.omit(prediction)), # Get ests and CIs\n",
        "         bin_pred = mean(prediction, na.rm = TRUE),\n",
        "         bin_prob = mean(survived, na.rm = TRUE),\n",
        "         se = sd(survived, na.rm = TRUE) / sqrt(n),\n",
        "         ul = bin_prob + qnorm(.975) * se,\n",
        "         ll = bin_prob - qnorm(.975) * se) %>%\n",
        "  ungroup() %>%\n",
        "  ggplot(aes(x = bin_pred, y = bin_prob, ymin = ll, ymax = ul)) +\n",
        "  geom_pointrange(size = 0.5, color = \"black\") +\n",
        "  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.1)) +\n",
        "  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.1)) +\n",
        "  geom_abline() + # 45 degree line indicating perfect calibration\n",
        "  geom_smooth(method = \"lm\", se = FALSE, linetype = \"dashed\",\n",
        "              color = \"black\", formula = y~-1 + x) +\n",
        "  # straight line fit through estimates\n",
        "  geom_smooth(aes(x = prediction, y = survived),\n",
        "              color = \"red\", se = FALSE, method = \"loess\") +\n",
        "  # loess fit through estimates\n",
        "  xlab(\"\") +\n",
        "  ylab(\"Observed Probability\") +\n",
        "  theme_minimal()+\n",
        "  xlab(\"Predicted Probability\")\n",
        "\n",
        "g2_ex <- ggplot(titanic, aes(x = prediction)) +\n",
        "  geom_histogram(fill = \"black\", bins = 200) +\n",
        "  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.1)) +\n",
        "  xlab(\"Histogram of Predicted Probability\") +\n",
        "  ylab(\"\") +\n",
        "  theme_minimal() +\n",
        "  theme(panel.grid.minor = element_blank())\n",
        "\n",
        "g_ex <- arrangeGrob(g1_ex, g2_ex, respect = TRUE, heights = c(1, 0.25), ncol = 1)\n",
        "grid.arrange(g_ex)"
      ],
      "id": "0a3985d1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Calibration: Brier Scores\n",
        "val.prob(titanic$prediction, titanic$survived, pl = FALSE)[\"Brier\"]\n",
        "\n",
        "#Calibration: Hosler-Lemeshow Test\n",
        "hoslem.test(titanic$survived, titanic$prediction, g = 2)\n",
        "hoslem.test(titanic$survived, titanic$prediction, g = 4)\n",
        "hoslem.test(titanic$survived, titanic$prediction, g = 6)\n",
        "hoslem.test(titanic$survived, titanic$prediction, g = 8)\n",
        "hoslem.test(titanic$survived, titanic$prediction, g = 10)\n",
        "\n",
        "#Calibration: Spiegelhalter's z\n",
        "val.prob(titanic$prediction, titanic$survived, pl = FALSE)[\"S:z\"]; val.prob(titanic$prediction, titanic$survived, pl = FALSE)[\"S:p\"]\n",
        "\n",
        "calibrationZ_ex <- val.prob(titanic$prediction, titanic$survived, pl = FALSE)[\"S:z\"]\n",
        "calibrationP_ex <- val.prob(titanic$prediction, titanic$survived, pl = FALSE)[\"S:p\"]"
      ],
      "id": "e1c3dacd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set a Cutoff ------------------------------------------------------------\n",
        "\n",
        "cutoff_ex <- 0.5\n",
        "\n",
        "titanic$diagnosis <- NA\n",
        "titanic$diagnosis[titanic$prediction < cutoff_ex] <- 0\n",
        "titanic$diagnosis[titanic$prediction >= cutoff_ex] <- 1"
      ],
      "id": "c292c0d0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prediction Accuracy at the Cutoff ---------------------------------------\n",
        "\n",
        "table(titanic$diagnosis, titanic$survived)\n",
        "\n",
        "TPvalue_ex <- length(which(titanic$diagnosis == 1 & titanic$survived == 1))\n",
        "TNvalue_ex <- length(which(titanic$diagnosis == 0 & titanic$survived == 0))\n",
        "FPvalue_ex <- length(which(titanic$diagnosis == 1 & titanic$survived == 0))\n",
        "FNvalue_ex <- length(which(titanic$diagnosis == 0 & titanic$survived == 1))\n",
        "\n",
        "Nvalue_ex <- sampleSize(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "\n",
        "selectionRatioValue_ex <- selectionRatio(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "baseRateValue_ex <- baseRate(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "\n",
        "percentAccuracyValue_ex <- percentAccuracy(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "percentAccuracyByChanceValue_ex <- percentAccuracyByChance(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "percentAccuracyPredictingFromBaseRateValue_ex <- percentAccuracyPredictingFromBaseRate(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "relativeImprovementOverChanceValue_ex <- relativeImprovementOverChance(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "relativeImprovementOverPredictingFromBaseRateValue_ex <- relativeImprovementOverPredictingFromBaseRate(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "\n",
        "sensitivityValue_ex <- TPrate_ex <- sensitivity(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "specificityValue_ex <- TNrate_ex <- specificity(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "\n",
        "FNrateValue_ex <- falseNegativeRate(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "FPrateValue_ex <- falsePositiveRate(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "\n",
        "positivePredictiveValueValue_ex <- positivePredictiveValue(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "negativePredictiveValueValue_ex <- negativePredictiveValue(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "\n",
        "falseDiscoveryRateValue_ex <- falseDiscoveryRate(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "falseOmissionRateValue_ex <- falseOmissionRate(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "\n",
        "diagnosticOddsRatioValue_ex <- diagnosticOddsRatio(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "positiveLikelihoodRatioValue_ex <- positiveLikelihoodRatio(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "negativeLikelihoodRatioValue_ex <- negativeLikelihoodRatio(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "\n",
        "youdenJValue_ex <- youdenJ(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "balancedAccuracyValue_ex <- balancedAccuracy(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "f1ScoreValue_ex <- fScore(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "mccValue_ex <- mcc(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "\n",
        "dPrimeValue_ex <- dPrimeSDT(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "betaValue_ex <- betaSDT(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "cValue_ex <- cSDT(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "AValue_ex <- aSDT(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "bValue_ex <- bSDT(TP = TPvalue_ex, TN = TNvalue_ex, FP = FPvalue_ex, FN = FNvalue_ex)\n",
        "\n",
        "predictingFromTheBaseRateValue_ex <- max(baseRateValue_ex, 1 - baseRateValue_ex) * 100\n",
        "increasedAccuracyValue_ex <- percentAccuracyValue_ex - predictingFromTheBaseRateValue_ex\n",
        "\n",
        "differenceBetweenPredictedAndObserved_ex <- miscalibration(predicted = titanic$prediction, actual = titanic$survived, cutoff = cutoff_ex)"
      ],
      "id": "67f49824",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Decision Theory Approach to Cutoff Specification ------------------------\n",
        "\n",
        "#Overall utility of a specific cutoff value: utilities-weighted sum of the probabilities of the four decision-making outcomes\n",
        "\n",
        "Uoverall(BR = baseRateValue_ex, HR = TPrate_ex, FAR = FPrateValue_ex, UH = 1, UM = 0, UCR = 0.75, UFA = 0.25)\n",
        "Uoverall(BR = baseRateValue_ex, HR = TPrate_ex, FAR = FPrateValue_ex, UH = 1, UM = 0, UCR = 1, UFA = 0)\n",
        "Uoverall(BR = baseRateValue_ex, HR = TPrate_ex, FAR = FPrateValue_ex, UH = 0.75, UM = 0.25, UCR = 1, UFA = 0)\n",
        "\n",
        "Uoverall(BR = baseRateValue_ex, HR = TPrate_ex, FAR = FPrateValue_ex, UH = 1, UM = 0, UCR = 0.25, UFA = 0)\n",
        "\n",
        "Uoverall_ex <- Uoverall(BR = baseRateValue_ex, HR = TPrate_ex, FAR = FPrateValue_ex, UH = 1, UM = 0, UCR = 0.25, UFA = 0)"
      ],
      "id": "5f9e821e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Utility ratio: user-perceived relative importance of decisions about negative versus positive cases\n",
        "\n",
        "utilityRatio(UH = 1, UM = 0, UCR = (1/2), UFA = (1/2))\n",
        "utilityRatio(UH = 1, UM = 0, UCR = (2/3), UFA = (1/3))\n",
        "utilityRatio(UH = 1, UM = 0, UCR = 0.75, UFA = 0.25)\n",
        "\n",
        "utilityRatio(UH = 1, UM = 0, UCR = 1, UFA = 0)\n",
        "\n",
        "utilityRatio(UH = 0.75, UM = 0.25, UCR = 1, UFA = 0)\n",
        "utilityRatio(UH = (2/3), UM = (1/3), UCR = 1, UFA = 0)\n",
        "utilityRatio(UH = (1/2), UM = (1/2), UCR = 1, UFA = 0)\n",
        "\n",
        "utilityRatio(UH = 1, UM = 0, UCR = (1/3), UFA = 0)\n",
        "utilityRatio(UH = 1, UM = 0, UCR = 0.25, UFA = 0)\n",
        "\n",
        "utilityRatio_ex <- utilityRatio(UH = 1, UM = 0, UCR = 0.25, UFA = 0)"
      ],
      "id": "ca839e7f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Information Theory Approach to Cutoff Specification ---------------------\n",
        "\n",
        "Igain(BR = baseRateValue_ex, HR = TPrate_ex, FAR = FPrateValue_ex)\n",
        "Igain_ex <- Igain(BR = baseRateValue_ex, HR = TPrate_ex, FAR = FPrateValue_ex)"
      ],
      "id": "108aa624",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Accuracy at Every Possible Cutoff ---------------------------------------\n",
        "\n",
        "#Specify utility of each outcome\n",
        "utilityHits_ex <- 1\n",
        "utilityMisses_ex <- 0\n",
        "utilityCorrectRejections_ex <- 0.25\n",
        "utilityFalseAlarms_ex <- 0\n",
        "\n",
        "possibleCutoffs_ex <- unique(na.omit(titanic$prediction))\n",
        "possibleCutoffs_ex <- possibleCutoffs_ex[order(possibleCutoffs_ex)]\n",
        "possibleCutoffs_ex <- c(possibleCutoffs_ex, max(possibleCutoffs_ex, na.rm = TRUE) + 0.01)\n",
        "\n",
        "accuracyVariables_ex <- c(\"cutoff\", \"TP\", \"TN\", \"FP\", \"FN\", \"differenceBetweenPredictedAndObserved\")\n",
        "\n",
        "accuracyStats_ex <- data.frame(matrix(nrow = length(possibleCutoffs_ex), ncol = length(accuracyVariables_ex)))\n",
        "names(accuracyStats_ex) <- accuracyVariables_ex\n",
        "\n",
        "for(i in 1:length(possibleCutoffs_ex)){\n",
        "  cutoff <- possibleCutoffs_ex[i]\n",
        "  \n",
        "  titanic$diagnosis <- NA\n",
        "  titanic$diagnosis[titanic$prediction < cutoff] <- 0\n",
        "  titanic$diagnosis[titanic$prediction >= cutoff] <- 1\n",
        "  \n",
        "  accuracyStats_ex[i, \"cutoff\"] <- cutoff\n",
        "  accuracyStats_ex[i, \"TP\"] <- length(which(titanic$diagnosis == 1 & titanic$survived == 1))\n",
        "  accuracyStats_ex[i, \"TN\"] <- length(which(titanic$diagnosis == 0 & titanic$survived == 0))\n",
        "  accuracyStats_ex[i, \"FP\"] <- length(which(titanic$diagnosis == 1 & titanic$survived == 0))\n",
        "  accuracyStats_ex[i, \"FN\"] <- length(which(titanic$diagnosis == 0 & titanic$survived == 1))\n",
        "  \n",
        "  accuracyStats_ex[i, \"differenceBetweenPredictedAndObserved\"] <- miscalibration(predicted = titanic$prediction, actual = titanic$survived, cutoff = cutoff)\n",
        "}\n",
        "\n",
        "accuracyStats_ex$N <- sampleSize(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "\n",
        "accuracyStats_ex$selectionRatio <- selectionRatio(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "accuracyStats_ex$baseRate <- baseRate(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "\n",
        "accuracyStats_ex$percentAccuracy <- percentAccuracy(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "accuracyStats_ex$percentAccuracyByChance <- percentAccuracyByChance(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "accuracyStats_ex$relativeImprovementOverChance <- relativeImprovementOverChance(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "accuracyStats_ex$relativeImprovementOverPredictingFromBaseRate <- relativeImprovementOverPredictingFromBaseRate(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "\n",
        "accuracyStats_ex$sensitivity <- accuracyStats_ex$TPrate <- sensitivity(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "accuracyStats_ex$specificity <- accuracyStats_ex$TNrate <- specificity(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "accuracyStats_ex$FNrate <- falseNegativeRate(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "accuracyStats_ex$FPrate <- falsePositiveRate(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "\n",
        "accuracyStats_ex$positivePredictiveValue <- positivePredictiveValue(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "accuracyStats_ex$negativePredictiveValue <- negativePredictiveValue(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "accuracyStats_ex$falseDiscoveryRate <- falseDiscoveryRate(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "accuracyStats_ex$falseOmissionRate <- falseOmissionRate(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "\n",
        "accuracyStats_ex$diagnosticOddsRatio <- diagnosticOddsRatio(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "accuracyStats_ex$positiveLikelihoodRatio <- positiveLikelihoodRatio(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "accuracyStats_ex$negativeLikelihoodRatio <- negativeLikelihoodRatio(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "\n",
        "accuracyStats_ex$youdenJ <- youdenJ(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "accuracyStats_ex$balancedAccuracy <- balancedAccuracy(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "accuracyStats_ex$f1Score <- fScore(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "accuracyStats_ex$mcc <- mcc(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "\n",
        "accuracyStats_ex$dPrimeSDT <- dPrimeSDT(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "accuracyStats_ex$betaSDT <- betaSDT(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "accuracyStats_ex$cSDT <- cSDT(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "accuracyStats_ex$ASDT <- aSDT(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "accuracyStats_ex$bSDT <- bSDT(TP = accuracyStats_ex$TP, TN = accuracyStats_ex$TN, FP = accuracyStats_ex$FP, FN = accuracyStats_ex$FN)\n",
        "\n",
        "accuracyStats_ex$overallUtility <- Uoverall(BR = accuracyStats_ex$baseRate, HR = accuracyStats_ex$TPrate, FAR = accuracyStats_ex$FPrate, UH = utilityHits_ex, UM = utilityMisses_ex, UCR = utilityCorrectRejections_ex, UFA = utilityFalseAlarms_ex)\n",
        "accuracyStats_ex$utilityRatio <- utilityRatio(UH = utilityHits_ex, UM = utilityMisses_ex, UCR = utilityCorrectRejections_ex, UFA = utilityFalseAlarms_ex)\n",
        "accuracyStats_ex$informationGain <- Igain(BR = accuracyStats_ex$baseRate, HR = accuracyStats_ex$TPrate, FAR = accuracyStats_ex$FPrate)\n",
        "\n",
        "#Replace NaN and INF values with NA\n",
        "is.nan.data.frame <- function(x)\n",
        "  do.call(cbind, lapply(x, is.nan))\n",
        "\n",
        "accuracyStats_ex[is.nan.data.frame(accuracyStats_ex)] <- NA\n",
        "accuracyStats_ex <- do.call(data.frame, lapply(accuracyStats_ex, function(x) replace(x, is.infinite(x), NA)))\n",
        "\n",
        "#All accuracy stats\n",
        "accuracyStats_ex"
      ],
      "id": "47a2d55c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Youden Index (maximum combination of sensitivity and specificity)\n",
        "youdenIndex_ex <- coords(roc(data = titanic, response = survived, predictor = prediction, smooth = FALSE), x = \"best\", best.method = \"youden\")[[1]]\n",
        "closestToTheTopLeft_ex <- coords(roc(data = titanic, response = survived, predictor = prediction, smooth = FALSE), x = \"best\", best.method = \"closest.topleft\")[[1]]\n",
        "\n",
        "#Accuracy stats at cutoff of Youden Index\n",
        "accuracyStats_ex[head(which(accuracyStats_ex$cutoff >= youdenIndex_ex), 1),]\n",
        "accuracyStats_ex[which(accuracyStats_ex$youdenJ == max(accuracyStats_ex$youdenJ, na.rm = TRUE)),]"
      ],
      "id": "2f316dae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Accuracy stats at cutoff where the ROC plot is closest to the Top Left\n",
        "accuracyStats_ex[head(which(accuracyStats_ex$cutoff >= closestToTheTopLeft_ex), 1),]"
      ],
      "id": "1ca2bc7a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Cutoff that optimizes:\n",
        "percentAccuracyCutoff_ex <- accuracyStats_ex$cutoff[which(accuracyStats_ex$percentAccuracy == max(accuracyStats_ex$percentAccuracy, na.rm = TRUE))]\n",
        "percentAccuracyByChanceCutoff_ex <- accuracyStats_ex$cutoff[which(accuracyStats_ex$percentAccuracyByChance == max(accuracyStats_ex$percentAccuracyByChance, na.rm = TRUE))]\n",
        "relativeImprovementOverChanceCutoff_ex <- accuracyStats_ex$cutoff[which(accuracyStats_ex$relativeImprovementOverChance == max(accuracyStats_ex$relativeImprovementOverChance, na.rm = TRUE))]\n",
        "relativeImprovementOverPredictingFromBaseRateCutoff_ex <- accuracyStats_ex$cutoff[which(accuracyStats_ex$relativeImprovementOverPredictingFromBaseRate == max(accuracyStats_ex$relativeImprovementOverPredictingFromBaseRate, na.rm = TRUE))]\n",
        "sensitivityCutoff_ex <- accuracyStats_ex$cutoff[which(accuracyStats_ex$sensitivity == max(accuracyStats_ex$sensitivity, na.rm = TRUE))]\n",
        "specificityCutoff_ex <- accuracyStats_ex$cutoff[which(accuracyStats_ex$specificity == max(accuracyStats_ex$specificity, na.rm = TRUE))]\n",
        "positivePredictiveValueCutoff_ex <- accuracyStats_ex$cutoff[which(accuracyStats_ex$positivePredictiveValue == max(accuracyStats_ex$positivePredictiveValue, na.rm = TRUE))]\n",
        "negativePredictiveValueCutoff_ex <- accuracyStats_ex$cutoff[which(accuracyStats_ex$negativePredictiveValue == max(accuracyStats_ex$negativePredictiveValue, na.rm = TRUE))]\n",
        "diagnosticOddsRatioCutoff_ex <- accuracyStats_ex$cutoff[which(accuracyStats_ex$diagnosticOddsRatio == max(accuracyStats_ex$diagnosticOddsRatio, na.rm = TRUE))]\n",
        "positiveLikelihoodRatioCutoff_ex <- accuracyStats_ex$cutoff[which(accuracyStats_ex$positiveLikelihoodRatio == max(accuracyStats_ex$positiveLikelihoodRatio, na.rm = TRUE))]\n",
        "negativeLikelihoodRatioCutoff_ex <- accuracyStats_ex$cutoff[which(accuracyStats_ex$negativeLikelihoodRatio == min(accuracyStats_ex$negativeLikelihoodRatio, na.rm = TRUE))]\n",
        "youdenJCutoff_ex <- accuracyStats_ex$cutoff[which(accuracyStats_ex$youdenJ == max(accuracyStats_ex$youdenJ, na.rm = TRUE))]\n",
        "balancedAccuracyCutoff_ex <- accuracyStats_ex$cutoff[which(accuracyStats_ex$balancedAccuracy == max(accuracyStats_ex$balancedAccuracy, na.rm = TRUE))]\n",
        "f1ScoreCutoff_ex <- accuracyStats_ex$cutoff[which(accuracyStats_ex$f1Score == max(accuracyStats_ex$f1Score, na.rm = TRUE))]\n",
        "mccCutoff_ex <- accuracyStats_ex$cutoff[which(accuracyStats_ex$mcc == max(accuracyStats_ex$mcc, na.rm = TRUE))]\n",
        "dPrimeSDTCutoff_ex <- accuracyStats_ex$cutoff[which(accuracyStats_ex$dPrimeSDT == max(accuracyStats_ex$dPrimeSDT, na.rm = TRUE))]\n",
        "betaSDTCutoff_ex <- accuracyStats_ex$cutoff[which(abs(accuracyStats_ex$betaSDT) == min(abs(accuracyStats_ex$betaSDT), na.rm = TRUE))]\n",
        "cSDTCutoff_ex <- accuracyStats_ex$cutoff[which(abs(accuracyStats_ex$cSDT) == min(abs(accuracyStats_ex$cSDT), na.rm = TRUE))]\n",
        "ASDTCutoff_ex <- accuracyStats_ex$cutoff[which(accuracyStats_ex$ASDT == max(accuracyStats_ex$ASDT, na.rm = TRUE))]\n",
        "bSDTCutoff_ex <- accuracyStats_ex$cutoff[which(abs(accuracyStats_ex$bSDT) == min(abs(accuracyStats_ex$bSDT), na.rm = TRUE))]\n",
        "differenceBetweenPredictedAndObservedCutoff_ex <- accuracyStats_ex$cutoff[which(abs(accuracyStats_ex$differenceBetweenPredictedAndObserved) == min(abs(accuracyStats_ex$differenceBetweenPredictedAndObserved), na.rm = TRUE))]\n",
        "overallUtilityCutoff_ex <- accuracyStats_ex$cutoff[which(accuracyStats_ex$overallUtility == max(accuracyStats_ex$overallUtility, na.rm = TRUE))]\n",
        "informationGainCutoff_ex <- accuracyStats_ex$cutoff[which(accuracyStats_ex$informationGain == max(accuracyStats_ex$informationGain, na.rm = TRUE))]"
      ],
      "id": "018f185e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Positive and Negative Predictive Value ----------------------------------\n",
        "\n",
        "sensitivity <- .9\n",
        "specificity <- .95\n",
        "baseRate <- .05\n",
        "positivePredictiveValue <- (sensitivity*baseRate)/(sensitivity*baseRate+(1-specificity)*(1-baseRate))\n",
        "negativePredictiveValue <- (specificity*(1-baseRate))/(specificity*(1-baseRate)+(1-sensitivity)*baseRate)\n",
        "\n",
        "prYouHadCovid <- 1 - negativePredictiveValue\n",
        "prFriendHadCovid <- positivePredictiveValue"
      ],
      "id": "4f315d99",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Bayes' Theorem ----------------------------------------------------------\n",
        "\n",
        "#p(C \\mid R)=(p(R \\mid C)∙p(C))/(p(R))\n",
        "\n",
        "#p(disease \\mid positiveTest) = (p(positiveTest \\mid disease)∙p(disease))/(p(positiveTest))\n",
        "\n",
        "pDisease <- .005\n",
        "pPositiveTest <- .02\n",
        "pPositiveTestGivenDisease <- .99\n",
        "\n",
        "pDiseaseGivenPositiveTest <- (pPositiveTestGivenDisease * pDisease)/pPositiveTest * 100\n",
        "\n",
        "# Posttest Probability\n",
        "probGivenTestA_ex <- posttestProbability(pretestProb = .005, SN = .95, SP = .90)\n",
        "probGivenTestAthenB_ex <- posttestProbability(pretestProb = probGivenTestA_ex, SN = .80, SP = .95)\n",
        "\n",
        "pctGivenTestA_ex <- probGivenTestA_ex * 100\n",
        "pctGivenTestAthenB_ex <- probGivenTestAthenB_ex * 100"
      ],
      "id": "31ae5d86",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Questions {#exercisesQuestions-prediction}\n",
        "\n",
        "Note: Several of the following questions use data from the survivorship of the Titanic accident.\n",
        "The data are publicly available (https://hbiostat.org/data/; archived at https://perma.cc/B4AV-YH4V).\n",
        "The Titanic data file for these exercises is located on the book's page of the Open Science Framework (https://osf.io/3pwza) and in the GitHub repo (https://github.com/isaactpetersen/Principles-Psychological-Assessment/tree/main/Data).\n",
        "Every record in the data set represents a passenger—including the passenger's age, sex, passenger class, number of siblings/spouses aboard (`sibsp`), number of parents/children aboard (`parch`) and, whether the passenger survived the accident.\n",
        "I used these variables to create a prediction model (based on a logistic regression model using Leave-10-out cross-validation) for whether the passenger survived the accident.\n",
        "The model's prediction for the passenger's likelihood of survival are in the variable called \"prediction\".\n",
        "\n",
        "1. What are the two main types of prediction accuracy?\n",
        "Define each.\n",
        "How can you quantify each?\n",
        "What is an example of an index that combines both main types of prediction accuracy?\n",
        "2. Provide the following indexes of overall prediction accuracy for the prediction model in predicting whether a passenger survived the Titanic:\n",
        "    a. Mean error (bias)\n",
        "    b. Mean absolute error (MAE)\n",
        "    c. Mean squared error (MSE)\n",
        "    d. Root mean squared error (RMSE)\n",
        "    e. Mean percentage error (MPE)\n",
        "    f. Mean absolute percentage error (MAPE)\n",
        "    g. Symmetric mean absolute percentage error (sMAPE)\n",
        "    h. Mean absolute scaled error (MASE)\n",
        "    i. Root mean squared log error (RMSLE)\n",
        "    j. Coefficient of determination ($R^2$)\n",
        "    k. Adjusted $R^2$ ($R^2_{adj}$)\n",
        "    l. Predictive $R^2$\n",
        "3. Based on the mean error for the prediction model you found in `2a`, what does this indicate?\n",
        "4. Create two receiver operating characteristic (ROC) curves for the prediction model in predicting whether a passenger survived the Titanic: one empirical ROC curve and one smooth ROC curve.\n",
        "5. What is the area under the ROC curve (AUC) for the prediction model in predicting whether a passenger survived the Titanic?\n",
        "What does this indicate?\n",
        "6. Create a calibration plot.\n",
        "Are the predictions well-calibrated?\n",
        "Provide empirical evidence and support your inferences with interpretation of the calibration plot.\n",
        "If the predictions are miscalibrated, describe the type of miscalibration present.\n",
        "7. You predict that a passenger survived the Titanic if their predicted probability of survival is .50 or greater.\n",
        "Create a confusion matrix (2x2 matrix of prediction accuracy) for Titanic survival using this threshold.\n",
        "Make sure to include the marginal cells.\n",
        "Label each cell.\n",
        "Enter the number and proportion in each cell.\n",
        "8. Using the 2x2 prediction matrix, identify or calculate the following:\n",
        "    a. Selection ratio\n",
        "    b. Base rate\n",
        "    c. Percent accuracy\n",
        "    d. Percent accuracy by chance\n",
        "    e. Percent accuracy predicting from the base rate\n",
        "    f. Relative improvement over chance (ROIC)\n",
        "    g. Relative improvement over predicting from the base rate\n",
        "    h. Sensitivity (true positive rate [TPR])\n",
        "    i. Specificity (true negative rate [TNR])\n",
        "    j. False negative rate (FNR)\n",
        "    k. False positive rate (FPR)\n",
        "    l. Positive predictive value (PPV)\n",
        "    m. Negative predictive value (NPV)\n",
        "    n. False discovery rate (FDR)\n",
        "    o. False omission rate (FOR)\n",
        "    p. Diagnostic odds ratio\n",
        "    q. Youden's J statistic\n",
        "    r. Balanced accuracy\n",
        "    s. $F_1$ score\n",
        "    t. Matthews correlation coefficient (MCC)\n",
        "    u. Positive likelihood ratio\n",
        "    v. Negative likelihood ratio\n",
        "    w. $d'$ sensitivity\n",
        "    x. $A$ (non-parametric) sensitivity \n",
        "    y. $\\beta$ bias\n",
        "    z. $c$ bias  \n",
        "    aa. $b$ (non-parametric) bias  \n",
        "    ab. Miscalibration (mean difference between predicted and observed values; based on 10 groups)  \n",
        "    ac. Information gain\n",
        "9. In terms of percent accuracy, how much more accurate are the predictions compared to predicting from the base rate?\n",
        "What would happen to sensitivity and specificity if you raise the selection ratio?\n",
        "What would happen if you lower the selection ratio?\n",
        "10. For your assessment goals, it is 4 times more important to identify survivors than to identify non-survivors.\n",
        "Consistent with these assessment goals, you specify the following utility for each of the four outcomes: hits: 1; misses: 0; correct rejections: 0.25; false alarms: 0.\n",
        "What is the utility ratio?\n",
        "What is the overall utility ($U_\\text{overall}$) of the current cutoff?\n",
        "What cutoff has the highest overall utility?\n",
        "11. Find the optimal cutoff threshold that optimizes each of the following criteria:\n",
        "    a. Youden's J statistic\n",
        "    b. Closest to the top left of the ROC curve\n",
        "    c. Percent accuracy\n",
        "    d. Percent accuracy by chance\n",
        "    e. Relative improvement over chance (ROIC)\n",
        "    f. Relative improvement over predicting from the base rate\n",
        "    g. Sensitivity (true positive rate [TPR])\n",
        "    h. Specificity (true negative rate [TNR])\n",
        "    i. Positive predictive value (PPV)\n",
        "    j. Negative predictive value (NPV)\n",
        "    k. Diagnostic odds ratio\n",
        "    l. Balanced accuracy\n",
        "    m. $F_1$ score\n",
        "    n. Matthews correlation coefficient (MCC)\n",
        "    o. Positive likelihood ratio\n",
        "    p. Negative likelihood ratio\n",
        "    q. $d'$ sensitivity\n",
        "    r. $A$ (non-parametric) sensitivity\n",
        "    s. $\\beta$ bias\n",
        "    t. $c$ bias\n",
        "    u. $b$ (non-parametric) bias\n",
        "    v. Miscalibration (mean difference between predicted and observed values; based on 10 groups)\n",
        "    w. Overall utility\n",
        "    x. Information gain\n",
        "12.\tA company develops a test that seeks to determine whether someone has been previously infected with a novel coronavirus (COVID-75) based on the presence of antibodies in their blood.\n",
        "You take the test and your test result is negative (i.e., the test says that you have not been infected).\n",
        "Your friend takes the test and their test result is positive for coronavirus (i.e., the test says that your friend has been infected).\n",
        "Assume the prevalence of the coronavirus is 5%, the sensitivity of the test is .90, and the specificity of the test is .95.\n",
        "    a. What is the probability that you actually had the coronavirus?\n",
        "    b. What is the probability that your friend actually had the coronavirus?\n",
        "    c. Your friend thinks that, given their positive test, that they have the antibodies (and thus may have immunity).\n",
        "\tWhat is the problem with your friend's logic?\n",
        "    d. What logical fallacy is your friend demonstrating?\n",
        "    e. Why is it challenging to interpret a positive test in this situation?\n",
        "13.\tYou just took a screening test for a genetic disease.\n",
        "Your test result is positive (i.e., the tests says that you have the disease).\n",
        "Assume the probability of having the disease is 0.5%, the probability of a positive test is 2%, and the probability of a positive test if you have the disease is 99%.\n",
        "What is the probability that you have the genetic disease?\n",
        "14. You just took a screening test (Test A) for a virus.\n",
        "Your test result is positive.\n",
        "Assume the base rate of the virus is .05%.\n",
        "Test A has a sensitivity of .95 and a specificity of .90.\n",
        "    a. What is your probability of having the virus after testing positive on Test A?\n",
        "    b. After getting the results back from Test A, the physician wants greater confidence regarding whether you have the virus given its low base rate, so the physician orders a second test, Test B.\n",
        "You test positive on Test B.\n",
        "Test B has a sensitivity of .80 and a specificity of .95.\n",
        "Assuming the errors of the Tests A and B are independent, what is your updated probability of having the virus?\n",
        "\n",
        "### Answers {#exercisesAnswers-prediction}\n",
        "\n",
        "1. The two main types of prediction accuracy are discrimination and calibration.\n",
        "Discrimination refers to the ability of a prediction model to separate/differentiate data into classes (e.g., survived versus died).\n",
        "Calibration for a prediction model refers to how well the predicted probability of an event (e.g., survival) matches the true probability of an event.\n",
        "You can quantify discrimination with AUC; you can quantify various aspects of discrimination with sensitivity, specificity, positive predictive value, and negative predictive value).\n",
        "You can quantify degree of (poor) calibration with Spiegelhalter's $z$, though other metrics also exist (e.g., Brier scores and the Hosmer-Lemeshow goodness-of-fit statistic).\n",
        "$R^2$ is an overall index of accuracy that combines both discrimination and calibration.\n",
        "2.\n",
        "    a. Mean error (bias): $`r format(round(meanError_ex, 4), scientific = FALSE)`$\n",
        "    b. Mean absolute error (MAE): $`r apa(meanAbsoluteError_ex, decimals = 2)`$\n",
        "    c. Mean squared error (MSE): $`r apa(meanSquaredError_ex, decimals = 2)`$\n",
        "    d. Root mean squared error (RMSE): $`r apa(rootMeanSquaredError_ex, decimals = 2)`$\n",
        "    e. Mean percentage error (MPE): undefined, but when dropping undefined values: $`r apa(meanPercentageError_ex, decimals = 2)`\\%$\n",
        "    f. Mean absolute percentage error (MAPE): undefined, but when dropping undefined values: $`r apa(meanAbsolutePercentageError_ex, decimals = 2)`\\%$\n",
        "    g. Symmetric mean absolute percentage error (sMAPE): $`r apa(symmetricMeanAbsolutePercentageError_ex, decimals = 2)`\\%$\n",
        "    h. Mean absolute scaled error (MASE): $`r apa(meanAbsoluteScaledError_ex, decimals = 2)`$\n",
        "    i. Root mean squared log error (RMSLE): $`r apa(rootMeanSquaredLogError_ex, decimals = 2)`$\n",
        "    j. Coefficient of determination ($R^2$): $`r apa(rsquared_ex, decimals = 2, leading = FALSE)`$\n",
        "    k. Adjusted $R^2$ ($R^2_{adj}$): $`r apa(rsquaredAdj_ex, decimals = 2, leading = FALSE)`$\n",
        "    l. Predictive $R^2$: $`r apa(predictiveRsquaredValue_ex, decimals = 2, leading = FALSE)`$\n",
        "3.\n",
        "The small mean error/bias $(`r format(round(meanError_ex, 4), scientific = FALSE)`)$ indicates that predictions did not consistently under- or over-estimate the actual values to a considerable degree.\n",
        "4. Empirical ROC curve:\n"
      ],
      "id": "e389a14c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot(rocCurve_ex, legacy.axes = TRUE, asp = NA)"
      ],
      "id": "e7b5a723",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Smooth ROC curve:\n"
      ],
      "id": "3545fdab"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot(rocCurveSmooth_ex, legacy.axes = TRUE, asp = NA)"
      ],
      "id": "a8478dd5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. The AUC is $`r apa(auc_ex, decimals = 2, leading = FALSE)`$.\n",
        "AUC indicates the probability that a randomly selected case has a higher test result than a randomly selected control.\n",
        "Thus, the probability is $`r as.integer(auc_ex * 100)`\\%$ that a randomly selected passenger who survived had a higher predicted probability of survival (based on the prediction model) than a randomly selected passenger who died.\n",
        "The AUC of $`r apa(auc_ex, decimals = 2, leading = FALSE)`$ indicates that the prediction model was moderately accurate in terms of discrimination.\n",
        "6. Calibration plot:\n"
      ],
      "id": "73e7f0d3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grid.arrange(g_ex)"
      ],
      "id": "2ede28f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. In general, the predictions are well-calibrated.\n",
        "There is not significant miscalibration according to Spiegehalter's $z$ $(`r apa(calibrationZ_ex, decimals = 2)`, p = `r apa(calibrationP_ex, decimals = 2, leading = FALSE)`)$.\n",
        "This is verified graphically in the calibration plot, in which the predicted probabilities fall mostly near the actual/observed probabilities.\n",
        "However, based on the calibration plot, there does appear to be some miscalibration.\n",
        "When the predicted probability of survival was ~60%, the actual probability of survival was lower (~40%).\n",
        "This pattern of miscalibration is known as overprediction, as depicted in Figure 1 of @Lindhiem2020.\n",
        "\n",
        "7. The 2x2 prediction matrix is below:\n"
      ],
      "id": "04cc182b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "knitr::include_graphics(\"./Images/2x2-Matrix.png\")"
      ],
      "id": "88543fb7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "8.\n",
        "    a. Selection ratio: $`r apa(selectionRatioValue_ex, decimals = 2, leading = FALSE)`$\n",
        "    b. Base rate: $`r apa(baseRateValue_ex, decimals = 2, leading = FALSE)`$\n",
        "    c. Percent accuracy: $`r as.integer(percentAccuracyValue_ex)`\\%$\n",
        "    d. Percent accuracy by chance: $`r as.integer(percentAccuracyByChanceValue_ex)`\\%$\n",
        "    e. Percent accuracy predicting from the base rate: $`r as.integer(percentAccuracyPredictingFromBaseRateValue_ex)`\\%$\n",
        "    f. Relative improvement over chance (ROIC): $`r apa(relativeImprovementOverChanceValue_ex, decimals = 2, leading = FALSE)`$\n",
        "    g. Relative improvement over predicting from the base rate: $`r apa(relativeImprovementOverPredictingFromBaseRateValue_ex, decimals = 2, leading = FALSE)`$\n",
        "    h. Sensitivity (true positive rate [TPR]): $`r apa(sensitivityValue_ex, decimals = 2, leading = FALSE)`$\n",
        "    i. Specificity (true negative rate [TNR]): $`r apa(specificityValue_ex, decimals = 2, leading = FALSE)`$\n",
        "    j. False negative rate (FNR): $`r apa(FNrateValue_ex, decimals = 2, leading = FALSE)`$\n",
        "    k. False positive rate (FPR): $`r apa(FPrateValue_ex, decimals = 2, leading = FALSE)`$\n",
        "    l. Positive predictive value (PPV): $`r apa(positivePredictiveValueValue_ex, decimals = 2, leading = FALSE)`$\n",
        "    m. Negative predictive value (NPV): $`r apa(negativePredictiveValueValue_ex, decimals = 2, leading = FALSE)`$\n",
        "    n. False discovery rate (FDR): $`r apa(falseDiscoveryRateValue_ex, decimals = 2, leading = FALSE)`$\n",
        "    o. False omission rate (FOR): $`r apa(falseOmissionRateValue_ex, decimals = 2, leading = FALSE)`$\n",
        "    p. Diagnostic odds ratio: $`r apa(diagnosticOddsRatioValue_ex, decimals = 2, leading = TRUE)`$\n",
        "    q. Youden's J statistic: $`r apa(youdenJValue_ex, decimals = 2, leading = FALSE)`$\n",
        "    r. Balanced accuracy: $`r apa(balancedAccuracyValue_ex, decimals = 2, leading = FALSE)`$\n",
        "    s. $F_1$ score: $`r apa(f1ScoreValue_ex, decimals = 2, leading = TRUE)`$\n",
        "    t. Matthews correlation coefficient (MCC): $`r apa(mccValue_ex, decimals = 2, leading = FALSE)`$\n",
        "    u. Positive likelihood ratio: $`r apa(positiveLikelihoodRatioValue_ex, decimals = 2, leading = TRUE)`$\n",
        "    v. Negative likelihood ratio: $`r apa(negativeLikelihoodRatioValue_ex, decimals = 2, leading = TRUE)`$\n",
        "    w. $d'$ sensitivity: $`r apa(dPrimeValue_ex, decimals = 2)`$\n",
        "    x. $A$ (non-parametric) sensitivity: $`r apa(AValue_ex, decimals = 2)`$\n",
        "    y. $\\beta$ bias: $`r apa(betaValue_ex, decimals = 2)`$\n",
        "    z. $c$ bias: $`r apa(cValue_ex, decimals = 2)`$\n",
        "    aa. $b$ (non-parametric) bias: $`r apa(bValue_ex, decimals = 2)`$\n",
        "    ab. Miscalibration (mean difference between predicted and observed values): $`r apa(differenceBetweenPredictedAndObserved_ex, decimals = 2, leading = TRUE)`$\n",
        "    ac. Information gain: $`r apa(Igain_ex, decimals = 2, leading = TRUE)`$\n",
        "\n",
        "9. Predicting from the base rate would have a percent accuracy of $`r as.integer(predictingFromTheBaseRateValue_ex)`\\%$.\n",
        "So, the predictions increase the percent accuracy by $`r as.integer(increasedAccuracyValue_ex)`\\%$.\n",
        "If you raise the selection ratio (i.e., predict more people survived) sensitivity will increase whereas specificity will decrease.\n",
        "If you lower the selection ratio, specificity will increase and sensitivity will decrease.\n",
        "\n",
        "10. The utility ratio is $`r apa(utilityRatio_ex, decimals = 2)`$.\n",
        "The overall utility ($U_\\text{overall}$) of the cutoff is $`r apa(Uoverall_ex, decimals = 2)`$.\n",
        "The cutoff with the highest overall utility is $`r apa(overallUtilityCutoff_ex, decimals = 3)`$.\n",
        "\n",
        "11. The cutoff that optimizes each of the following criteria:\n",
        "    a. Youden's J statistic: $`r apa(youdenIndex_ex, decimals = 3)`$\n",
        "    b. Closest to the top left of the ROC curve: $`r apa(closestToTheTopLeft_ex, decimals = 3)`$\n",
        "    c. Percent accuracy: $`r apa(percentAccuracyCutoff_ex, decimals = 3)`$\n",
        "    d. Percent accuracy by chance: 1 (i.e., predicting from the base rate—that nobody will survive)\n",
        "    e. Relative improvement over chance (ROIC): $`r apa(relativeImprovementOverChanceCutoff_ex, decimals = 3, leading = FALSE)`$\n",
        "    f. Relative improvement over predicting from the base rate: $`r apa(relativeImprovementOverPredictingFromBaseRateCutoff_ex, decimals = 3, leading = FALSE)`$\n",
        "    g. Sensitivity (true positive rate [TPR]): 0 (i.e., predicting that everyone will survive will minimize false negatives)\n",
        "    h. Specificity: 1 (i.e., predicting that nobody will survive will minimize false positives)\n",
        "    i. Positive predictive value (PPP): $`r apa(positivePredictiveValueCutoff_ex, decimals = 3)`$\n",
        "    j. Negative predictive value (NPV): $`r apa(negativePredictiveValueCutoff_ex, decimals = 3)`$\n",
        "    k. Diagnostic odds ratio: $`r apa(diagnosticOddsRatioCutoff_ex, decimals = 3)`$\n",
        "    l. Balanced accuracy: $`r apa(balancedAccuracyCutoff_ex, decimals = 3)`$\n",
        "    m. $F_1$ score: $`r apa(f1ScoreCutoff_ex, decimals = 3)`$\n",
        "    n. Matthews correlation coefficient (MCC): $`r apa(mccCutoff_ex, decimals = 3)`$\n",
        "    o. Positive likelihood ratio: $`r apa(positiveLikelihoodRatioCutoff_ex, decimals = 3)`$\n",
        "    p. Negative likelihood ratio: $`r apa(negativeLikelihoodRatioCutoff_ex, decimals = 3)`$\n",
        "    q. Miscalibration (mean difference between predicted and observed values): $`r apa(min(differenceBetweenPredictedAndObservedCutoff_ex), decimals = 3)`–`r apa(max(differenceBetweenPredictedAndObservedCutoff_ex), decimals = 3)`$\n",
        "    r. $d'$ sensitivity: $`r apa(dPrimeSDTCutoff_ex, decimals = 3)`$\n",
        "    s. $A$ (non-parametric) sensitivity $`r apa(ASDTCutoff_ex, decimals = 3)`$\n",
        "    t. $\\beta$ bias $`r apa(betaSDTCutoff_ex, decimals = 3)`$\n",
        "    u. $c$ bias $`r apa(cSDTCutoff_ex, decimals = 3)`$\n",
        "    v. $b$ (non-parametric) bias $`r apa(bSDTCutoff_ex, decimals = 3)`$\n",
        "    w. Overall utility: $`r apa(overallUtilityCutoff_ex, decimals = 3)`$\n",
        "    x. Information gain: $`r apa(informationGainCutoff_ex, decimals = 3)`$\n",
        "    \n",
        "12.\n",
        "    a. Based on negative predictive value (i.e., the probability of no disease given a negative test), the probability that you actually had the coronavirus is less than 1 in 100 $(`r apa(prYouHadCovid, decimals = 3, leading = FALSE)`)$.\n",
        "    b. Based on positive predictive value (i.e., the probability of disease given a positive test), the probability that your friend actually had the coronavirus is less than 50% $(`r apa(prFriendHadCovid, decimals = 2, leading = FALSE)`)$.\n",
        "    c. The problem with your friend's logic is that your friend is assuming they have the antibodies when chances are more likely that they do not.\n",
        "    d. Your friend is likely demonstrating the fallacy of base rate neglect.\n",
        "    e. A positive test on a screening device is hard to interpret in this situation because of the low base rate.\n",
        "\t\"Even with a very accurate test, the fewer people in a population who have a condition, the more likely it is that an individual's positive result is wrong.\"\n",
        "\tFor more info, see here: https://www.scientificamerican.com/article/coronavirus-antibody-tests-have-a-mathematical-pitfall/ (archived at https://perma.cc/GL9F-EVPH)\n",
        "13. According to Bayes' theorem, the probability that you have the disease is $`r apa(pDiseaseGivenPositiveTest, decimals = 2)`\\%$.\n",
        "For more info, see here: https://www.scientificamerican.com/article/what-is-bayess-theorem-an/ (archived at https://perma.cc/GM6B-5MEP)\n",
        "14.\n",
        "    a. According to Bayes' theorem, the probability that you have the virus after testing positive on Test A is $`r apa(pctGivenTestA_ex, decimals = 1)`\\%$.\n",
        "    b. According to Bayes' theorem, the updated probability that you have the virus after testing positive on both Tests A and B is $`r apa(pctGivenTestAthenB_ex, decimals = 1)`\\%$.\n"
      ],
      "id": "799c0dd2"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "ir",
      "language": "R",
      "display_name": "R",
      "path": "/Users/corradocaudek/Library/Jupyter/kernels/ir"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}