{
  "hash": "45f29be9a0dbb629256f735c171f424d",
  "result": {
    "engine": "knitr",
    "markdown": "# Metodi di stima dell'affidabilità {#sec-ctt-methods-reliability}\n\n**Preparazione del Notebook**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> source()\npacman::p_load(modelsummary, ltm)\n```\n:::\n\n\n\n\n## Introduzione\n\nI punteggi ottenuti da test psicologici possono variare per differenze tra item, occasioni di somministrazione o modalità di valutazione. La CTT affronta questo problema introducendo il concetto di affidabilità, intesa come stabilità e coerenza delle misure ottenute, distinguendo tra \"punteggio vero\" e \"errore di misurazione\".\n\nNel capitolo precedente abbiamo visto che l'affidabilità riflette la proporzione della varianza dovuta al punteggio vero rispetto alla varianza totale. Il problema successivo è stimare accuratamente l'affidabilità, considerando diverse modalità di errore.\n\n## Come Stimare l'Affidabilità\n\nPer stimare l'affidabilità ($\\rho^2_{XT}$), dobbiamo affrontare la difficoltà legata all'impossibilità di osservare direttamente il punteggio vero o l'errore di misurazione. La strategia di stima dipende da come definiamo e interpretiamo l'errore di misurazione ($\\sigma^2_E$):\n  \n1. **Affidabilità delle Forme Parallele**:\n    - considera l’errore come differenza tra punteggi ottenuti da forme equivalenti del test.\n\n2. **Consistenza Interna**:\n    - valuta quanto gli item all’interno dello stesso test siano omogenei rispetto al costrutto misurato (es. Alpha di Cronbach).\n\n3. **Affidabilità Test-Retest (Coerenza Temporale)**:\n    - misura l’errore come variazione dei punteggi nel tempo, attraverso somministrazioni ripetute dello stesso test.\n\nLa principale differenza tra questi metodi è nella definizione operativa e nel calcolo della varianza d’errore ($\\sigma^2_E$).\n\n\n## Affidabilità come Consistenza Interna\n  \n  La consistenza interna valuta l’omogeneità tra item. Nella CTT, esistono tre principali modelli teorici che descrivono come i punteggi veri di diversi item si relazionano tra loro (cfr. @sec-ctt-foundations):\n\n### Item paralleli\n  \n**Definizione in CTT**:  \n  \n- $\\tau$-equivalenti con intercetta nulla e fattore di scala pari a 1,  \n- stessa varianza dell’errore di misura.\n\nFormule generiche:  \n\n  $$\n    \\begin{cases}\n    X_1 = T + E_1 \\\\\n    X_2 = T + E_2 \\\\\n    \\end{cases}\n    $$\n    \ncon $\\mathbb{E}[E_1] = \\mathbb{E}[E_2] = 0$ e $\\mathrm{Var}(E_1) = \\mathrm{Var}(E_2)$.\n\nIn questo caso, la **correlazione $\\mathrm{corr}(X_1, X_2)$ coincide con l’affidabilità**, data da\n\n$$\n  \\rho_{X_1 X_2} \\;=\\;\n  \\frac{\\sigma_T^2}{\\sigma_T^2 + \\sigma_E^2}.\n  $$\n\n**Esempio in R.**\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\n\nn <- 100000 # numero di osservazioni\n\n# 1) Generiamo il punteggio vero T\nT <- rnorm(n, mean = 50, sd = 10)\n\n# 2) Generiamo errori di misura con stessa varianza\nE1 <- rnorm(n, mean = 0, sd = 5)\nE2 <- rnorm(n, mean = 0, sd = 5)\n\n# 3) Costruiamo i due item paralleli\nX1_parallel <- T + E1\nX2_parallel <- T + E2\n\n# 4) Calcoliamo la correlazione tra i due item\ncorr_par <- cor(X1_parallel, X2_parallel)\n\n# 5) Calcoliamo la \"affidabilità\" teorica di uno qualunque di questi item\n#    secondo CTT: var(T) / var(X1_parallel)\nrel_par <- var(T) / var(X1_parallel)\n\ncat(\"Item paralleli:\\n\")\n#> Item paralleli:\ncat(\"Correlazione (X1, X2):\", round(corr_par, 2), \"\\n\")\n#> Correlazione (X1, X2): 0.8\ncat(\"Affidabilità teorica  :\", round(rel_par, 2), \"\\n\")\n#> Affidabilità teorica  : 0.8\n```\n:::\n\n\n\n\n**Risultato**: La correlazione tra $X_1$ e $X_2$ dovrebbe approssimare molto da vicino la stima dell’affidabilità: se i due item sono paralleli, “vedono” lo stesso punteggio vero con la stessa qualità di misura.\n\n\n### Item $\\tau$-equivalenti\n  \n**Definizione in CTT**:  \n\n- Stesso coefficiente di pendenza ($b_{ij} = 1$),  \n- Possibile **shift costante** (intercetta diversa) o differenze nelle varianze d’errore,  \n- In generale, $\\mathrm{corr}(X_1, X_2)$ non è uguale all’affidabilità, anche se i punteggi veri differiscono solo per un termine costante.\n\nFormule generiche (possibilità di shift di media e/o errori differenti):  \n\n  $$\n    \\begin{cases}\n    X_1 = T + E_1,\\\\\n    X_2 = (T + c) + E_2,\n    \\end{cases}\n    $$\ndove $c \\neq 0$ è uno shift nella media o $\\mathrm{Var}(E_1)\\neq \\mathrm{Var}(E_2)$.\n\n**Esempio in R.**\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 1) Punteggio vero T\nT <- rnorm(n, mean = 50, sd = 10)\n\n# 2) Generiamo errori con varianze diverse\nE1_tau <- rnorm(n, mean = 0, sd = 4)\nE2_tau <- rnorm(n, mean = 0, sd = 6)\n\n# 3) Aggiungiamo un piccolo shift (c)\nc_shift <- 5\n\n# 4) Costruiamo i due item tau-equivalenti (stessa \"pendenza\" = 1)\nX1_tau <- T + E1_tau\nX2_tau <- (T + c_shift) + E2_tau\n\n# 5) Correlazione\ncorr_tau <- cor(X1_tau, X2_tau)\n\n# 6) \"Affidabilità\" teorica di X1_tau (ad es.)\nrel_tau <- var(T) / var(X1_tau)\n\ncat(\"\\nItem tau-equivalenti:\\n\")\n#> \n#> Item tau-equivalenti:\ncat(\"Correlazione (X1, X2):\", round(corr_tau, 3), \"\\n\")\n#> Correlazione (X1, X2): 0.796\ncat(\"Affidabilità teorica  :\", round(rel_tau, 3), \"\\n\")\n#> Affidabilità teorica  : 0.867\n```\n:::\n\n\n\n\n**Risultato**: Qui la correlazione tra $X_1$ e $X_2$ non coincide con la stima di affidabilità, perché abbiamo violato una delle condizioni di parallelismo (stessa varianza d’errore e/o zero shift). Pur avendo la stessa “pendenza” ($b=1$), lo shift e la differente varianza d’errore fanno sì che $\\mathrm{corr}(X_1, X_2)\\neq \\mathrm{var}(T)/\\mathrm{var}(X_1)$.\n\n###  Item congenerici\n  \n**Definizione in CTT**: \n\n- Gli item possono avere **pendenze (slopes)** diverse e **intercette** diverse.  \n- In formula: $X_1 = a_1 + b_1 T + E_1$, $X_2 = a_2 + b_2 T + E_2$.  \n- I rapporti tra varianze e correlazioni diventano più complessi, e non è possibile che la mera correlazione tra $X_1$ e $X_2$ coincida con l’affidabilità di uno dei due, se non per casi particolari.\n\n**Esempio in R.**\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 1) Punteggio vero T\nT <- rnorm(n, mean = 50, sd = 10)\n\n# 2) Generiamo errori con varianze (o distribuzioni) diverse\nE1_cong <- rnorm(n, mean = 0, sd = 4)\nE2_cong <- rnorm(n, mean = 0, sd = 8)\n\n# 3) Fattori di scala e intercette diverse\na1 <- 10\nb1 <- 1.2\na2 <- -5\nb2 <- 0.8\n\n# 4) Costruiamo i due item congenerici\nX1_cong <- a1 + b1 * T + E1_cong\nX2_cong <- a2 + b2 * T + E2_cong\n\n# 5) Correlazione\ncorr_cong <- cor(X1_cong, X2_cong)\n\n# 6) Affidabilità teorica di X1_cong (ad es.)\n#    var( T' ) / var( X1_cong ),\n#    dove T' = b1 * T (non va scordato che a1 è un shift)\n#    ma in CTT spesso si assume T' come 'vera' per X1\nvar_true_X1 <- var(b1 * T) # var(b1*T) = b1^2 * var(T)\nvar_X1 <- var(X1_cong)\nrel_cong <- var_true_X1 / var_X1\n\ncat(\"\\nItem congenerici:\\n\")\n#> \n#> Item congenerici:\ncat(\"Correlazione (X1, X2):\", round(corr_cong, 3), \"\\n\")\n#> Correlazione (X1, X2): 0.673\ncat(\"Affidabilità teorica di X1:\", round(rel_cong, 3), \"\\n\")\n#> Affidabilità teorica di X1: 0.899\n```\n:::\n\n\n\n\n**Risultato**: La correlazione $\\mathrm{corr}(X_1, X_2)$ non coincide con $\\mathrm{var}(b_1 T)/\\mathrm{var}(X_1)$. Gli item congenerici “vedono” il costrutto latente con pendenze e intercette diverse, quindi non c’è motivo che la semplice correlazione tra $X_1$ e $X_2$ equivalga all’affidabilità di uno dei due.\n\n\n**Conclusioni.**\n  \n1. **Item paralleli**: stessa media, stessa varianza vera (e d’errore), $\\mathrm{corr}(X_1, X_2) =$ affidabilità.  \n2. **Item $\\tau$-equivalenti**: stessa “pendenza” ($b=1$), ma possibili shift di media o differenze di varianza d’errore; la correlazione tra i due item **non** equivale all’affidabilità di un singolo item.  \n3. **Item congenerici**: pendenza e intercetta diverse; la correlazione tra i due item si discosta ancora di più dall’affidabilità.\n\n\n### Coefficiente Alpha di Cronbach\n\nI coefficiente Alpha di Cronbach è l'indice più utilizzato fornito dalla CTT per misurare la consistenza interna. Il coefficiente Alpha è adatto per item $\\tau$-equivalenti:\n\n$$ \n\\alpha = \\frac{k}{k-1} \\left(1 - \\frac{\\sum \\sigma_i^2}{\\sigma_X^2}\\right) ,\n$$ {#eq-cronbach-application}\n\ndove:\n\n- $k$ è il numero di item,\n- $\\sigma_i^2$ è la varianza dell'item i,\n- $\\sigma_X^2$ è la varianza totale del test.\n\nUna derivazione della formula del coefficiente alpha di Cronbach sarà fornita nel capitolo @sec-fa-reliability.\n\nL’Alpha di Cronbach fornisce una stima conservativa (limite inferiore) dell'affidabilità solo quando le assunzioni del modello $\\tau$-equivalente sono rispettate. In caso contrario, può sovrastimare l'affidabilità. In altre parole, per utilizzare il coefficiente alpha di Cronbach **è necessario che gli item soddisfino la condizione di $\\tau$-equivalenza**.\n\n::: {#exm-}\nPer illustrare la procedura di calcolo del coefficiente $\\alpha$, useremo i dati `bfi` contenuti nel pacchetto `psych`. Il dataframe `bfi` comprende 25 item di autovalutazione della personalità. Sono riportati i dati di 2800 soggetti. Ci concentreremo qui sulla sottoscala *Openness*:\n\n- O1: *Am full of ideas*; \n- O2: *Avoid difficult reading material*; \n- O3: *Carry the conversation to a higher level*; \n- O4: *Spend time reflecting on things*; \n- O5: *Will not probe deeply into a subject*. \n\nLeggiamo i dati in R.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(bfi, package = \"psych\")\nhead(bfi[c(\"O1\", \"O2\", \"O3\", \"O4\", \"O5\")])\n#>       O1 O2 O3 O4 O5\n#> 61617  3  6  3  4  3\n#> 61618  4  2  4  3  3\n#> 61620  4  2  5  5  2\n#> 61621  3  3  4  3  5\n#> 61622  3  3  4  3  3\n#> 61623  4  3  5  6  1\n```\n:::\n\n\n\n\nEsaminiamo la correlazione tra gli item della sottoscale Openness.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncor(\n  bfi[c(\"O1\", \"O2\", \"O3\", \"O4\", \"O5\")],\n  use = \"pairwise.complete.obs\"\n) |>\n  round(2)\n#>       O1    O2    O3    O4    O5\n#> O1  1.00 -0.21  0.40  0.18 -0.24\n#> O2 -0.21  1.00 -0.26 -0.07  0.32\n#> O3  0.40 -0.26  1.00  0.19 -0.31\n#> O4  0.18 -0.07  0.19  1.00 -0.18\n#> O5 -0.24  0.32 -0.31 -0.18  1.00\n```\n:::\n\n\n\n\nÈ necessario ricodificare due item.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbfi$O2r <- 7 - bfi$O2\nbfi$O5r <- 7 - bfi$O5\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncor(\n  bfi[c(\"O1\", \"O2r\", \"O3\", \"O4\", \"O5r\")],\n  use = \"pairwise.complete.obs\"\n) |>\n  round(2)\n#>       O1  O2r   O3   O4  O5r\n#> O1  1.00 0.21 0.40 0.18 0.24\n#> O2r 0.21 1.00 0.26 0.07 0.32\n#> O3  0.40 0.26 1.00 0.19 0.31\n#> O4  0.18 0.07 0.19 1.00 0.18\n#> O5r 0.24 0.32 0.31 0.18 1.00\n```\n:::\n\n\n\n\nConsideriamo la matrice di varianze e covarianze della sottoscala Openness. \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nC <- cov(\n  bfi[c(\"O1\", \"O2r\", \"O3\", \"O4\", \"O5r\")],\n  use = \"pairwise.complete.obs\"\n)\nC |> round(2)\n#>       O1  O2r   O3   O4  O5r\n#> O1  1.28 0.38 0.54 0.25 0.36\n#> O2r 0.38 2.45 0.50 0.13 0.67\n#> O3  0.54 0.50 1.49 0.29 0.50\n#> O4  0.25 0.13 0.29 1.49 0.29\n#> O5r 0.36 0.67 0.50 0.29 1.76\n```\n:::\n\n\n\n\nCalcoliamo alpha:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np <- 5\nalpha <- (p / (p - 1)) * (1 - tr(C) / sum(C))\nalpha\n#> [1] 0.6002\n```\n:::\n\n\n\n\nLo stesso risultato si ottiene utilizzando la funzione `alpha()`\ncontenuta nel pacchetto `psych`:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npsych::alpha(C)\n#> \n#> Reliability analysis   \n#> Call: psych::alpha(x = C)\n#> \n#>   raw_alpha std.alpha G6(smc) average_r S/N median_r\n#>        0.6      0.61    0.57      0.24 1.5     0.23\n#> \n#>     95% confidence boundaries \n#>       lower alpha upper\n#> Feldt -0.49   0.6  0.95\n#> \n#>  Reliability if an item is dropped:\n#>     raw_alpha std.alpha G6(smc) average_r S/N  var.r med.r\n#> O1       0.53      0.53    0.48      0.22 1.1 0.0092  0.23\n#> O2r      0.57      0.57    0.51      0.25 1.3 0.0076  0.22\n#> O3       0.50      0.50    0.44      0.20 1.0 0.0071  0.20\n#> O4       0.61      0.62    0.56      0.29 1.6 0.0044  0.29\n#> O5r      0.51      0.53    0.47      0.22 1.1 0.0115  0.20\n#> \n#>  Item statistics \n#>        r r.cor r.drop\n#> O1  0.65  0.52   0.39\n#> O2r 0.60  0.43   0.33\n#> O3  0.69  0.59   0.45\n#> O4  0.52  0.29   0.22\n#> O5r 0.66  0.52   0.42\n```\n:::\n\n\n\n:::\n\n### Coefficiente KR-20\n\nLa formula di Kuder-Richardson-20 (KR-20) è un caso particolare del coefficiente $\\alpha$ di Cronbach. Se ogni item è dicotomico, il coefficiente $\\alpha$ di Cronbach diventa il KR-20. Il coefficiente Coefficiente KR-20 si calcola con la formula:\n\n$$\n\\mathrm{KR}_{20} \n\\;=\\; \\frac{k}{k - 1}\n\\left(\n1 \\;-\\; \\frac{\\sum_{i=1}^k p_i \\,\\bigl(1 - p_i\\bigr)}{\\sigma_X^2}\n\\right),\n$$ {#eq-kr20}\n\ndove:\n\n- $k$ è il numero di item nel test,\n- $p_i$ è la proporzione di risposte esatte all’item $i$,\n- $\\sigma_{X}^{2}$ è la varianza del punteggio totale.\n\n::: {#exm-}\nConsideriamo il data-set `LSAT` contenuto nel pacchetto `ltm`.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nKR20 <- function(responses) {\n  # Get number of items (N) and individuals\n  n.items <- ncol(responses)\n  n.persons <- nrow(responses)\n  # get p_j for each item\n  p <- colMeans(responses)\n  # Get total scores (X)\n  x <- rowSums(responses)\n  # observed score variance\n  var.x <- var(x) * (n.persons - 1) / n.persons\n  # Apply KR-20 formula\n  rel <- (n.items / (n.items - 1)) * (1 - sum(p * (1 - p)) / var.x)\n  return(rel)\n}\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(LSAT)\nhead(LSAT)\n#>   Item 1 Item 2 Item 3 Item 4 Item 5\n#> 1      0      0      0      0      0\n#> 2      0      0      0      0      0\n#> 3      0      0      0      0      0\n#> 4      0      0      0      0      1\n#> 5      0      0      0      0      1\n#> 6      0      0      0      0      1\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nKR20(LSAT)\n#> [1] 0.295\n```\n:::\n\n\n\n:::\n\n\n### Coefficiente KR-21\n\nIl coefficiente KR-21 si ottiene da KR-20 **assumendo che tutti gli item abbiano la stessa difficoltà**, ossia la stessa proporzione di risposte corrette $\\bar p$. In tale condizione, la formula si semplifica a:\n\n$$\n\\mathrm{KR}_{21} \n\\;=\\; \n\\frac{k}{k - 1}\n\\Biggl(\n1 \n- \n\\frac{k\\,\\bar{p}\\,\\bigl(1 - \\bar{p}\\bigr)}{\\sigma_X^2}\n\\Biggr),\n$$ {#eq-kr21}\n\ndove:\n\n- $k$ è il numero di item,\n- $\\bar p = \\frac{1}{k}\\sum_{i=1}^k p_i$ è la media delle proporzioni di risposte corrette,\n- $\\sigma_X^2$ è la varianza del **punteggio totale** del test (ovvero la varianza della somma di tutti gli item).\n\nSe, invece, si vuole esprimere $\\bar p$ in funzione di $\\sum_{i=1}^k p_i$, la formula rimane comunque la stessa, ma si “nasconde” la somma all’interno del termine $\\bar p$.\n\n**Cosa cambia rispetto a KR-20?**\n\n1. **KR-20 (generale)**  \n   $$\n   \\mathrm{KR}_{20}\n   \\;=\\;\n   \\frac{k}{k-1}\n   \\Bigl(\n     1 - \\frac{\\sum_{i=1}^k p_i(1 - p_i)}{\\sigma_X^2}\n   \\Bigr).\n   $$\n\n   - **Non** richiede che tutti gli item abbiano la stessa $\\,p_i$.  \n   - La sommatoria $\\,\\sum_{i=1}^k p_i(1-p_i)$ cattura la **variazione** di difficoltà tra i singoli item.\n\n2. **KR-21 (caso speciale)**  \n   $$\n   \\mathrm{KR}_{21}\n   \\;=\\;\n   \\frac{k}{k - 1}\n   \\Bigl(\n     1 - \\frac{k\\,\\bar{p}\\,(1 - \\bar{p})}{\\sigma_X^2}\n   \\Bigr),\n   $$\n   con $\\bar{p} = \\frac{1}{k}\\sum_{i=1}^k p_i$.\n\n   - **Assume** esplicitamente $\\,p_i = \\bar{p}$ (cioè tutti gli item hanno la **stessa** difficoltà).  \n   - In tal caso, la sommatoria di KR-20 si riduce a $k\\,\\bar{p}(1-\\bar{p})$.\n\n**In sintesi**: KR-21 è una **semplificazione** di KR-20 che diventa valida solo sotto l’assunzione (spesso irrealistica) di “item ugualmente difficili”. Per questo, nella pratica, KR-21 è meno utilizzato di KR-20, in quanto meno flessibile e più restrittivo.\n\n### Formula di Spearman-Brown\n\nLa **formula di Spearman-Brown** consente di stimare l’affidabilità di un test composto da $p$ **item paralleli**, partendo dalla stima dell’affidabilità di un singolo item ($\\rho_1$):\n\n$$\n\\rho_p \n= \\frac{p \\,\\rho_1}{(p - 1)\\,\\rho_1 + 1}.\n$$ {#eq-spearman-brown}\n\nIn questo contesto, **$\\rho_1$** rappresenta la correlazione (o affidabilità) media di un item rispetto al costrutto che si intende misurare, mentre **$p$** è il numero di item totali. Una derivazione della formula del coefficiente di Spearman-Brown sarà fornita nel capitolo @sec-fa-reliability.\n\n::: {#exm-}\nStimiamo l’affidabilità della sottoscala Openness di un questionario, assumendo che gli item siano **paralleli**. Di seguito, otteniamo la matrice di correlazione degli item e calcoliamo la correlazione media tra di essi, usandola come $\\rho_1$.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nR <- cor(\n  bfi[c(\"O1\", \"O2r\", \"O3\", \"O4\", \"O5r\")],\n  use = \"pairwise.complete.obs\"\n)\nround(R, 2)\n#>       O1  O2r   O3   O4  O5r\n#> O1  1.00 0.21 0.40 0.18 0.24\n#> O2r 0.21 1.00 0.26 0.07 0.32\n#> O3  0.40 0.26 1.00 0.19 0.31\n#> O4  0.18 0.07 0.19 1.00 0.18\n#> O5r 0.24 0.32 0.31 0.18 1.00\n\n# Numero di item\np <- 5\n\n# Calcolo della correlazione media tra item (stima di rho_1)\n\n# Sostituiamo la diagonale con NA\ndiag(R) <- NA\n# Calcoliamo la media di tutti i valori non-NA\nro_1 <- mean(R, na.rm = TRUE)\n\nro_1\n#> [1] 0.2365\n```\n:::\n\n\n\n\nInfine, applichiamo la formula di Spearman-Brown per ottenere la stima dell’affidabilità dell’intera scala:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrho_p <- (p * ro_1) / ((p - 1) * ro_1 + 1)\nrho_p\n#> [1] 0.6077\n```\n:::\n\n\n\n:::\n\nIn sintesi, la **Spearman-Brown** combina il numero degli item ($p$) con l’affidabilità di un singolo item ($\\rho_1$) per stimare la **consistenza interna globale** di una scala composta da item paralleli. \n\n### L’indice $\\omega$ di McDonald\n\nL’indice $\\omega$ di McDonald è un metodo per stimare l’affidabilità come consistenza interna in presenza di item congenerici. Maggiori approfondimenti sul suo calcolo e utilizzo saranno forniti nel @sec-fa-reliability.\n\n## Affidabilità delle forme alternative\n\nL’affidabilità delle forme alternative si riferisce alla **coerenza** tra i punteggi ottenuti da versioni diverse, ma **equivalenti**, di uno stesso test. Lo scopo principale di queste forme è ridurre gli effetti di pratica e memoria, ma è essenziale assicurarsi che le diverse versioni siano effettivamente **parallele** o almeno molto simili in termini di **contenuto** e **difficoltà**.\n\nPer valutarne l’affidabilità, si esamina la **correlazione** tra i punteggi delle varie forme. Nei termini della CTT, se le forme alternative possono essere considerate **parallele** (cioè stesse medie e varianze vere, più varianze d’errore identiche), allora la loro correlazione corrisponde alla **affidabilità**.\n\n\n## Affidabilità Test-Retest\n\nL’affidabilità test-retest valuta **la stabilità** dei punteggi ottenuti somministrando lo stesso test a **distanza di tempo** sullo stesso gruppo di persone. Questo metodo è particolarmente indicato per costrutti considerati **relativamente stabili** (ad esempio, intelligenza e tratti di personalità). Di contro, risulta **inadeguato** per costrutti che possono subire **variazioni rapide o frequenti**, poiché i mutamenti nei punteggi potrebbero riflettere reali cambiamenti del costrutto piuttosto che una mancanza di attendibilità del test.\n\n## Affidabilità dei Punteggi Compositi\n\nL’affidabilità di un punteggio composito si riferisce alla **stabilità** di un indice costruito combinando più sottoscale o item. Di norma, un punteggio composito risulta **più affidabile** rispetto a ciascun sottopunteggio considerato individualmente, perché l’errore specifico di ogni sottoscale/item tende a “compensarsi” quando i punteggi sono tra loro **positivamente correlati**.\n\n### Esempio numerico\n\nSupponiamo di avere due subtest, $X_1$ e $X_2$, ciascuno con:  \n\n- varianza del **punteggio vero** pari a $25$,  \n- varianza dell’**errore di misura** pari a $10$,  \n- covarianza tra i punteggi veri dei due subtest pari a $15$.\n\n1. **Varianza del punteggio composito $Z$** (sola parte vera). Se $Z$ è la somma dei due subtest:  \n   $$\n   \\mathrm{Var}(Z_{\\text{vero}}) \n   = \\mathrm{Var}(X_1^{\\text{vero}}) + \\mathrm{Var}(X_2^{\\text{vero}}) + 2\\,\\mathrm{Cov}(X_1^{\\text{vero}},X_2^{\\text{vero}})\n   = 25 + 25 + 2 \\cdot 15 = 80.\n   $$\n\n2. **Varianza totale del composito** (punteggio vero + errore). Dato che ciascun subtest ha 10 di varianza d’errore, la varianza totale di $Z$ diventa:  \n   $$\n   \\mathrm{Var}(Z_{\\text{totale}}) \n   = \\underbrace{25 + 25}_{\\text{parte vera}} \n   + \\underbrace{10 + 10}_{\\text{parte errore}}\n   + \\underbrace{2 \\cdot 15}_{\\text{covarianza vera}}\n   = 35 + 35 + 2 \\cdot 15 = 100.\n   $$\n\n::: {.callout-tip title=\"Nota\" collapse=\"true\"}\n\nLa varianza totale del punteggio composito $Z$ è data dalla somma della varianza della componente vera, della varianza della componente di errore e del doppio della covarianza tra le due variabili, in base alle proprietà della varianza di una somma di variabili aleatorie.\n\nSe consideriamo un punteggio composito $Z$ ottenuto come somma di due subtest $X_1$ e $X_2$, la varianza di $Z$ è data dalla formula:\n\n$$\n\\text{Var}(Z) = \\text{Var}(X_1 + X_2) = \\text{Var}(X_1) + \\text{Var}(X_2) + 2\\cdot\\text{Cov}(X_1, X_2)\n$$\n\nCiascun subtest $X_1$ e $X_2$ è composto da due componenti:\n\n- La parte vera $X_1^{\\text{vero}}$ e $X_2^{\\text{vero}}$\n- La parte di errore $X_1^{\\text{errore}}$ e $X_2^{\\text{errore}}$\n\nPossiamo quindi scrivere:\n\n$$\nX_1 = X_1^{\\text{vero}} + X_1^{\\text{errore}}\n$$\n\n$$\nX_2 = X_2^{\\text{vero}} + X_2^{\\text{errore}}\n$$\n\nUtilizzando la proprietà della varianza per una somma di variabili, la varianza totale di $Z$ diventa:\n\n$$\n\\text{Var}(Z) = \\text{Var}(X_1^{\\text{vero}} + X_1^{\\text{errore}} + X_2^{\\text{vero}} + X_2^{\\text{errore}})\n$$\n\nEspandendo il termine:\n\n$$\n\\text{Var}(Z) = \\text{Var}(X_1^{\\text{vero}}) + \\text{Var}(X_2^{\\text{vero}}) + \\text{Var}(X_1^{\\text{errore}}) + \\text{Var}(X_2^{\\text{errore}})\n+ 2\\cdot\\text{Cov}(X_1^{\\text{vero}}, X_2^{\\text{vero}})\n$$\n\nDato che si assume che gli errori non siano correlati tra loro né con le parti vere ($\\text{Cov}(X_1^{\\text{vero}}, X_1^{\\text{errore}}) = 0$ e simili), tutte le altre covarianze sono nulle.\n\nOra, sostituendo i valori numerici:\n\n1. La varianza della parte vera è:\n\n   $$\n   \\text{Var}(X_1^{\\text{vero}}) + \\text{Var}(X_2^{\\text{vero}}) = 25 + 25 = 50\n   $$\n\n2. La varianza della parte di errore è:\n\n   $$\n   \\text{Var}(X_1^{\\text{errore}}) + \\text{Var}(X_2^{\\text{errore}}) = 10 + 10 = 20\n   $$\n\n3. La covarianza tra le parti vere contribuisce con:\n\n   $$\n   2\\cdot\\text{Cov}(X_1^{\\text{vero}}, X_2^{\\text{vero}}) = 2 \\cdot 15 = 30\n   $$\n\nPertanto, la varianza totale di $Z$ è:\n\n$$\n\\text{Var}(Z) = 50 + 20 + 30 = 100\n$$\n\nIl termine $2\\cdot\\text{Cov}(X_1^{\\text{vero}}, X_2^{\\text{vero}})$ appare perché, quando calcoliamo la varianza di una somma, dobbiamo considerare non solo le varianze individuali, ma anche la covarianza tra le due variabili, che rappresenta la loro interdipendenza. Questo termine indica quanto i due punteggi veri tendono a variare insieme in modo sistematico.\n\n:::\n\n3. **Affidabilità del composito**. Il coefficiente di affidabilità corrisponde alla proporzione di varianza vera rispetto alla varianza totale:  \n   $$\n   \\rho_Z\n   = \\frac{\\mathrm{Var}(Z_{\\text{vero}})}{\\mathrm{Var}(Z_{\\text{totale}})}\n   = \\frac{80}{100} = 0.80.\n   $$\n\n4. **Confronto con un singolo subtest**. Per un solo subtest:  \n   - $\\mathrm{Var}(\\text{vero}) = 25$.  \n   - $\\mathrm{Var}(\\text{totale}) = 25 + 10 = 35$.  \n   $$\n   \\rho_{X_i}\n   = \\frac{25}{35} \n   \\approx 0.714.\n   $$\n\nL’affidabilità del **punteggio composito** ($0.80$) risulta quindi maggiore rispetto a quella di un **singolo subtest** ($\\approx 0.71$). Questa differenza è dovuta al fatto che la covarianza vera tra i due subtest (15) **aumenta** la porzione di varianza attribuibile a differenze reali, mentre la parte di errore si “diluisce” nel composito.\n\n### Dimostrazione pratica in R\n\nPer confermare questi calcoli, possiamo simulare dati in R:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\n\nn <- 10000 # numero di soggetti\n\n# Creiamo due punteggi veri correlati\n# Varianza = 25 => sd = 5\n# Covarianza = 15 => correlazione = 15/(5*5) = 0.60\nlibrary(MASS)\nSigma_true <- matrix(c(\n  25, 15,\n  15, 25\n), nrow = 2)\nX_true <- MASS::mvrnorm(\n  n = n,\n  mu = c(0, 0),\n  Sigma = Sigma_true\n)\n\n# Aggiungiamo errori indipendenti (varianza 10 => sd = ~3.162)\nE1 <- rnorm(n, 0, sqrt(10))\nE2 <- rnorm(n, 0, sqrt(10))\n\n# Subtest osservati (X1 e X2)\nX1 <- X_true[, 1] + E1\nX2 <- X_true[, 2] + E2\n\n# Composito\nZ <- X1 + X2\n\n# Calcoliamo varianza vera stimata\nvar_true_X1 <- var(X_true[, 1]) # Dovrebbe ~25\nvar_true_X2 <- var(X_true[, 2]) # Dovrebbe ~25\ncov_true <- cov(X_true[, 1], X_true[, 2]) # Dovrebbe ~15\nc(var_true_X1, var_true_X2, cov_true)\n#> [1] 24.84 25.08 14.93\n\n# Verifichiamo varianza e affidabilità osservate\nvar_X1 <- var(X1)\nvar_X2 <- var(X2)\nvar_Z <- var(Z)\nc(var_X1, var_X2, var_Z)\n#> [1] 35.46 34.46 99.52\n\n# Stima affidabilità subtest 1\nreliability_X1 <- var_true_X1 / var_X1\n\n# Stima affidabilità subtest 2\nreliability_X2 <- var_true_X2 / var_X2\n\n# Stima affidabilità punteggio composito\n# var(Z_vero) = var_true_X1 + var_true_X2 + 2 * cov_true\nvar_true_Z <- var_true_X1 + var_true_X2 + 2 * cov_true\nreliability_Z <- var_true_Z / var_Z\n\ncat(\"Affidabilità subtest 1:\", round(reliability_X1, 3), \"\\n\")\n#> Affidabilità subtest 1: 0.701\ncat(\"Affidabilità subtest 2:\", round(reliability_X2, 3), \"\\n\")\n#> Affidabilità subtest 2: 0.728\ncat(\"Affidabilità composito :\", round(reliability_Z, 3), \"\\n\")\n#> Affidabilità composito : 0.802\n```\n:::\n\n\n\n\nEseguendo questo codice, troveremo valori prossimi a quelli **attesi** teoricamente: circa 0.71 per i singoli subtest e circa 0.80 per il punteggio composito.\n\n**Conclusioni.**\n\n- **Maggiore affidabilità**: Combinare più subtest (o item) **correlati** tende a migliorare l’affidabilità del punteggio totale, perché aumenta la porzione di varianza vera rispetto alla varianza d’errore.  \n- **Vantaggio pratico**: Un punteggio composito è spesso considerato più  **rappresentativo** del costrutto di interesse, specialmente in ambito psicometrico, dove il singolo item o subtest può essere soggetto a un errore specifico più elevato.\n\n\n## Affidabilità dei Punteggi Differenza\n\nQuando si parla di “punteggio differenza”, ci si riferisce generalmente a un valore ottenuto sottraendo due misure, ad esempio i punteggi di un test somministrato **prima** e **dopo** un trattamento (punteggi “pre” e “post”). L’affidabilità di questo punteggio differenza si collega alla sua **stabilità** come indicatore, ovvero alla coerenza con cui misura la variazione tra pre e post.\n\nNella pratica, tali punteggi differenza spesso risultano **meno affidabili** rispetto ai punteggi originali, soprattutto quando le due misure sono fortemente correlate. Questo accade perché, all’aumentare della correlazione tra i due test, gran parte della varianza in comune “si annulla” nella differenza, lasciando i punteggi differenza più esposti alla variabilità casuale.\n\n### 1. Formula Generale \n\nSe $X$ e $Y$ sono le due misure (ad es. “pre” e “post”), l’affidabilità del punteggio differenza $(X - Y)$ può essere calcolata con la formula di Lord:\n\n$$\nr_{dd} \n= \n\\frac{0.5\\,[r_{xx} + r_{yy}] - r_{xy}}{1 - r_{xy}},\n$$ {#eq-diff-scores-lord}\n\ndove:\n\n- $r_{xx}$ e $r_{yy}$ sono le affidabilità (per esempio Cronbach $\\alpha$ o test-retest) delle due misure $X$ e $Y$;  \n- $r_{xy}$ è la correlazione tra $X$ e $Y$;  \n- $r_{dd}$ è l’affidabilità del punteggio differenza $(X - Y)$.\n\n::: {.callout-tip title=\"Dimostrazione\" collapse=\"true\"}\n\n**Passo 1: Definizione dell'affidabilità di una variabile**\n\nL'affidabilità di una variabile è definita come la proporzione di varianza vera sulla varianza totale. Se una variabile $X$ è composta da una componente vera $T_X$ e un errore $E_X$, possiamo scrivere:\n\n$$\nX = T_X + E_X\n$$\n\nL'affidabilità di $X$ è data da:\n\n$$\nr_{xx} = \\frac{\\text{Var}(T_X)}{\\text{Var}(X)}\n$$\n\nAnalogamente, per $Y$:\n\n$$\nY = T_Y + E_Y\n$$\n\n$$\nr_{yy} = \\frac{\\text{Var}(T_Y)}{\\text{Var}(Y)}\n$$\n\n**Passo 2: Definizione del punteggio differenza**\n\nIl punteggio differenza è definito come:\n\n$$\nD = X - Y\n$$\n\nLa varianza del punteggio differenza è:\n\n$$\n\\text{Var}(D) = \\text{Var}(X - Y)\n$$\n\nUtilizzando la proprietà della varianza:\n\n$$\n\\text{Var}(D) = \\text{Var}(X) + \\text{Var}(Y) - 2\\text{Cov}(X, Y)\n$$\n\nPoiché la covarianza è:\n\n$$\n\\text{Cov}(X, Y) = r_{xy} \\cdot \\sigma_X \\cdot \\sigma_Y\n$$\n\nsi può riscrivere la varianza del punteggio differenza come:\n\n$$\n\\text{Var}(D) = \\sigma_X^2 + \\sigma_Y^2 - 2 r_{xy} \\sigma_X \\sigma_Y\n$$\n\n**Passo 3: Calcolo della componente vera del punteggio differenza**\n\nAnalogamente a quanto fatto per $X$ e $Y$, scriviamo la componente vera del punteggio differenza:\n\n$$\nT_D = T_X - T_Y\n$$\n\nLa sua varianza è:\n\n$$\n\\text{Var}(T_D) = \\text{Var}(T_X - T_Y)\n$$\n\nUtilizzando la stessa proprietà della varianza:\n\n$$\n\\text{Var}(T_D) = \\text{Var}(T_X) + \\text{Var}(T_Y) - 2\\text{Cov}(T_X, T_Y)\n$$\n\nPoiché l'affidabilità è definita come il rapporto tra la varianza vera e la varianza osservata:\n\n$$\n\\text{Var}(T_X) = r_{xx} \\text{Var}(X), \\quad \\text{Var}(T_Y) = r_{yy} \\text{Var}(Y)\n$$\n\n$$\n\\text{Cov}(T_X, T_Y) = r_{xy} \\sqrt{r_{xx} r_{yy}} \\sigma_X \\sigma_Y\n$$\n\nQuindi:\n\n$$\n\\text{Var}(T_D) = r_{xx} \\sigma_X^2 + r_{yy} \\sigma_Y^2 - 2 r_{xy} \\sqrt{r_{xx} r_{yy}} \\sigma_X \\sigma_Y\n$$\n\n**Passo 4: Calcolo dell'affidabilità del punteggio differenza**\n\nL'affidabilità del punteggio differenza è data da:\n\n$$\nr_{dd} = \\frac{\\text{Var}(T_D)}{\\text{Var}(D)}\n$$\n\nSostituendo le espressioni trovate:\n\n$$\nr_{dd} = \\frac{r_{xx} \\sigma_X^2 + r_{yy} \\sigma_Y^2 - 2 r_{xy} \\sqrt{r_{xx} r_{yy}} \\sigma_X \\sigma_Y}{\\sigma_X^2 + \\sigma_Y^2 - 2 r_{xy} \\sigma_X \\sigma_Y}\n$$\n\nSe assumiamo che $\\sigma_X = \\sigma_Y$ (varianza uguale per entrambe le misure), possiamo semplificare la formula.\n\n**Semplificazione della varianza del punteggio differenza**\n\nAbbiamo definito la varianza del punteggio differenza come:\n\n$$\n\\text{Var}(D) = \\sigma_X^2 + \\sigma_Y^2 - 2 r_{xy} \\sigma_X \\sigma_Y\n$$\n\nSostituendo $\\sigma_X = \\sigma_Y = \\sigma$:\n\n$$\n\\text{Var}(D) = \\sigma^2 + \\sigma^2 - 2 r_{xy} \\sigma \\sigma\n$$\n\n$$\n\\text{Var}(D) = 2\\sigma^2 (1 - r_{xy})\n$$\n\n**Semplificazione della varianza della componente vera del punteggio differenza**\n\nAnalogamente, la varianza della componente vera del punteggio differenza è:\n\n$$\n\\text{Var}(T_D) = r_{xx} \\sigma_X^2 + r_{yy} \\sigma_Y^2 - 2 r_{xy} \\sqrt{r_{xx} r_{yy}} \\sigma_X \\sigma_Y\n$$\n\nSostituendo $\\sigma_X = \\sigma_Y = \\sigma$:\n\n$$\n\\text{Var}(T_D) = r_{xx} \\sigma^2 + r_{yy} \\sigma^2 - 2 r_{xy} \\sqrt{r_{xx} r_{yy}} \\sigma \\sigma\n$$\n\n$$\n\\text{Var}(T_D) = \\sigma^2 \\left( r_{xx} + r_{yy} - 2 r_{xy} \\sqrt{r_{xx} r_{yy}} \\right)\n$$\n\n**Calcolo dell'affidabilità del punteggio differenza**\n\nL'affidabilità del punteggio differenza è:\n\n$$\nr_{dd} = \\frac{\\text{Var}(T_D)}{\\text{Var}(D)}\n$$\n\nSostituendo le espressioni ottenute per $\\text{Var}(T_D)$ e $\\text{Var}(D)$:\n\n$$\nr_{dd} = \\frac{\\sigma^2 \\left( r_{xx} + r_{yy} - 2 r_{xy} \\sqrt{r_{xx} r_{yy}} \\right)}{2\\sigma^2 (1 - r_{xy})}\n$$\n\nSemplificando $\\sigma^2$:\n\n$$\nr_{dd} = \\frac{r_{xx} + r_{yy} - 2 r_{xy} \\sqrt{r_{xx} r_{yy}}}{2(1 - r_{xy})}\n$$\n\nQuesta è la formula intermedia che, riorganizzata, porta alla **formula di Lord**.\n\n**Passo 5: Ottenimento della formula di Lord**\n\nRiscriviamo il numeratore:\n\n$$\nr_{xx} + r_{yy} - 2 r_{xy} \\sqrt{r_{xx} r_{yy}} = (r_{xx} + r_{yy}) - 2 r_{xy} \\sqrt{r_{xx} r_{yy}}\n$$\n\nFattorizziamo il numeratore estraendo un fattore $0.5$:\n\n$$\nr_{dd} = \\frac{0.5 (r_{xx} + r_{yy}) - r_{xy} \\sqrt{r_{xx} r_{yy}}}{1 - r_{xy}}\n$$\n\nSe $r_{xx} = r_{yy}$, possiamo riscrivere ulteriormente come:\n\n$$\nr_{dd} = \\frac{0.5 (r_{xx} + r_{yy}) - r_{xy}}{1 - r_{xy}}\n$$\n\nQuesta è la **formula di Lord**, che descrive l'affidabilità del punteggio differenza in termini delle affidabilità delle misure individuali e della loro correlazione.\n\n---\n\n**Intuizione:**  \n\n- Se $r_{xy}$ è alto, significa che le due misure sono molto correlate e la differenza tra loro ha una varianza ridotta, quindi la sua affidabilità diminuisce.  \n- Se $r_{xx}$ e $r_{yy}$ sono elevate, significa che le misure originali sono più affidabili, aumentando l'affidabilità del punteggio differenza.  \n- Se $r_{xy}$ è basso, il punteggio differenza cattura una maggiore variabilità vera, aumentando $r_{dd}$.\n:::\n\n\n**Interpretazione:**  \n\n- se $r_{xy}$ è molto alto, allora la parte di varianza in comune tra i due test è ampia e, di conseguenza, la differenza $(X - Y)$ finisce per catturare soprattutto la varianza dovuta all’errore di misura, perdendo stabilità (affidabilità più bassa); \n- viceversa, quando la correlazione è bassa, $X$ e $Y$ condividono meno varianza (e di conseguenza meno “errore comune”), permettendo al punteggio differenza di essere relativamente più stabile.\n\n\n### Esempio in R (funzione `rdd()`)\n\nUn modo per osservare come varia $r_{dd}$ al variare di $r_{xy}$ è creare una funzione in R che applichi la formula di Lord. Di seguito un esempio:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Definizione della formula\nrdd <- function(rxx, ryy, rxy) {\n  (0.5 * (rxx + ryy) - rxy) / (1 - rxy)\n}\n\n# Esempio: rxx = 0.9, ryy = 0.8, rxy varia da 0.01 a 0.81\nrxx <- 0.9\nryy <- 0.8\ncorr_values <- seq(0.01, 0.81, by = 0.1)\n\nrdd_values <- rdd(rxx, ryy, corr_values)\nrdd_values\n#> [1] 0.8485 0.8315 0.8101 0.7826 0.7458 0.6939 0.6154 0.4828 0.2105\n```\n:::\n\n\n\n\nStampando `rdd_values` si nota che l’affidabilità del punteggio differenza **diminuisce** man mano che aumenta la correlazione tra $X$ e $Y$, confermando la spiegazione teorica.\n\n## Esempio Completo di Simulazione in R\n\nPer dimostrare in modo empirico la relazione tra correlazione delle misure e affidabilità del punteggio differenza, possiamo simulare due test con caratteristiche desiderate (affidabilità, correlazione, varianza) e confrontare l’affidabilità empirica con il valore teorico previsto dalla formula di Lord.\n\n**Passaggio 1: Impostazione dei Parametri.**\n\nDefiniamo:\n\n- $r_{xx}$ e $r_{yy}$: affidabilità di $X$ e $Y$.  \n- $r_{xy}$: correlazione vera tra i costrutti misurati da $X$ e $Y$.  \n- $\\mathrm{Var}(T_x)$ e $\\mathrm{Var}(T_y)$: varianze delle parti “vere” di $X$ e $Y$.  \n- `n`: numero di soggetti da simulare.  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\n\n# Parametri\nr_xx <- 0.9 # affidabilità X\nr_yy <- 0.8 # affidabilità Y\nr_xy <- 0.5 # correlazione vera tra i costrutti\nn <- 100000\n\n# Varianze delle parti vere\nvar_Tx <- 100\nvar_Ty <- 100\nsd_Tx <- sqrt(var_Tx) # 10\nsd_Ty <- sqrt(var_Ty) # 10\n\n# Covarianza vera\ncov_TxTy <- r_xy * sd_Tx * sd_Ty # = 50\n\n# Matrice var-cov per (T_x, T_y)\nSigma_true <- matrix(\n  c(\n    var_Tx, cov_TxTy,\n    cov_TxTy, var_Ty\n  ),\n  nrow = 2\n)\n```\n:::\n\n\n\n\n**Passaggio 2: Calcolo delle Varianze d’Errore.**\n\nDalla Teoria Classica dei Test (TCT) sappiamo che:\n\n$$\nr_{xx} \n= \n\\frac{\\mathrm{Var}(T_x)}{\\mathrm{Var}(T_x) + \\mathrm{Var}(E_x)}\n\\quad\\Longrightarrow\\quad\n\\mathrm{Var}(E_x) = \\frac{\\mathrm{Var}(T_x)}{r_{xx}} - \\mathrm{Var}(T_x).\n$$\n\nIn modo analogo per $Y$. Queste varianze d’errore ci serviranno per generare i punteggi osservati.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvar_Ex <- var_Tx * (1 / r_xx - 1)\nvar_Ey <- var_Ty * (1 / r_yy - 1)\n\nsd_Ex <- sqrt(var_Ex)\nsd_Ey <- sqrt(var_Ey)\n\nvar_Ex\n#> [1] 11.11\nvar_Ey\n#> [1] 25\n```\n:::\n\n\n\n\n**Passaggio 3: Generazione dei Dati.**\n\n1. Generiamo la parte vera $(T_x, T_y)$ con varianze e covarianza definite da `Sigma_true`.  \n2. Generiamo gli errori $(E_x, E_y)$ come variabili indipendenti, con le varianze appena trovate.  \n3. Sommiamo le parti vere e gli errori per ottenere i punteggi osservati $X$ e $Y$.  \n4. Calcoliamo la differenza $D = X - Y$.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# (1) Parte vera\nT <- MASS::mvrnorm(\n  n = n,\n  mu = c(0, 0),\n  Sigma = Sigma_true\n)\n\nT_x <- T[, 1]\nT_y <- T[, 2]\n\n# (2) Errori indipendenti\nE_x <- rnorm(n, mean = 0, sd = sd_Ex)\nE_y <- rnorm(n, mean = 0, sd = sd_Ey)\n\n# (3) Punteggi osservati\nX <- T_x + E_x\nY <- T_y + E_y\n\n# (4) Punteggio differenza\nD <- X - Y\n```\n:::\n\n\n\n\n**Passaggio 4: Stima dell’Affidabilità Empirica di $D$.**\n\nIn TCT, l’affidabilità è il rapporto tra la varianza vera e la varianza totale.  \nPer il punteggio differenza $D$:\n\n- parte vera: $T_d = T_x - T_y$;  \n- varianza vera: $\\mathrm{Var}(T_d)$;  \n- varianza totale osservata: $\\mathrm{Var}(D)$;  \n- affidabilità: \n\n  $$\n  r_{dd} = \\frac{\\mathrm{Var}(T_d)}{\\mathrm{Var}(D)}.\n  $$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nT_d <- T_x - T_y\nvar_Td <- var(T_d)\nvar_D <- var(D)\n\nrdd_empirical <- var_Td / var_D\nrdd_empirical\n#> [1] 0.7415\n```\n:::\n\n\n\n\n**Passaggio 5: Confronto con la Formula di Lord.**\n\nRichiamiamo la formula teorica:\n\n$$\nr_{dd} \n= \\frac{0.5 \\,[r_{xx} + r_{yy}] - r_{xy}}{1 - r_{xy}}.\n$$\n\nE applichiamola ai nostri parametri:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrdd_theoretical <- (0.5 * (r_xx + r_yy) - r_xy) / (1 - r_xy)\n\ncat(\"Affidabilità differenza (empirica):       \", round(rdd_empirical, 3), \"\\n\")\n#> Affidabilità differenza (empirica):        0.742\ncat(\"Affidabilità differenza (formula Lord):   \", round(rdd_theoretical, 3), \"\\n\")\n#> Affidabilità differenza (formula Lord):    0.7\n```\n:::\n\n\n\n\nIdealmente, i due valori coincidono (o sono molto simili) se la simulazione rispecchia bene i presupposti della formula (grande campione, corretta generazione di errori, ecc.).\n\nIn sintesi:\n\n1. **all’aumentare di $r_{xy}$**, diminuisce in genere l’affidabilità del punteggio differenza, poiché crescono le porzioni di varianza in comune tra $X$ e $Y$, che si sottraggono a vicenda;   \n2. **se $r_{xy}$ è più basso**, i punteggi di $X$ e $Y$ condividono meno varianza: la parte vera delle differenze risulta più “solida”, innalzando il potenziale di affidabilità del punteggio differenza.\n\nQuesto esempio dimostra chiaramente perché i punteggi differenza (ad esempio, nei confronti pre-post) possano spesso presentare un’affidabilità inferiore rispetto ai singoli punteggi originali, specialmente in presenza di misure molto simili o fortemente correlate.\n\n\n## Scelta del Coefficiente di Affidabilità\n\nLa selezione del coefficiente di affidabilità più adeguato dipende dal tipo di test, dal contesto d’uso e dallo scopo della misurazione.\n\n1. **Affidabilità Test-Retest**  \n   - Valuta la **stabilità nel tempo** della misura.  \n   - Indicata quando si vuole verificare se un costrutto permane stabile (o se varia) in assenza di fattori esterni che possano influenzarlo.\n\n2. **Consistenza Interna** (ad esempio, Cronbach’s Alpha, KR-20)  \n   - Appropriata con **una singola somministrazione** del test.  \n   - Utile soprattutto quando il test è progettato per misurare **un unico costrutto** e gli item dovrebbero, teoricamente, essere omogenei tra loro.\n\n3. **Affidabilità di Forme Alternative**  \n   - Rilevante quando esistono **versioni diverse (parallele o equivalenti)** di uno stesso test, e si desidera misurare se forniscono punteggi coerenti.  \n   - Aiuta a ridurre l’effetto dell’apprendimento o della familiarità con un’unica forma del test.\n\n4. **Affidabilità Inter-Valutatori**  \n   - Necessaria in presenza di **giudizi soggettivi** (ad esempio, valutazioni cliniche o osservazioni dirette).  \n   - Stima l’accordo tra più valutatori, misurando in che misura le discrepanze siano dovute a differenze reali nel comportamento/situazione osservata e non a differenze di criterio tra osservatori.\n\n\n### Linee Guida Generali sui Valori di Affidabilità\n\nSebbene le soglie di accettabilità varino in base allo scopo specifico del test e al contesto (clinico, educativo, di ricerca), le indicazioni più comuni sono:\n\n- **≥ 0.90**: raccomandato per decisioni di grande importanza, come diagnosi cliniche o valutazioni ad alto impatto.  \n- **≥ 0.80**: standard desiderabile per test di rendimento o di personalità, dove serve un livello di precisione piuttosto elevato.  \n- **≥ 0.70**: spesso considerato accettabile per screening didattici o contesti in cui il rischio di errore è più tollerabile.  \n- **≥ 0.60**: talvolta considerato sufficiente in contesti di ricerca di base o di gruppo, ma occorre molta cautela al di sotto di 0.70 perché la variabilità dovuta all’errore può influenzare notevolmente l’interpretazione dei punteggi.\n\nIn sintesi, la decisione su quale coefficiente adottare e su quale soglia considerare “accettabile” deve sempre tener conto:  \n- della **finalità** del test (diagnostico, selettivo, esplorativo);  \n- della **natura del costrutto** misurato (omogeneo o multi-dimensionale);  \n- del **contesto** (clinico, organizzativo, educativo, di ricerca).  \n\nUn’analisi delle esigenze e degli obiettivi specifici permette di scegliere con maggiore precisione quale forma di affidabilità risulti più indicata e quali valori siano ritenuti adeguati.\n\n## Riflessioni Conclusive\n\nLa stima dell’affidabilità non costituisce un mero esercizio metodologico, bensì un **passaggio cruciale** per validare la solidità di qualsiasi misura psicologica. Scegliere l’indice più adeguato in base al contesto, al tipo di test e alla natura del costrutto consente di:\n\n- **ridurre il rischio di interpretazioni errate**, poiché una misura non affidabile mette in dubbio i risultati e le eventuali decisioni che ne conseguono;  \n- **contestualizzare correttamente i risultati**, valutando come il tipo di errore (temporale, inter-valutatore, differenze di forme, ecc.) possa incidere sull’uso finale del punteggio;  \n- **guidare l’ottimizzazione del test**, rivelando quali aree dello strumento possono essere migliorate per incrementare la coerenza e la precisione della misurazione.\n\nIntegrare queste considerazioni nella fase di progettazione e valutazione di una misura consente di sviluppare strumenti più sensibili e appropriati alle finalità dell’indagine psicologica, garantendo **robustezza** e **credibilità** alle conclusioni tratte.\n\n## Session Info\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.4.2 (2024-10-31)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.3.2\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] ltm_1.2-0          polycor_0.8-1      msm_1.8.2         \n#>  [4] modelsummary_2.3.0 ggokabeito_0.1.0   see_0.11.0        \n#>  [7] MASS_7.3-65        viridis_0.6.5      viridisLite_0.4.2 \n#> [10] ggpubr_0.6.0       ggExtra_0.10.1     gridExtra_2.3     \n#> [13] patchwork_1.3.0    bayesplot_1.11.1   semTools_0.5-6    \n#> [16] semPlot_1.1.6      lavaan_0.6-19      psych_2.4.12      \n#> [19] scales_1.3.0       markdown_1.13      knitr_1.50        \n#> [22] lubridate_1.9.4    forcats_1.0.0      stringr_1.5.1     \n#> [25] dplyr_1.1.4        purrr_1.0.4        readr_2.1.5       \n#> [28] tidyr_1.3.1        tibble_3.2.1       ggplot2_3.5.1     \n#> [31] tidyverse_2.0.0    here_1.0.1        \n#> \n#> loaded via a namespace (and not attached):\n#>   [1] rstudioapi_0.17.1   jsonlite_1.9.1      magrittr_2.0.3     \n#>   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#>   [7] nloptr_2.2.1        rmarkdown_2.29      vctrs_0.6.5        \n#>  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#>  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#>  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#>  [19] emmeans_1.10.7      zoo_1.8-13          admisc_0.37        \n#>  [22] igraph_2.1.4        mime_0.13           lifecycle_1.0.4    \n#>  [25] pkgconfig_2.0.3     Matrix_1.7-3        R6_2.6.1           \n#>  [28] fastmap_1.2.0       rbibutils_2.3       shiny_1.10.0       \n#>  [31] digest_0.6.37       OpenMx_2.21.13      fdrtool_1.2.18     \n#>  [34] colorspace_2.1-1    rprojroot_2.0.4     Hmisc_5.2-3        \n#>  [37] timechange_0.3.0    abind_1.4-8         compiler_4.4.2     \n#>  [40] withr_3.0.2         glasso_1.11         htmlTable_2.4.3    \n#>  [43] backports_1.5.0     carData_3.0-5       ggsignif_0.6.4     \n#>  [46] corpcor_1.6.10      gtools_3.9.5        tools_4.4.2        \n#>  [49] pbivnorm_0.6.0      foreign_0.8-88      zip_2.3.2          \n#>  [52] httpuv_1.6.15       nnet_7.3-20         glue_1.8.0         \n#>  [55] quadprog_1.5-8      nlme_3.1-167        promises_1.3.2     \n#>  [58] lisrelToR_0.3       grid_4.4.2          checkmate_2.3.2    \n#>  [61] cluster_2.1.8.1     reshape2_1.4.4      generics_0.1.3     \n#>  [64] gtable_0.3.6        tzdb_0.5.0          data.table_1.17.0  \n#>  [67] hms_1.1.3           car_3.1-3           tables_0.9.31      \n#>  [70] sem_3.1-16          pillar_1.10.1       rockchalk_1.8.157  \n#>  [73] later_1.4.1         splines_4.4.2       lattice_0.22-6     \n#>  [76] survival_3.8-3      kutils_1.73         tidyselect_1.2.1   \n#>  [79] miniUI_0.1.1.1      pbapply_1.7-2       reformulas_0.4.0   \n#>  [82] stats4_4.4.2        xfun_0.51           expm_1.0-0         \n#>  [85] qgraph_1.9.8        arm_1.14-4          stringi_1.8.4      \n#>  [88] pacman_0.5.1        boot_1.3-31         evaluate_1.0.3     \n#>  [91] codetools_0.2-20    mi_1.1              cli_3.6.4          \n#>  [94] RcppParallel_5.1.10 rpart_4.1.24        xtable_1.8-4       \n#>  [97] Rdpack_2.6.3        munsell_0.5.1       Rcpp_1.0.14        \n#> [100] coda_0.19-4.1       png_0.1-8           XML_3.99-0.18      \n#> [103] parallel_4.4.2      jpeg_0.1-10         lme4_1.1-36        \n#> [106] mvtnorm_1.3-3       openxlsx_4.2.8      rlang_1.1.5        \n#> [109] multcomp_1.4-28     mnormt_2.1.1\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}