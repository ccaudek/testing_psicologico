{
  "hash": "b18097e29e29f97e3e4752bacfbc8605",
  "result": {
    "engine": "knitr",
    "markdown": "# Modello di Regressione Logistica {#sec-mokken-logistic-reg}\n\n::: callout-important\n## In questo capitolo imparerai:\n\n- A comprendere le limitazioni del modello di regressione lineare applicato a variabili binarie.  \n- A conoscere il modello di regressione logistica e i suoi componenti principali: aleatoria, sistematica e funzione di legame.  \n- A interpretare i coefficienti della regressione logistica in termini di logit, odds ratio e probabilità.  \n- A calcolare e visualizzare le probabilità predette utilizzando modelli logistici in R.  \n- A distinguere tra il modello lineare nei logit e il modello non lineare nelle probabilità.  \n:::\n\n::: callout-tip\n## Prerequisiti\n\n- Si consiglia di ripassare i concetti fondamentali dell'analisi di regressione. Per approfondire, si rimanda al materiale didattico dell'insegnamento di Psicometria disponibile al seguente [link](https://ccaudek.github.io/psicometria-r/).\n:::\n\n::: callout-caution\n## Preparazione del Notebook\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\n  car, LaplacesDemon, gmodels, effects, psych, jtools, see, sjPlot\n)\n```\n:::\n\n\n\n:::\n\n\n## Introduzione\n\nPrima di affrontare i modelli parametrici e non parametrici della Teoria della Risposta all’Item (IRT), è fondamentale acquisire una solida comprensione del modello di regressione logistica. Questo modello, ampiamente utilizzato per l’analisi di dati categorici, rappresenta un punto di riferimento cruciale per comprendere i principi che i modelli IRT sviluppano ed estendono. \n\nLa regressione logistica consente di stimare la probabilità di un evento in funzione di una o più variabili predittive, fornendo una base teorica utile per interpretare la probabilità che un esaminando risponda correttamente a un item. Tuttavia, mentre il modello di regressione logistica si limita a considerare la relazione tra le variabili predittive e l’evento, i modelli IRT aggiungono un livello di complessità: integrano l’abilità individuale dell’esaminando e le caratteristiche specifiche degli item, permettendo un’analisi più articolata.\n\nNonostante alcune similitudini, i modelli IRT si differenziano profondamente dalla regressione logistica. Essi non solo modellano simultaneamente le proprietà degli item e le abilità degli individui, ma considerano anche le interdipendenze tra le risposte agli item e tra gli item stessi, superando l’assunzione di indipendenza tra le osservazioni tipica della regressione logistica. \n\nIn questo capitolo, esploreremo il modello di regressione logistica come punto di partenza per comprendere le basi teoriche e applicative dei modelli IRT, fornendo una transizione chiara e graduale verso l’approfondimento di questa teoria fondamentale.\n\n## Modello di Regressione Logistica per Variabili Binarie\n\nLa regressione logistica è uno strumento utile per analizzare la relazione tra una **variabile dipendente dicotomica** (che assume due valori, ad esempio \"successo\" e \"fallimento\") e una o più **variabili indipendenti** (che possono essere sia quantitative che qualitative). È particolarmente utile per modellare situazioni in cui vogliamo stimare la probabilità che un evento si verifichi in base alle caratteristiche di un individuo o di una situazione.\n\n### Definizione del Modello\n\nConsideriamo $n$ osservazioni indipendenti, dove $Y_i$ rappresenta l'osservazione $i$-esima della variabile dipendente (ad esempio, \"successo\" o \"fallimento\"), con $i = 1, \\dots, n$. Ogni osservazione è associata a un insieme di variabili esplicative $(x_{1}, \\dots, x_{p})$. L'obiettivo è stimare la probabilità di successo, denotata con $\\pi_i$, data la combinazione delle variabili esplicative:\n\n$$\nP(Y = 1 \\mid X = x_i) = \\pi_i.\n$$\n\nIn questo contesto, $Y$ segue una distribuzione di Bernoulli, quindi può assumere solo due valori:\n\n$$\ny_i =\n\\begin{cases}\n1 & \\text{se si verifica un successo (osservazione $i$)}, \\\\\n0 & \\text{se si verifica un fallimento.}\n\\end{cases}\n$$\n\nLe probabilità associate sono:\n\n- $\\pi_i$ per il successo, \n- $1 - \\pi_i$ per il fallimento.\n\nLa **media condizionata** $\\mathbb{E}(Y \\mid X = x)$ rappresenta la probabilità attesa di successo per un dato valore $x$ delle variabili esplicative:\n\n$$\n\\mathbb{E}(Y \\mid X = x) = \\pi_i.\n$$\n\n### Esempio Pratico\n\nImmaginiamo un dataset con 100 soggetti, in cui:\n\n- `age` è una variabile esplicativa che indica l’età,\n- `chd` è la variabile dipendente che indica la presenza (`chd = 1`) o l’assenza (`chd = 0`) di disturbi cardiaci.\n\nLa probabilità condizionata $\\pi_i$ indica la probabilità di osservare disturbi cardiaci in un certo gruppo d’età:\n\n$$\n\\pi_i = P(Y = 1 \\mid X = x).\n$$\n\nPer valori discreti della variabile `age`, possiamo calcolare la proporzione di individui con $Y = 1$ (cioè con disturbi cardiaci) per ogni gruppo di età. Queste proporzioni rappresentano una **stima non parametrica** della funzione di regressione tra `chd` e `age`.\n\n### Visualizzazione dei Dati\n\nCon il dataset, possiamo calcolare queste proporzioni e rappresentarle graficamente:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nchdage <- rio::import(\n    here::here(\"data\", \"logistic_reg\", \"chdage_dat.txt\")\n)\n\n# Calcolo delle proporzioni di Y = 1 (chd) per età\nprop_data <- chdage %>%\n    group_by(age) %>%\n    summarise(prop_chd = mean(chd))\n\n# Creazione del grafico con smoothing\nggplot(prop_data, aes(x = age, y = prop_chd)) +\n    geom_point() + # Punti proporzione\n    geom_smooth(method = \"loess\", span = 0.7) + # Smoothing LOESS\n    labs(\n        title = \"Relazione tra Età e Disturbi Cardiaci\",\n        x = \"Età\",\n        y = \"Proporzione di CHD = 1\"\n    )\n```\n\n::: {.cell-output-display}\n![](01_logistic_regr_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n### Interpretazione della Relazione\n\nOsservando il grafico:\n\n- Per valori bassi di `age`, la proporzione di `Y = 1` (presenza di disturbi cardiaci) è vicina a 0.\n- Per valori elevati di `age`, la proporzione di `Y = 1` tende a 1.\n- Per valori intermedi di `age`, la proporzione aumenta gradualmente, seguendo un andamento sigmoidale.\n\nQuesto andamento riflette la natura probabilistica del fenomeno: la probabilità di disturbi cardiaci cresce con l'età, ma non è una crescita lineare.\n\n### Vantaggio del Modello Logistico\n\nSebbene la stima non parametrica (ad esempio LOESS) possa fornire un quadro generale, la regressione logistica permette di modellare questa relazione utilizzando una semplice funzione. Questo approccio è particolarmente vantaggioso quando ci sono più variabili esplicative, consentendo di quantificare come ciascuna contribuisce alla probabilità di un evento.\n\n## Modello Lineare nelle Probabilità\n\nIl **modello lineare nelle probabilità** rappresenta un primo approccio per descrivere la relazione tra una variabile dipendente binaria (ad esempio \"successo\" o \"fallimento\") e una variabile indipendente continua o categoriale. La sua formulazione è data da:\n\n$$\nY_i = \\alpha + \\beta X_i + \\varepsilon_i,\n$$\n\ndove:\n\n- $\\alpha$ è l’intercetta,\n- $\\beta$ è il coefficiente di regressione,\n- $\\varepsilon_i$ rappresenta l’errore, che si assume distribuito normalmente con media 0 e varianza costante ($\\varepsilon_i \\sim \\mathcal{N}(0, 1)$).\n\nIl valore atteso di $Y_i$ è quindi \n\n$$\\mathbb{E}(Y_i) = \\alpha + \\beta X_i,$$\n\nche porta alla stima della probabilità di successo $\\pi_i$ come:\n\n$$\n\\pi_i = \\alpha + \\beta X_i.\n$$\n\n### Limiti del Modello Lineare nelle Probabilità\n\nNonostante la semplicità, il modello lineare nelle probabilità presenta alcune problematiche. \n\n1. **Valori predetti fuori dall’intervallo [0,1].**\nPoiché $\\pi_i = \\alpha + \\beta X_i$ è una funzione lineare, i valori predetti di $\\pi_i$ possono essere negativi o superiori a 1, il che è incompatibile con l’interpretazione di $\\pi_i$ come probabilità.\n\n2. **Assunzione di normalità degli errori.**\nLa variabile dipendente $Y_i$ è binaria (0 o 1), ma l’errore $\\varepsilon_i$ non segue una distribuzione normale. Ad esempio, se $Y_i = 1$, l’errore sarà:\n\n$$\n\\varepsilon_i = 1 - \\mathbb{E}(Y_i) = 1 - (\\alpha + \\beta X_i) = 1 - \\pi_i.\n$$\n\nAnalogamente, se $Y_i = 0$, l’errore sarà:\n\n$$\n\\varepsilon_i = 0 - \\mathbb{E}(Y_i) = 0 - (\\alpha + \\beta X_i) = - \\pi_i.\n$$\n\nPertanto, gli errori sono dicotomici e non normali.\n\n3. **Problemi di omoschedasticità.**\n\nNel modello lineare nelle probabilità, la varianza degli errori dipende dalla media $\\pi_i$, quindi non è costante. La varianza degli errori si calcola come:\n\n$$\n\\mathbb{V}(\\varepsilon_i) = (1-\\pi_i)\\pi_i.\n$$\ndove $\\pi_i$ varia in funzione di $X_i$. Dato che $\\pi_i$ dipende da $x$, ciò significa che la varianza non è costante in funzione di $x$. Questa eteroschedasticità viola una delle assunzioni fondamentali del metodo dei minimi quadrati.\n\n4. **Linearità irrealistica.**\nLa relazione tra $X_i$ e la probabilità di successo non è sempre lineare nella realtà. Ad esempio, per valori estremi di $X_i$, una relazione lineare può portare a predizioni improbabili (valori negativi o superiori a 1) e non cattura l’andamento sigmoidale tipico di molti fenomeni.\n\n## Modello Lineare nelle Probabilità Vincolato\n\nUn tentativo di risolvere il problema dei valori predetti fuori dall’intervallo [0,1] consiste nell’introdurre vincoli:\n\n$$\n\\pi=\n\\begin{cases}\n  0                           &\\text{se $\\alpha + \\beta X < 0$},\\\\\n  \\alpha + \\beta X           &\\text{se $0 \\leq \\alpha + \\beta X \\leq 1$},\\\\\n  1 &\\text{se $\\alpha + \\beta X > 1$}.\n\\end{cases}\n$$\n\nTuttavia, questo approccio presenta diversi limiti:\n\n- **Dipendenza critica dai valori estremi di $\\pi$:** I valori di $\\pi = 0$ e $\\pi = 1$ dipendono fortemente dai valori più bassi e più alti di $X_i$, che possono variare tra campioni.\n- **Cambiamenti bruschi nella pendenza:** La curva di regressione subisce variazioni improvvise vicino agli estremi, risultando poco realistica.\n- **Complicazioni con più variabili esplicative:** Quando il numero di variabili indipendenti aumenta, il modello diventa instabile e difficile da interpretare.\n\n## Regressione Logistica\n\nLa regressione logistica offre una soluzione efficace per modellare probabilità, garantendo che i valori previsti siano sempre compresi nell'intervallo $[0,1]$. Invece di specificare un modello direttamente per le probabilità condizionate $\\pi_i$, si definisce un modello lineare per una loro trasformazione: il logaritmo degli odds, noto come *logit*. Questa trasformazione risolve il problema del vincolo imposto dall’intervallo delle probabilità.\n\n### Modello Logistico e Logit\n\nIl logit, definito come il logaritmo naturale del rapporto tra probabilità di successo e probabilità di fallimento, è dato da:\n\n$$\n\\eta_i = \\log_e \\frac{\\pi_i}{1-\\pi_i} = \\alpha + \\beta x_i,\n$$\n\ndove:\n\n- $\\eta_i$ è il logit, sempre un numero reale,\n- $\\alpha$ e $\\beta$ sono i parametri del modello,\n- $x_i$ è la variabile esplicativa.\n\nQuesto approccio consente di modellare $\\pi_i$ come una funzione non lineare di $x_i$, espressa dalla funzione logistica:\n\n$$\n\\pi_i = \\frac{1}{1 + e^{-(\\alpha + \\beta x_i)}}.\n$$\n\n### Caratteristiche del Modello Logistico\n\nIl modello logistico presenta i seguenti vantaggi:\n\n- **Intervallo limitato**: garantisce che $\\pi_i$ sia sempre compreso tra 0 e 1.\n- **Relazione sigmoidale**: rappresenta una transizione fluida tra probabilità basse e alte in funzione di $x_i$.\n- **Adatto a variabili dicotomiche**: rispetta la natura della variabile dipendente.\n\n### Relazione tra Probabilità, Odds e Logit\n\nLa relazione tra probabilità ($P$), odds ($O$) e logit ($L$) è illustrata nella tabella seguente:\n\n| Probabilità ($P$) | Odds ($O = P / (1-P)$) | Logit ($L = \\ln(O)$) |\n|---------------------|-------------------------|-------------------------|\n| 0.01                | 0.01 / 0.99 = 0.0101   | $-4.60$              |\n| 0.50                | 0.50 / 0.50 = 1.0000   | $0.00$               |\n| 0.99                | 0.99 / 0.01 = 99.0000  | $4.60$               |\n\nIl logit trasforma l’intervallo $[0,1]$ della probabilità in tutta la linea reale, semplificando l’uso di modelli lineari.\n\n### Logit Empirici e Relazione Lineare\n\nPer visualizzare la relazione tra variabili trasformate, è possibile calcolare i *logit empirici*. Consideriamo un esempio con 8 intervalli della variabile `age`, calcolando il logit degli odds per ciascun gruppo. La relazione risultante è lineare, come mostrato dal seguente codice:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat1 <- chdage %>%\n    mutate(age_c = ntile(age, 8)) %>%\n    group_by(age_c) %>%\n    summarise(\n        age_bin_center = (min(age) + max(age)) / 2,\n        proportion_heart_disease = mean(chd)\n    )\n\nxc <- dat1$age_bin_center\nyc <- dat1$proportion_heart_disease\nlogit_y <- log(yc / (1 - yc))\nfit <- lm(logit_y ~ xc)\n\nplot(\n    xc, logit_y,\n    xlab = \"Età\", ylab = \"Logit(Y)\",\n    main = \"Relazione Lineare tra Logit e Età\", type = \"n\"\n)\npoints(xc, logit_y, cex = 2)\nabline(fit)\n```\n\n::: {.cell-output-display}\n![](01_logistic_regr_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n### Modello Logistico Applicato\n\nUtilizzando un modello logistico, possiamo rappresentare l’andamento sigmoidale della probabilità condizionata:\n\n$$\n\\pi_i = \\frac{e^{\\alpha + \\beta x_i}}{1 + e^{\\alpha + \\beta x_i}}.\n$$\n\nIn R, il modello può essere stimato come segue:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfm <- glm(chd ~ age, family = binomial(link = \"logit\"), data = chdage)\nlogit_hat <- fm$coef[1] + fm$coef[2] * chdage$age\npi_hat <- exp(logit_hat) / (1 + exp(logit_hat))\n\nplot(chdage$age, pi_hat,\n    xlab = \"Età\",\n    ylab = \"P(CHD)\",\n    main = \"Probabilità di Malattia Cardiaca\", type = \"n\"\n)\nlines(chdage$age, pi_hat)\npoints(dat1$age_bin_center, dat1$proportion_heart_disease, cex = 2)\n```\n\n::: {.cell-output-display}\n![](01_logistic_regr_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\nUn’alternativa per visualizzare i risultati è l’uso del pacchetto `sjPlot`:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot_model(fm, type = \"pred\", terms = \"age\") +\n  labs(y = \"Probabilità di Malattia Cardiaca\")\n```\n\n::: {.cell-output-display}\n![](01_logistic_regr_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\nIn conclusione, la regressione logistica rappresenta un metodo robusto e flessibile per modellare probabilità, superando i limiti del modello lineare nelle probabilità. La sua capacità di rappresentare relazioni non lineari e rispettare i vincoli probabilistici la rende ideale per l’analisi di variabili dipendenti binarie.\n\n## Modelli Lineari Generalizzati (GLM)\n\nI **Modelli Lineari Generalizzati (GLM)** rappresentano una potente estensione del modello lineare classico, progettata per affrontare i limiti che emergono con variabili risposta di natura non continua o con varianze non costanti. Nel caso di una variabile risposta binaria, come nel modello di regressione logistica, il modello lineare classico incontra diverse difficoltà:\n\n1. **Distribuzione Binomiale**: La variabile risposta $Y_i$ segue una distribuzione binomiale (con parametro $n_i$, tipicamente $n_i = 1$ per dati individuali), incompatibile con l’assunzione di normalità.\n2. **Vincoli sulle Probabilità**: Specificare un modello lineare come $\\pi_i = \\beta_0 + \\beta_1 x_i$ può portare a stime di probabilità fuori dall’intervallo $[0,1]$.\n3. **Varianze Non Costanti**: La varianza dei residui, calcolata come $V(\\varepsilon_i) = \\pi_i (1 - \\pi_i)$, varia in funzione di $\\pi_i$.\n\nI GLM affrontano queste sfide consentendo di specificare una relazione tra la media attesa della variabile risposta e le variabili esplicative attraverso una funzione di legame. Questi modelli includono varianti come:\n\n- **Regressione Lineare**: Per variabili dipendenti continue.\n- **Regressione Logistica**: Per variabili risposta binarie.\n- **Modello Loglineare di Poisson**: Per conteggi o frequenze in tabelle di contingenza.\n\n### Struttura dei GLM\n\nUn GLM si compone di tre elementi principali:\n\n1. **Componente Aleatoria**: Specifica la distribuzione della variabile risposta $Y_i$, ad esempio:\n   - Normale per variabili continue,\n   - Binomiale per variabili binarie,\n   - Poisson per conteggi.\n\n2. **Componente Sistematica**: Definisce la relazione lineare tra le variabili esplicative e una trasformazione della media attesa della variabile risposta. È rappresentata dal predittore lineare:\n\n   $$\n   \\eta_i = \\alpha + \\sum_{j} \\beta_j X_{ij}.\n   $$\n\n3. **Funzione di Legame**: Trasforma la media attesa $\\mathbb{E}(Y_i)$ in modo che sia modellata linearmente rispetto a $\\eta_i$. Ad esempio, nella regressione logistica, il legame è dato dal logit.\n\n| Componente Aleatoria | Funzione di Legame | Applicazione               |\n|-----------------------|--------------------|----------------------------|\n| Gaussiana             | Identità          | Regressione lineare        |\n| Binomiale             | Logit             | Regressione logistica      |\n| Poisson               | Logaritmo         | Modello loglineare         |\n\n### Componente Sistematica\n\nLa componente sistematica descrive come le variabili esplicative ($X_{ij}$) influenzano il predittore lineare $\\eta_i$. Per $k$ osservazioni e $p$ variabili esplicative, il predittore lineare è definito come:\n\n$$\n\\eta_i = \\alpha + \\sum_{j} \\beta_j X_{ij},\n$$\n\ndove:\n\n- $\\alpha$ è l’intercetta,\n- $\\beta_j$ sono i coefficienti delle variabili esplicative.\n\n### Componente Aleatoria\n\nLa componente aleatoria assume che le osservazioni $Y_i$ siano realizzazioni indipendenti di una variabile casuale. Per una variabile risposta binaria:\n\n$$\nY_i \\sim \\text{Bin}(n_i, \\pi_i),\n$$\n\ndove $n_i = 1$ per dati individuali.\n\n### Funzione di Legame\n\nLa funzione di legame $g(\\cdot)$ connette la media attesa $\\mathbb{E}(Y_i) = \\pi_i$ alla componente sistematica $\\eta_i$. Per la regressione logistica, il legame è dato dal logit:\n\n$$\n\\eta_i = g(\\pi_i) = \\ln\\left(\\frac{\\pi_i}{1-\\pi_i}\\right).\n$$\n\nLa funzione legame è invertibile, consentendo di esprimere la probabilità $\\pi_i$ come funzione del predittore lineare:\n\n$$\n\\pi_i = \\frac{e^{\\eta_i}}{1 + e^{\\eta_i}} = \\frac{e^{\\alpha + \\sum_j \\beta_j X_{ij}}}{1 + e^{\\alpha + \\sum_j \\beta_j X_{ij}}}.\n$$\n\nQuesta relazione permette di ottenere un modello non lineare per le probabilità $\\pi_i$.\n\n### Visualizzazione della Funzione Logistica\n\nLa funzione logistica, che rappresenta il legame tra il predittore lineare $\\eta_i$ e la probabilità $\\pi_i$, ha un andamento sigmoidale:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- seq(-5, 5, length.out = 100)\nprob <- plogis(x)  # Inversa del logit\nplot(x, prob, type = \"l\", \n     main = \"Funzione Logistica\", \n     ylab = \"Probabilità (\\u03c0)\", \n     xlab = \"Valori di \\u03b7\")\n```\n\n::: {.cell-output-display}\n![](01_logistic_regr_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n### Applicazioni dei GLM\n\nI GLM sono particolarmente utili in contesti in cui:\n\n- La variabile risposta non è continua (es. binaria o discreta),\n- Le varianze non sono costanti,\n- La relazione tra media e predittore è non lineare.\n\nNella regressione logistica, la combinazione di queste componenti consente di descrivere in modo accurato la probabilità di successo in funzione delle variabili esplicative.\n\n## Regressione Logistica con R\n\nLa regressione logistica può essere implementata in R utilizzando la funzione `glm()` (Generalized Linear Model). Questo metodo consente di stimare i parametri del modello, tenendo conto della distribuzione della variabile risposta e della funzione di legame appropriata.\n\nPer stimare i parametri del modello sui dati dell'esempio, si utilizza il seguente codice:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfm <- glm(chd ~ age,\n    family = binomial(link = \"logit\"),\n    data = chdage\n)\n```\n:::\n\n\n\n\n- **`family = binomial`**: Specifica che la variabile risposta segue una distribuzione binomiale (necessaria per una variabile binaria come `chd`).\n- **`link = \"logit\"`**: Indica che la funzione di legame utilizzata è il logit.\n\nL'output del modello può essere visualizzato con la funzione `summary()`:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(fm)\n#> \n#> Call:\n#> glm(formula = chd ~ age, family = binomial(link = \"logit\"), data = chdage)\n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)\n#> (Intercept)  -5.3095     1.1337   -4.68  2.8e-06\n#> age           0.1109     0.0241    4.61  4.0e-06\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 136.66  on 99  degrees of freedom\n#> Residual deviance: 107.35  on 98  degrees of freedom\n#> AIC: 111.4\n#> \n#> Number of Fisher Scoring iterations: 4\n```\n:::\n\n\n\n\nL'output mostra i coefficienti stimati ($\\alpha$ e $\\beta$), i loro errori standard e altre statistiche. Per i dati dell'esempio, i risultati principali sono:\n\n1. Intercetta ($\\alpha = -5.309$): Indica il log-odds di sviluppare CHD a età 0.\n2. Coefficiente di Età ($\\beta = 0.111$): Indica come i log-odds di CHD cambiano per ogni anno aggiuntivo di età.\n\nLe equazioni risultanti sono:\n\n- **Logit stimato**:  \n  $$\n  \\hat{\\eta}(x) = -5.309 + 0.111 \\cdot \\text{age}.\n  $$\n\n- **Probabilità stimata**:  \n  $$\n  \\hat{\\pi}(x) = \\frac{e^{-5.309 + 0.111 \\cdot \\text{age}}}{1 + e^{-5.309 + 0.111 \\cdot \\text{age}}}.\n  $$\n\n### Interpretazione dei Coefficienti\n\nLa comprensione dei coefficienti del modello di regressione logistica può essere suddivisa in tre livelli: log-odds, odds ratio e probabilità predette.\n\n### Interpretazione Basata sui Log-Odds\n\n1. **Intercetta ($-5.309$)**:  \n   - Rappresenta i log-odds di sviluppare CHD quando l'età è 0.\n   - Un valore negativo suggerisce che la probabilità di CHD è molto bassa a età 0.\n\n2. **Coefficiente di Età ($0.111$)**:  \n   - Ogni anno aggiuntivo di età aumenta i log-odds di CHD di 0.111.  \n   - Un coefficiente positivo indica che il rischio di CHD aumenta con l'età.\n\n### Interpretazione Attraverso l'Odds Ratio\n\nPer una comprensione più intuitiva, il coefficiente di età può essere trasformato in un odds ratio esponenziando il valore del coefficiente:\n\n$$\n\\text{Odds Ratio per Età} = e^{0.111} \\approx 1.12.\n$$\n\n- **Significato**:  \n  - Un odds ratio di 1.12 implica che per ogni anno di età in più, gli odds di sviluppare CHD aumentano del 12%.\n  - Se l'odds ratio fosse pari a 1, ciò indicherebbe che l'età non influisce sul rischio di CHD.\n\n### Interpretazione Basata sulle Probabilità Predette\n\nIl modo più diretto per interpretare l'impatto delle variabili esplicative è attraverso le probabilità predette. Le probabilità mostrano come il rischio di CHD varia con l'età.\n\nPossiamo calcolare e visualizzare le probabilità predette per diverse età utilizzando il pacchetto `jtools` con la funzione `effect_plot()`:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\neffect_plot(fm,\n    pred = age, interval = TRUE, plot.points = TRUE,\n    jitter = 0.05\n)\n```\n\n::: {.cell-output-display}\n![](01_logistic_regr_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n- **`pred = age`**: Indica che vogliamo calcolare le probabilità predette in funzione di `age`.\n- **`interval = TRUE`**: Aggiunge intervalli di confidenza per le stime.\n- **`plot.points = TRUE`**: Mostra i punti osservati sui dati originali.\n\nQuesto grafico rappresenta la relazione sigmoidale tra età e probabilità di CHD, fornendo una rappresentazione intuitiva e accessibile anche a chi non ha una formazione avanzata in statistica.\n\nIn conclusione, la regressione logistica in R, tramite `glm()`, è uno strumento versatile per analizzare variabili binarie. L'interpretazione dei coefficienti attraverso log-odds, odds ratio e probabilità predette offre molteplici prospettive utili per comprendere l'effetto delle variabili esplicative e comunicare i risultati in modo chiaro ed efficace.\n\n## Riflessioni Conclusive\n\nNel caso di una variabile dipendente binaria $Y_i$, il tradizionale modello di regressione lineare risulta inadatto, principalmente a causa della natura discreta di $Y_i$, della varianza non costante e della necessità di vincolare i valori predetti all’intervallo $[0, 1]$. Queste limitazioni vengono superate applicando un modello lineare non direttamente alla probabilità $\\pi_i$ (il valore atteso di $Y_i$), ma a una sua trasformazione: il logit.\n\nNel modello di regressione logistica, la **componente sistematica** esprime i logit, definiti come il logaritmo naturale degli odds, come una funzione lineare dei predittori:\n\n$$\n\\ln \\frac{\\pi_i}{1-\\pi_i} = \\alpha + \\beta X_i.\n$$\n\nQuesto rende il modello **lineare nei logit**, semplificando la relazione tra variabili esplicative e odds. La funzione logit è invertibile, e la trasformazione inversa (antilogit) consente di esprimere le probabilità $\\pi_i$ in funzione del predittore lineare $\\eta_i = \\alpha + \\beta X_i$:\n\n$$\n\\pi_i = \\frac{\\exp(\\alpha + \\beta X_i)}{1 + \\exp(\\alpha + \\beta X_i)}.\n$$\n\nQuesta relazione rende il modello **non lineare rispetto alle probabilità**, ma garantisce che i valori predetti rimangano nell’intervallo $[0, 1]$.\n\nNel contesto della regressione logistica, il **valore atteso** della variabile dipendente $Y_i$, condizionato ai valori dei predittori, rappresenta la probabilità che $Y_i$ assuma il valore 1:\n\n$$\n\\mathbb{E}(Y \\mid x_i) = Pr(Y = 1 \\mid X = x_i) \\equiv \\pi_i.\n$$\n\nQuesto valore può essere interpretato come la proporzione di individui nella popolazione con $Y = 1$ per una data combinazione di valori $X = x_i$.\n\nLa **componente aleatoria** del modello considera $Y_i$ come una variabile aleatoria binomiale, con due scenari principali:\n\n1. **Dati raggruppati**: Quando le osservazioni sono aggregate, la variabile risposta segue una distribuzione binomiale con parametro $n_i$, dove $n_i$ rappresenta il numero di osservazioni per ogni gruppo omogeneo di predittori.\n2. **Dati individuali**: Quando ogni osservazione è indipendente, $n_i = 1$ per tutte le unità.\n\nLa funzione logistica:\n\n$$\n\\Lambda(\\eta) = \\frac{\\exp(\\eta)}{1 + \\exp(\\eta)},\n$$\n\nè stata scelta come funzione di legame per trasformare il predittore lineare $\\eta_i = \\alpha + \\beta X_i$ nelle probabilità $\\pi_i$. Questa funzione è preferita per la sua semplicità interpretativa e per il fatto che produce un andamento sigmoidale, che descrive bene molte relazioni probabilistiche.\n\nIn conclusione, il modello di regressione logistica risolve elegantemente le limitazioni del modello lineare applicato a variabili binarie, fornendo un approccio flessibile e interpretabile:\n\n- **Lineare nei logit**: Il modello sfrutta la semplicità di una relazione lineare per descrivere log-odds.\n- **Non lineare nelle probabilità**: La funzione logistica garantisce che le probabilità predette siano sempre comprese nell’intervallo $[0, 1]$.\n- **Adatto a variabili binarie**: La componente aleatoria binomiale riflette la natura discreta della variabile dipendente.\n\nQuesto modello si dimostra particolarmente utile in ambiti dove le variabili risposta sono dicotomiche, offrendo interpretazioni intuitive tramite logit, odds e probabilità. La sua flessibilità consente di essere applicato sia a dati individuali sia a dati raggruppati, rendendolo uno strumento fondamentale per analisi statistiche moderne.\n\n## Session Info\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.4.2 (2024-10-31)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.3.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] sjPlot_2.8.17        jtools_2.3.0         effects_4.2-2       \n#>  [4] gmodels_2.19.1       LaplacesDemon_16.1.6 car_3.1-3           \n#>  [7] carData_3.0-5        ggokabeito_0.1.0     see_0.10.0          \n#> [10] MASS_7.3-64          viridis_0.6.5        viridisLite_0.4.2   \n#> [13] ggpubr_0.6.0         ggExtra_0.10.1       gridExtra_2.3       \n#> [16] patchwork_1.3.0      bayesplot_1.11.1     semTools_0.5-6      \n#> [19] semPlot_1.1.6        lavaan_0.6-19        psych_2.4.12        \n#> [22] scales_1.3.0         markdown_1.13        knitr_1.49          \n#> [25] lubridate_1.9.4      forcats_1.0.0        stringr_1.5.1       \n#> [28] dplyr_1.1.4          purrr_1.0.4          readr_2.1.5         \n#> [31] tidyr_1.3.1          tibble_3.2.1         ggplot2_3.5.1       \n#> [34] tidyverse_2.0.0      here_1.0.1          \n#> \n#> loaded via a namespace (and not attached):\n#>   [1] splines_4.4.2       later_1.4.1         R.oo_1.27.0        \n#>   [4] datawizard_1.0.0    XML_3.99-0.18       rpart_4.1.24       \n#>   [7] lifecycle_1.0.4     Rdpack_2.6.2        rstatix_0.7.2      \n#>  [10] rprojroot_2.0.4     globals_0.16.3      lattice_0.22-6     \n#>  [13] insight_1.0.2       rockchalk_1.8.157   backports_1.5.0    \n#>  [16] survey_4.4-2        magrittr_2.0.3      openxlsx_4.2.8     \n#>  [19] Hmisc_5.2-2         rmarkdown_2.29      httpuv_1.6.15      \n#>  [22] qgraph_1.9.8        zip_2.3.2           RColorBrewer_1.1-3 \n#>  [25] pbapply_1.7-2       DBI_1.2.3           minqa_1.2.8        \n#>  [28] multcomp_1.4-28     abind_1.4-8         quadprog_1.5-8     \n#>  [31] R.utils_2.12.3      nnet_7.3-20         TH.data_1.1-3      \n#>  [34] sandwich_3.1-1      listenv_0.9.1       gdata_3.0.1        \n#>  [37] arm_1.14-4          performance_0.13.0  parallelly_1.42.0  \n#>  [40] codetools_0.2-20    tidyselect_1.2.1    ggeffects_2.2.0    \n#>  [43] farver_2.1.2        lme4_1.1-36         broom.mixed_0.2.9.6\n#>  [46] stats4_4.4.2        base64enc_0.1-3     jsonlite_1.8.9     \n#>  [49] Formula_1.2-5       survival_3.8-3      emmeans_1.10.7     \n#>  [52] tools_4.4.2         rio_1.2.3           Rcpp_1.0.14        \n#>  [55] glue_1.8.0          mnormt_2.1.1        mgcv_1.9-1         \n#>  [58] xfun_0.50           withr_3.0.2         fastmap_1.2.0      \n#>  [61] mitools_2.4         boot_1.3-31         digest_0.6.37      \n#>  [64] mi_1.1              timechange_0.3.0    R6_2.6.1           \n#>  [67] mime_0.12           estimability_1.5.1  colorspace_2.1-1   \n#>  [70] gtools_3.9.5        jpeg_0.1-10         R.methodsS3_1.8.2  \n#>  [73] generics_0.1.3      data.table_1.16.4   corpcor_1.6.10     \n#>  [76] htmlwidgets_1.6.4   pkgconfig_2.0.3     sem_3.1-16         \n#>  [79] gtable_0.3.6        furrr_0.3.1         htmltools_0.5.8.1  \n#>  [82] png_0.1-8           snakecase_0.11.1    reformulas_0.4.0   \n#>  [85] rstudioapi_0.17.1   tzdb_0.4.0          reshape2_1.4.4     \n#>  [88] coda_0.19-4.1       checkmate_2.3.2     nlme_3.1-167       \n#>  [91] nloptr_2.1.1        zoo_1.8-12          sjlabelled_1.2.0   \n#>  [94] parallel_4.4.2      miniUI_0.1.1.1      foreign_0.8-88     \n#>  [97] pillar_1.10.1       grid_4.4.2          vctrs_0.6.5        \n#> [100] promises_1.3.2      OpenMx_2.21.13      xtable_1.8-4       \n#> [103] cluster_2.1.8       htmlTable_2.4.3     evaluate_1.0.3     \n#> [106] pbivnorm_0.6.0      mvtnorm_1.3-3       cli_3.6.4          \n#> [109] kutils_1.73         compiler_4.4.2      rlang_1.1.5        \n#> [112] ggsignif_0.6.4      labeling_0.4.3      fdrtool_1.2.18     \n#> [115] plyr_1.8.9          sjmisc_2.8.10       stringi_1.8.4      \n#> [118] pander_0.6.5        munsell_0.5.1       lisrelToR_0.3      \n#> [121] pacman_0.5.1        Matrix_1.7-2        sjstats_0.19.0     \n#> [124] hms_1.1.3           glasso_1.11         future_1.34.0      \n#> [127] shiny_1.10.0        haven_2.5.4         rbibutils_2.3      \n#> [130] igraph_2.1.4        broom_1.0.7         RcppParallel_5.1.10\n```\n:::\n",
    "supporting": [
      "01_logistic_regr_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}