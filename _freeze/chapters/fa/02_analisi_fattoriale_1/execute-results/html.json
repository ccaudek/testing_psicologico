{
  "hash": "c4d204ad469b791996b27b5c82ca6b9b",
  "result": {
    "engine": "knitr",
    "markdown": "# Il modello unifattoriale {#sec-fa-unifactor-model}\n\n::: {.chapterintro data-latex=\"\"}\nL'analisi fattoriale è un modello statistico che consente di spiegare le correlazioni tra variabili osservate mediante la loro saturazione in uno o più fattori generali. In questo modello, le $p$ variabili osservate (item) sono considerate condizionalmente indipendenti rispetto a $m$ variabili latenti chiamate fattori. L'obiettivo dell'analisi fattoriale è di interpretare questi fattori come costrutti teorici inosservabili. Ad esempio, l'analisi fattoriale può essere utilizzata per spiegare le correlazioni tra le prestazioni di un gruppo di individui in una serie di compiti mediante il concetto di intelligenza. In questo modo, l'analisi fattoriale aiuta a identificare i costrutti cui gli item si riferiscono e a stabilire in che misura ciascun item rappresenta il costrutto. Il modello può essere unifattoriale ($m = 1$) o multifattoriale ($m > 1$), e in questo capitolo si introdurrà il modello unifattoriale che assume l'esistenza di un unico fattore comune latente.\n:::\n\n**Prerequisiti**\n\n- Leggere il capitolo 6, *Factor Analysis and Principal Component Analysis*, del testo *Principles of psychological assessment* di @petersen2024principles. \n\n**Concetti e Competenze Chiave**\n\n- Correlazione parziale\n- Teoria dei due fattori\n- Annullamento della tetrade\n\n**Preparazione del Notebook**\n\n\n\n\n::: {.cell vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\n# Carica il file _common.R per impostazioni di pacchetti e opzioni\nhere::here(\"code\", \"_common.R\") |> source()\n\n# Carica pacchetti aggiuntivi\npacman::p_load(corrplot, tidyr, kableExtra, lavaanPlot, lavaanExtra)\n```\n:::\n\n\n\n\n## Introduzione\n\nNel contesto dell'analisi fattoriale, i costrutti teorici di interesse vengono rappresentati da variabili latenti, che riflettono le comunanze sottostanti tra un insieme di variabili manifeste (ovvero misurabili). Le variabili manifeste, come gli item di un questionario o gli indici derivati da un compito comportamentale, sono direttamente osservabili e vengono solitamente rappresentate graficamente come quadrati. Al contrario, le variabili latenti descrivono costrutti ipotetici non direttamente osservabili, come un fattore latente di intelligenza o memoria, e sono rappresentate come cerchi nei diagrammi.\n\nIl legame tra variabili latenti e manifeste si esprime tramite i *carichi fattoriali*, cioè percorsi che collegano una variabile latente a una variabile osservabile. L'intensità di un carico fattoriale indica la quota di varianza osservata spiegata dal fattore latente. In altre parole, il carico fattoriale riflette la capacità di una data variabile manifesta di rappresentare il costrutto latente.\n\nDal punto di vista matematico, l’analisi fattoriale descrive ogni misura osservabile $y$ come la combinazione lineare del punteggio latente $\\xi$ relativo al costrutto di interesse e di un elemento di errore non osservato $\\delta$. In questo modello, il valore di $y$ è interpretato come il prodotto del punteggio latente, ponderato da un coefficiente di carico $\\lambda$, a cui si somma un termine di errore specifico $\\delta_y$:\n\n$$\ny = \\lambda \\xi + \\delta_y.\n$$\n\nUn esempio concreto può aiutare a chiarire questo concetto: supponiamo di usare una bilancia non perfettamente affidabile per misurare il peso corporeo. Ogni lettura non rifletterà solo il peso reale della persona, ma includerà anche una componente di errore dovuta alla bilancia stessa, manifestando variazioni casuali tra una misurazione e l’altra.\n\nQuando si dispone di più misure osservabili $y$ che rappresentano il medesimo costrutto latente $\\xi$, diventa possibile stimare con maggiore accuratezza sia il punteggio reale latente $\\xi$ sia la componente di errore di misura $\\delta$. Questo permette di migliorare la precisione interpretativa dei dati e di ottenere una rappresentazione più affidabile del costrutto teorico di interesse.\n\n## Modello monofattoriale\n\nCon $p$ variabili manifeste $y_i$, il caso più semplice è quello di un solo fattore comune: \n\n$$\n\\begin{equation}\ny_i = \\mu_i + \\lambda_{i} \\xi +  1 \\cdot \\delta_i \\qquad i=1, \\dots, p,\n\\end{equation}\n$$ {#eq-mod-unifattoriale}\n\ndove $\\xi$ rappresenta il fattore comune a tutte le $y_i$, $\\delta_i$ sono i fattori specifici o unici di ogni variabile osservata e $\\lambda_i$ sono le saturazioni (o pesi) fattoriali le quali stabiliscono il peso del fattore latente su ciascuna variabile osservata.\n\nIl modello di analisi fattoriale e il modello di regressione possono sembrare simili, ma presentano alcune differenze importanti. In primo luogo, sia il fattore comune $\\xi$ sia i fattori specifici $\\delta_i$ sono inosservabili, il che rende tutto ciò che si trova a destra dell'uguaglianza incognito. In secondo luogo, l'analisi di regressione e l'analisi fattoriale hanno obiettivi diversi. L'analisi di regressione mira a individuare le variabili esplicative, osservabili direttamente, che sono in grado di spiegare la maggior parte della varianza della variabile dipendente. Al contrario, il problema dell'analisi unifattoriale consiste nell'identificare la variabile esplicativa inosservabile che è in grado di spiegare la maggior parte della covarianza tra le variabili osservate.\n\nSolitamente, per comodità, si assume che la media delle variabili osservate $y_i$ sia zero, ovvero $\\mu_i=0$. Ciò equivale a considerare gli scarti delle variabili rispetto alle rispettive medie. Il modello unifattoriale assume che le variabili osservate siano il risultato della combinazione lineare di un fattore comune $\\xi$ e dei fattori specifici $\\delta_i$, ovvero:\n\n$$\n\\begin{equation}\ny_i -\\mu_i = \\lambda_i \\xi + 1 \\cdot \\delta_i,\n\\end{equation}\n$$ {#eq-mod-monofattoriale}\n\ndove $\\lambda_i$ è la saturazione o il peso della variabile $i$-esima sul fattore comune e $\\delta_i$ rappresenta il fattore specifico della variabile $i$-esima. Si assume che il fattore comune abbia media zero e varianza unitaria, mentre i fattori specifici abbiano media zero, varianza $\\psi_{i}$ e siano incorrelati tra loro e con il fattore comune. Nel modello unifattoriale, l'interdipendenza tra le variabili è completamente spiegata dal fattore comune.\n\nLe ipotesi precedenti consentono di ricavare la covarianza tra la variabile osservata $y_i$ e il fattore comune, la varianza della variabile osservata $y_i$ e la covarianza tra due variabili osservate $y_i$ e $y_k$. L'obiettivo della discussione in questo capitolo è appunto quello di analizzare tali grandezze statistiche.\n\n## Correlazione parziale\n\nPrima di entrare nel dettaglio del modello statistico dell'analisi fattoriale, è importante chiarire il concetto di correlazione parziale. Si attribuisce spesso a Charles Spearman la nascita dell'analisi fattoriale. Nel 1904, Spearman pubblicò un articolo intitolato \"General Intelligence, Objectively Determined and Measured\" in cui propose la Teoria dei Due Fattori. In questo articolo, dimostrò come fosse possibile identificare un fattore inosservabile a partire da una matrice di correlazioni, utilizzando il metodo dell'annullamento della tetrade (*tetrad differences*). L'annullamento della tetrade è un'applicazione della teoria della correlazione parziale che mira a stabilire se, controllando un insieme di variabili inosservabili chiamate fattori $\\xi_j$, le correlazioni tra le variabili osservabili $Y_i$, al netto degli effetti lineari delle $\\xi_j$, diventino statisticamente nulle.\n\nPossiamo considerare un esempio con tre variabili: $Y_1$, $Y_2$ e $F$. La correlazione tra $Y_1$ e $Y_2$, $r_{1,2}$, può essere influenzata dalla presenza di $F$. Per calcolare la correlazione parziale tra $Y_1$ e $Y_2$ al netto dell'effetto lineare di $F$, dobbiamo trovare le componenti di $Y_1$ e $Y_2$ che sono linearmente indipendenti da $F$.\n\nPer fare ciò, dobbiamo trovare la componente di $Y_1$ che è ortogonale a $F$. Possiamo calcolare i residui $E_1$ del modello:\n\n$$\nY_1 = b_{01} + b_{11}F + E_1.\n$$ {#eq-mod-reg-mult-fa}\n\nLa componente di $Y_1$ linearmente indipendente da $F$ è quindi data dai residui $E_1$. Possiamo eseguire un'operazione analoga per $Y_2$ per trovare la sua componente ortogonale a $F$. Calcolando la correlazione tra le due componenti così ottenute si ottiene la correlazione parziale tra $Y_1$ e $Y_2$ al netto dell'effetto lineare di $F$.\n\nL'@eq-corr-parz consente di calcolare la correlazione parziale tra $Y_1$ e $Y_2$ al netto dell'effetto di $F$ a partire dalle correlazioni semplici tra le tre variabili $Y_1$, $Y_2$ e $F$. \n\n$$\n\\begin{equation}\nr_{1,2 \\mid F} = \\frac{r_{12} - r_{1F}r_{2F}}{\\sqrt{(1-r_{1F}^2)(1-r_{2F}^2)}}.\n\\end{equation}\n$$ {#eq-corr-parz}\n\nIn particolare, la correlazione parziale $r_{1,2 \\mid F}$ è data dalla differenza tra la correlazione $r_{12}$ tra $Y_1$ e $Y_2$ e il prodotto tra le correlazioni $r_{1F}$ e $r_{2F}$ tra ciascuna delle due variabili e $F$, il tutto diviso per la radice quadrata del prodotto delle differenze tra 1 e i quadrati delle correlazioni tra $Y_1$ e $F$ e tra $Y_2$ e $F$. In altre parole, la formula tiene conto dell'effetto di $F$ sulle correlazioni tra $Y_1$ e $Y_2$ per ottenere una stima della relazione diretta tra le due variabili, eliminando l'effetto del fattore comune.\n\nConsideriamo un esempio numerico. Sia $f$ una variabile su cui misuriamo $n$ valori\n\n\n\n\n::: {.cell layout-align=\"center\" vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\nset.seed(123)\nn <- 1000\nf <- rnorm(n, 24, 12)\n```\n:::\n\n\n\n\nSiano $y_1$ e $y_2$ funzioni lineari di $f$, a cui viene aggiunta una componente d'errore gaussiano:\n\n\n\n\n::: {.cell layout-align=\"center\" vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\ny1 <- 10 + 7 * f + rnorm(n, 0, 50)\ny2 <- 3  + 2 * f + rnorm(n, 0, 50)\n```\n:::\n\n\n\n\nLa correlazione tra $y_1$ e $y_2$ ($r_{12}= 0.355$) deriva dal fatto che $\\hat{y}_1$ e $\\hat{y}_2$ sono entrambe funzioni lineari di $f$:\n\n\n\n\n::: {.cell layout-align=\"center\" vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\nY <- cbind(y1, y2, f)\ncor(Y) |>\n    round(3)\n#>       y1    y2     f\n#> y1 1.000 0.380 0.867\n#> y2 0.380 1.000 0.423\n#> f  0.867 0.423 1.000\n```\n:::\n\n\n\n\nEseguiamo le regressioni di $y_1$ su $f$ e di $y_2$ su $F$:\n\n\n\n\n::: {.cell layout-align=\"center\" vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\nfm1 <- lm(y1 ~ f)\nfm2 <- lm(y2 ~ f)\n```\n:::\n\n\n\n\nNella regressione, ciascuna osservazione $y_{i1}$ viene scomposta in due componenti linearmente indipendenti, i valori adattati $\\hat{y}_{i}$ e i residui, $e_{i}$: $y_i = \\hat{y}_i + e_1$. Nel caso di $y_1$ abbiamo\n\n\n\n\n::: {.cell layout-align=\"center\" vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\ncbind(y1, y1.hat=fm1$fit, e=fm1$res, fm1$fit+fm1$res) |>\n    head() |>\n    round(3)\n#>      y1 y1.hat       e      \n#> 1  81.1    131  -49.38  81.1\n#> 2 106.7    160  -53.04 106.7\n#> 3 308.0    318   -9.81 308.0\n#> 4 177.3    186   -8.97 177.3\n#> 5  61.4    191 -130.09  61.4\n#> 6 374.1    332   42.43 374.1\n```\n:::\n\n\n\n\nLo stesso può dirsi di $y_2$. La correlazione parziale $r_{12 \\mid f}$\ntra $y_1$ e $y_2$ dato $f$ è uguale alla correlazione di Pearson tra i\nresidui $e_1$ e $e_2$ calcolati mediante i due modelli di regressione\ndescritti sopra:\n\n\n\n\n::: {.cell layout-align=\"center\" vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\ncor(fm1$res, fm2$res)\n#> [1] 0.0283\n```\n:::\n\n\n\n\n\\\nLa correlazione parziale tra $y_1$ e $y_2$ al netto di $f$ è .02829. Per i dati esaminati sopra, dunque, la correlazione parziale tra le variabili $y_1$ e $y_2$ diventa uguale a zero se la variabile $f$ viene controllata (ovvero, se escludiamo da $y_1$ e da $y_2$ l'effetto lineare\ndi $f$). \n\nIl fatto che la correlazione parziale sia zero significa che la\ncorrelazione che abbiamo osservato tra $y_1$ e $y_2$ ($r = 0.355$) non\ndipendeva dall'effetto che una variabile $y$ esercitava sull'altra, ma\nbensì dal fatto che c'era una terza variabile, $f$, che influenzava sia\n$y_1$ sia $y_2$. In altre parole, le variabili $y_1$ e $y_2$ sono\ncondizionalmente indipendenti dato $f$. Ciò significa, come abbiamo\nvisto sopra, che la componente di $y_1$ linearmente indipendente da $f$\nè incorrelata con la componente di $y_2$ linearmente indipendente da\n$f$.\n\nLa correlazione che abbiamo calcolato tra i residui di due modelli di\nregressione è identica alla correlazione che viene calcolata\napplicando l'@eq-corr-parz:\n\n\n\n\n::: {.cell layout-align=\"center\" vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\nR <- cor(Y)\n\n(R[1, 2] - R[1, 3] * R[2, 3]) / \n  sqrt((1 - R[1, 3]^2) * (1- R[2, 3]^2)) |>\n  round(3)\n#> [1] 0.0283\n```\n:::\n\n\n\n\n## Principio base dell'analisi fattoriale\n\nAttualmente, l'inferenza statistica nell'analisi fattoriale spesso si svolge mediante il calcolo di stime della massima verosimiglianza ottenute mediante procedure iterative. All'inizio dell'analisi fattoriale, tuttavia, la procedura di estrazione dei fattori faceva leva sulle relazioni invarianti che il modello fattoriale impone agli elementi della matrice di covarianza delle variabili osservate. Il più conosciuto tra tali invarianti è la *tetrade* che si presenta nei modelli ad un fattore.\n\nLa tetrade è una combinazione di quattro correlazioni. Se l'associazione osservata tra le variabili dipende effettivamente dal fatto che le  variabili in questione sono state causalmente generate da un fattore comune inosservabile, allora è possibile generare una combinazione delle correlazioni tra le variabili che porta all'annullamento della tetrade. In altre parole, l'analisi fattoriale si chiede se esiste un insieme esiguo di $m<p$ variabili inosservabili che rendono significativamente nulle tutte le correlazioni parziali tra le $p$ variabili osservate al netto dei fattori comuni. Se il metodo della correlazione parziale consente di identificare $m$ variabili latenti, allora lo psicologo conclude che tali fattori corrispondono agli $m$ costrutti che intende misurare.\n\nPer chiarire il metodo dell'annullamento della tetrade consideriamo la matrice di correlazioni riportata nella Tabella successiva. Nella tabella, la correlazione parziale tra ciascuna coppia di variabili \n$y_i$, $y_j$ (con $i \\neq j$) dato $\\xi$ è sempre uguale a zero. Ad esempio, la correlazione parziale tra $y_3$ e $y_5$ dato $\\xi$ è:\n\n$$\n\\begin{align}\n  r_{35 \\mid \\xi} &= \\frac{r_{35} - r_{3\\xi}r_{5\\xi}}\n  {\\sqrt{(1-r_{3\\xi}^2)(1-r_{5\\xi}^2)}} \\notag \\\\[12pt]\n  &= \\frac{0.35 - 0.7 \\times 0.5}\n  {\\sqrt{(1-0.7^2)(1-0.5^2)}} = 0. \\notag\n\\end{align}\n$$\n\nLo stesso risultato si trova per qualunque altra coppia di variabili $y_i$ e $y_j$, ovvero $r_{ij \\mid \\xi} = 0$.\n\n|       | $\\xi$ | $y_1$ | $y_2$ | $y_3$ | $y_4$ | $y_5$ |\n|-------|-------|-------|-------|-------|-------|-------|\n| $\\xi$ | **1.00** |       |       |       |       |       |\n| $y_1$ | **0.90** | 1.00  |       |       |       |       |\n| $y_2$ | **0.80** | 0.72  | 1.00  |       |       |       |\n| $y_3$ | **0.70** | 0.63  | 0.56  | 1.00  |       |       |\n| $y_4$ | **0.60** | 0.54  | 0.48  | 0.42  | 1.00  |       |\n| $y_5$ | **0.50** | 0.45  | 0.40  | 0.35  | 0.30  | 1.00  |\n  \nPossiamo dunque dire che, per la matrice di correlazioni della Tabella, esiste un'unica variabile $\\xi$ la quale, quando viene controllata, spiega tutte le \n\n$$p(p-1)/2 = 5(5-1)/2=10$$ \n\ncorrelazioni tra le variabili $y$. Questo risultato non è sorprendente, in quanto la  matrice di correlazioni della Tabella è stata costruita in modo tale da possedere tale proprietà.\n\nMa supponiamo di essere in una situazione diversa, ovvero di avere osservato soltanto le variabili $y_i$ e di non conoscere $\\xi$. In tali circostanze ci possiamo porre la seguente domanda: Esiste una variabile inosservabile $\\xi$ la quale, se venisse controllata, renderebbe uguali a zero tutte le correlazioni parziali tra le variabili $y$? Se una tale variabile inosservabile esiste, ed è in grado di spiegare tutte le correlazioni tra le variabili osservate $y$, allora essa viene chiamata *fattore*. Arriviamo dunque alla seguente definizione: \n\nUn fattore è una variabile inosservabile in grado di rendere significativamente nulle tutte le correlazioni parziali tra le variabili manifeste.\n\n## Vincoli sulle correlazioni\n\nCome si può stabilire se esiste una variabile inosservabile in grado di rendere nulle tutte le correlazioni parziali tra le variabili osservate? Riscriviamo l'@eq-corr-parz per specificare la correlazione parziale tra le variabili $y_i$ e $y_j$ dato $\\xi$:\n\n$$\n\\begin{align}\n  r_{ij \\mid \\xi} &= \\frac{r_{ij} - r_{i\\xi}r_{j\\xi}}\n  {\\sqrt{(1-r_{i\\xi}^2)(1-r_{j\\xi}^2)}} \n\\end{align}\n$$\n\nAffinché $r_{ij \\mid \\xi}$ sia uguale a zero è necessario che\n\n$$\nr_{ij} - r_{i\\xi}r_{j\\xi}=0\n$$\n\novvero\n\n$$\n\\begin{equation}\nr_{ij} = r_{i\\xi}r_{j\\xi}.\n\\end{equation}\n$$\n\nIn altri termini, se esiste un fattore non osservato $\\xi$ in grado di rendere uguali a zero tutte le correlazioni parziali $r_{ih \\mid \\xi}$, allora la correlazione tra ciascuna coppia di variabili $y$ deve essere uguale al prodotto delle correlazioni tra ciascuna $y$ e il fattore latente $\\xi$. Questo è il principio base dell'analisi fattoriale. \n\n## Teoria dei Due Fattori\n\nPer fare un esempio concreto relativo al metodo dell'annullamento della tetrade, esaminiamo la matrice di correlazioni originariamente analizzata da Spearman. Spearman (1904) raccolse alcune misure di capacità intellettuale su un piccolo numero di studenti di una scuola superiore. Nello specifico, esaminò i voti di tali studenti nelle seguenti materie: studio dei classici ($c$), letteratura inglese ($e$) e abilità matematiche ($m$). Considerò anche la prestazione in un compito di discriminazione dell'altezza di suoni (\"pitch discrimination\") ($p$), ovvero un'abilità diversa da quelle richieste nei test scolastici.  \n\nSecondo la Teoria dei Due Fattori, le prestazioni relative ad un determinato compito intellettuale possiedono una componente comune (detta fattore 'g') con le prestazioni in un qualunque altro compito intellettuale e una componente specifica a quel determinato compito. Il modello dell'intelligenza di Spearman prevede dunque due fattori, uno generale e uno specifico (detto fattore 's'). Il fattore 'g' costituisce la componente invariante dell'abilità intellettiva, mente il fattore 's' è una componente che varia da condizione a condizione. \n\nCome è possibile stabilire se esiste una variabile latente in grado di spiegare le correlazioni tra le variabili osservate da Spearman? Lo strumento proposto da Spearman per rispondere a questa domanda è *l'annullamento della tetrade*. L'annullamento della tetrade utilizza i vincoli sulle correlazioni che derivano dalla definizione di correlazione parziale. In precedenza abbiamo visto che la correlazione parziale tra le variabili $y$ indicizzate da $i$ e $j$, al netto dell'effetto di $\\xi$, è nulla se\n\n$$\nr_{ij} = r_{i\\xi}r_{j\\xi}.\n$$\n\nNel caso dei dati di Spearman, dunque, le correlazioni parziali sono nulle se \nla correlazione tra ''studi classici'' e ''letteratura inglese'' è uguale al prodotto della correlazione tra ''studi classici'' e il fattore $\\xi$ e della correlazione tra ''letteratura inglese'' e il fattore $\\xi$. Inoltre, la correlazione tra ''studi classici'' e ''abilità matematica'' deve essere uguale al prodotto della correlazione tra ''studi classici'' e il fattore $\\xi$ e della correlazione tra ''abilità matematica'' e il fattore $\\xi$; e così via.\n\nLe correlazioni tra le variabili manifeste e il fattore latente sono dette \\textit{saturazioni fattoriali} e vengono denotate con la lettera $\\lambda$. Se il modello di Spearman è corretto, avremo che\n\n$$r_{ec}=\\lambda_e \\times \\lambda_{c},$$ \n\ndove $r_{ec}$ è la correlazione tra ''letteratura inglese'' (e) e ''studi classici'' (c), $\\lambda_e$ è la correlazione tra ''letteratura inglese'' e $\\xi$, e $\\lambda_{c}$ è la correlazione tra ''studi classici'' e $\\xi$. \n\nAllo stesso modo, la correlazione tra ''studi classici'' e ''matematica'' (m) dovrà essere uguale a \n\n$$\\lambda_c \\times \\lambda_m,$$ \n\neccetera. \n\n## Annullamento della tetrade\n\nDate le correlazioni tra tre coppie di variabili manifeste, il metodo dell'annullamento della tetrade\n\n> in una matrice di correlazione, si selezionino quattro coefficienti nelle posizioni che marcano gli angoli di un rettangolo. La differenza tra i prodotti dei coefficienti che giacciono sulle due diagonali di tale rettangolo costituisce la differenza delle tetradi e deve essere uguale a zero.\n\nrende possibile stimare i valori delle saturazioni fattoriali $\\lambda$. Ad esempio, per le variabili $c$, $m$ ed $e$, possiamo scrivere le seguenti tre equazioni in tre incognite:\n\n$$\n\\begin{align}\n  r_{cm} &= \\lambda_c \\times \\lambda_m, \\notag \\\\\n  r_{em} &= \\lambda_e \\times \\lambda_m,  \\\\\n  r_{ce} &= \\lambda_c \\times \\lambda_e. \\notag\n\\end{align}\n$$\n\nRisolvendo il precedente sistema di equazioni lineari, il coefficiente di saturazione $\\lambda_m$ della variabile $y_m$ nel fattore comune $\\xi$, ad esempio, pu{\\`o} essere calcolato a partire dalle correlazioni tra le variabili manifeste $c$, $m$, ed $e$ nel modo seguente\\footnote{\nLa terza delle equazioni del sistema lineare può essere riscritta come $\\lambda_c = \\frac{r_{ce}}{\\lambda_e}$.\n\nUtilizzando tale risultato, la prima equazione diventa $r_{cm} = \\frac{r_{ce}}{\\lambda_e}\\lambda_m$. \nDalla seconda equazione otteniamo $\\lambda_e = \\frac{r_{em}}{\\lambda_m}$. Sostituendo questo risultato nell'equazione precedente otteniamo $r_{cm} = \\frac{r_{ce}}{r_{em}}\\lambda_m^2$, quindi $\\lambda_m^2 = \\frac{r_{cm} r_{em} }{r_{ce}}$.\n\nVerifichiamo: $\\frac{r_{cm} r_{em}}{r_{ce}} = \\frac{\\lambda_c \\lambda_m \\lambda_e \\lambda_m}{\\lambda_c \\lambda_e} = \\lambda_m^2$. \n\n$$\n\\begin{align}\n  \\lambda_m &= \\sqrt{\n    \\frac{r_{cm} r_{em}}{r_{ce}}\n    }. \n\\end{align}\n$$ {#eq-tetradi}\n\nLo stesso vale per le altre due saturazioni $\\lambda_c$ e $\\lambda_e$.\n\nNel suo articolo del 1904, Spearman osservò le seguenti correlazioni tra \nle variabili $Y_c$, $Y_e$, $Y_m$ e $Y_p$:\n\n$$\n\\begin{array}{ccccc}\n  \\hline\n    & Y_C & Y_E & Y_M & Y_P \\\\\n  \\hline\n  Y_C & 1.00 & 0.78 & 0.70 & 0.66 \\\\\n  Y_E &   & 1.00 & 0.64 & 0.54 \\\\\n  Y_M &   &   & 1.00 & 0.45 \\\\\n  Y_P &   &   &   & 1.00 \\\\\n  \\hline\n\\end{array}\n$$\n\nUtilizzando l'@eq-tetradi, mediante le correlazioni $r_{cm}$, $r_{em}$, e $r_{ce}$ fornite dalla tabella precedente, la saturazione $\\lambda_m$ diventa uguale a:\n\n$$\n\\begin{align}\n  \\hat{\\lambda}_m &= \\sqrt{ \\frac{r_{cm} r_{em}}{r_{ce}} } = \\sqrt{\n    \\frac{0.70 \\times 0.64}{0.78} } = 0.76. \\notag\n\\end{align}\n$$\n\nÈ importante notare che il metodo dell'annullamento della tetrade produce risultati falsificabili. \nInfatti, ci sono modi diversi per calcolare la stessa saturazione fattoriale. \nSe il modello fattoriale è corretto si deve ottenere lo stesso risultato in tutti i casi.  \nNel caso presente, la saturazione fattoriale $\\lambda_m$ può essere calcolata in altri due modi:\n\n$$\n\\begin{align}\n  \\hat{\\lambda}_m &= \\sqrt{ \\frac{r_{cm} r_{mp}}{r_{cp}} } = \\sqrt{ \\frac{0.78 \\times 0.45}{0.66} } = 0.69, \\notag \\\\\n  \\hat{\\lambda}_m &= \\sqrt{ \\frac{r_{em} r_{mp}}{r_{ep}} } = \\sqrt{\n    \\frac{0.64 \\times 0.45}{0.54} } = 0.73. \\notag\n\\end{align}\n$$\n\nI tre valori che sono stati ottenuti sono molto simili. Qual è allora la stima migliore di  $\\lambda_m$? \n\n## Metodo del centroide\n\nLa soluzione più semplice è quella di fare la media di questi tre valori ($\\bar{\\lambda}_m = 0.73$). \nUn metodo migliore (meno vulnerabile ai valori anomali) è dato dal rapporto tra la\nsomma dei numeratori e dei denominatori:\n\n$$\n\\begin{align}\n  \\hat{\\lambda}_m &= \\sqrt{ \\frac{0.70 \\times 0.64 + 0.78 \\times 0.45 + 0.64\n      \\times 0.45}{0.78+0.66+0.54} } = 0.73 \\notag\n\\end{align}\n$$\n\nIn questo caso, i due metodi danno lo stesso risultato. Le altre tre saturazioni fattoriali\ntrovate mediante il metodo del centroide sono: \n\n$$\\hat{\\lambda}_c = 0.97, \\quad \\hat{\\lambda}_e = 0.84, \\quad \\hat{\\lambda}_p = 0.65.$$ \n\nIn conclusione, \n\n$$\n\\boldsymbol{\\hat{\\Lambda}}'=\n(\\hat{\\lambda}_c, \\hat{\\lambda}_e, \\hat{\\lambda}_m, \\hat{\\lambda}_p) = (0.97, 0.84, 0.73, 0.65).\n$$ \n\nQuesto risultato è la soluzione proposta da Spearman nel suo articolo del 1904 per risolvere il problema di determinare le saturazioni fattoriali di un modello con un fattore comune latente.\n\n## Introduzione a `lavaan`\n\nAttualmente, l'analisi fattoriale viene svolta mediante software. Il pacchetto R più ampiamente utilizzato per condurre l'analisi fattoriale è `lavaan`. \n\n### Sintassi del modello\n\nAl cuore del pacchetto `lavaan` si trova la \"sintassi del modello\". La sintassi del modello è una descrizione del modello da stimare. In questa sezione, spieghiamo brevemente gli elementi della sintassi del modello `lavaan`. \n\nNell'ambiente R, una formula di regressione ha la seguente forma:\n\n```\ny ~ x1 + x2 + x3 + x4\n```\n\nIn questa formula, la tilde (\"~\") è l'operatore di regressione. Sul lato sinistro dell'operatore, abbiamo la variabile dipendente (y), e sul lato destro abbiamo le variabili indipendenti, separate dall'operatore \"+\" . In `lavaan`, un modello tipico è semplicemente un insieme (o sistema) di formule di regressione, in cui alcune variabili (che iniziano con una 'f' qui sotto) possono essere latenti. Ad esempio:\n\n```\ny ~ f1 + f2 + x1 + x2\nf1 ~ f2 + f3\nf2 ~ f3 + x1 + x2\n```\n\nSe abbiamo variabili latenti in una qualsiasi delle formule di regressione, dobbiamo \"definirle\" elencando i loro indicatori (manifesti o latenti). Lo facciamo utilizzando l'operatore speciale \"=~\", che può essere letto come \"è misurato da\". Ad esempio, per definire le tre variabili latenti f1, f2 e f3, possiamo usare la sintassi seguente:\n\n```\nf1 =~ y1 + y2 + y3\nf2 =~ y4 + y5 + y6\nf3 =~ y7 + y8 + y9 + y10\n```\n\nInoltre, le varianze e le covarianze sono specificate utilizzando un operatore \"doppia tilde\", ad esempio:\n\n```\ny1 ~~ y1 # varianza\ny1 ~~ y2 # covarianza\nf1 ~~ f2 # covarianza\n```\n\nE infine, le intercette per le variabili osservate e latenti sono semplici formule di regressione con solo una intercetta (esplicitamente indicato dal numero \"1\") come unico predittore:\n\n```\ny1 ~ 1\nf1 ~ 1\n```\n\nUtilizzando questi quattro tipi di formule, è possibile descrivere una vasta gamma di modelli di variabili latenti. L'attuale insieme di tipi di formula è riassunto nella tabella sottostante.\n\n| tipo di formula | operatore |  mnemonic |\n| --------------- | --------- | --------- |\n| definizione variabile latente | =~ | è misurato da |\n| regressione | ~ | viene regredito su |\n| (co)varianza (residuale) | ~~  |è correlato con |\n| intercetta | ~ 1 | intercetta |\n\nUna sintassi completa del modello lavaan è semplicemente una combinazione di questi tipi di formule, racchiusi tra virgolette singole. Ad esempio:\n\n```\nmy_model <- ' \n  # regressions\n  y1 + y2 ~ f1 + f2 + x1 + x2\n  f1 ~ f2 + f3\n  f2 ~ f3 + x1 + x2\n\n  # latent variable definitions \n  f1 =~ y1 + y2 + y3 \n  f2 =~ y4 + y5 + y6 \n  f3 =~ y7 + y8 + y9\n  \n  # variances and covariances \n  y1 ~~ y1 \n  y1 ~~ y2 \n  f1 ~~ f2\n\n  # intercepts \n  y1 ~ 1 \n  f1 ~ 1\n'\n```\n\nPer adattare il modello ai dati usiamo la seguente sintassi.\n\n```\nfit <- cfa(model = my_model, data = my_data)\n```\n\n### Un esempio concreto\n\nAnalizziamo nuovamente i dati di Spearman che abbiamo esaminato in precedenza usando `lavaan`. La matrice completa dei dati di Spearman è messa a disposizione da @kan2019extending. \n\nSpecifichiamo il nome delle variabili manifeste\n\n\n\n\n::: {.cell layout-align=\"center\" vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\nvarnames <- c(\n  \"Classics\", \"French\", \"English\", \"Math\", \"Pitch\", \"Music\"\n)\n```\n:::\n\n\n\n\ne il loro numero\n\n\n\n\n::: {.cell layout-align=\"center\" vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\nny <- length(varnames)\n```\n:::\n\n\n\n\nCreiamo la matrice di correlazione:\n\n\n\n\n::: {.cell layout-align=\"center\" vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\nspearman_cor_mat <- matrix(\n  c(\n    1.00,  .83,  .78,  .70,  .66,  .63,\n     .83, 1.00,  .67,  .67,  .65,  .57,\n     .78,  .67, 1.00,  .64,  .54,  .51,\n     .70,  .67,  .64, 1.00,  .45,  .51,\n     .66,  .65,  .54,  .45, 1.00,  .40,\n     .63,  .57,  .51,  .51,  .40, 1.00\n  ),\n  ny, ny,\n  byrow = TRUE,\n  dimnames = list(varnames, varnames)\n)\nspearman_cor_mat\n#>          Classics French English Math Pitch Music\n#> Classics     1.00   0.83    0.78 0.70  0.66  0.63\n#> French       0.83   1.00    0.67 0.67  0.65  0.57\n#> English      0.78   0.67    1.00 0.64  0.54  0.51\n#> Math         0.70   0.67    0.64 1.00  0.45  0.51\n#> Pitch        0.66   0.65    0.54 0.45  1.00  0.40\n#> Music        0.63   0.57    0.51 0.51  0.40  1.00\n```\n:::\n\n\n\n\nSpecifichiamo l'ampiezza campionaria:\n\n\n\n\n::: {.cell layout-align=\"center\" vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\nn <- 33\n```\n:::\n\n\n\n\nDefiniamo il modello unifattoriale in `lavaan`. L'operatore `=~` si può leggere dicendo che la variabile latente a sinistra dell'operatore viene identificata dalle variabili manifeste elencate a destra dell'operatore e separate dal segno `+`. Per il caso presente, il modello dei due fattori di Spearman può essere specificato come segue.\n\n\n\n\n::: {.cell layout-align=\"center\" vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\nspearman_mod <- \"\n  g =~ Classics + French + English + Math + Pitch + Music\n\"\n```\n:::\n\n\n\n\nAdattiamo il modello ai dati con la funzione `cfa()`:\n\n\n\n\n::: {.cell layout-align=\"center\" vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\nfit1 <- lavaan::cfa(\n  spearman_mod,\n  sample.cov = spearman_cor_mat,\n  sample.nobs = n,\n  std.lv = TRUE\n)\n```\n:::\n\n\n\n\nLa funzione `cfa()` è una funzione dedicata per adattare modelli di analisi fattoriale confermativa. Il primo argomento è il modello specificato dall'utente. Il secondo argomento è il dataset che contiene le variabili osservate. L'argomento `std.lv = TRUE` specifica che imponiamo una varianza pari a 1 a tutte le variabili latenti comuni (nel caso presente, solo una). Ciò consente di  stimare le saturazioni fattoriali.\n\nUna volta adattato il modello, la funzione `summary()` ci consente di esaminare la soluzione ottenuta:\n\n\n\n\n::: {.cell layout-align=\"center\" vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\nout = summary(\n  fit1, \n  fit.measures = TRUE, \n  standardized = TRUE\n)\nout\n#> lavaan 0.6-19 ended normally after 23 iterations\n#> \n#>   Estimator                                         ML\n#>   Optimization method                           NLMINB\n#>   Number of model parameters                        12\n#> \n#>   Number of observations                            33\n#> \n#> Model Test User Model:\n#>                                                       \n#>   Test statistic                                 2.913\n#>   Degrees of freedom                                 9\n#>   P-value (Chi-square)                           0.968\n#> \n#> Model Test Baseline Model:\n#> \n#>   Test statistic                               133.625\n#>   Degrees of freedom                                15\n#>   P-value                                        0.000\n#> \n#> User Model versus Baseline Model:\n#> \n#>   Comparative Fit Index (CFI)                    1.000\n#>   Tucker-Lewis Index (TLI)                       1.086\n#> \n#> Loglikelihood and Information Criteria:\n#> \n#>   Loglikelihood user model (H0)               -212.547\n#>   Loglikelihood unrestricted model (H1)       -211.091\n#>                                                       \n#>   Akaike (AIC)                                 449.094\n#>   Bayesian (BIC)                               467.052\n#>   Sample-size adjusted Bayesian (SABIC)        429.622\n#> \n#> Root Mean Square Error of Approximation:\n#> \n#>   RMSEA                                          0.000\n#>   90 Percent confidence interval - lower         0.000\n#>   90 Percent confidence interval - upper         0.000\n#>   P-value H_0: RMSEA <= 0.050                    0.976\n#>   P-value H_0: RMSEA >= 0.080                    0.016\n#> \n#> Standardized Root Mean Square Residual:\n#> \n#>   SRMR                                           0.025\n#> \n#> Parameter Estimates:\n#> \n#>   Standard errors                             Standard\n#>   Information                                 Expected\n#>   Information saturated (h1) model          Structured\n#> \n#> Latent Variables:\n#>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n#>   g =~                                                                  \n#>     Classics          0.942    0.129    7.314    0.000    0.942    0.956\n#>     French            0.857    0.137    6.239    0.000    0.857    0.871\n#>     English           0.795    0.143    5.545    0.000    0.795    0.807\n#>     Math              0.732    0.149    4.923    0.000    0.732    0.743\n#>     Pitch             0.678    0.153    4.438    0.000    0.678    0.689\n#>     Music             0.643    0.155    4.142    0.000    0.643    0.653\n#> \n#> Variances:\n#>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n#>    .Classics          0.083    0.051    1.629    0.103    0.083    0.086\n#>    .French            0.234    0.072    3.244    0.001    0.234    0.242\n#>    .English           0.338    0.094    3.610    0.000    0.338    0.349\n#>    .Math              0.434    0.115    3.773    0.000    0.434    0.447\n#>    .Pitch             0.510    0.132    3.855    0.000    0.510    0.526\n#>    .Music             0.556    0.143    3.893    0.000    0.556    0.573\n#>     g                 1.000                               1.000    1.000\n```\n:::\n\n\n\n\nL'output di `lavaan` si divide in tre sezioni principali:\n\n1. **Intestazione**: Le prime nove righe dell'output costituiscono l'intestazione, che fornisce informazioni chiave sul modello e sull'analisi, tra cui:\n   - **Versione di lavaan**: specifica la versione del pacchetto utilizzata.\n   - **Esito dell'ottimizzazione**: indica se l'algoritmo di ottimizzazione è terminato correttamente e il numero di iterazioni necessarie.\n   - **Stimatore**: mostra il metodo di stima utilizzato, come `ML` (massima verosimiglianza).\n   - **Ottimizzatore**: specifica l'algoritmo di ottimizzazione (es., `NLMINB`) utilizzato per trovare i parametri migliori per lo stimatore selezionato.\n   - **Numero di parametri del modello**: fornisce il totale dei parametri stimati nel modello (es., 12).\n   - **Numero di osservazioni**: indica il numero di dati effettivamente utilizzati nell'analisi (es., 33).\n   - **\"Model Test User Model\"**: contiene la statistica di test, i gradi di libertà e il valore p per il modello specificato dall'utente.\n\n2. **Misure di adattamento**: Questa sezione, visibile solo con `fit.measures = TRUE`, presenta una serie di indicatori di adattamento del modello, iniziando dalla riga \"Model Test Baseline Model\" e terminando con il valore per l'SRMR. Questi indicatori forniscono informazioni sulla bontà del modello rispetto ai dati.\n\n3. **Stime dei parametri**: Questa sezione contiene le stime dei parametri e inizia con dettagli tecnici sul metodo utilizzato per calcolare gli errori standard. Di seguito, sono elencati i parametri liberi e fissi del modello, in genere con ordine che parte dalle variabili latenti, seguito dalle covarianze e dalle varianze residue. Le colonne includono:\n   - **Estimate**: indica il valore stimato (non standardizzato) per ciascun parametro, rappresentando il peso del collegamento tra il costrutto latente (es., `g`) e le variabili osservate.\n   - **Std.err**: l'errore standard per ogni stima, utile per valutare l'accuratezza della stima.\n   - **Z-value**: la statistica di Wald, calcolata dividendo la stima per il suo errore standard.\n   - **P(>|z|)**: il valore p, utilizzato per testare l'ipotesi nulla che la stima sia zero nella popolazione.\n\n**Ulteriori dettagli sulle colonne:**\n\n- **Estimate**: Fornisce lo stimatore di massima verosimiglianza per i pesi dei percorsi, rappresentando l'effetto diretto di ogni costrutto latente sulle variabili osservate.\n- **Std.lv**: Questi valori sono standardizzati solo rispetto alle variabili latenti, permettendo un confronto all'interno del modello indipendentemente dalle unità di misura originali.\n- **Std.all**: Fornisce le stime completamente standardizzate, considerando sia le variabili latenti sia quelle osservate, facilitando un confronto dei coefficienti in termini di deviazioni standard.\n\nNella sezione delle **Varianze**, si osserva un punto prima dei nomi delle variabili osservate. Questo formato indica che sono variabili endogene, cioè predette dalle variabili latenti, e che il valore riportato rappresenta la varianza residua non spiegata dai predittori. Invece, le variabili latenti, che non hanno un punto prima del loro nome, sono considerate esogene; i valori riportati sono le loro varianze totali stimate.\n\nÈ possibile semplificare l'output dalla funzione `summary()` in maniera tale da stampare solo la tabella completa delle stime dei parametri e degli errori standard. Qui usiamo `coef(fit1)`.\n\n\n\n\n::: {.cell layout-align=\"center\" vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\ncoef(fit1) \n#>        g=~Classics          g=~French         g=~English            g=~Math \n#>              0.942              0.857              0.795              0.732 \n#>           g=~Pitch           g=~Music Classics~~Classics     French~~French \n#>              0.678              0.643              0.083              0.234 \n#>   English~~English         Math~~Math       Pitch~~Pitch       Music~~Music \n#>              0.338              0.434              0.510              0.556\n```\n:::\n\n\n\n\nUsando `parameterEstimates`, l'output diventa il seguente.\n\n\n\n\n::: {.cell layout-align=\"center\" vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\nparameterEstimates(fit1, standardized = TRUE)\n#>         lhs op      rhs   est    se    z pvalue ci.lower ci.upper std.lv\n#> 1         g =~ Classics 0.942 0.129 7.31  0.000    0.689    1.194  0.942\n#> 2         g =~   French 0.857 0.137 6.24  0.000    0.588    1.127  0.857\n#> 3         g =~  English 0.795 0.143 5.54  0.000    0.514    1.076  0.795\n#> 4         g =~     Math 0.732 0.149 4.92  0.000    0.441    1.024  0.732\n#> 5         g =~    Pitch 0.678 0.153 4.44  0.000    0.379    0.978  0.678\n#> 6         g =~    Music 0.643 0.155 4.14  0.000    0.339    0.948  0.643\n#> 7  Classics ~~ Classics 0.083 0.051 1.63  0.103   -0.017    0.183  0.083\n#> 8    French ~~   French 0.234 0.072 3.24  0.001    0.093    0.376  0.234\n#> 9   English ~~  English 0.338 0.094 3.61  0.000    0.154    0.522  0.338\n#> 10     Math ~~     Math 0.434 0.115 3.77  0.000    0.208    0.659  0.434\n#> 11    Pitch ~~    Pitch 0.510 0.132 3.85  0.000    0.251    0.769  0.510\n#> 12    Music ~~    Music 0.556 0.143 3.89  0.000    0.276    0.836  0.556\n#> 13        g ~~        g 1.000 0.000   NA     NA    1.000    1.000  1.000\n#>    std.all\n#> 1    0.956\n#> 2    0.871\n#> 3    0.807\n#> 4    0.743\n#> 5    0.689\n#> 6    0.653\n#> 7    0.086\n#> 8    0.242\n#> 9    0.349\n#> 10   0.447\n#> 11   0.526\n#> 12   0.573\n#> 13   1.000\n```\n:::\n\n\n\n\nCon opportuni parametri possiamo semplificare l'output nel modo seguente.\n\n\n\n\n::: {.cell layout-align=\"center\" vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\nparameterEstimates(fit1, standardized = TRUE) |>\n  dplyr::filter(op == \"=~\") |>\n  dplyr::select(\n    \"Latent Factor\" = lhs,\n    Indicator = rhs,\n    B = est,\n    SE = se,\n    Z = z,\n    \"p-value\" = pvalue,\n    Beta = std.all\n  ) \n#>   Latent.Factor Indicator     B    SE    Z p.value  Beta\n#> 1             g  Classics 0.942 0.129 7.31       0 0.956\n#> 2             g    French 0.857 0.137 6.24       0 0.871\n#> 3             g   English 0.795 0.143 5.54       0 0.807\n#> 4             g      Math 0.732 0.149 4.92       0 0.743\n#> 5             g     Pitch 0.678 0.153 4.44       0 0.689\n#> 6             g     Music 0.643 0.155 4.14       0 0.653\n```\n:::\n\n\n\n\nEsaminiamo la matrice delle correlazioni residue:\n\n\n\n\n::: {.cell layout-align=\"center\" vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\nresiduals(fit1, type = \"cor\")$cov \n#>          Clsscs French Englsh   Math  Pitch  Music\n#> Classics  0.000                                   \n#> French   -0.003  0.000                            \n#> English   0.008 -0.033  0.000                     \n#> Math     -0.011  0.023  0.040  0.000              \n#> Pitch     0.001  0.050 -0.016 -0.062  0.000       \n#> Music     0.005  0.001 -0.017  0.024 -0.050  0.000\n```\n:::\n\n\n\n\nCreiamo un qq-plot dei residui:\n\n\n\n\n::: {.cell layout-align=\"center\" vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\nres1 <- residuals(fit1, type = \"cor\")$cov\nres1[upper.tri(res1, diag = TRUE)] <- NA\nv1 <- as.vector(res1)\nv2 <- v1[!is.na(v1)]\n\ntibble(v2) %>% \n  ggplot(aes(sample = v2)) + \n  stat_qq() + \n  stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](02_analisi_fattoriale_1_files/figure-html/unnamed-chunk-20-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n## Diagrammi di percorso\n\nIl pacchetto `semPlot` consente di disegnare diagrammi di percorso per vari modelli SEM. La funzione `semPaths` prende in input un oggetto creato da `lavaan` e disegna il diagramma, con diverse opzioni disponibili. Il diagramma prodotto controlla le dimensioni dei caratteri/etichette, la visualizzazione dei residui e il colore dei percorsi/coefficienti. Sono disponibili queste e molte altre opzioni di controllo. \n\n\n\n\n::: {.cell layout-align=\"center\" vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\nsemPaths(\n    fit1,\n    \"std\",\n    posCol = c(\"black\"),\n    edge.label.cex = 1.2,\n    whatLabels = \"std\", \n    edge.width = 0.3, # Imposta lo spessore delle linee \n    fade = FALSE # Disabilita il fading\n)\n```\n\n::: {.cell-output-display}\n![](02_analisi_fattoriale_1_files/figure-html/unnamed-chunk-21-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\nIl calcolo delle saturazioni fattoriali con il metodo del centroide aveva prodotto il seguente risultato: \n\n- classici (Cls): 0.97\n- inglese (Eng): 0.84\n- matematica (Mth): 0.73\n- pitch discrimination (Ptc): 0.65\n\nSi noti la somiglianza con i valori ottenuti mediante il metodo di massima verosimiglianza riportati nella figura.\n\n## Analisi fattoriale esplorativa\n\nQuando abbiamo un'unica variabile latente, l'analisi fattoriale confermativa si riduce al caso dell'analisi fattoriale esplorativa. Esaminiamo qui sotto la sintassi per l'analisi fattoriale esplorativa in `lavaan`. \n\nSpecifichiamo il modello.\n\n\n\n\n::: {.cell layout-align=\"center\" vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\nefa_model <- '\n    efa(\"efa\")*g =~ Classics + French + English + Math + Pitch + Music\n'\n```\n:::\n\n\n\n\nAdattiamo il modello ai dati.\n\n\n\n\n::: {.cell layout-align=\"center\" vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\nfit2 <- lavaan::cfa(\n  efa_model,\n  sample.cov = spearman_cor_mat,\n  sample.nobs = n,\n  std.lv = TRUE\n)\n```\n:::\n\n\n\n\nEsaminiamo la soluzione ottenuta.\n\n\n\n\n::: {.cell layout-align=\"center\" lines_to_next_cell='2' vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\nsummary(fit2, standardized = TRUE) \n#> lavaan 0.6-19 ended normally after 3 iterations\n#> \n#>   Estimator                                         ML\n#>   Optimization method                           NLMINB\n#>   Number of model parameters                        12\n#> \n#>   Rotation method                       GEOMIN OBLIQUE\n#>   Geomin epsilon                                 0.001\n#>   Rotation algorithm (rstarts)                GPA (30)\n#>   Standardized metric                             TRUE\n#>   Row weights                                     None\n#> \n#>   Number of observations                            33\n#> \n#> Model Test User Model:\n#>                                                       \n#>   Test statistic                                 2.913\n#>   Degrees of freedom                                 9\n#>   P-value (Chi-square)                           0.968\n#> \n#> Parameter Estimates:\n#> \n#>   Standard errors                             Standard\n#>   Information                                 Expected\n#>   Information saturated (h1) model          Structured\n#> \n#> Latent Variables:\n#>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n#>   g =~ efa                                                              \n#>     Classics          0.942    0.129    7.314    0.000    0.942    0.956\n#>     French            0.857    0.137    6.239    0.000    0.857    0.871\n#>     English           0.795    0.143    5.545    0.000    0.795    0.807\n#>     Math              0.732    0.149    4.923    0.000    0.732    0.743\n#>     Pitch             0.678    0.153    4.438    0.000    0.678    0.689\n#>     Music             0.643    0.155    4.142    0.000    0.643    0.653\n#> \n#> Variances:\n#>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n#>    .Classics          0.083    0.051    1.629    0.103    0.083    0.086\n#>    .French            0.234    0.072    3.244    0.001    0.234    0.242\n#>    .English           0.338    0.094    3.610    0.000    0.338    0.349\n#>    .Math              0.434    0.115    3.773    0.000    0.434    0.447\n#>    .Pitch             0.510    0.132    3.855    0.000    0.510    0.526\n#>    .Music             0.556    0.143    3.893    0.000    0.556    0.573\n#>     g                 1.000                               1.000    1.000\n```\n:::\n\n::: {.cell layout-align=\"center\" vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\nsemPaths(\n    fit2,\n    \"std\",\n    posCol = c(\"black\"),\n    edge.label.cex = 1.2,\n    whatLabels = \"std\",\n    edge.width = 0.3, # Imposta lo spessore delle linee\n    fade = FALSE # Disabilita il fading\n)\n```\n\n::: {.cell-output-display}\n![](02_analisi_fattoriale_1_files/figure-html/unnamed-chunk-25-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n## Riflessioni Conclusive\n\nIn questo capitolo, abbiamo introdotto il metodo dell’annullamento della tetrade, che permette di stimare le saturazioni in un modello monofattoriale. Abbiamo anche illustrato come questo metodo sia, in effetti, un’applicazione del concetto di correlazione parziale.\n\nUn aspetto fondamentale nella costruzione dei test psicologici riguarda la determinazione del numero di fattori o tratti sottostanti al set di indicatori in esame. La teoria classica dei test presuppone che un test sia monofattoriale, cioè che gli indicatori riflettano un unico tratto latente. La mancata monodimensionalità introduce difficoltà nell’applicare i principi della teoria classica ai punteggi di un test che non soddisfa tale proprietà.\n\nL’analisi della dimensionalità di un insieme di indicatori rappresenta, quindi, una fase cruciale nel processo di costruzione di un test. Solitamente, questa valutazione viene effettuata attraverso l’analisi fattoriale. In questo capitolo, abbiamo descritto le proprietà di base del modello unifattoriale, gettando le fondamenta per una comprensione più approfondita della dimensionalità e dell'influenza di un singolo tratto latente sugli indicatori.\n\n## Session Info\n\n\n\n\n::: {.cell layout-align=\"center\" vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.4.2 (2024-10-31)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.3\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] lavaanExtra_0.2.1 lavaanPlot_0.8.1  kableExtra_1.4.0  corrplot_0.95    \n#>  [5] ggokabeito_0.1.0  see_0.9.0         MASS_7.3-64       viridis_0.6.5    \n#>  [9] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#> [13] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#> [17] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#> [21] knitr_1.49        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#> [25] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n#> [29] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#> \n#> loaded via a namespace (and not attached):\n#>   [1] RColorBrewer_1.1-3  rstudioapi_0.17.1   jsonlite_1.8.9     \n#>   [4] magrittr_2.0.3      TH.data_1.1-3       estimability_1.5.1 \n#>   [7] farver_2.1.2        nloptr_2.1.1        rmarkdown_2.29     \n#>  [10] vctrs_0.6.5         minqa_1.2.8         base64enc_0.1-3    \n#>  [13] rstatix_0.7.2       htmltools_0.5.8.1   broom_1.0.7        \n#>  [16] Formula_1.2-5       htmlwidgets_1.6.4   plyr_1.8.9         \n#>  [19] sandwich_3.1-1      emmeans_1.10.6      zoo_1.8-12         \n#>  [22] igraph_2.1.4        mime_0.12           lifecycle_1.0.4    \n#>  [25] pkgconfig_2.0.3     Matrix_1.7-2        R6_2.5.1           \n#>  [28] fastmap_1.2.0       rbibutils_2.3       shiny_1.10.0       \n#>  [31] numDeriv_2016.8-1.1 digest_0.6.37       OpenMx_2.21.13     \n#>  [34] fdrtool_1.2.18      colorspace_2.1-1    rprojroot_2.0.4    \n#>  [37] Hmisc_5.2-2         labeling_0.4.3      timechange_0.3.0   \n#>  [40] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n#>  [43] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#>  [46] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n#>  [49] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#>  [52] foreign_0.8-88      zip_2.3.1           httpuv_1.6.15      \n#>  [55] nnet_7.3-20         glue_1.8.0          quadprog_1.5-8     \n#>  [58] DiagrammeR_1.0.11   nlme_3.1-167        promises_1.3.2     \n#>  [61] lisrelToR_0.3       grid_4.4.2          checkmate_2.3.2    \n#>  [64] cluster_2.1.8       reshape2_1.4.4      generics_0.1.3     \n#>  [67] gtable_0.3.6        tzdb_0.4.0          data.table_1.16.4  \n#>  [70] hms_1.1.3           xml2_1.3.6          car_3.1-3          \n#>  [73] sem_3.1-16          pillar_1.10.1       rockchalk_1.8.157  \n#>  [76] later_1.4.1         splines_4.4.2       lattice_0.22-6     \n#>  [79] survival_3.8-3      kutils_1.73         tidyselect_1.2.1   \n#>  [82] miniUI_0.1.1.1      pbapply_1.7-2       reformulas_0.4.0   \n#>  [85] svglite_2.1.3       stats4_4.4.2        xfun_0.50          \n#>  [88] qgraph_1.9.8        arm_1.14-4          visNetwork_2.1.2   \n#>  [91] stringi_1.8.4       yaml_2.3.10         pacman_0.5.1       \n#>  [94] boot_1.3-31         evaluate_1.0.3      codetools_0.2-20   \n#>  [97] mi_1.1              cli_3.6.3           RcppParallel_5.1.10\n#> [100] rpart_4.1.24        systemfonts_1.2.1   xtable_1.8-4       \n#> [103] Rdpack_2.6.2        munsell_0.5.1       Rcpp_1.0.14        \n#> [106] coda_0.19-4.1       png_0.1-8           XML_3.99-0.18      \n#> [109] parallel_4.4.2      jpeg_0.1-10         lme4_1.1-36        \n#> [112] mvtnorm_1.3-3       openxlsx_4.2.8      rlang_1.1.5        \n#> [115] multcomp_1.4-26     mnormt_2.1.1\n```\n:::\n",
    "supporting": [
      "02_analisi_fattoriale_1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}