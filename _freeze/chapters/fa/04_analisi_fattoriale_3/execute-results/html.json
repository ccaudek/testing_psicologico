{
  "hash": "e74b461eabac0074cc31130c263b916a",
  "result": {
    "engine": "knitr",
    "markdown": "# Il modello multifattoriale {#sec-fa-multifactor-model}\n\n::: callout-important\n## In questo capitolo imparerai a\n\n- calcolare e interpretare la correlazione parziale;\n- capire la teoria dei due fattori;\n- applicare e comprendere il metodo dell'annullamento della tetrade.\n:::\n\n::: callout-tip\n## Prerequisiti\n\n- Leggere il capitolo 6, *Factor Analysis and Principal Component Analysis*, del testo *Principles of psychological assessment* di @petersen2024principles. \n:::\n\n::: callout-caution\n## Preparazione del Notebook\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\n\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(lavaan, semPlot, corrplot, tidyr, tidySEM, kableExtra)\n```\n:::\n\n\n\n\n:::\n\n## Fattori ortogonali e teoria multifattoriale\n\nLa *teoria dei due fattori* di Spearman, secondo cui la prestazione in\ncompiti cognitivi sarebbe spiegata da un fattore generale (*g*) comune a\ntutte le variabili e da un fattore specifico per ognuna di esse, ha\ninfluenzato gli studi sull’intelligenza per diversi anni. Tuttavia, con\nil passare del tempo, è emersa la necessità di spiegare in modo più\narticolato la covariazione tra più variabili osservabili. A questo scopo\nThurstone (1945) propose la *teoria multifattoriale*, in base alla quale\nla covariazione tra le variabili manifeste (*test*, *misure*) non può\nessere riconducibile a un singolo fattore generale, ma deve essere\nspiegata dall’azione congiunta di *diversi fattori* comuni (ognuno\nrelativo ad alcuni soltanto dei test).\n\n### Dal modello a due fattori al modello multifattoriale\n\nNel modello multifattoriale di Thurstone, si assume che ogni variabile\nmanifesta $Y_i$ dipenda da:\n\n1. **Un insieme di $m$ fattori comuni** ($\\xi_1,\\dots,\\xi_m$), che\n   spiegano la correlazione tra variabili diverse. Questi fattori sono\n   detti *comuni* perché intervengono in più variabili manifeste.\n2. **Un fattore specifico** (o *fattore unico*, indicato con\n   $\\delta_i$) proprio di ogni variabile manifesta. Questo fattore\n   spiega la parte di varianza *non* condivisa con le altre variabili e\n   viene spesso trattato come termine di errore o rumore statistico.\n\nIn presenza di $p$ variabili manifeste $Y_1,\\dots,Y_p$, l’ipotesi\nfondamentale è che vi siano molti meno fattori comuni ($m$) rispetto\nal numero di variabili ($p$), così da avere un modello *parsimonioso*:\npoche variabili latenti riescono a spiegare un gran numero di variabili\nosservate.\n\n### Notazione\n\n- Le **variabili manifeste** $Y$ sono indicizzate da $i=1,\\dots,p$.\n- Le **variabili latenti a fattore comune** ($\\xi$) sono indicizzate\n  da $j=1,\\dots,m$.\n- I **fattori specifici** ($\\delta$) sono indicizzati da\n  $i=1,\\dots,p$. Ciascun $\\delta_i$ agisce *soltanto* su\n  $Y_i$.\n- Le **saturazioni fattoriali** $\\lambda_{ij}$ sono i parametri che\n  quantificano l’importanza del fattore $\\xi_j$ nella composizione\n  della variabile osservabile $Y_i$.\n\nInoltre, $\\mu_i$ è la media della $i$-esima variabile manifesta\n$Y_i$. Per semplicità, si assume che i fattori latenti abbiano media\nzero, il che permette di separare nettamente la componente sistematica\n($\\xi_j$) dalla media $\\mu_i$.\n\n### Equazioni del modello multifattoriale\n\n1. **Caso senza fattori comuni** (solo fattori specifici):\n\n   $$\n   \\begin{cases}\n     Y_{1k} = \\mu_1 + \\delta_{1k}, \\\\\n     \\vdots \\\\\n     Y_{ik} = \\mu_i + \\delta_{ik}, \\\\\n     \\vdots \\\\\n     Y_{pk} = \\mu_p + \\delta_{pk}.\n   \\end{cases}\n   $$\n\n   In questo scenario, ogni variabile $Y_i$ dipende solo dalla propria\n   media $\\mu_i$ e dal fattore specifico $\\delta_i$. Poiché\n   $\\delta_i$ non è condiviso tra più variabili, le $Y_i$ risulterebbero\n   incorrelate.\n\n2. **Caso con $m$ fattori comuni** (oltre ai fattori specifici):\n\n   $$\n   \\begin{cases}\n     Y_1 - \\mu_1 &= \\lambda_{11}\\,\\xi_1 + \\dots + \\lambda_{1k}\\,\\xi_k + \\dots + \\lambda_{1m}\\,\\xi_m + \\delta_1, \\\\\n     \\vdots & \\\\\n     Y_i - \\mu_i &= \\lambda_{i1}\\,\\xi_1 + \\dots + \\lambda_{ik}\\,\\xi_k + \\dots + \\lambda_{im}\\,\\xi_m + \\delta_i, \\\\\n     \\vdots & \\\\\n     Y_p - \\mu_p &= \\lambda_{p1}\\,\\xi_1 + \\dots + \\lambda_{pk}\\,\\xi_k + \\dots + \\lambda_{pm}\\,\\xi_m + \\delta_p.\n   \\end{cases}\n   $$\n\n   Ogni variabile manifesta $Y_i$ viene dunque vista come **combinazione\n   lineare** di tutti e soli i fattori comuni $\\xi_j$ e di un fattore\n   specifico $\\delta_i$ che la riguarda esclusivamente.\n\nRiassumendo, nel modello multifattoriale:\n\n- $\\xi_j$ (con $j=1,\\dots,m$) è la $j$-esima variabile latente a\n  fattore comune;\n- $\\lambda_{ij}$ è il *peso fattoriale* o *saturazione* che misura\n  quanto il fattore $\\xi_j$ contribuisce a definire la variabile\n  osservabile $Y_i$;\n- $\\delta_i$ è il fattore specifico, esclusivo della variabile $Y_i$.\n\n### Assunzioni del modello multifattoriale\n\nPer semplificare l’identificazione e l’interpretazione del modello,\nvengono poste alcune assunzioni chiave:\n\n1. **Media e varianza dei fattori comuni**:\n\n   - $\\mathbb{E}(\\xi_j)=0$ per ogni $j=1,\\dots,m$.  \n     (*Non avendo unità di misura proprie, si impone una media nulla per\n     rendere il modello più semplice da stimare.*)\n\n   - $\\mathbb{V}(\\xi_j)=1$.  \n     (*Analogamente, si normalizza la varianza dei fattori comuni a 1.*)\n\n2. **Incorrelazione tra i fattori comuni**:  \n   $$\n   \\text{Cov}(\\xi_j, \\xi_h)=0 \\quad \\text{per} \\ j \\neq h.\n   $$  \n   Se i fattori sono incorrelati, si parla di *fattori ortogonali*.\n   Questa è un’ipotesi tipica nelle prime formulazioni, ma **può essere\n   rilassata** nei modelli a fattori *obliqui*, dove si ammette la\n   possibilità che i fattori comuni siano correlati tra loro.\n\n3. **Incorrelazione tra i fattori specifici**:  \n   $$\n   \\text{Cov}(\\delta_i,\\delta_k)=0 \\quad \\text{per} \\ i \\neq k,\n   $$  \n   con $\\mathbb{E}(\\delta_i) = 0$ e $\\mathbb{V}(\\delta_i) = \\psi_{ii}$.\n   La quantità $\\psi_{ii}$ è la *varianza specifica* (o *unicità*) di\n   $Y_i$.\n\n4. **Incorrelazione tra fattori comuni e fattori specifici**:  \n   $$\n   \\text{Cov}(\\xi_j, \\delta_i)=0\n   \\quad \\text{per ogni} \\ i=1,\\dots,p \\ \\text{e} \\ j=1,\\dots,m.\n   $$\n\nIn sintesi, il modello multifattoriale di Thurstone propone che la\nrelazione tra le variabili osservate sia spiegabile da un numero\nrelativamente piccolo di fattori latenti *ortogonali* (per via\ndell’ipotesi di incorrelazione), ognuno dei quali influenza *solo alcune*\ndelle variabili. Ciascuna variabile riceve inoltre un contributo unico\nda un fattore specifico. Grazie a queste ipotesi, si ottiene una\nstruttura lineare parsimoniosa, utile sia in ambito di ricerca (per\nevidenziare eventuali dimensioni latenti comuni) sia in contesti\napplicativi (ad esempio nella costruzione e validazione di test\npsicologici).\n\n---\n\n**Nota**: Nella pratica, i fattori estratti con metodi come l’Analisi\ndei Fattori (*Factor Analysis*) possono anche essere obliqui (cioè\ncorrelati tra loro) qualora l’ipotesi di ortogonalità non sia realistica\no non sia empiricamente supportata dai dati. L’ortogonalità non è dunque\nun dogma, bensì un’opzione che semplifica l’interpretazione ma che\npotrebbe non essere sempre adeguata a rappresentare la realtà\npsicologica sottostante.\n\n\n## Interpretazione dei parametri del modello\n\n### Covarianza tra variabili e fattori\n\nSupponiamo che le variabili manifeste $Y_i$ abbiano media nulla\n$\\bigl(\\mathbb{E}(Y_i)=0\\bigr)$. In questo caso, la covarianza tra una\nvariabile $Y_i$ e un fattore comune $\\xi_j$ coincide esattamente con\nla corrispondente saturazione fattoriale $\\lambda_{ij}$. Mostriamo\nquesto risultato nel dettaglio:\n\n$$\n\\begin{aligned}\n  \\text{Cov}(Y_i, \\xi_j) &= \\mathbb{E}(Y_i \\,\\xi_j)\\\\\n  &= \\mathbb{E}\\Bigl[\\bigl(\\lambda_{i1} \\,\\xi_1 + \\dots + \\lambda_{im} \\,\\xi_m + \\delta_i\\bigr)\\,\\xi_j \\Bigr]\\\\\n  &= \\lambda_{i1}\\,\\underbrace{\\mathbb{E}(\\xi_1\\,\\xi_j)}_{=0}\n     + \\dots \n     + \\lambda_{ij}\\,\\underbrace{\\mathbb{E}(\\xi_j^2)}_{=1}\n     + \\dots \n     + \\lambda_{im}\\,\\underbrace{\\mathbb{E}(\\xi_m\\,\\xi_j)}_{=0}\n     + \\underbrace{\\mathbb{E}(\\delta_i\\,\\xi_j)}_{=0} \\\\\n  &= \\lambda_{ij}.\n\\end{aligned}\n$$\n\n- La prima e l’ultima eguaglianza seguono dall’ipotesi che i fattori\n  comuni $\\xi_j$ abbiano media zero e siano incorrelati tra loro\n  ($\\text{Cov}(\\xi_j,\\xi_h) = 0$ per $j \\neq h$) e con i fattori\n  specifici $\\delta_i$.  \n- Inoltre, poiché $\\mathbb{V}(\\xi_j)=1$, si ha\n  $\\mathbb{E}(\\xi_j^2)=1$.  \n\nIn sintesi, **le saturazioni fattoriali\n$\\lambda_{ij}$ misurano la covarianza tra la variabile manifesta\n$Y_i$ e il fattore latente $\\xi_j$**, a patto che $\\mathbb{E}(Y_i)=0$.\n\n#### Saturazioni e correlazioni nel caso di variabili standardizzate\n\nSe le variabili $Y_i$ sono ulteriormente *standardizzate*, ossia\n$\\mathbb{V}(Y_i)=1$, allora la saturazione fattoriale $\\lambda_{ij}$\ndiventa la *correlazione* tra $Y_i$ e $\\xi_j$. In tal caso,\nscriveremo\n\n$$\nr_{ij} \\;=\\; \\lambda_{ij}.\n$$\n\nCiò fornisce un’interpretazione ancora più immediata delle saturazioni\nfattoriali: in presenza di fattori latenti standardizzati, ciascuna\n$\\lambda_{ij}$ riflette quanto il fattore $\\xi_j$ è correlato con\nla variabile osservabile $Y_i$.\n\n\n### Espressione fattoriale della varianza\n\nNel modello multifattoriale, analogamente a quanto avviene nel modello\nmonofattoriale, la varianza di ciascuna variabile manifesta $Y_i$ può\nessere scomposta in due componenti:\n\n1. Una **componente comune**, detta *comunalità*, che riflette la porzione\n   di varianza di $Y_i$ spiegata dai fattori comuni.\n2. Una **componente specifica**, detta *unicità*, che rappresenta la\n   porzione di varianza di $Y_i$ non spiegata dai fattori comuni ed è\n   attribuibile al fattore specifico $\\delta_i$.\n\nSupponendo che $\\mathbb{E}(Y_i)=0$ per ogni $i$, la varianza della\nvariabile $Y_i$ è:\n\n$$\n\\mathbb{V}(Y_i) \n= \\mathbb{E}\\Bigl[\\bigl(\\lambda_{i1}\\,\\xi_1 + \\dots + \\lambda_{im}\\,\\xi_m + \\delta_i\\bigr)^2\\Bigr].\n$$\n\n#### Sviluppo del polinomio\n\nSviluppando il **quadrato** del termine tra parentesi, ricordiamo che il\nquadrato di una somma include:\n\n1. La somma dei quadrati di tutti i termini.\n2. Il doppio prodotto di ogni termine con ciascuno degli altri termini\n   successivi.\n\nIn formula:\n\n$$\n(a + b + c)^2 \\;=\\; a^2 + b^2 + c^2 \\;+\\; 2ab + 2ac + 2bc.\n$$\n\nApplicando lo stesso principio al nostro caso:\n\n$$\n\\Bigl(\\lambda_{i1}\\,\\xi_1 + \\dots + \\lambda_{im}\\,\\xi_m + \\delta_i\\Bigr)^2\n= \\sum_{j=1}^m \\lambda_{ij}^2\\,\\xi_j^2 \\;+\\; \\delta_i^2\n  \\;+\\; \\text{(termini di doppio prodotto)}.\n$$\n\n#### Valore atteso: contributo dei singoli fattori e delle loro interazioni\n\nPer calcolare $\\mathbb{V}(Y_i)$, prendiamo il **valore atteso** di\nquesto polinomio:\n\n- **Termini al quadrato dei fattori comuni**:  \n  Poiché $\\mathbb{V}(\\xi_j) = 1$ e $\\mathbb{E}(\\xi_j^2) = 1$, il\n  contributo di ciascun fattore comune $\\xi_j$ è\n  $\\lambda_{ij}^2$.\n- **Termine al quadrato del fattore specifico**:  \n  $\\mathbb{E}(\\delta_i^2) = \\psi_{ii}$, dove $\\psi_{ii}$ è la\n  varianza specifica della variabile $Y_i$.\n- **Termini di doppio prodotto**:  \n  Grazie all’ipotesi di ortogonalità, la covarianza tra fattori comuni\n  diversi è nulla ($\\mathbb{E}(\\xi_j\\,\\xi_h) = 0$ per $j \\neq h$), e\n  la covarianza tra fattori comuni e specifici è anch’essa nulla\n  ($\\mathbb{E}(\\delta_i\\,\\xi_j) = 0$). Di conseguenza, tutti i doppi\n  prodotti si annullano e non contribuiscono alla varianza totale.\n\n#### Risultato finale\n\nIn conclusione, la varianza di $Y_i$ si ottiene sommando i contributi\ndi tutti i fattori comuni e del fattore specifico:\n\n$$\n\\mathbb{V}(Y_i) \n= \\lambda_{i1}^2 + \\lambda_{i2}^2 + \\dots + \\lambda_{im}^2 + \\psi_{ii}\n= \\sum_{j=1}^m \\lambda_{ij}^2 + \\psi_{ii}.\n$$\n\n- **Comunalità** $\\,h_i^2 = \\sum_{j=1}^m \\lambda_{ij}^2$: misura la\n  quota di varianza di $Y_i$ spiegata dai fattori comuni.\n- **Unicità** $\\,\\psi_{ii}$: rappresenta la varianza *non* spiegata\n  dai fattori comuni, associata esclusivamente al fattore specifico\n  $\\delta_i$.\n\nIn sintesi, per ogni variabile $\\,Y_i$, la **somma** di comunalità e\nunicità deve coincidere con la sua varianza totale, a conferma che\nl’analisi fattoriale scompone ciascuna variabile in una parte “comune”\ne una parte “specifica”.\n\n\n### Espressione fattoriale della covarianza\n\nA titolo di esempio, consideriamo il caso di $p=5$ variabili osservate\ne $m=2$ fattori ortogonali. Assumiamo inoltre che le variabili\nmanifeste siano state *centrate* (ossia abbiano media nulla), così da\npoter omettere i termini costanti. In queste condizioni, il modello\nmultifattoriale si scrive:\n\n$$\n\\begin{cases}\n  Y_1 = \\lambda_{11}\\,\\xi_1 + \\lambda_{12}\\,\\xi_2 + \\delta_1, \\\\\n  Y_2 = \\lambda_{21}\\,\\xi_1 + \\lambda_{22}\\,\\xi_2 + \\delta_2, \\\\\n  Y_3 = \\lambda_{31}\\,\\xi_1 + \\lambda_{32}\\,\\xi_2 + \\delta_3, \\\\\n  Y_4 = \\lambda_{41}\\,\\xi_1 + \\lambda_{42}\\,\\xi_2 + \\delta_4, \\\\\n  Y_5 = \\lambda_{51}\\,\\xi_1 + \\lambda_{52}\\,\\xi_2 + \\delta_5.\n\\end{cases}\n$$\n\nRicordiamo che:\n\n- $\\xi_1$ e $\\xi_2$ sono i due fattori comuni, con\n  $\\mathbb{E}(\\xi_j) = 0$, $\\mathbb{V}(\\xi_j)=1$ e\n  $\\mathrm{Cov}(\\xi_1,\\xi_2)=0$.\n- $\\delta_i$ è il fattore specifico associato a $Y_i$, con\n  $\\mathbb{E}(\\delta_i)=0$ e\n  $\\mathrm{Cov}(\\delta_i,\\delta_k)=0$ per $i \\neq k$.\n- I fattori comuni sono incorrelati con i fattori specifici\n  ($\\mathrm{Cov}(\\xi_j,\\delta_i)=0$).\n\n#### Calcolo esplicito di una covarianza\n\nMostriamo, nello specifico, come si ottiene la covarianza\n$\\mathrm{Cov}(Y_1,Y_2)$. Supponendo $\\mathbb{E}(Y_1)=\\mathbb{E}(Y_2)=0$,\n\n$$\n\\begin{aligned}\n  \\mathrm{Cov}(Y_1, Y_2) \n    &= \\mathbb{E}\\bigl(Y_1\\,Y_2\\bigr) \\\\\n    &= \\mathbb{E}\\Bigl[\n       (\\lambda_{11} \\,\\xi_1 + \\lambda_{12} \\,\\xi_2 + \\delta_1)\n       \\,(\\lambda_{21} \\,\\xi_1 + \\lambda_{22} \\,\\xi_2 + \\delta_2)\n       \\Bigr].\n\\end{aligned}\n$$\n\nSviluppando il prodotto e facendo uso delle ipotesi di ortogonalità, si\nottiene:\n\n1. $\\lambda_{11}\\,\\lambda_{21}\\,\\mathbb{E}(\\xi_1^2)$. Qui, siccome\n   $\\mathrm{Var}(\\xi_1)=1$, abbiamo $\\mathbb{E}(\\xi_1^2)=1$.\n2. $\\lambda_{11}\\,\\lambda_{22}\\,\\mathbb{E}(\\xi_1\\,\\xi_2)=0$ perché\n   $\\mathrm{Cov}(\\xi_1,\\xi_2)=0$.\n3. $\\lambda_{11}\\,\\mathbb{E}(\\xi_1\\,\\delta_2)=0$ perché i fattori\n   comuni e i fattori specifici sono incorrelati.\n4. $\\lambda_{12}\\,\\lambda_{21}\\,\\mathbb{E}(\\xi_2\\,\\xi_1)=0$ per la\n   stessa ragione del punto 2.\n5. $\\lambda_{12}\\,\\lambda_{22}\\,\\mathbb{E}(\\xi_2^2)$. Analogamente,\n   $\\mathrm{Var}(\\xi_2)=1$, quindi $\\mathbb{E}(\\xi_2^2)=1$.\n6. $\\lambda_{12}\\,\\mathbb{E}(\\xi_2\\,\\delta_2)=0$ (incorrelazione tra\n   fattori).\n7. $\\lambda_{21}\\,\\mathbb{E}(\\xi_1\\,\\delta_1)=0$ (stessa ragione).\n8. $\\lambda_{22}\\,\\mathbb{E}(\\xi_2\\,\\delta_1)=0$ (stessa ragione).\n9. $\\mathbb{E}(\\delta_1\\,\\delta_2)=0$ (i fattori specifici sono\n   incorrelati tra loro).\n\nCon tutti i termini di doppio prodotto che si annullano, rimane solo la\nsomma dei termini che includono $\\xi_1^2$ e $\\xi_2^2$:\n\n$$\n\\mathrm{Cov}(Y_1, Y_2) \n= \\lambda_{11}\\,\\lambda_{21} \\,+\\, \\lambda_{12}\\,\\lambda_{22}.\n$$\n\n#### Interpretazione\n\nIn generale, la covarianza tra due variabili manifeste $Y_\\ell$ e\n$Y_m$ in un modello multifattoriale con $m$ fattori ortogonali si\npuò interpretare come **la somma dei prodotti tra le saturazioni nei\nfattori comuni condivisi**:\n\n$$\n\\mathrm{Cov}(Y_\\ell, Y_m) \n= \\sum_{j=1}^m \\lambda_{\\ell j}\\,\\lambda_{mj}.\n$$\n\nQuesto vuol dire che due variabili $Y_\\ell$ e $Y_m$ risultano\ncorrelate soltanto nella misura in cui condividono (in senso letterale)\ngli stessi fattori comuni. Se in una determinata posizione $j$ una\nvariabile ha saturazione prossima a zero, il contributo di quel fattore\nalla covarianza tra le due variabili sarà molto basso (o nullo).\n\n---\n\nQuesto esempio illustra in modo concreto come, nel modello\nmultifattoriale, la *covarianza* osservata tra due variabili sia\nl’effetto cumulativo dei **prodotti delle loro saturazioni** nei fattori\ncomuni. La struttura di queste saturazioni diventa quindi fondamentale\nper interpretare quali fattori latenti spiegano la correlazione tra le\nvarie misure osservate.\n\n::: {#exm-} \nConsideriamo, a titolo di esempio, i dati presentati da @brown2015confirmatory relativi a un campione di 250 pazienti che hanno completato un programma di psicoterapia. Su ciascun soggetto sono state raccolte otto misure di personalità, corrispondenti ad altrettante scale:\n\n- **N1**: anxiety  \n- **N2**: hostility  \n- **N3**: depression  \n- **N4**: self-consciousness  \n- **E1**: warmth  \n- **E2**: gregariousness  \n- **E3**: assertiveness  \n- **E4**: positive emotions  \n\nLe prime quattro scale riflettono diverse sfaccettature del\n**neuroticismo** (N), mentre le ultime quattro sono riferite\nall’**estroversione** (E). Di seguito vengono mostrate le deviazioni\nstandard di ciascuna scala, oltre alla matrice di correlazioni osservate\n(qui denominata `psychot_cor_mat`) e alla dimensione campionaria\n($n=250$).\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvarnames <- c(\"N1\", \"N2\", \"N3\", \"N4\", \"E1\", \"E2\", \"E3\", \"E4\")\nsds <- '5.7  5.6  6.4  5.7  6.0  6.2  5.7  5.6'\n\ncors <- '\n    1.000\n    0.767  1.000 \n    0.731  0.709  1.000 \n    0.778  0.738  0.762  1.000 \n    -0.351  -0.302  -0.356  -0.318  1.000 \n    -0.316  -0.280  -0.300  -0.267  0.675  1.000 \n    -0.296  -0.289  -0.297  -0.296  0.634  0.651  1.000 \n    -0.282  -0.254  -0.292  -0.245  0.534  0.593  0.566  1.000\n'\n\npsychot_cor_mat <- getCov(cors, names = varnames)\nn_obs <- 250\n```\n:::\n\n\n\n\n\n\n### Analisi fattoriale esplorativa\n\nApplichiamo un’**analisi fattoriale esplorativa** (EFA) con metodo di\nstima a **massima verosimiglianza** e ipotizziamo la presenza di\n**due fattori comuni incorrelati**. In R, il comando utilizzato è:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit_efa <- factanal(\n  covmat = psychot_cor_mat,\n  factors = 2,\n  rotation = \"varimax\",\n  n.obs = n_obs\n)\n```\n:::\n\n\n\n\n\nDalle saturazioni fattoriali (ottenute tramite `fit_efa$loadings`)\nemerge la presenza di **due fattori**:\n\n1. Il **primo fattore** satura principalmente sulle scale di\n   **neuroticismo** (N1, N2, N3, N4).  \n2. Il **secondo fattore** satura principalmente sulle scale di\n   **estroversione** (E1, E2, E3, E4).\n\n### Covarianze (o correlazioni) riprodotte dal modello\n\nUna volta stimati i fattori e le relative saturazioni\n$\\boldsymbol{\\Lambda}$, possiamo confrontare la matrice di\ncorrelazioni osservate con quella **riprodotta** dal modello fattoriale.\n\n#### Esempio di correlazione riprodotta\n\nPer illustrare il concetto, consideriamo la correlazione riprodotta tra\nN1 (prima variabile) e N2 (seconda variabile), che secondo il modello a\n**due fattori incorrelati** si ottiene sommando i prodotti delle\nsaturazioni sui singoli fattori:\n\n$$\nr_{12}^{(\\text{modello})} \n= \\lambda_{11}\\,\\lambda_{21} + \\lambda_{12}\\,\\lambda_{22},\n$$\n\ndove:\n\n- $\\lambda_{1j}$ è la saturazione della variabile $Y_1$ sul fattore\n  $j$.\n- $\\lambda_{2j}$ è la saturazione della variabile $Y_2$ sul fattore\n  $j$.\n  \nNell’esempio in R, otteniamo le saturazioni fattoriali:\n  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlambda <- fit_efa$loadings \nlambda\n#> \n#> Loadings:\n#>    Factor1 Factor2\n#> N1  0.854  -0.228 \n#> N2  0.826  -0.194 \n#> N3  0.811  -0.233 \n#> N4  0.865  -0.186 \n#> E1 -0.202   0.773 \n#> E2 -0.139   0.829 \n#> E3 -0.158   0.771 \n#> E4 -0.147   0.684 \n#> \n#>                Factor1 Factor2\n#> SS loadings      2.923   2.526\n#> Proportion Var   0.365   0.316\n#> Cumulative Var   0.365   0.681\n```\n:::\n\n\n\n\n\n\nL’espressione\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlambda[1, 1] * lambda[2, 1] + lambda[1, 2] * lambda[2, 2]\n#> [1] 0.7493\n```\n:::\n\n\n\n\n\nrestituisce un valore molto simile alla correlazione empirica 0.767,\nconfermando la bontà dell’adattamento del modello per questa coppia di\nvariabili.\n\n#### Intera matrice di correlazioni riprodotte\n\nL’intera **matrice di correlazioni riprodotte** dal modello è data da:\n\n$$\n\\mathbf{R}_{\\text{riprodotta}}\n=\n\\boldsymbol{\\Lambda}\\,\\boldsymbol{\\Lambda}^{\\mathsf{T}}\n+\n\\boldsymbol{\\Psi},\n$$\n\ndove $\\boldsymbol{\\Lambda}$ è la matrice delle saturazioni fattoriali\n(e righe corrispondono alle variabili, colonne ai fattori) e\n$\\boldsymbol{\\Psi}$ è la matrice diagonale contenente le varianze\nspecifiche (unicità) stimate. In R, possiamo calcolare questa matrice\ncon:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nRr <- lambda %*% t(lambda) + diag(fit_efa$uniq)\nRr |> round(2)\n#>       N1    N2    N3    N4    E1    E2    E3    E4\n#> N1  1.00  0.75  0.75  0.78 -0.35 -0.31 -0.31 -0.28\n#> N2  0.75  1.00  0.71  0.75 -0.32 -0.28 -0.28 -0.25\n#> N3  0.75  0.71  1.00  0.74 -0.34 -0.31 -0.31 -0.28\n#> N4  0.78  0.75  0.74  1.00 -0.32 -0.27 -0.28 -0.25\n#> E1 -0.35 -0.32 -0.34 -0.32  1.00  0.67  0.63  0.56\n#> E2 -0.31 -0.28 -0.31 -0.27  0.67  1.00  0.66  0.59\n#> E3 -0.31 -0.28 -0.31 -0.28  0.63  0.66  1.00  0.55\n#> E4 -0.28 -0.25 -0.28 -0.25  0.56  0.59  0.55  1.00\n```\n:::\n\n\n\n\n\nConfrontando `Rr` con la matrice di correlazioni osservate\n(`psychot_cor_mat`), otteniamo l’**errore di riproduzione**, ovvero la\ndifferenza tra i valori osservati e quelli teorici previsti dal modello:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npsychot_cor_mat - Rr |> round(3)\n#>        N1     N2     N3     N4     E1     E2     E3     E4\n#> N1  0.000  0.018 -0.014 -0.003 -0.003 -0.009  0.015 -0.001\n#> N2  0.018  0.000 -0.006 -0.013  0.015 -0.004 -0.008  0.000\n#> N3 -0.014 -0.006  0.000  0.017 -0.012  0.006  0.011 -0.013\n#> N4 -0.003 -0.013  0.017  0.000  0.000  0.007 -0.016  0.009\n#> E1 -0.003  0.015 -0.012  0.000  0.000  0.006  0.006 -0.024\n#> E2 -0.009 -0.004  0.006  0.007  0.006  0.000 -0.010  0.006\n#> E3  0.015 -0.008  0.011 -0.016  0.006 -0.010  0.000  0.016\n#> E4 -0.001  0.000 -0.013  0.009 -0.024  0.006  0.016  0.000\n```\n:::\n\n\n\n\n\nIdealmente, se il modello fattoriale si adatta bene ai dati, tale\ndifferenza risulterà piccola per tutte le coppie di variabili\n(osservate).\n\n**In sintesi**, questo esempio dimostra come un modello con **due fattori\nincorrelati** sia in grado di spiegare le correlazioni tra le otto scale\ndi personalità. Il primo fattore, caricato dalle scale di neuroticismo,\ne il secondo, caricato da quelle di estroversione, confermano così la\nstruttura a due dimensioni attesa.\n:::\n\n\n## Fattori obliqui\n\nNel modello fattoriale, può accadere che i **fattori comuni** non siano ortogonali, ma **correlati tra loro**. In tal caso si parla di **fattori obliqui**. Anche in questa situazione, è possibile esprimere:\n\n- la **covarianza teorica** tra una variabile manifesta $Y_i$ e un fattore comune $\\xi_j$,\n- la **covarianza teorica** tra due variabili manifeste,\n- la **comunalità** di ciascuna variabile manifesta,\n\nma le formule diventano più complesse rispetto al caso ortogonale, perché occorre tener conto anche delle **covarianze tra i fattori comuni**.\n\n### Covarianza teorica tra variabili manifeste e fattori comuni\n\nNel modello multifattoriale con $m$ fattori comuni, ciascuna variabile manifesta è modellata come:\n\n$$\nY_i = \\lambda_{i1} \\xi_1 + \\dots + \\lambda_{im} \\xi_m + \\delta_i .\n$$ {#eq-modello-obliquo}\n\nVogliamo calcolare la **covarianza teorica** tra la variabile $Y_i$ e un fattore comune $\\xi_j$. Sfruttando la linearità del valore atteso, abbiamo:\n\n$$\n\\begin{aligned}\n\\mathrm{Cov}(Y_i, \\xi_j)\n&= \\mathbb{E}(Y_i \\cdot \\xi_j) \\\\\n&= \\mathbb{E}\\left[\n\\left(\\sum_{h=1}^{m} \\lambda_{ih} \\xi_h + \\delta_i \\right) \\xi_j\n\\right] \\\\\n&= \\sum_{h=1}^{m} \\lambda_{ih} \\cdot \\mathrm{Cov}(\\xi_h, \\xi_j)\n+ \\underbrace{\\mathrm{Cov}(\\delta_i, \\xi_j)}_{=0} \\\\\n&= \\sum_{h=1}^{m} \\lambda_{ih} \\cdot \\phi_{hj},\n\\end{aligned}\n$$\n\ndove $\\phi_{hj} = \\mathrm{Cov}(\\xi_h, \\xi_j)$ è l’elemento $(h,j)$ della matrice di covarianze tra i fattori comuni, denotata con $\\boldsymbol{\\Phi}$.\n\n**Esempio con 3 fattori comuni.** \n\nLa covarianza tra $Y_1$ e il primo fattore $\\xi_1$ è:\n\n$$\n\\mathrm{Cov}(Y_1, \\xi_1) = \\lambda_{11} + \\lambda_{12} \\cdot \\phi_{21} + \\lambda_{13} \\cdot \\phi_{31}.\n$$\n\n\n### Varianza teorica di una variabile manifesta\n\nPartendo dall'@eq-modello-obliquo, vogliamo calcolare la **varianza teorica** di $Y_i$. Sviluppiamo il quadrato:\n\n$$\n\\mathrm{Var}(Y_i) = \\mathbb{E}\\left[Y_i^2\\right] = \\mathbb{E}\\left[\n\\left(\\sum_{j=1}^m \\lambda_{ij} \\xi_j + \\delta_i \\right)^2\n\\right].\n$$\n\nSviluppando il quadrato e distribuendo il valore atteso:\n\n$$\n\\mathrm{Var}(Y_i) =\n\\sum_{j=1}^m \\lambda_{ij}^2 + \n2 \\sum_{j<k} \\lambda_{ij} \\lambda_{ik} \\cdot \\phi_{jk} + \n\\psi_{ii},\n$$\n\ndove $\\psi_{ii} = \\mathrm{Var}(\\delta_i)$ è l’unicità.\n\n**Esempio con tre fattori.** La varianza teorica di $Y_1$ sarà:\n\n$$\n\\begin{aligned}\n\\mathrm{Var}(Y_1) =\\; & \\lambda_{11}^2 + \\lambda_{12}^2 + \\lambda_{13}^2 + \\\\\n& 2 \\lambda_{11} \\lambda_{12} \\phi_{12} +\n  2 \\lambda_{11} \\lambda_{13} \\phi_{13} +\n  2 \\lambda_{12} \\lambda_{13} \\phi_{23} + \\\\\n& \\psi_{11}.\n\\end{aligned}\n$$\n\n\n### Covarianza teorica tra due variabili manifeste\n\nNel caso di due variabili $Y_1$ e $Y_2$ spiegate da due fattori obliqui $\\xi_1$ e $\\xi_2$, abbiamo:\n\n$$\n\\begin{aligned}\n\\mathrm{Cov}(Y_1, Y_2) =\\; &\n\\lambda_{11} \\lambda_{21} + \n\\lambda_{12} \\lambda_{22} + \\\\\n& \\lambda_{11} \\lambda_{22} \\cdot \\phi_{12} +\n  \\lambda_{12} \\lambda_{21} \\cdot \\phi_{12}.\n\\end{aligned}\n$$\n\nRaccogliendo $\\phi_{12}$:\n\n$$\n\\mathrm{Cov}(Y_1, Y_2) = \n\\lambda_{11} \\lambda_{21} +\n\\lambda_{12} \\lambda_{22} +\n\\phi_{12} (\\lambda_{11} \\lambda_{22} + \\lambda_{12} \\lambda_{21}).\n$$\n\n\n### Forma matriciale del modello\n\nNel caso generale, con $p$ variabili e $m$ fattori obliqui, la **matrice di covarianze teoriche** è:\n\n$$\n\\boldsymbol{\\Sigma} =\n\\boldsymbol{\\Lambda} \\boldsymbol{\\Phi} \\boldsymbol{\\Lambda}^\\mathsf{T} +\n\\boldsymbol{\\Psi},\n$$\n\ndove:\n\n- $\\boldsymbol{\\Lambda}$: matrice $p \\times m$ delle saturazioni fattoriali;\n- $\\boldsymbol{\\Phi}$: matrice $m \\times m$ di covarianze tra i fattori comuni (non più diagonale);\n- $\\boldsymbol{\\Psi}$: matrice diagonale $p \\times p$ delle unicità.\n\n\n## Applicazione con R: modello con fattori obliqui\n\nTorniamo ai dati di personalità esaminati in precedenza. Applichiamo ora un’analisi fattoriale con **rotazione obliqua (oblimin)**, che consente ai due fattori di essere correlati:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn_obs <- 250\n\nefa_result <- fa(\n  psychot_cor_mat,\n  nfactors = 2,\n  n.obs = n_obs,\n  rotate = \"oblimin\"\n)\n```\n:::\n\n\n\n\n\nVisualizziamo la struttura del modello:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfa.diagram(efa_result)\n```\n\n::: {.cell-output-display}\n![](04_analisi_fattoriale_3_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n### Saturazioni fattoriali e parametri del modello\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlambda <- matrix(efa_result$loadings[, 1:2], nrow = 8, ncol = 2)\n\nrownames(lambda) <- c(\"N1\", \"N2\", \"N3\", \"N4\", \"E1\", \"E2\", \"E3\", \"E4\")\ncolnames(lambda) <- c(\"Factor1\", \"Factor2\")\nlambda\n#>      Factor1  Factor2\n#> N1  0.877076 -0.01578\n#> N2  0.852281  0.01128\n#> N3  0.826584 -0.03685\n#> N4  0.898763  0.03121\n#> E1 -0.048589  0.77187\n#> E2  0.034700  0.85566\n#> E3  0.002815  0.79292\n#> E4 -0.007885  0.69545\n```\n:::\n\n\n\n\n\nMatrice di intercorrelazioni tra i fattori:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nPhi <- efa_result$Phi\nPhi\n#>         MR1     MR2\n#> MR1  1.0000 -0.4314\n#> MR2 -0.4314  1.0000\n```\n:::\n\n\n\n\n\nUnicità:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nPsi <- diag(efa_result$uniquenesses)\nround(Psi, 2)\n#>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n#> [1,] 0.22 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n#> [2,] 0.00 0.28 0.00 0.00 0.00 0.00 0.00 0.00\n#> [3,] 0.00 0.00 0.29 0.00 0.00 0.00 0.00 0.00\n#> [4,] 0.00 0.00 0.00 0.22 0.00 0.00 0.00 0.00\n#> [5,] 0.00 0.00 0.00 0.00 0.37 0.00 0.00 0.00\n#> [6,] 0.00 0.00 0.00 0.00 0.00 0.29 0.00 0.00\n#> [7,] 0.00 0.00 0.00 0.00 0.00 0.00 0.37 0.00\n#> [8,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.51\n```\n:::\n\n\n\n\n\n### Matrice delle correlazioni riprodotte\n\nCostruiamo la matrice di correlazioni riprodotte dal modello obliquo:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nR_hat <- lambda %*% Phi %*% t(lambda) + Psi\nround(R_hat, 2)\n#>       N1    N2    N3    N4    E1    E2    E3    E4\n#> N1  1.00  0.75  0.75  0.78 -0.35 -0.31 -0.31 -0.28\n#> N2  0.75  1.00  0.71  0.75 -0.32 -0.28 -0.28 -0.25\n#> N3  0.75  0.71  1.00  0.74 -0.34 -0.31 -0.31 -0.28\n#> N4  0.78  0.75  0.74  1.00 -0.32 -0.27 -0.28 -0.25\n#> E1 -0.35 -0.32 -0.34 -0.32  1.00  0.67  0.63  0.55\n#> E2 -0.31 -0.28 -0.31 -0.27  0.67  1.00  0.67  0.59\n#> E3 -0.31 -0.28 -0.31 -0.28  0.63  0.67  1.00  0.55\n#> E4 -0.28 -0.25 -0.28 -0.25  0.55  0.59  0.55  1.00\n```\n:::\n\n\n\n\n\nDifferenza con la matrice osservata:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nround(psychot_cor_mat - R_hat, 2)\n#>       N1    N2    N3    N4    E1    E2    E3    E4\n#> N1  0.00  0.02 -0.01  0.00  0.00 -0.01  0.01  0.00\n#> N2  0.02  0.00  0.00 -0.01  0.01  0.00 -0.01  0.00\n#> N3 -0.01  0.00  0.00  0.02 -0.01  0.01  0.01 -0.01\n#> N4  0.00 -0.01  0.02  0.00  0.00  0.01 -0.02  0.01\n#> E1  0.00  0.01 -0.01  0.00  0.00  0.01  0.01 -0.02\n#> E2 -0.01  0.00  0.01  0.01  0.01  0.00 -0.01  0.01\n#> E3  0.01 -0.01  0.01 -0.02  0.01 -0.01  0.00  0.01\n#> E4  0.00  0.00 -0.01  0.01 -0.02  0.01  0.01  0.00\n```\n:::\n\n\n\n\n\n### Correlazione riprodotta tra due variabili\n\nPer esempio, la correlazione tra N1 e N2 predetta dal modello è:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlambda[1,1] * lambda[2,1] +\nlambda[1,2] * lambda[2,2] +\nlambda[1,1] * lambda[2,2] * Phi[1,2] +\nlambda[1,2] * lambda[2,1] * Phi[1,2]\n#> [1] 0.7489\n```\n:::\n\n\n\n\n\nQuesto valore dovrebbe essere molto vicino al valore osservato:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npsychot_cor_mat[1, 2]\n#> [1] 0.767\n```\n:::\n\n\n\n\n\n**In sintesi**, nel modello a **fattori obliqui**, la struttura delle correlazioni tra variabili manifeste dipende **non solo dalle saturazioni**, ma anche dalla **correlazione tra i fattori comuni**. Questo tipo di modello è più flessibile e spesso più realistico in psicologia, dove i costrutti latenti tendono a essere interdipendenti.\n\n## Riflessioni Conclusive\n\nIn questo capitolo abbiamo esaminato il modello di analisi fattoriale comune, distinguendo tra l’ipotesi di **fattori ortogonali** e quella più generale di **fattori obliqui**. Abbiamo visto che, mentre i modelli ortogonali permettono una formulazione più semplice e interpretazioni più immediate, i modelli obliqui risultano più flessibili e realistici, poiché ammettono correlazioni tra i fattori latenti. Abbiamo inoltre analizzato come le covarianze tra le variabili osservate possano essere espresse in funzione delle saturazioni fattoriali, delle intercorrelazioni tra i fattori e delle unicità specifiche. Infine, abbiamo illustrato come queste relazioni teoriche si traducono concretamente nell’applicazione empirica dell’analisi fattoriale esplorativa, mediante l’uso del software R. Comprendere la struttura fattoriale sottostante a un insieme di variabili psicologiche osservate consente di sintetizzare l’informazione in modo più parsimonioso, rivelando le dimensioni latenti che organizzano il comportamento osservato.\n\n## Session Info\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.4.2 (2024-10-31)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.3.2\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] kableExtra_1.4.0  tidySEM_0.2.7     OpenMx_2.21.13    corrplot_0.95    \n#>  [5] ggokabeito_0.1.0  see_0.11.0        MASS_7.3-65       viridis_0.6.5    \n#>  [9] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#> [13] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#> [17] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#> [21] knitr_1.50        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#> [25] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#> [29] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#> \n#> loaded via a namespace (and not attached):\n#>   [1] splines_4.4.2         later_1.4.1           XML_3.99-0.18        \n#>   [4] rpart_4.1.24          fastDummies_1.7.5     lifecycle_1.0.4      \n#>   [7] Rdpack_2.6.3          rstatix_0.7.2         rprojroot_2.0.4      \n#>  [10] StanHeaders_2.32.10   globals_0.16.3        lattice_0.22-6       \n#>  [13] rockchalk_1.8.157     backports_1.5.0       magrittr_2.0.3       \n#>  [16] openxlsx_4.2.8        Hmisc_5.2-3           rmarkdown_2.29       \n#>  [19] httpuv_1.6.15         tmvnsim_1.0-2         qgraph_1.9.8         \n#>  [22] zip_2.3.2             pkgbuild_1.4.6        pbapply_1.7-2        \n#>  [25] minqa_1.2.8           multcomp_1.4-28       abind_1.4-8          \n#>  [28] quadprog_1.5-8        nnet_7.3-20           TH.data_1.1-3        \n#>  [31] sandwich_3.1-1        inline_0.3.21         listenv_0.9.1        \n#>  [34] arm_1.14-4            proto_1.0.0           parallelly_1.42.0    \n#>  [37] texreg_1.39.4         svglite_2.1.3         codetools_0.2-20     \n#>  [40] xml2_1.3.8            tidyselect_1.2.1      farver_2.1.2         \n#>  [43] lme4_1.1-36           matrixStats_1.5.0     stats4_4.4.2         \n#>  [46] base64enc_0.1-3       jsonlite_1.9.1        progressr_0.15.1     \n#>  [49] Formula_1.2-5         survival_3.8-3        emmeans_1.10.7       \n#>  [52] systemfonts_1.2.1     dbscan_1.2.2          tools_4.4.2          \n#>  [55] Rcpp_1.0.14           glue_1.8.0            mnormt_2.1.1         \n#>  [58] xfun_0.51             MplusAutomation_1.1.1 loo_2.8.0            \n#>  [61] withr_3.0.2           fastmap_1.2.0         boot_1.3-31          \n#>  [64] digest_0.6.37         mi_1.1                timechange_0.3.0     \n#>  [67] R6_2.6.1              mime_0.13             estimability_1.5.1   \n#>  [70] colorspace_2.1-1      gtools_3.9.5          jpeg_0.1-10          \n#>  [73] generics_0.1.3        data.table_1.17.0     corpcor_1.6.10       \n#>  [76] httr_1.4.7            htmlwidgets_1.6.4     pkgconfig_2.0.3      \n#>  [79] sem_3.1-16            gtable_0.3.6          bain_0.2.11          \n#>  [82] htmltools_0.5.8.1     carData_3.0-5         blavaan_0.5-8        \n#>  [85] png_0.1-8             reformulas_0.4.0      rstudioapi_0.17.1    \n#>  [88] tzdb_0.5.0            reshape2_1.4.4        curl_6.2.1           \n#>  [91] coda_0.19-4.1         checkmate_2.3.2       nlme_3.1-167         \n#>  [94] nloptr_2.2.1          zoo_1.8-13            parallel_4.4.2       \n#>  [97] miniUI_0.1.1.1        nonnest2_0.5-8        foreign_0.8-88       \n#> [100] pillar_1.10.1         grid_4.4.2            vctrs_0.6.5          \n#> [103] RANN_2.6.2            promises_1.3.2        car_3.1-3            \n#> [106] xtable_1.8-4          cluster_2.1.8.1       GPArotation_2024.3-1 \n#> [109] htmlTable_2.4.3       evaluate_1.0.3        pbivnorm_0.6.0       \n#> [112] gsubfn_0.7            mvtnorm_1.3-3         cli_3.6.4            \n#> [115] kutils_1.73           compiler_4.4.2        rlang_1.1.5          \n#> [118] rstantools_2.4.0      future.apply_1.11.3   ggsignif_0.6.4       \n#> [121] fdrtool_1.2.18        plyr_1.8.9            stringi_1.8.4        \n#> [124] rstan_2.32.7          pander_0.6.6          QuickJSR_1.6.0       \n#> [127] munsell_0.5.1         lisrelToR_0.3         CompQuadForm_1.4.3   \n#> [130] V8_6.0.2              pacman_0.5.1          Matrix_1.7-3         \n#> [133] hms_1.1.3             glasso_1.11           future_1.34.0        \n#> [136] shiny_1.10.0          rbibutils_2.3         igraph_2.1.4         \n#> [139] broom_1.0.7           RcppParallel_5.1.10\n```\n:::\n",
    "supporting": [
      "04_analisi_fattoriale_3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}