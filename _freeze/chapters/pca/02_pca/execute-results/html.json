{
  "hash": "e652a9deaeae529686d6bc9eae81c18a",
  "result": {
    "engine": "knitr",
    "markdown": "# Analisi delle componenti principali {#sec-pca}\n\n::: callout-note\n## In questo capitolo imparerai a\n\n- eseguire la PCA usando l'algebra lineare;\n- eseguire la PCA usando R.\n:::\n\n::: callout-tip\n## Prerequisiti\n\n- Leggere il capitolo *Factor Analysis and Principal Component Analysis* del testo di @petersen2024principles. \n:::\n\n::: callout-important\n## Preparazione del Notebook\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(readr)\n```\n:::\n\n\n\n\n:::\n\n\n## Introduzione\n\nL'**Analisi delle Componenti Principali (PCA)** √® una tecnica statistica utilizzata per **ridurre la dimensionalit√† di un insieme di dati**. Il suo obiettivo √® quello di semplificare analisi complesse **conservando quanta pi√π informazione possibile**, cio√® spiegando la maggior parte della **varianza** presente nei dati originali.\n\nLa PCA trasforma un insieme di variabili iniziali, spesso **correlate tra loro**, in un nuovo insieme di variabili **non correlate** dette **componenti principali**. Queste componenti sono **combinazioni lineari** delle variabili originali e sono ordinate in base alla quantit√† di varianza che riescono a spiegare.\n\n## Perch√© Usare la PCA in Psicologia?\n\nIn psicologia, ci troviamo spesso a lavorare con dati ad alta dimensionalit√†: questionari con decine di item, batterie di test, o set di dati raccolti tramite studi longitudinali. La PCA √® utile perch√©:\n\n- **semplifica l‚Äôinterpretazione dei dati**, riducendo molte variabili a poche dimensioni latenti;\n- **elimina ridondanze**: se due o pi√π variabili sono altamente correlate, la PCA pu√≤ rappresentarle con un'unica componente;\n- **favorisce la visualizzazione**, soprattutto nei casi in cui si riescano a ridurre i dati a 2 o 3 componenti principali;\n- **prepara i dati per analisi successive** come regressioni o modelli strutturali, evitando collinearit√† tra predittori.\n\n> üß© *Esempio tipico*: un questionario su tratti di personalit√† con 50 item pu√≤ essere ridotto a 5 componenti principali che riflettono le dimensioni dei Big Five.\n\n## Cos'√® la Varianza Totale?\n\nLa **varianza totale** rappresenta la somma della variabilit√† presente in ciascuna variabile del dataset.\n\nSe abbiamo tre variabili, la varianza totale sar√†:\n\n$$\n\\text{Varianza Totale} = \\sigma_1^2 + \\sigma_2^2 + \\sigma_3^2 ,\n$$\n\ndove $\\sigma_i^2$ √® la varianza della variabile $i$-esima.\n\nNella PCA:\n\n- ogni **autovalore** (o *eigenvalue*) rappresenta la quantit√† di varianza spiegata da una componente principale;\n- la **somma degli autovalori** √® pari alla varianza totale del dataset (dopo la standardizzazione, essa sar√† uguale al numero di variabili).\n\n\n## Un Nuovo Sistema di Coordinate\n\nLa PCA pu√≤ essere intesa come una **rotazione del sistema di riferimento** nello spazio delle variabili:\n\n1. la **prima componente principale (PC1)** √® la direzione lungo cui la varianza dei dati √® massima;\n2. la **seconda componente (PC2)** √® perpendicolare alla prima (cio√® ortogonale) e spiega la massima varianza residua;\n3. le successive componenti seguono lo stesso principio.\n\nQuesto nuovo sistema √® costruito in modo tale che le componenti siano **non correlate tra loro** (ortogonali) e spiegano, progressivamente, meno varianza.\n\n> üí° *Nota importante*: la PCA non elimina variabili, ma le **riorganizza** in modo da ridurre la complessit√† informativa.\n\n## Riduzione della Dimensionalit√†\n\nLa PCA consente di rappresentare i dati originali in un nuovo spazio, pi√π compatto, ma informativamente ricco.\n\n### Cosa significa \"ridurre la dimensionalit√†\"?\n\n- **Riduzione**: da *p* variabili iniziali (es. 30 item), possiamo ottenere *k* componenti principali (es. 3 o 5), dove *k < p*;\n- **Obiettivo**: mantenere una soglia prefissata di varianza spiegata, ad esempio il 70% o l‚Äô80%.\n\n> üéØ *Esempio concreto*: In uno studio sui Big Five, la PCA pu√≤ ridurre un set di 100 item a 5 componenti, ciascuna interpretabile come una delle dimensioni di personalit√†.\n\n\n## Interpretazione dei Risultati della PCA\n\nLa PCA produce due insiemi principali di risultati:\n\n1. **Punteggi delle componenti principali (scores)**  \nPer ogni partecipante (o unit√† osservata), si ottiene un punteggio per ciascuna componente. Questi punteggi possono essere utilizzati come **nuove variabili sintetiche**.\n\n2. **Varianza spiegata (eigenvalues)**  \nL‚Äôimportanza di ciascuna componente √® misurata dalla proporzione di varianza che essa spiega:\n\n$$\n\\text{Proporzione di varianza spiegata dalla PC}_i = \\frac{\\lambda_i}{\\sum_{j=1}^{p} \\lambda_j} ,\n$$\n\n  dove $\\lambda_i$ √® l'autovalore della componente *i*.\n\n> üîç *Interpretazione tipica*: Se la PC1 spiega il 50% della varianza e la PC2 un ulteriore 30%, possiamo dire che **le prime due componenti spiegano l'80% della variabilit√† totale**.\n\n\n## Geometria della PCA: Assi, Autovalori e Autovettori\n\nLa PCA si basa su concetti fondamentali dell‚Äôalgebra lineare:\n\n- **autovalori (eigenvalues)**: quantificano la varianza spiegata da ogni componente;\n- **autovettori (eigenvectors)**: definiscono le direzioni lungo cui si osserva la varianza massima nei dati.\n\nNel piano bidimensionale:\n\n- ogni autovettore √® un **asse di una nuova base ortogonale**;\n- la **lunghezza dell‚Äôasse** √® proporzionale alla **radice quadrata dell‚Äôautovalore** corrispondente;\n- le osservazioni vengono **proiettate ortogonalmente** su questi assi per ottenere i punteggi delle componenti principali.\n\n\n## Visualizzazione: Scree Plot e Biplot\n\n### Scree Plot\n\nLo **Scree Plot** √® un grafico che mostra gli autovalori ordinati per componente:\n\n- permette di **determinare quante componenti mantenere**;\n- un \"gomito\" nel grafico indica il punto oltre il quale le componenti aggiuntive spiegano poca varianza.\n\n### Biplot\n\nIl **biplot** mostra simultaneamente:\n\n- **i punteggi delle osservazioni** (punti);\n- **i contributi delle variabili originali** (frecce).\n\n> üéì *Guida all'interpretazione del biplot*:  \n> - **Correlazione positiva**: variabili con frecce vicine tra loro (angolo piccolo tra i vettori);  \n> - **Correlazione negativa**: variabili con frecce dirette in versi opposti (angolo ‚âà 180¬∞);  \n> - **Nessuna correlazione**: variabili con frecce quasi perpendicolari (angolo ‚âà 90¬∞).  \n> \n> *Ogni angolo riflette l'intensit√† della relazione tra le variabili analizzate.*\n\n\n## Tutorial in R \n\n### Obiettivo del tutorial\n\nApplicare la PCA passo dopo passo su un dataset simulato con due variabili correlate, comprendendo ogni fase del processo:\n\n- creazione dei dati e visualizzazione;\n- standardizzazione;\n- calcolo della PCA con algebra lineare;\n- interpretazione geometrica (autovalori, autovettori);\n- visualizzazioni: ellissi, componenti, scree plot;\n- proiezione dei dati nello spazio delle componenti;\n- verifica con `prcomp()`;\n- costruzione del biplot.\n\n### Passo 1: Creare un dataset\n\nGeneriamo un dataset con due variabili correlate.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Generiamo due variabili correlate\nset.seed(123)\nX <- data.frame(\n  x1 = rnorm(100, mean = 5, sd = 2),\n  x2 = rnorm(100, mean = 10, sd = 3)\n)\n\n# Aggiungiamo una correlazione lineare tra x1 e x2\nX$x2 <- 0.8 * X$x1 + rnorm(100, mean = 0, sd = 1)\n```\n:::\n\n\n\n\n\n### Passo 2: Standardizzare i dati\n\nLa standardizzazione √® essenziale nella PCA quando le variabili hanno scale diverse. Dopo la standardizzazione, ogni variabile ha media = 0 e deviazione standard = 1.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Standardizziamo le variabili\nX_scaled <- scale(X)\n```\n:::\n\n\n\n\n\nVisualizziamo i dati standardizzati:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(X_scaled, aes(x = x1, y = x2)) +\n  geom_point(shape = 19) +\n  coord_fixed(ratio = 1) + # Imposta l'aspect ratio a 1\n  labs(\n    x = \"x1 standardizzata\",\n    y = \"x2 standardizzata\",\n    title = \"Dati standardizzati (asp = 1)\"\n  )\n```\n\n::: {.cell-output-display}\n![](02_pca_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\nAggiungiamo un‚Äôellisse di confidenza per mostrare la distribuzione:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncar::dataEllipse(\n  X_scaled[, 1], X_scaled[, 2],\n  levels = 0.95, lty = 2,\n  asp = 1,\n  xlab = \"x1\", ylab = \"x2\"\n)\n```\n\n::: {.cell-output-display}\n![](02_pca_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n### Passo 3: Calcolare la matrice di covarianza\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncov_matrix <- cov(X_scaled)\ncov_matrix\n#>        x1     x2\n#> x1 1.0000 0.8177\n#> x2 0.8177 1.0000\n```\n:::\n\n\n\n\n\n### Passo 4: Calcolare autovalori e autovettori\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\neigen_decomp <- eigen(cov_matrix)\neigenvalues <- eigen_decomp$values       # Varianza spiegata (autovalori)\neigenvectors <- eigen_decomp$vectors     # Direzioni principali (autovettori)\n```\n:::\n\n\n\n\n\nStampiamo i risultati:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\neigenvalues\n#> [1] 1.8177 0.1823\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\neigenvectors\n#>        [,1]    [,2]\n#> [1,] 0.7071 -0.7071\n#> [2,] 0.7071  0.7071\n```\n:::\n\n\n\n\n\nVerifichiamo che gli autovettori siano ortogonali (prodotto scalare = 0):\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nt(eigenvectors[,1]) %*% eigenvectors[,2]\n#>           [,1]\n#> [1,] 2.237e-17\n```\n:::\n\n\n\n\n\n### Visualizzare le direzioni principali\n\nCalcoliamo le pendenze degli autovettori:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nev1_slope <- eigenvectors[2, 1] / eigenvectors[1, 1]\nev2_slope <- eigenvectors[2, 2] / eigenvectors[1, 2]\n```\n:::\n\n\n\n\n\nVisualizziamo il grafico con gli autovettori sovrapposti:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata.frame(zx = X_scaled[, 1], zy = X_scaled[, 2]) |>\n  ggplot(aes(x = zx, y = zy)) +\n  geom_point(size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  geom_abline(slope = ev1_slope, color = \"red\", linewidth = 1.2) +\n  geom_abline(slope = ev2_slope, color = \"blue\", linewidth = 1.2) +\n  ggtitle(\"Autovettori: direzioni delle componenti principali\") \n```\n\n::: {.cell-output-display}\n![](02_pca_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n### Scree Plot ‚Äì Percentuale di varianza spiegata\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Percentuale di varianza spiegata\nvar_per <- tibble(\n  PC = c(\"PC1\", \"PC2\"),\n  Percent = eigenvalues / sum(eigenvalues) * 100\n)\n\nggplot(var_per, aes(x = PC, y = Percent)) +\n  geom_col(fill = \"skyblue\", color = \"black\", width = 0.5) +\n  ylab(\"Varianza spiegata (%)\") +\n  xlab(\"Componente Principale\") +\n  ggtitle(\"Scree Plot\") \n```\n\n::: {.cell-output-display}\n![](02_pca_files/figure-html/unnamed-chunk-13-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\nVerifica: somma degli autovalori ‚âà somma delle varianze\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsum(eigenvalues)\n#> [1] 2\nsum(apply(X_scaled, 2, var))  # Varianza totale\n#> [1] 2\n```\n:::\n\n\n\n\n\n### Passo 5: Visualizzazione geometrica (ellisse + assi principali)\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncar::dataEllipse(\n  X_scaled[, 1], X_scaled[, 2],\n  levels = 0.95, lty = 2,\n  xlim = c(-3, 3), ylim = c(-3, 3),\n  asp = 1,\n  xlab = \"x1\", ylab = \"x2\"\n)\n\n# Disegniamo gli assi in base agli autovettori\nk <- 2.5\narrows(0, 0,\n       k * sqrt(eigenvalues[1]) * eigenvectors[1, 1],\n       k * sqrt(eigenvalues[1]) * eigenvectors[2, 1],\n       col = \"red\", lwd = 2, code = 2)\n\narrows(0, 0,\n       k * sqrt(eigenvalues[2]) * eigenvectors[1, 2],\n       k * sqrt(eigenvalues[2]) * eigenvectors[2, 2],\n       col = \"red\", lwd = 2, code = 2)\n```\n\n::: {.cell-output-display}\n![](02_pca_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n### Passo 6: Proiezione dei dati (calcolo dei punteggi)\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npc_scores <- as.matrix(X_scaled) %*% eigenvectors\ncolnames(pc_scores) <- c(\"PC1\", \"PC2\")\n\nhead(pc_scores)  # Mostra le prime osservazioni nel nuovo spazio\n#>           PC1     PC2\n#> [1,] -0.05606  0.9523\n#> [2,]  0.04512  0.5418\n#> [3,]  1.98607 -0.2887\n#> [4,]  0.15352  0.1844\n#> [5,] -0.17413 -0.2344\n#> [6,]  2.12408 -0.3930\n```\n:::\n\n\n\n\n\nVisualizziamo le osservazioni nello spazio delle componenti:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npc_df <- as.data.frame(pc_scores)\n\n# Grafico ggplot dei punteggi delle componenti principali\nggplot(pc_df, aes(x = PC1, y = PC2)) +\n  geom_point(color = \"blue\", size = 2) +\n  labs(\n    title = \"Punteggi delle Componenti Principali\",\n    x = \"PC1\",\n    y = \"PC2\"\n  ) +\n  coord_fixed()\n```\n\n::: {.cell-output-display}\n![](02_pca_files/figure-html/unnamed-chunk-17-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n### Passo 7: Confronto con `prcomp`\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calcolo automatico della PCA\npca_auto <- prcomp(X, scale. = TRUE)\nsummary(pca_auto)\n#> Importance of components:\n#>                          PC1    PC2\n#> Standard deviation     1.348 0.4270\n#> Proportion of Variance 0.909 0.0912\n#> Cumulative Proportion  0.909 1.0000\n```\n:::\n\n\n\n\n\nVerifica: confronta punteggi calcolati a mano con quelli di `prcomp`\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(pca_auto$x)  # Punteggi calcolati da prcomp\n#>           PC1     PC2\n#> [1,] -0.05606 -0.9523\n#> [2,]  0.04512 -0.5418\n#> [3,]  1.98607  0.2887\n#> [4,]  0.15352 -0.1844\n#> [5,] -0.17413  0.2344\n#> [6,]  2.12408  0.3930\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(pc_scores)   # Punteggi calcolati a mano\n#>           PC1     PC2\n#> [1,] -0.05606  0.9523\n#> [2,]  0.04512  0.5418\n#> [3,]  1.98607 -0.2887\n#> [4,]  0.15352  0.1844\n#> [5,] -0.17413 -0.2344\n#> [6,]  2.12408 -0.3930\n```\n:::\n\n\n\n\n\n### Passo 8: Biplot\n\nIl **biplot** permette di visualizzare contemporaneamente:\n\n- le **osservazioni** (rappresentate da numeri), proiettate nello spazio delle componenti principali;\n- le **variabili originali** (`x1`, `x2`), rappresentate come **frecce rosse**, che indicano come ciascuna variabile contribuisce alla definizione delle componenti.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbiplot(pca_auto, scale = 0,\n       main = \"Biplot delle Componenti Principali\",\n       xlab = \"PC1\", ylab = \"PC2\")\n```\n\n::: {.cell-output-display}\n![](02_pca_files/figure-html/unnamed-chunk-21-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n**Distribuzione delle osservazioni**\n\n- I numeri neri rappresentano le 100 osservazioni.\n- Le osservazioni si distribuiscono principalmente **lungo la direzione della prima componente principale (PC1)**, che si estende orizzontalmente.\n- C‚Äô√® **relativamente poca variabilit√† lungo la seconda componente (PC2)**, che √® verticale. Questo conferma che **quasi tutta la varianza** √® catturata da PC1, come osservato nei passaggi precedenti (Scree Plot, autovalori).\n\n**Frecce delle variabili originali**\n\nNel biplot vediamo due frecce:\n\n- la freccia **x1** punta verso l‚Äôalto a destra;\n- la freccia **x2** punta verso il basso a destra;\n- entrambe le frecce sono **allineate in parte con l'asse orizzontale (PC1)**, ma puntano in **direzioni opposte lungo PC2**.\n\nInterpretazione geometrica:\n\n- **Entrambe le variabili contribuiscono positivamente a PC1**, perch√© le componenti orizzontali delle frecce sono entrambe > 0.\n- La componente **verticale di x1 √® positiva**, quella di **x2 √® negativa**, quindi le due variabili sono **positivamente correlate in generale**, ma **divergono leggermente lungo PC2**.\n\nQuesto pattern √® coerente con i coefficienti degli autovettori:\n\n```r\npca_auto$rotation\n```\n\nEs.:\n\n```\n           PC1      PC2\nx1       0.71     0.71\nx2       0.71    -0.71\n```\n\nCosa significa?\n\n- La **PC1** √® la somma bilanciata di `x1` e `x2`, e rappresenta la **dimensione comune** tra le due variabili.\n- La **PC2** √® la loro differenza: rappresenta una direzione lungo cui `x1` e `x2` si muovono in modo opposto. Ma in questo caso, la varianza lungo PC2 √® molto piccola ‚Üí **questo secondo asse √® poco informativo**.\n\nLunghezza delle frecce:\n\n- le frecce hanno **lunghezza simile** ‚Üí entrambe le variabili sono **ben rappresentate** dallo spazio PC1‚ÄìPC2;\n- in un biplot, la lunghezza di una freccia indica **quanto bene quella variabile √® spiegata dalle componenti principali**.\n\n\n**Conclusioni sull'interpretazione del biplot.**\n\n- Il biplot mostra che la struttura dei dati pu√≤ essere **riassunta da una sola dimensione latente (PC1)**.\n- Le due variabili `x1` e `x2` sono **entrambe fortemente associate a PC1**, e **debolmente differenziate** da PC2.\n\n\n## Riflessioni Conclusive\n\nAbbiamo completato un'analisi PCA manuale e automatica, verificando ogni passaggio attraverso:\n\n- algebra lineare (autovalori/autovettori);\n- visualizzazione geometrica (ellisse, autovettori);\n- scree plot;\n- proiezione dei dati;\n- biplot.\n\nQuesta struttura permette di **collegare i concetti teorici alla loro implementazione pratica**, fornendo agli studenti una comprensione pi√π profonda della PCA.\n\n\n## ‚úçÔ∏è Suggerimenti per gli Studenti\n\n- Quando standardizzare? **Sempre**, se le variabili sono su scale diverse (es. punteggi da 0 a 10, da 1 a 100);\n- Quando usare la PCA? Quando **l‚Äôobiettivo √® descrittivo**, esplorare la struttura latente o semplificare l‚Äôanalisi;\n- Quando non usarla? Se le variabili **non sono correlate**: la PCA non sar√† utile;\n- Come interpretare le componenti? Serve **analizzare i coefficienti** degli autovettori per capire il contributo delle variabili originali a ciascuna componente.\n\n\n## Informazioni sull'Ambiente di Sviluppo {.unnumbered} \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.4.2 (2024-10-31)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.3.2\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] ggokabeito_0.1.0  see_0.11.0        MASS_7.3-65       viridis_0.6.5    \n#>  [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#>  [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#> [13] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#> [17] knitr_1.50        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#> [21] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#> [25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#> \n#> loaded via a namespace (and not attached):\n#>   [1] rstudioapi_0.17.1   jsonlite_1.9.1      magrittr_2.0.3     \n#>   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#>   [7] nloptr_2.2.1        rmarkdown_2.29      vctrs_0.6.5        \n#>  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#>  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#>  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#>  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#>  [22] mime_0.13           lifecycle_1.0.4     pkgconfig_2.0.3    \n#>  [25] Matrix_1.7-3        R6_2.6.1            fastmap_1.2.0      \n#>  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#>  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#>  [34] rprojroot_2.0.4     Hmisc_5.2-3         labeling_0.4.3     \n#>  [37] timechange_0.3.0    abind_1.4-8         compiler_4.4.2     \n#>  [40] withr_3.0.2         glasso_1.11         htmlTable_2.4.3    \n#>  [43] backports_1.5.0     carData_3.0-5       ggsignif_0.6.4     \n#>  [46] corpcor_1.6.10      gtools_3.9.5        tools_4.4.2        \n#>  [49] pbivnorm_0.6.0      foreign_0.8-88      zip_2.3.2          \n#>  [52] httpuv_1.6.15       nnet_7.3-20         glue_1.8.0         \n#>  [55] quadprog_1.5-8      nlme_3.1-167        promises_1.3.2     \n#>  [58] lisrelToR_0.3       grid_4.4.2          checkmate_2.3.2    \n#>  [61] cluster_2.1.8.1     reshape2_1.4.4      generics_0.1.3     \n#>  [64] gtable_0.3.6        tzdb_0.5.0          data.table_1.17.0  \n#>  [67] hms_1.1.3           car_3.1-3           sem_3.1-16         \n#>  [70] pillar_1.10.1       rockchalk_1.8.157   later_1.4.1        \n#>  [73] splines_4.4.2       lattice_0.22-6      survival_3.8-3     \n#>  [76] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n#>  [79] pbapply_1.7-2       reformulas_0.4.0    stats4_4.4.2       \n#>  [82] xfun_0.51           qgraph_1.9.8        arm_1.14-4         \n#>  [85] stringi_1.8.4       yaml_2.3.10         pacman_0.5.1       \n#>  [88] boot_1.3-31         evaluate_1.0.3      codetools_0.2-20   \n#>  [91] mi_1.1              cli_3.6.4           RcppParallel_5.1.10\n#>  [94] rpart_4.1.24        xtable_1.8-4        Rdpack_2.6.3       \n#>  [97] munsell_0.5.1       Rcpp_1.0.14         coda_0.19-4.1      \n#> [100] png_0.1-8           XML_3.99-0.18       parallel_4.4.2     \n#> [103] jpeg_0.1-10         lme4_1.1-36         mvtnorm_1.3-3      \n#> [106] openxlsx_4.2.8      rlang_1.1.5         multcomp_1.4-28    \n#> [109] mnormt_2.1.1\n```\n:::\n",
    "supporting": [
      "02_pca_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}