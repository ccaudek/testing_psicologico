{
  "hash": "3aae6a7a289be62e07b638f60a46926c",
  "result": {
    "engine": "knitr",
    "markdown": "# Analisi delle componenti principali\n\n::: callout-note\n## In questo capitolo imparerai a\n\n- eseguire la PCA usando l'algebra lineare;\n- eseguire la PCA usando R.\n:::\n\n::: callout-tip\n## Prerequisiti\n\n- Leggere il capitolo *Factor Analysis and Principal Component Analysis* del testo di @petersen2024principles. \n:::\n\n::: callout-important\n## Preparazione del Notebook\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(readr)\n```\n:::\n\n\n\n:::\n\n## Introduzione\n\nL'Analisi delle Componenti Principali (PCA) è una tecnica di riduzione dei dati che permette di semplificare analisi complesse riducendo un grande numero di variabili correlate a un insieme più piccolo di **componenti principali**. Queste componenti sono nuove variabili calcolate come combinazioni lineari delle variabili originali, progettate per spiegare la massima varianza possibile nei dati.\n\nIn psicologia, la PCA è ampiamente utilizzata per:\n\n- Ridurre il numero di variabili in studi con molti questionari o test psicometrici.\n- Identificare le dimensioni sottostanti a un set di item (ad esempio, esplorare le dimensioni latenti di una scala).\n- Preparare i dati per analisi successive (ad esempio, in regressioni o modelli strutturali).\n\n### Perché Usare la PCA in Psicologia?\n\nQuando un grande numero di variabili è fortemente correlato, può essere difficile interpretare i dati. In questi casi, la PCA permette di semplificare l'analisi mantenendo gran parte dell'informazione originale:\n\n- Le **componenti principali** catturano la varianza condivisa tra le variabili, fornendo un riepilogo efficace dei dati.\n- Se le prime componenti principali spiegano una quota sostanziale della **varianza totale**, possiamo ridurre il numero di variabili senza perdere significative informazioni.\n\n### Cos'è la Varianza Totale?\n\nLa varianza totale rappresenta la quantità complessiva di variabilità nei dati. Nella PCA, è definita come la somma delle varianze delle variabili originali. Ad esempio, se abbiamo un dataset con tre variabili, la varianza totale è:\n\n$$\n\\text{Varianza Totale} = \\sigma_1^2 + \\sigma_2^2 + \\sigma_3^2 ,\n$$\n\ndove $\\sigma_i^2$ è la varianza della variabile $i$-esima.\n\nNella PCA:\n\n- Gli **autovalori** (eigenvalues) rappresentano la varianza spiegata da ciascuna componente principale.\n- La somma degli autovalori corrisponde alla varianza totale dei dati:\n\n$$\n\\text{Somma degli autovalori} = \\text{Varianza Totale}\n$$\n\n### Un Nuovo Sistema di Coordinate\n\nLa PCA può essere interpretata come una **ridescrizione dei dati** in un nuovo sistema di assi coordinati. Questi nuovi assi (le componenti principali) sono calcolati come segue:\n\n1. Le componenti principali sono orientate lungo le direzioni di massima varianza nei dati.\n2. La **prima componente principale (PC1)** è la direzione che spiega la massima quantità di varianza.\n3. La **seconda componente principale (PC2)** è ortogonale alla prima e spiega la successiva maggiore quantità di varianza, e così via.\n\nQuesto significa che la PCA non elimina le variabili, ma le ricombina in modo tale da rappresentare i dati in un sistema più semplice e interpretabile.\n\n**Esempio Pratico.** Supponiamo di avere 10 variabili in un questionario psicologico, molte delle quali sono fortemente correlate. Con la PCA, potremmo scoprire che le prime due componenti principali spiegano l’80% della varianza totale. In questo caso, potremmo ridurre l’analisi a queste due componenti, semplificando notevolmente l’interpretazione.\n\n\n### Riduzione della Dimensionalità\n\nL’obiettivo principale della PCA è dunque quello di identificare il **minor numero di componenti** che spiegano la maggior parte della varianza nei dati. In psicologia, questo è particolarmente utile quando:\n\n- Si vuole ridurre il numero di variabili per facilitare l'interpretazione.\n- Si cerca di individuare dimensioni sottostanti (ad esempio, in uno studio sui tratti di personalità).\n\nAd esempio, in uno studio sui Big Five, la PCA potrebbe ridurre centinaia di item iniziali alle cinque dimensioni principali.\n\n### Interpretazione dei Risultati\n\nLa PCA produce due risultati principali:\n\n1. **Punteggi delle Componenti Principali**:\n   Ogni osservazione ottiene un punteggio per ciascuna componente principale, che rappresenta la sua posizione nel nuovo spazio.\n\n2. **Varianza Spiegata**:\n   La proporzione di varianza spiegata da ciascuna componente principale è un indicatore della sua importanza: $\\text{Varianza Spiegata per PC} = \\text{Autovalore della PC} / \\text{Somma degli autovalori}$. \n\nSe, ad esempio, la PC1 spiega il 60% della varianza e la PC2 il 20%, possiamo concludere che le prime due componenti rappresentano l’80% della variabilità nei dati.\n\nIn sintesi, la PCA è uno strumento potente per semplificare e interpretare dataset complessi in psicologia, soprattutto quando ci troviamo di fronte a molte variabili correlate. Questo metodo non solo facilita l'analisi, ma può anche fornire una nuova prospettiva sulle relazioni tra le variabili, evidenziando dimensioni latenti che altrimenti potrebbero non essere immediatamente evidenti.\n\n## Tutorial\n\nEsaminiamo qui di seguito l'analisi delle componenti principali passo passo.\n\n### Passo 1: Creare un dataset\n\nPer cominciare, generiamo un dataset di esempio per applicare la PCA.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Generare un dataset con due variabili correlate\nset.seed(123)\nX <- data.frame(\n  x1 = rnorm(100, mean = 5, sd = 2),\n  x2 = rnorm(100, mean = 10, sd = 3)\n)\nX$x2 <- 0.8 * X$x1 + rnorm(100, mean = 0, sd = 1)  # Introduciamo correlazione\n```\n:::\n\n\n\n\n### Passo 2: Standardizzare i dati\n\nPrima di calcolare la PCA, è importante standardizzare le variabili (sottrarre la media e dividere per la deviazione standard) per garantire che abbiano lo stesso peso.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Centrare e scalare le variabili\nX_scaled <- scale(X)\n\n# Plot con aspect ratio 1\nplot(X_scaled[, 1], X_scaled[, 2], asp = 1, \n     col = \"blue\", pch = 19, \n     main = \"Dati standardizzati con aspect ratio = 1\",\n     xlab = \"Variabile x1 standardizzata\",\n     ylab = \"Variabile x2 standardizzata\")\n```\n\n::: {.cell-output-display}\n![](02_pca_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncar::dataEllipse(\n  X_scaled[, 1], X_scaled[, 2],\n  levels = 0.95,\n  lty = 2,\n  ylim = c(-3, 3),\n  xlim = c(-3, 3),\n  asp = 1, \n)\n```\n\n::: {.cell-output-display}\n![](02_pca_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n### Passo 3: Calcolare la matrice di covarianza\n\nLa PCA utilizza la matrice di covarianza per calcolare le componenti principali.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncov_matrix <- cov(X_scaled)\nprint(cov_matrix)\n#>       x1    x2\n#> x1 1.000 0.818\n#> x2 0.818 1.000\n```\n:::\n\n\n\n\n### Passo 4: Calcolare autovalori e autovettori\n\nUtilizziamo l'algebra lineare per ottenere gli autovalori e gli autovettori della matrice di covarianza.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\neigen_decomp <- eigen(cov_matrix)\neigenvalues <- eigen_decomp$values       # Autovalori\neigenvectors <- eigen_decomp$vectors     # Autovettori\nprint(eigenvalues)\n#> [1] 1.818 0.182\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprint(eigenvectors)\n#>       [,1]   [,2]\n#> [1,] 0.707 -0.707\n#> [2,] 0.707  0.707\n```\n:::\n\n\n\n\nGli autovalori rappresentano la varianza spiegata dalle componenti principali, mentre gli autovettori indicano le direzioni delle componenti principali.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# First eigenvector \nev_1 <- eigen_decomp$vectors[, 1]\n\n# Slope of the first eigenvector\nev1_m <- ev_1[2] / ev_1[1]\n\n# Second eigenvector \nev_2 <- eigen_decomp$vectors[, 2]\n\n# Slope of the second eigenvector\nev2_m <- ev_2[2] / ev_2[1]\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Scatter plot showing the span of both eigenvectors \ndata.frame(zx=X_scaled[, 1], zy= X_scaled[, 2])  |>\nggplot(aes(x = zx, y = zy)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  geom_abline(slope = ev1_m, color = \"blue\", linewidth = 0.7) +\n  geom_abline(slope = ev2_m, color = \"red\", linewidth = 0.7) \n```\n\n::: {.cell-output-display}\n![](02_pca_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\nGli autovettori sono ortogonali:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprint(ev_1 %*% ev_2)\n#>          [,1]\n#> [1,] 2.24e-17\n```\n:::\n\n\n\n\nGeneriamo uno Scree Plot.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calculate the estimated variance for each eigenvalue\ne_var <- eigen_decomp$values / (length(X_scaled[, 1]) - 1)\n\n# Data frame with variance percentages\nvar_per <- tibble(\n  PC  = c(\"PC1\", \"PC2\"),\n  PER = c(e_var) * 100 / sum(e_var) # Calculate the percentage\n    )\n\n# Scree plot \nggplot(var_per, aes(x = PC, y = PER)) +\n  geom_col(width = 0.5, color = \"black\") +\n  xlab(\"Principal component\") +\n  ylab(\"Percentage of variation (%)\") \n```\n\n::: {.cell-output-display}\n![](02_pca_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\nVerifichiamo che la somma degli autovalori sia uguale alla varianza totale.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvar(X_scaled[, 1]) + var(X_scaled[, 2])\n#> [1] 2\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\neigen_decomp$values |> sum()\n#> [1] 2\n```\n:::\n\n\n\n\nGli autovettori ottenuti utilizzando la funzione `eigen()` sono normalizzati. Ciò significa che la loro lunghezza è uguale a 1:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nt(as.matrix(eigen_decomp$vectors[, 1])) %*% \n  as.matrix(eigen_decomp$vectors[, 1]) \n#>      [,1]\n#> [1,]    1\n```\n:::\n\n\n\n\nUtilizziamo le informazioni degli autovettori e degli autovalori per disegnare gli assi dell'ellisse: \n\n- gli autovettori determinano la direzione degli assi; \n- la radice quadrata degli autovalori è proporzionale alla lunghezza degli assi dell'ellisse.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncar::dataEllipse(\n  X_scaled[, 1], X_scaled[, 2],\n  levels = 0.95,\n  lty = 2,\n  ylim = c(-3, 3),\n  xlim = c(-3, 3), \n  asp = 1\n)\nk <- 2.5\narrows(\n  0, 0, \n  k * sqrt(eigen_decomp$values[1]) * eigen_decomp$vectors[1],\n  k * sqrt(eigen_decomp$values[1]) * eigen_decomp$vectors[2],\n  code = 2, \n  col = \"red\", \n  lwd = 2\n)\narrows(\n  0, 0, \n  k * sqrt(eigen_decomp$values[2]) * eigen_decomp$vectors[1],\n  k * sqrt(eigen_decomp$values[2]) * -eigen_decomp$vectors[2],\n  code = 2, \n  col = \"red\", \n  lwd = 2\n)\n```\n\n::: {.cell-output-display}\n![](02_pca_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\nTale analisi si può estendere a qualunque numero di variabili. Per\nesempio, nel caso di tre variabili, possiamo pensare di disegnare un\nellisoide attorno ad una nube di punti nello spazio tridimensionale.\nAnche in questo caso, gli autovalori e gli associati autovettori\ncorrisponderanno agli assi dell'elissoide.\n\n### Passo 5: Proiettare i dati sulle componenti principali \n\nPer calcolare i punteggi delle Componenti Principali, dobbiamo **proiettare ortogonalmente** i punti originali del dataset sulle nuove coordinate, definite dalle direzioni principali (autovettori). Questo processo ci permette di rappresentare ogni osservazione nello spazio delle componenti principali.\n\nNell'algebra lineare, la **proiezione ortogonale** consiste nel trovare la posizione di un punto su una retta o un piano, in modo che il vettore risultante sia **perpendicolare** alla direzione di proiezione. \n\nNel contesto della PCA:\n\n1. Gli **autovettori** rappresentano le direzioni principali (componenti principali) lungo cui la varianza dei dati è massimizzata.\n2. Proiettare un punto sui componenti principali significa calcolare la sua posizione lungo queste nuove direzioni.\n\n#### Formulazione Matematica\n\nConsideriamo le seguenti matrici:\n\n- **$\\mathbf{X}_{\\text{scaled}}$**: la matrice dei dati standardizzati, in cui ogni riga rappresenta un'osservazione e ogni colonna una variabile.\n- **$\\mathbf{V}$**: la matrice degli autovettori, le cui colonne rappresentano le nuove direzioni principali.\n\nLa proiezione dei dati nello spazio delle componenti principali si calcola come:\n\n$$\n\\mathbf{Z} = \\mathbf{X}_{\\text{scaled}} \\cdot \\mathbf{V}\n$$\n\ndove:\n\n- **$\\mathbf{Z}$** è la matrice dei **punteggi delle componenti principali**.\n- Ogni riga di $\\mathbf{Z}$ rappresenta un'osservazione trasformata nello spazio delle componenti principali.\n- Ogni colonna di $\\mathbf{Z}$ corrisponde a una componente principale (ad esempio, PC1, PC2).\n\n#### Implementazione in R\n\nIn R, questo calcolo può essere realizzato attraverso il prodotto matrice-matrice. Ecco il codice per calcolare i punteggi delle componenti principali:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calcolo dei punteggi delle componenti principali\npc_scores <- as.matrix(X_scaled) %*% eigenvectors\ncolnames(pc_scores) <- c(\"PC1\", \"PC2\")  # Etichettare le componenti principali\n```\n:::\n\n\n\n\nPer verificare i risultati, possiamo visualizzare i primi punteggi calcolati:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Stampare i primi punteggi delle componenti principali\nprint(head(pc_scores))\n#>          PC1    PC2\n#> [1,] -0.0561  0.952\n#> [2,]  0.0451  0.542\n#> [3,]  1.9861 -0.289\n#> [4,]  0.1535  0.184\n#> [5,] -0.1741 -0.234\n#> [6,]  2.1241 -0.393\n```\n:::\n\n\n\n\n#### Interpretazione dei Punteggi\n\nOgni valore in `pc_scores` rappresenta la posizione dell'osservazione nello spazio trasformato delle componenti principali:\n\n- La **PC1** è la direzione lungo cui si osserva la massima varianza dei dati.\n- La **PC2** è la direzione ortogonale successiva con la seconda massima varianza, e così via.\n\n## Passo 6: Confrontare con l'output di `prcomp`\n\nUtilizziamo la funzione prcomp di R per confermare i risultati.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npca <- prcomp(X, scale. = TRUE)\nprint(pca)\n#> Standard deviations (1, .., p=2):\n#> [1] 1.348 0.427\n#> \n#> Rotation (n x k) = (2 x 2):\n#>      PC1    PC2\n#> x1 0.707  0.707\n#> x2 0.707 -0.707\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Confronto tra i punteggi calcolati manualmente e quelli di prcomp\nprint(head(pca$x))\n#>          PC1    PC2\n#> [1,] -0.0561 -0.952\n#> [2,]  0.0451 -0.542\n#> [3,]  1.9861  0.289\n#> [4,]  0.1535 -0.184\n#> [5,] -0.1741  0.234\n#> [6,]  2.1241  0.393\n```\n:::\n\n\n\n\n### Passo 7: Visualizzare la proiezione dei dati\n\nPossiamo visualizzare i punti originali proiettati sulle componenti principali.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Grafico del dataset originale\nplot(\n  X_scaled, \n  col = \"blue\", pch = 19, \n  main = \"Dati originali e componenti principali\",\n  asp = 1\n)\nabline(0, eigenvectors[2,1] / eigenvectors[1,1], col = \"red\", lwd = 2)  \n# Prima componente\nabline(0, eigenvectors[2,2] / eigenvectors[1,2], col = \"green\", lwd = 2)  \n# Seconda componente\n```\n\n::: {.cell-output-display}\n![](02_pca_files/figure-html/unnamed-chunk-20-1.png){fig-align='center' width=70%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Grafico delle componenti principali\nplot(\n  pc_scores, \n  col = \"blue\", pch = 19, \n  main = \"Punteggi delle componenti principali\",\n  asp = 1)\n```\n\n::: {.cell-output-display}\n![](02_pca_files/figure-html/unnamed-chunk-21-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n## Biplot\n\nIl **biplot** è uno strumento grafico che combina la visualizzazione dei punteggi delle componenti principali e delle variabili originali in un unico grafico. Questo permette di:\n\n- Interpretare la relazione tra le variabili originali.\n- Visualizzare come le osservazioni (campioni) si distribuiscono nello spazio delle componenti principali.\n- Identificare cluster, outlier, o pattern nei dati.\n\nUn biplot combina due tipi di informazioni:\n\n1. **I punteggi delle componenti principali** (proiezioni delle osservazioni sulle componenti principali), rappresentati come punti.\n2. **I carichi delle variabili originali** sulle componenti principali (autovettori), rappresentati come frecce.\n\nLe frecce indicano:\n\n- La direzione della variabilità spiegata da ciascuna variabile.\n- La correlazione tra le variabili e le componenti principali.\n\n### Come Creare un Biplot in R\n\nPer creare un biplot in R possiamo utilizzare `prcomp`. Supponiamo di avere già calcolato la PCA con la funzione `prcomp`:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# PCA con prcomp\npca <- prcomp(X, scale. = TRUE)\n```\n:::\n\n\n\n\nIl biplot si visualizza nel modo seguente.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Creare un biplot\nbiplot(\n  pca, scale = 0, \n  main = \"Biplot delle Componenti Principali\", \n  xlab = \"PC1\", ylab = \"PC2\"\n)\n```\n\n::: {.cell-output-display}\n![](02_pca_files/figure-html/unnamed-chunk-23-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n- **`scale = 0`**: Evita di ridimensionare le frecce e i punteggi per semplificare l'interpretazione.\n\nNel grafico:\n\n- **I punti** rappresentano le osservazioni, proiettate sulle componenti principali.\n- **Le frecce** rappresentano le variabili originali, con:\n  - La lunghezza della freccia che indica la forza della correlazione con le componenti principali.\n  - L’angolo tra due frecce che rappresenta la correlazione tra le due variabili:\n    - Un angolo piccolo indica una correlazione positiva.\n    - Un angolo di 90° indica una correlazione nulla.\n    - Un angolo ampio (vicino a 180°) indica una correlazione negativa.\n\n### Interpretazione\n\nIn psicologia, il biplot è particolarmente utile per:\n\n1. **Identificare pattern nei dati**: Ad esempio, come i partecipanti si distribuiscono lungo dimensioni psicologiche latenti (es. tratti di personalità).\n2. **Esaminare le relazioni tra variabili**: Le frecce possono evidenziare cluster di variabili correlate che rappresentano dimensioni psicologiche (es. ansia, stress, depressione).\n3. **Valutare l'adeguatezza della PCA**: Se le frecce delle variabili sono lunghe e ben distribuite lungo le componenti principali, ciò suggerisce che la PCA sta spiegando bene la varianza delle variabili.\n\nIn sostanza, il biplot è uno strumento grafico che semplifica l'interpretazione della PCA. Combina in un unico diagramma sia le informazioni sulle variabili originali che sulla loro proiezione nello spazio delle componenti principali, offrendo una visione d'insieme chiara e immediata dei dati.\n\n## Informazioni sull'Ambiente di Sviluppo {.unnumbered} \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.4.2 (2024-10-31)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.3.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] ggokabeito_0.1.0  see_0.10.0        MASS_7.3-65       viridis_0.6.5    \n#>  [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#>  [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#> [13] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#> [17] knitr_1.49        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#> [21] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#> [25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#> \n#> loaded via a namespace (and not attached):\n#>   [1] rstudioapi_0.17.1   jsonlite_1.9.0      magrittr_2.0.3     \n#>   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#>   [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n#>  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#>  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#>  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#>  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#>  [22] mime_0.12           lifecycle_1.0.4     pkgconfig_2.0.3    \n#>  [25] Matrix_1.7-2        R6_2.6.1            fastmap_1.2.0      \n#>  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#>  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#>  [34] rprojroot_2.0.4     Hmisc_5.2-2         labeling_0.4.3     \n#>  [37] timechange_0.3.0    abind_1.4-8         compiler_4.4.2     \n#>  [40] withr_3.0.2         glasso_1.11         htmlTable_2.4.3    \n#>  [43] backports_1.5.0     carData_3.0-5       ggsignif_0.6.4     \n#>  [46] corpcor_1.6.10      gtools_3.9.5        tools_4.4.2        \n#>  [49] pbivnorm_0.6.0      foreign_0.8-88      zip_2.3.2          \n#>  [52] httpuv_1.6.15       nnet_7.3-20         glue_1.8.0         \n#>  [55] quadprog_1.5-8      nlme_3.1-167        promises_1.3.2     \n#>  [58] lisrelToR_0.3       grid_4.4.2          checkmate_2.3.2    \n#>  [61] cluster_2.1.8       reshape2_1.4.4      generics_0.1.3     \n#>  [64] gtable_0.3.6        tzdb_0.4.0          data.table_1.17.0  \n#>  [67] hms_1.1.3           car_3.1-3           sem_3.1-16         \n#>  [70] pillar_1.10.1       rockchalk_1.8.157   later_1.4.1        \n#>  [73] splines_4.4.2       lattice_0.22-6      survival_3.8-3     \n#>  [76] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n#>  [79] pbapply_1.7-2       reformulas_0.4.0    stats4_4.4.2       \n#>  [82] xfun_0.51           qgraph_1.9.8        arm_1.14-4         \n#>  [85] stringi_1.8.4       yaml_2.3.10         pacman_0.5.1       \n#>  [88] boot_1.3-31         evaluate_1.0.3      codetools_0.2-20   \n#>  [91] mi_1.1              cli_3.6.4           RcppParallel_5.1.10\n#>  [94] rpart_4.1.24        xtable_1.8-4        Rdpack_2.6.2       \n#>  [97] munsell_0.5.1       Rcpp_1.0.14         coda_0.19-4.1      \n#> [100] png_0.1-8           XML_3.99-0.18       parallel_4.4.2     \n#> [103] jpeg_0.1-10         lme4_1.1-36         mvtnorm_1.3-3      \n#> [106] openxlsx_4.2.8      rlang_1.1.5         multcomp_1.4-28    \n#> [109] mnormt_2.1.1\n```\n:::\n\n\n\n\n## Bibliografia {.unnumbered}\n",
    "supporting": [
      "02_pca_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}