{
  "hash": "4cc318afc78b0143a0a577afa8b7c774",
  "result": {
    "engine": "knitr",
    "markdown": "# Elementi di algebra lineare\n\n::: callout-note\n## In questo capitolo imparerai a\n\n- comprendere e applicare i concetti fondamentali di algebra lineare necessari per l'analisi e l'assessment psicologico.\n:::\n\n::: callout-important\n## Preparazione del Notebook\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\n```\n:::\n\n\n\n\n:::\n\n## Introduzione \n\nQuesto capitolo presenta alcune nozioni di base dell'algebra lineare, una branca della matematica essenziale per la comprensione e l'analisi dei modelli di regressione lineare. \n\n## Rappresentazione dei Vettori\n\nNell'algebra lineare, un vettore, che rappresenta una lista ordinata di scalari, è solitamente indicato con una lettera minuscola in grassetto, come $\\mathbf{v}$. Gli elementi di un vettore sono generalmente indicati con un indice, ad esempio $\\mathbf{v}_1$ si riferisce al primo elemento del vettore $\\mathbf{v}$.\n\nUn vettore $\\mathbf{v}$ di $n$ elementi può essere rappresentato sia come una colonna che come una riga, a seconda della convenzione scelta. Ad esempio, un vettore colonna di $n$ elementi è scritto come:\n\n$$\n\\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix},\n$$\n\nmentre un vettore riga appare come:\n\n$$\n\\mathbf{v} = \\begin{bmatrix} v_1 & v_2 & \\cdots & v_n \\end{bmatrix}.\n$$\n\nQuesta notazione consente di visualizzare chiaramente i singoli elementi del vettore e di riferirsi a ciascuno di essi in modo specifico.\n\nUna lista di $n$ scalari organizzata in un vettore $\\mathbf{v}$ è chiamata \"dimensione\" del vettore. Formalmente, si esprime come $\\mathbf{v} \\in \\mathbb{R}^n$, indicando che il vettore $\\mathbf{v}$ appartiene all'insieme di tutti i vettori reali di dimensione $n$.\n\n\n## Visualizzazione Geometrica dei Vettori\n\nI vettori possono essere rappresentati come frecce in uno spazio $n$-dimensionale, con l'origine come punto di partenza e la punta della freccia che corrisponde alle coordinate specificate dal vettore. La norma $L_2$ (o lunghezza) di un vettore, denotata come $\\|\\mathbf{v}\\|$, rappresenta la distanza euclidea dall'origine alla punta del vettore.\n\nPer un vettore $\\mathbf{v} = [v_1, v_2, \\ldots, v_n]$, la norma è definita come:\n\n$$\n\\|\\mathbf{v}\\| = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}.\n$$\n\n### Esempio Numerico\n\nConsideriamo un vettore in uno spazio bidimensionale, ad esempio $\\mathbf{v} = [3, 4]$. Geometricamente, questo vettore parte dall'origine $(0, 0)$ e termina nel punto $(3, 4)$ del piano cartesiano.\n\nPer calcolare la norma $L_2$ di questo vettore, applichiamo la formula:\n\n$$\n\\|\\mathbf{v}\\| = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5.\n$$\n\nQuindi, la norma del vettore $\\mathbf{v} = [3, 4]$ è 5, che rappresenta la lunghezza della freccia dal punto di origine $(0, 0)$ al punto $(3, 4)$ nello spazio bidimensionale.\n\n### Rappresentazione Geometrica\n\n```\ny\n^\n|       * (3, 4)\n|      /\n|     /\n|    /\n|   /\n|  /\n| / \n|/____________> x\n(0, 0)\n```\n\nIn questo diagramma, il punto `*` rappresenta la fine del vettore $\\mathbf{v}$ e la linea inclinata mostra il vettore stesso che parte dall'origine. L'altezza della linea fino al punto `(3, 4)` rappresenta visivamente la norma del vettore, che è la distanza di 5 unità dall'origine.\n\nQuesto esempio illustra chiaramente la relazione tra la rappresentazione numerica di un vettore e la sua interpretazione geometrica, facilitando la comprensione della lunghezza del vettore e della sua direzione nello spazio bidimensionale.\n\nSebbene noi siamo principalmente limitati a ragionare su spazi bidimensionali (2D) e tridimensionali (3D), i dati che raccogliamo spesso risiedono in spazi di dimensioni superiori. L'algebra lineare permette di ragionare e sviluppare intuizioni su vettori e spazi di dimensioni molto più elevate, superando i limiti della visualizzazione diretta.\n\n## Operazioni di Base sui Vettori\n\n### 1. Moltiplicazione di un Vettore per uno Scalare\n\nLa moltiplicazione di un vettore per uno scalare produce un nuovo vettore. Questa operazione può essere interpretata come una \"scalatura\" del vettore nello spazio: il vettore risultante mantiene la stessa direzione dell'originale, ma la sua lunghezza viene modificata in base allo scalare.\n\nSe $\\mathbf{v} = [v_1, v_2, \\ldots, v_n]$ è un vettore e $c$ è uno scalare, la moltiplicazione del vettore per lo scalare è data da:\n\n$$\nc\\mathbf{v} = [cv_1, cv_2, \\ldots, cv_n]\n$$\n\n### 2. Addizione di Vettori\n\nÈ possibile sommare due vettori della stessa dimensione. La somma vettoriale si ottiene sommando gli elementi corrispondenti di ciascun vettore.\n\nSe $\\mathbf{u} = [u_1, u_2, \\ldots, u_n]$ e $\\mathbf{v} = [v_1, v_2, \\ldots, v_n]$ sono due vettori di dimensione $n$, la loro somma è:\n\n$$\n\\mathbf{u} + \\mathbf{v} = [u_1 + v_1, u_2 + v_2, \\ldots, u_n + v_n]\n$$\n\n### 3. Prodotto Scalare (o Prodotto Interno)\n\nIl prodotto scalare tra due vettori della stessa dimensione è uno scalare che fornisce informazioni sull'angolo tra i vettori nello spazio. Formalmente, il prodotto scalare di $\\mathbf{u} = [u_1, u_2, \\ldots, u_n]$ e $\\mathbf{v} = [v_1, v_2, \\ldots, v_n]$ è definito come:\n\n$$\n\\mathbf{u} \\cdot \\mathbf{v} = u_1v_1 + u_2v_2 + \\cdots + u_nv_n\n$$\n\nQuesto prodotto scalare può anche essere espresso in termini dell'angolo $\\theta$ tra i vettori:\n\n$$\n\\mathbf{u} \\cdot \\mathbf{v} = \\|\\mathbf{u}\\| \\|\\mathbf{v}\\| \\cos(\\theta)\n$$\n\nSe due vettori sono ortogonali, ovvero formano un angolo di $90^\\circ$ tra loro, il loro prodotto scalare è zero: $\\mathbf{u} \\cdot \\mathbf{v} = 0$.\n\n### 4. Prodotto Scalare di un Vettore con Se Stesso\n\nIl prodotto scalare di un vettore con se stesso fornisce il quadrato della sua lunghezza. Se $\\mathbf{v} = [v_1, v_2, \\ldots, v_n]$, allora:\n\n$$\n\\mathbf{v} \\cdot \\mathbf{v} = v_1^2 + v_2^2 + \\cdots + v_n^2 = \\|\\mathbf{v}\\|^2\n$$\n\nQueste operazioni di base sui vettori sono fondamentali per molte applicazioni in matematica, fisica, informatica e altre scienze, fornendo una struttura potente per analizzare e risolvere problemi in spazi multidimensionali.\n\n### Vettori in R\n\nIn R, possiamo creare un vettore con tre elementi usando la funzione `c()`:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Creazione di un vettore\nv <- c(1, 2, 3)\nv\n#> [1] 1 2 3\n```\n:::\n\n\n\n\n\nIn questo esempio, `v` è un vettore con tre elementi: 1, 2 e 3.\n\nPer eseguire il prodotto tra un vettore e uno scalare, possiamo semplicemente moltiplicare il vettore per lo scalare. Questo moltiplica ogni elemento del vettore per lo scalare:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Scalari e vettori\na <- 5\n\n# Prodotto vettore-scalare\nva <- v * a\nva\n#> [1]  5 10 15\n```\n:::\n\n\n\n\n\nIl risultato sarà `[5, 10, 15]`.\n\nIl prodotto interno (o prodotto scalare) tra due vettori si può calcolare con la funzione `sum()` per ottenere la somma dei prodotti degli elementi corrispondenti:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Un altro vettore\nv2 <- c(4, 5, 6)\n\n# Prodotto interno\nprodotto_interno <- sum(v * v2)\nprodotto_interno\n#> [1] 32\n```\n:::\n\n\n\n\n\nIl risultato sarà `32`, dato che il prodotto interno è calcolato come $1*4 + 2*5 + 3*6 = 32$.\n\nIn alternativa, si può utilizzare la funzione `crossprod()` che calcola il prodotto interno in modo efficiente:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Prodotto interno con crossprod\nprodotto_interno2 <- crossprod(v, v2)\nprodotto_interno2\n#>      [,1]\n#> [1,]   32\n```\n:::\n\n\n\n\n\nLa funzione `crossprod()` restituisce una matrice $1 \\times 1$, quindi il risultato sarà simile.\n\nIl prodotto esterno tra due vettori produce una matrice dove ogni elemento è il prodotto degli elementi corrispondenti dei due vettori. In R, possiamo usare la funzione `outer()`:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Prodotto esterno\nprodotto_esterno <- outer(v, v2)\nprodotto_esterno\n#>      [,1] [,2] [,3]\n#> [1,]    4    5    6\n#> [2,]    8   10   12\n#> [3,]   12   15   18\n```\n:::\n\n\n\n\n\nIl risultato sarà una matrice in cui ogni elemento è il prodotto dei corrispondenti elementi dei vettori `v` e `v2`.\n\n## Matrici\n\nUna matrice è una struttura matematica bidimensionale costituita da elementi disposti in righe e colonne. Formalmente, una matrice $\\mathbf{A}$ di dimensioni $m \\times n$ (si legge \"m per n\") è un array rettangolare di numeri reali o complessi, denotato come:\n\n$$ \\mathbf{A} = (a_{ij})_{m \\times n} = \\begin{bmatrix} \na_{11} & a_{12} & \\cdots & a_{1n} \\\\ \na_{21} & a_{22} & \\cdots & a_{2n} \\\\ \n\\vdots & \\vdots & \\ddots & \\vdots \\\\ \na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix} $$\n\ndove $a_{ij}$ rappresenta l'elemento nella $i$-esima riga e $j$-esima colonna della matrice.\n\nLe matrici sono comunemente indicate con lettere maiuscole in grassetto, come $\\mathbf{A}$, $\\mathbf{B}$, $\\mathbf{C}$, etc. Una matrice con $m$ righe e $n$ colonne si dice di ordine $m \\times n$.\n\nIn molte matrici di dati, ogni elemento $a_{ij}$ è uno scalare che rappresenta il valore della $j$-esima variabile del $i$-esimo campione. Formalmente, possiamo indicare $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, il che significa che la matrice $\\mathbf{A}$ ha $m$ righe e $n$ colonne. Si dice comunemente che la \"dimensione\" di $\\mathbf{A}$ è $m \\times n$.\n\n### Matrici come Collezioni di Vettori Colonna\n\nLe matrici possono essere interpretate come collezioni di vettori colonna. Ad esempio, una matrice di dati può essere rappresentata come:\n\n$$\n\\mathbf{A} = \\begin{bmatrix} \\mathbf{a}_1 & \\mathbf{a}_2 & \\cdots & \\mathbf{a}_n \\end{bmatrix}\n$$\n\nIn questo caso, $\\mathbf{A}$ è composta da una sequenza di $n$ vettori colonna $\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n$, ciascuno dei quali è un vettore di dimensione $m$. Più precisamente, ogni vettore colonna $\\mathbf{a}_j$ rappresenta i dati di tutti i campioni per la $j$-esima variabile o feature.\n\n### Matrici come Collezioni di Vettori Riga\n\nIn alternativa, una matrice può essere vista come una collezione di vettori riga. In questo contesto, ogni riga di $\\mathbf{A}$ rappresenta tutte le variabili misurate per un dato campione:\n\n$$\n\\mathbf{A} = \\begin{bmatrix} \n\\mathbf{a}_1^T \\\\ \n\\mathbf{a}_2^T \\\\ \n\\vdots \\\\ \n\\mathbf{a}_m^T \n\\end{bmatrix}\n$$\n\nQui, la matrice $\\mathbf{A}$ è composta da $m$ vettori riga, denotati come $\\mathbf{a}_i^T$. Ognuno di questi vettori riga $\\mathbf{a}_i^T$ è di dimensione $n$, indicando che ciascun campione ha $n$ variabili o feature associate. \n\n### Trasposta di una Matrice\n\nIl simbolo $T$ rappresenta la trasposta di una matrice. La trasposta di una matrice, denotata con un apice $T$ (es. $\\mathbf{A}^T$), è un'operazione che trasforma ciascuna delle righe di $\\mathbf{A}$ in colonne di $\\mathbf{A}^T$. In altre parole, se $\\mathbf{A}$ ha dimensione $m \\times n$, allora $\\mathbf{A}^T$ avrà dimensione $n \\times m$:\n\n$$\n\\mathbf{A}^T = \\begin{bmatrix} \na_{11} & a_{21} & \\cdots & a_{m1} \\\\ \na_{12} & a_{22} & \\cdots & a_{m2} \\\\ \n\\vdots & \\vdots & \\ddots & \\vdots \\\\ \na_{1n} & a_{2n} & \\cdots & a_{mn} \n\\end{bmatrix}\n$$\n\nCon la trasposta, le variabili misurate diventano colonne e i campioni diventano righe. Essenzialmente, i vettori riga sono le trasposte dei vettori colonna. Questo concetto è molto utile in algebra lineare, poiché permette di passare facilmente da una rappresentazione dei dati a un'altra.\n\n\n### Matrici in R\n\nIn R, una matrice può essere creata utilizzando la funzione `matrix()`. Per esempio, possiamo creare una matrice 3x4 fornendo un vettore di elementi e specificando il numero di righe e colonne.\n\nEcco come definire una matrice 3x4:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Definizione della matrice 3x4\nM <- matrix(c(\n    1, 2, 3, 4, \n    5, 6, 7, 8, \n    9, 10, 11, 12\n    ), \n    nrow = 3, ncol = 4, byrow = TRUE)\n\nprint(\"Matrice originale:\")\n#> [1] \"Matrice originale:\"\nprint(M)\n#>      [,1] [,2] [,3] [,4]\n#> [1,]    1    2    3    4\n#> [2,]    5    6    7    8\n#> [3,]    9   10   11   12\n```\n:::\n\n\n\n\n\nQui, `byrow = TRUE` indica che i dati vengono inseriti riga per riga. Se si utilizza `byrow = FALSE`, i dati vengono inseriti colonna per colonna.\n\nIn R, puoi calcolare la trasposta di una matrice utilizzando la funzione `t()`:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calcolo della trasposta\ntrasposta <- t(M)\ntrasposta\n#>      [,1] [,2] [,3]\n#> [1,]    1    5    9\n#> [2,]    2    6   10\n#> [3,]    3    7   11\n#> [4,]    4    8   12\n```\n:::\n\n\n\n\n\n## Moltiplicazione tra Matrici\n\nLa moltiplicazione tra matrici è un'operazione fondamentale nell'algebra lineare. Per poter moltiplicare due matrici, è necessario che siano **conformabili**, il che significa che il numero di colonne della prima matrice deve essere uguale al numero di righe della seconda matrice.\n\nSe abbiamo una matrice $\\mathbf{A}$ di dimensioni $m \\times n$ (cioè, $m$ righe e $n$ colonne) e una matrice $\\mathbf{B}$ di dimensioni $n \\times p$ (cioè, $n$ righe e $p$ colonne), allora il prodotto delle due matrici $\\mathbf{A} \\mathbf{B}$ sarà una matrice $\\mathbf{C}$ di dimensioni $m \\times p$.\n\nIl prodotto tra due matrici $\\mathbf{A}$ e $\\mathbf{B}$ si ottiene calcolando il prodotto interno tra le righe della prima matrice e le colonne della seconda matrice.\n\nPer ciascun elemento $c_{ij}$ della matrice risultante $\\mathbf{C}$, si esegue il seguente calcolo:\n\n$$\nc_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj}.\n$$\n\nQuesto significa che l'elemento $c_{ij}$ è il risultato del prodotto interno tra la $i$-esima riga della matrice $\\mathbf{A}$ e la $j$-esima colonna della matrice $\\mathbf{B}$.\n\nLa moltiplicazione di una matrice per un vettore è un caso particolare della moltiplicazione tra matrici, dove il vettore può essere visto come una matrice con una delle dimensioni uguale a 1.\n\nSe $\\mathbf{A}$ è una matrice $m \\times n$ e $\\mathbf{x}$ è un vettore di dimensione $n$ (cioè una matrice di dimensione $n \\times 1$), allora il prodotto $\\mathbf{A} \\mathbf{x}$ è un vettore di dimensione $m$. Ogni elemento del vettore risultante è il prodotto interno tra una riga della matrice $\\mathbf{A}$ e il vettore $\\mathbf{x}$.\n\nConsideriamo le seguenti matrici:\n\n$$\n\\mathbf{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}, \\quad \\mathbf{B} = \\begin{bmatrix}\n7 & 8 \\\\\n9 & 10 \\\\\n11 & 12\n\\end{bmatrix}\n$$\n\n- $\\mathbf{A}$ è una matrice $2 \\times 3$.\n- $\\mathbf{B}$ è una matrice $3 \\times 2$.\n\nIl prodotto $\\mathbf{A} \\mathbf{B}$ è una matrice $2 \\times 2$ calcolata come segue:\n\n$$\n\\mathbf{C} = \\mathbf{A} \\mathbf{B} = \\begin{bmatrix}\n(1 \\cdot 7 + 2 \\cdot 9 + 3 \\cdot 11) & (1 \\cdot 8 + 2 \\cdot 10 + 3 \\cdot 12) \\\\\n(4 \\cdot 7 + 5 \\cdot 9 + 6 \\cdot 11) & (4 \\cdot 8 + 5 \\cdot 10 + 6 \\cdot 12)\n\\end{bmatrix}\n$$\n\nCalcolando ogni elemento:\n\n$$\n\\mathbf{C} = \\begin{bmatrix}\n58 & 64 \\\\\n139 & 154\n\\end{bmatrix}\n$$\n\nIn questo esempio, ogni elemento della matrice risultante $\\mathbf{C}$ è stato ottenuto calcolando il prodotto interno tra le righe di $\\mathbf{A}$ e le colonne di $\\mathbf{B}$.\n\n\n### Calcoli con Matrici in R\n\nIn R, il prodotto tra matrici può essere calcolato utilizzando l'operatore `%*%`.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Definizione della matrice A (2x3)\nA <- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3, byrow = TRUE)\n\n# Definizione della matrice B (3x2)\nB <- matrix(c(7, 8, 9, 10, 11, 12), nrow = 3, ncol = 2, byrow = TRUE)\n\n# Calcolo del prodotto A * B\nprodotto_AB <- A %*% B\nprint(\"Prodotto A * B usando l'operatore %*%:\")\n#> [1] \"Prodotto A * B usando l'operatore %*%:\"\nprodotto_AB\n#>      [,1] [,2]\n#> [1,]   58   64\n#> [2,]  139  154\n```\n:::\n\n\n\n\n\nIn R, `%*%` è l'operatore per il prodotto matriciale.\n\n### Matrice Identità e Matrice Inversa \n\n### Matrice Identità\n\nLa matrice identità, denotata come $\\mathbf{I}_n$, è una matrice quadrata di dimensione $n \\times n$ con tutti gli elementi sulla diagonale principale uguali a 1 e tutti gli altri elementi uguali a 0. Ad esempio, una matrice identità 3x3 è:\n\n$$\n\\mathbf{I}_3 = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n$$\n\nIn generale, una matrice identità di dimensione $n \\times n$ è:\n\n$$\n\\mathbf{I}_n = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}\n$$\n\nLa matrice identità ha la proprietà fondamentale di essere l'elemento neutro per la moltiplicazione matriciale. Per qualsiasi matrice $\\mathbf{A}$ di dimensioni $n \\times n$:\n\n$$\n\\mathbf{A} \\mathbf{I}_n = \\mathbf{A} \\quad \\text{e} \\quad \\mathbf{I}_n \\mathbf{A} = \\mathbf{A}.\n$$\n\nIn R, puoi creare una matrice identità utilizzando la funzione `diag()`:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Creazione della matrice identità 3x3\nI <- diag(3)\n\nprint(\"Matrice identità 3x3:\")\n#> [1] \"Matrice identità 3x3:\"\nprint(I)\n#>      [,1] [,2] [,3]\n#> [1,]    1    0    0\n#> [2,]    0    1    0\n#> [3,]    0    0    1\n```\n:::\n\n\n\n\n\nLa funzione `diag(3)` crea una matrice identità 3x3 con 1 lungo la diagonale principale e 0 altrove.\n\n### Determinante di una Matrice\n\nIl **determinante** è un numero associato a una matrice quadrata che fornisce informazioni essenziali sulle proprietà della matrice stessa. È uno scalare che può indicare se una matrice è invertibile, se un sistema di equazioni lineari ha una soluzione unica, e molto altro. \n\nIl determinante di una matrice può essere interpretato in diversi modi:\n\n1. In termini geometrici, il determinante di una matrice $2 \\times 2$ o $3 \\times 3$ rappresenta rispettivamente l'area o il volume del parallelogramma o del parallelepipedo definito dai vettori delle righe (o colonne) della matrice. Un determinante pari a zero indica che i vettori sono linearmente dipendenti e che l'area o il volume è nullo, suggerendo che la matrice non ha un'inversa.\n\n2. Algebraicamente, il determinante di una matrice quadrata può dirci se la matrice è **invertibile**. Se il determinante è diverso da zero, la matrice è invertibile, cioè esiste una matrice inversa tale che il prodotto delle due sia la matrice identità. Se il determinante è zero, la matrice non è invertibile.\n\n3. Nel contesto dei sistemi di equazioni lineari, se il determinante del coefficiente della matrice associata a un sistema è zero, il sistema può non avere soluzioni o avere un numero infinito di soluzioni. Se è diverso da zero, il sistema ha una soluzione unica.\n\n#### Calcolo del Determinante per una Matrice 2x2\n\nPer una matrice $2 \\times 2$:\n\n$$\n\\mathbf{A} = \\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n$$\n\nil determinante è calcolato come:\n\n$$\n\\det(\\mathbf{A}) = ad - bc\n$$\n\nQuesto semplice calcolo deriva dalla differenza tra il prodotto degli elementi della diagonale principale (dall'angolo superiore sinistro all'angolo inferiore destro) e il prodotto degli elementi della diagonale secondaria (dall'angolo superiore destro all'angolo inferiore sinistro).\n\n#### Calcolo del Determinante per Matrici di Dimensioni Superiori\n\nPer matrici di dimensioni superiori a $2 \\times 2$, il calcolo del determinante diventa più complesso. Un metodo comune per calcolare il determinante di matrici più grandi è l'*espansione di Laplace* o *espansione per cofattori*. Questo metodo si basa sulla ricorsione, calcolando il determinante attraverso una somma pesata di determinanti di matrici più piccole (minori) che si ottengono eliminando una riga e una colonna dalla matrice originale.\n\n### Calcolo del Determinante in R\n\nPer calcolare il determinante di una matrice quadrata in R, puoi usare la funzione `det()`. Questa funzione funziona per matrici quadrate di qualsiasi dimensione.\n\n#### Esempio con una matrice $2 \\times 2$\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Definizione di una matrice 2x2\nA <- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2, byrow = TRUE)\n\n# Calcolo del determinante\ndeterminante_A <- det(A)\ndeterminante_A\n#> [1] -2\n```\n:::\n\n\n\n\n\n#### Esempio con una matrice $3 \\times 3$\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Definizione di una matrice 3x3\nB <- matrix(c(6, 1, 1, 4, -2, 5, 2, 8, 7), nrow = 3, ncol = 3, byrow = TRUE)\n\n# Calcolo del determinante\ndeterminante_B <- det(B)\ndeterminante_B\n#> [1] -306\n```\n:::\n\n\n\n\n\nIn R, come in Python, il determinante è uno strumento fondamentale per comprendere le proprietà di una matrice. Può essere utilizzato per determinare:\n\n- **Invertibilità**: Se il determinante è $0$, la matrice non è invertibile.\n- **Trasformazioni geometriche**: Il valore del determinante descrive il fattore di scala della trasformazione rappresentata dalla matrice.\n- **Sistemi lineari**: Il determinante aiuta a identificare la singolarità dei sistemi di equazioni.\n\n### Inversa di una Matrice\n\nL'inversa di una matrice quadrata $\\mathbf{A}$, denotata come $\\mathbf{A}^{-1}$, è una matrice che, moltiplicata per $\\mathbf{A}$, restituisce la matrice identità $\\mathbf{I}_n$. L'inversa di una matrice esiste solo per matrici quadrate *non singolari*, ovvero matrici il cui determinante è diverso da zero.\n\nLa proprietà fondamentale dell'inversa è:\n\n$$\n\\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{I}_n \\quad \\text{e} \\quad \\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_n.\n$$\n\ndove $\\mathbf{I}_n$ è la matrice identità di dimensione $n \\times n$.\n\n#### Esempio: Calcolo dell'Inversa di una Matrice $2 \\times 2$\n\nPer una matrice $2 \\times 2$:\n\n$$\n\\mathbf{A} = \\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n$$\n\nl'inversa, se esiste, è data dalla formula:\n\n$$\n\\mathbf{A}^{-1} = \\frac{1}{ad-bc} \\begin{bmatrix}\nd & -b \\\\\n-c & a\n\\end{bmatrix}\n$$\n\ndove $ad-bc$ è il determinante della matrice $\\mathbf{A}$. L'inversa esiste solo se questo determinante è diverso da zero (cioè, se $\\mathbf{A}$ è non singolare).\n\n### Utilizzo dell'Inversa di una Matrice\n\nL'inversa di una matrice è particolarmente utile per risolvere sistemi di equazioni lineari. Ad esempio, consideriamo un sistema rappresentato in forma matriciale come $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$, dove $\\mathbf{A}$ è la matrice dei coefficienti, $\\mathbf{x}$ è il vettore delle variabili incognite e $\\mathbf{b}$ è il vettore dei termini noti.\n\nSe $\\mathbf{A}$ è una matrice invertibile, possiamo risolvere per $\\mathbf{x}$ moltiplicando entrambi i lati dell'equazione per $\\mathbf{A}^{-1}$:\n\n$$\n\\mathbf{A}^{-1} \\mathbf{A} \\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}.\n$$\n\nPoiché $\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}$, otteniamo:\n\n$$\n\\mathbf{I} \\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b},\n$$\n\n$$\n\\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}.\n$$\n\nQuesta proprietà è utile anche per altre applicazioni, come nella derivazione della formula per i coefficienti della regressione lineare.\n\nEcco come calcolare l'inversa di una matrice in R utilizzando la funzione `solve()`:\n\n---\n\n### Calcolo dell'Inversa in R\n\nIn R, possiamo calcolare l'inversa di una matrice quadrata (se invertibile) utilizzando la funzione `solve()`. È importante verificare che la matrice abbia un determinante diverso da zero, altrimenti non è invertibile.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Definizione di una matrice 2x2\nA <- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2, byrow = TRUE)\n\n# Calcolo dell'inversa\nA_inv <- solve(A)\n\ncat(\"Inversa di A:\\n\")\n#> Inversa di A:\nprint(A_inv)\n#>      [,1] [,2]\n#> [1,] -2.0  1.0\n#> [2,]  1.5 -0.5\n```\n:::\n\n\n\n\n\nPer verificare che l'inversa sia stata calcolata correttamente, possiamo moltiplicare la matrice originale $\\mathbf{A}$ per la sua inversa $\\mathbf{A}^{-1}$ e verificare che il risultato sia la matrice identità:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Prodotto di A e A_inv\nidentita <- A %*% A_inv\n\ncat(\"Prodotto di A e A_inv (matrice identità):\\n\")\n#> Prodotto di A e A_inv (matrice identità):\nprint(identita)\n#>      [,1]     [,2]\n#> [1,]    1 1.11e-16\n#> [2,]    0 1.00e+00\n```\n:::\n\n\n\n\n\nIn conclusione, l'inversa di una matrice è uno strumento potente e utile per diverse applicazioni, come:\n\n- **Risoluzione di sistemi di equazioni lineari**\n- **Trasformazioni geometriche**\n- **Analisi di modelli lineari**\n\nIn R, `solve()` rende semplice e veloce il calcolo dell'inversa, a patto che la matrice sia:\n\n1. **Quadrata**: Deve avere lo stesso numero di righe e colonne.\n2. **Invertibile**: Il determinante della matrice deve essere diverso da zero, altrimenti `solve()` restituirà un errore.\n\n## Regressione Lineare e Stima dei Coefficienti\n\nLa regressione lineare è una tecnica statistica utilizzata per modellare la relazione tra una variabile dipendente (o risposta) e una o più variabili indipendenti (o predittori). È possibile rappresentare questo modello in termini di algebra matriciale per semplificare il calcolo dei coefficienti.\n\n### Regressione Lineare Semplice\n\nLa regressione lineare semplice descrive una relazione lineare tra una variabile indipendente $x$ e una variabile dipendente $y$. Quando abbiamo un campione di $n$ osservazioni, il modello assume la seguente forma:\n\n$$\ny_i = \\beta_0 + \\beta_1 x_i + e_i, \\quad \\text{per} \\; i = 1, 2, \\ldots, n,\n$$\n\ndove:\n\n- $y_i$ è il valore osservato della variabile dipendente per l'osservazione $i$,\n- $\\beta_0$ è l'intercetta, che rappresenta il valore di $y$ quando $x = 0$,\n- $\\beta_1$ è il coefficiente di regressione, che indica quanto varia $y$ per una variazione unitaria di $x$,\n- $x_i$ è il valore della variabile indipendente per l'osservazione $i$,\n- $e_i$ è l'errore o residuo per l'osservazione $i$, rappresenta la differenza tra il valore osservato $y_i$ e il valore previsto $\\hat{y}_i = \\beta_0 + \\beta_1 x_i$.\n\nPer un campione di $n$ osservazioni, possiamo rappresentare la regressione lineare in forma matriciale, che rende il modello più compatto e facilita i calcoli statistici. La rappresentazione matriciale del modello di regressione lineare è:\n\n$$\n\\mathbf{y} = \\mathbf{Xb} + \\mathbf{e},\n$$\n\ndove:\n\n- $\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$ è il vettore delle osservazioni della variabile dipendente,\n- $\\mathbf{X} = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix}$ è la matrice di design, in cui la prima colonna è costituita da 1 per includere l'intercetta $\\beta_0$,\n- $\\mathbf{b} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}$ è il vettore dei coefficienti del modello,\n- $\\mathbf{e} = \\begin{bmatrix} e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{bmatrix}$ è il vettore degli errori o residui.\n\nQuesta forma matriciale sintetizza tutte le $n$ equazioni del modello di regressione lineare semplice in un'unica espressione compatta, che rappresenta la relazione tra le osservazioni della variabile dipendente $y$ e le corrispondenti osservazioni della variabile indipendente $x$, tenendo conto degli errori di previsione.\n\n### Regressione Lineare Multipla\n\nLa regressione lineare multipla estende la regressione lineare semplice includendo più variabili indipendenti, consentendo di modellare la relazione tra una variabile dipendente e diverse variabili indipendenti. Il modello di regressione lineare multipla per un campione di $n$ osservazioni con $p$ variabili indipendenti può essere scritto come:\n\n$$\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip} + e_i, \\quad \\text{per} \\; i = 1, 2, \\ldots, n,\n$$\n\ndove:\n\n- $y_i$ è il valore osservato della variabile dipendente per l'osservazione $i$,\n- $\\beta_0$ è l'intercetta del modello,\n- $\\beta_1, \\beta_2, \\ldots, \\beta_p$ sono i coefficienti di regressione associati alle variabili indipendenti,\n- $x_{i1}, x_{i2}, \\ldots, x_{ip}$ sono i valori delle variabili indipendenti per l'osservazione $i$,\n- $e_i$ è l'errore o residuo per l'osservazione $i$, che rappresenta la differenza tra il valore osservato $y_i$ e il valore previsto $\\hat{y}_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}$.\n\nIn termini matriciali, il modello di regressione lineare multipla può essere scritto come:\n\n$$\n\\mathbf{y} = \\mathbf{Xb} + \\mathbf{e},\n$$\n\ndove:\n\n- $\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$ è il vettore delle osservazioni della variabile dipendente, di dimensione $n \\times 1$,\n- $\\mathbf{X} = \\begin{bmatrix} 1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\ 1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & x_{n2} & \\cdots & x_{np} \\end{bmatrix}$ è la matrice di design, di dimensione $n \\times (p+1)$, dove la prima colonna è composta da 1 per includere l'intercetta $\\beta_0$,\n- $\\mathbf{b} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}$ è il vettore dei coefficienti, di dimensione $(p+1) \\times 1$,\n- $\\mathbf{e} = \\begin{bmatrix} e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{bmatrix}$ è il vettore degli errori o residui, di dimensione $n \\times 1$.\n\nL'equazione in forma matriciale esplicita per il campione di $n$ osservazioni con $p$ variabili indipendenti è:\n\n$$\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix} +\n\\begin{bmatrix}\ne_1 \\\\\ne_2 \\\\\n\\vdots \\\\\ne_n\n\\end{bmatrix}.\n$$\n\nIn questa rappresentazione:\n\n- Il prodotto $\\mathbf{Xb}$ rappresenta i valori previsti (o stimati) del modello come combinazione lineare delle colonne della matrice di design $\\mathbf{X}$, ponderata dai coefficienti $\\mathbf{b}$.\n- Il vettore $\\mathbf{e}$ rappresenta gli errori o residui, che sono le differenze tra i valori osservati $\\mathbf{y}$ e i valori previsti $\\mathbf{Xb}$.\n\nQuesta forma compatta e ordinata consente un'efficiente analisi statistica e facilita i calcoli necessari per stimare i coefficienti del modello di regressione [@caudek2001statistica].\n\n## Stima dei Coefficienti con il Metodo dei Minimi Quadrati\n\nPer ogni osservazione $i$, l'errore (o residuo) è definito come la differenza tra il valore osservato $y_i$ e il valore predetto $\\hat{y}_i$ dal modello:\n\n$$\ne_i = y_i - \\hat{y}_i,\n$$\n\ndove:\n\n- $y_i$ è il valore osservato dell'output per l'osservazione $i$,\n- $\\hat{y}_i$ è il valore predetto dal modello per l'osservazione $i$.\n\nIn forma matriciale, possiamo rappresentare l'errore per tutte le $n$ osservazioni come segue:\n\n$$\n\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}},\n$$\n\ndove:\n\n- $\\mathbf{e} = \\begin{bmatrix} e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{bmatrix}$ è il vettore degli errori o residui,\n- $\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$ è il vettore delle osservazioni della variabile dipendente,\n- $\\hat{\\mathbf{y}} = \\begin{bmatrix} \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_n \\end{bmatrix} = \\mathbf{Xb}$ è il vettore dei valori predetti dal modello.\n\nL'equazione matriciale esplicita per il vettore degli errori $\\mathbf{e}$ è quindi:\n\n$$\n\\mathbf{e} = \\mathbf{y} - \\mathbf{Xb}.\n$$\n\nQuesta equazione mostra che il vettore degli errori $\\mathbf{e}$ è la differenza tra il vettore delle osservazioni $\\mathbf{y}$ e il vettore dei valori predetti $\\hat{\\mathbf{y}} = \\mathbf{Xb}$. In altre parole, ogni elemento $e_i$ del vettore degli errori rappresenta la differenza tra il valore osservato $y_i$ e il valore predetto $\\hat{y}_i$ per l'osservazione $i$. \n\nL'obiettivo della regressione lineare è minimizzare la somma degli errori quadrati ($SSE$, Sum of Squared Errors) per tutte le osservazioni. Questa somma è data da:\n\n$$\n\\text{SSE} = \\sum_{i=1}^m e_i^2 = \\sum_{i=1}^m (y_i - \\hat{y}_i)^2.\n$$\n\nUtilizzando la notazione matriciale, possiamo esprimere la somma degli errori quadrati come:\n\n$$\n\\text{SSE} = \\mathbf{e}^T \\mathbf{e} = (\\mathbf{y} - \\mathbf{X} \\mathbf{b})^T (\\mathbf{y} - \\mathbf{X} \\mathbf{b}).\n$$\n\nIl problema di ottimizzazione per minimizzare la somma degli errori quadrati si traduce in:\n\n$$\n\\min_{\\mathbf{b}} (\\mathbf{y} - \\mathbf{X} \\mathbf{b})^T (\\mathbf{y} - \\mathbf{X} \\mathbf{b}),\n$$\n\ndove:\n\n- $\\mathbf{b}$ è il vettore dei coefficienti da stimare.\n- $\\mathbf{X}$ è la matrice di design che include tutte le osservazioni delle variabili indipendenti.\n- $\\mathbf{y}$ è il vettore delle osservazioni della variabile dipendente.\n\nPer trovare i coefficienti ottimali $\\mathbf{b}$, calcoliamo la derivata parziale dell'errore quadratico totale rispetto a $\\mathbf{b}$ e la impostiamo a zero:\n\n$$\n\\frac{\\partial}{\\partial \\mathbf{b}} (\\mathbf{y} - \\mathbf{X} \\mathbf{b})^T (\\mathbf{y} - \\mathbf{X} \\mathbf{b}) = -2 \\mathbf{X}^T (\\mathbf{y} - \\mathbf{X} \\mathbf{b}).\n$$\n\nImpostando questa derivata uguale a zero, otteniamo:\n\n$$\n-2 \\mathbf{X}^T (\\mathbf{y} - \\mathbf{X} \\mathbf{b}) = 0.\n$$\n\nSemplificando, possiamo riscrivere l'equazione come:\n\n$$\n\\mathbf{X}^T \\mathbf{y} = \\mathbf{X}^T \\mathbf{X} \\mathbf{b}.\n$$\n\nAssumendo che la matrice $\\mathbf{X}^T \\mathbf{X}$ sia invertibile, risolviamo per $\\mathbf{b}$:\n\n$$\n\\mathbf{b} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}.\n$$\n\nPer gli scopi presenti, non è necessario comprendere la derivazione formale in dettaglio. Tuttavia, possiamo fare un parallelo con il metodo dei minimi quadrati per il caso univariato per ottenere un'intuizione geometrica su cosa stiamo facendo.\n\nNel caso della regressione lineare semplice (univariata), minimizzare la somma degli errori quadrati significa trovare la retta che meglio si adatta ai dati in uno spazio bidimensionale (2D). Dal punto di vista geometrico, questo processo equivale a calcolare la derivata della funzione di errore rispetto ai coefficienti della retta, quindi impostando la derivata a zero per trovare il punto in cui la pendenza della tangente è piatta. In altre parole, cerchiamo il punto in cui la pendenza della funzione di errore è zero, che corrisponde a un minimo della funzione.\n\nNel caso della regressione lineare multipla, invece di lavorare in uno spazio bidimensionale, stiamo operando in uno spazio multidimensionale. Ogni dimensione aggiuntiva rappresenta una variabile indipendente (regressore) nel nostro modello. Quando prendiamo la derivata dell'errore quadratico totale rispetto ai coefficienti $\\mathbf{b}$ e la impostiamo a zero, stiamo essenzialmente cercando il punto in questo spazio multidimensionale in cui tutte le \"pendenze\" (derivate parziali) sono zero. Questo punto rappresenta il minimo dell'errore quadratico totale e corrisponde alla migliore stima dei coefficienti del nostro modello di regressione lineare, minimizzando l'errore di previsione su tutti i dati.\n\nQuindi, mentre nel caso univariato minimizzare l'errore quadratico trova la migliore linea retta che si adatta ai dati in 2D, nel caso multivariato troviamo il miglior piano o iperpiano che si adatta ai dati in uno spazio di dimensioni superiori.\n\n### Stima dei Coefficienti OLS\n\nQuesta formula:\n\n$$\n\\mathbf{b} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n$$\n\nè conosciuta come *stima dei minimi quadrati ordinari* (Ordinary Least Squares, OLS) per i coefficienti della regressione lineare multivariata. Essa fornisce i valori dei coefficienti $\\mathbf{b}$ che minimizzano la somma degli errori quadrati e, quindi, rappresenta la migliore approssimazione lineare dei dati osservati.\n\n\n### Simulazione di una Regressione Lineare Semplice in R\n\n#### Definizione dei dati e calcolo dei coefficienti dei minimi quadrati\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Scegli valori per i coefficienti veri\nb <- c(3.4, 12.35)  # Intercetta e pendenza\n\n# Simula n punti dati\nset.seed(123)  # Per riproducibilità\nn <- 30\ndata_mean <- 0\ndata_sd <- 1\ndata <- rnorm(n, mean = data_mean, sd = data_sd)  # Variabile indipendente\n\n# Aggiungi una colonna di 1s per la matrice di design\nx <- cbind(1, data)  # Matrice di design\n\n# Aggiungi rumore gaussiano\nnoise_mean <- 0\nnoise_sd <- 5\ne <- rnorm(n, mean = noise_mean, sd = noise_sd)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Simula i valori di y\ny <- x %*% b + e\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calcola i coefficienti stimati (minimi quadrati)\nb_hat <- solve(t(x) %*% x) %*% t(x) %*% y\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncat(\"Valori veri di b:\\n\")\n#> Valori veri di b:\nprint(b)\n#> [1]  3.40 12.35\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncat(\"Stima di b:\\n\")\n#> Stima di b:\nprint(b_hat)\n#>       [,1]\n#>       4.26\n#> data 11.68\n```\n:::\n\n\n\n\n\n#### Calcolo dei valori predetti e del coefficiente di determinazione ($R^2$)\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Valori predetti\ny_hat <- x %*% b_hat\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calcola R^2\nSS_res <- sum((y - y_hat)^2)  # Somma dei residui al quadrato\nSS_tot <- sum((y - mean(y))^2)  # Somma totale dei quadrati\nr2 <- 1 - (SS_res / SS_tot)\n\ncat(\"Coefficiente di determinazione (R^2):\", r2, \"\\n\")\n#> Coefficiente di determinazione (R^2): 0.8853\n```\n:::\n\n\n\n\n\n#### Rappresentazione Grafica dei Dati e della Regressione\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Costruiamo un data.frame\ndf <- data.frame(x = data, y = y)\n\n# Data frame per le due linee\nlines_df <- data.frame(\n  intercept = c(b[1], b_hat[1]),\n  slope     = c(b[2], b_hat[2]),\n  tipo      = c(\"Valori veri\", \"Stima\")\n)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_abline(data = lines_df,\n              aes(intercept = intercept, slope = slope, \n                  color = tipo, linetype = tipo),\n              linewidth = 1) +\n  scale_color_manual(values = c(\"Valori veri\" = \"black\", \"Stima\" = \"red\")) +\n  scale_linetype_manual(values = c(\"Valori veri\" = \"dashed\", \"Stima\" = \"solid\")) +\n  labs(\n    title = \"Regressione Lineare Semplice\",\n    x = \"x\",\n    y = \"y\",\n    color = \"Linea\",\n    linetype = \"Linea\"\n  ) +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    legend.position = \"bottom\",                 \n    legend.box = \"horizontal\",                 \n    legend.title = element_text(face = \"bold\")\n  )\n```\n\n::: {.cell-output-display}\n![](01_linear_algebra_files/figure-html/unnamed-chunk-22-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\n#### Analisi con il Pacchetto `lm` \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Modello di regressione con lm()\nlm_model <- lm(y ~ data)\nsummary(lm_model)\n#> \n#> Call:\n#> lm(formula = y ~ data)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#>  -8.04  -2.53  -1.08   3.47  10.06 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)    4.260      0.767    5.55  6.1e-06\n#> data          11.680      0.794   14.70  1.1e-14\n#> \n#> Residual standard error: 4.2 on 28 degrees of freedom\n#> Multiple R-squared:  0.885,\tAdjusted R-squared:  0.881 \n#> F-statistic:  216 on 1 and 28 DF,  p-value: 1.07e-14\n```\n:::\n\n\n\n\n\n- La matrice `x` è la matrice di design, che include una colonna di 1 per l'intercetta.\n- La funzione `solve()` calcola i coefficienti dei minimi quadrati usando l'equazione $(X'X)^{-1}X'Y$.\n- La somma dei quadrati residui ($SS_{res}$) e la somma totale dei quadrati ($SS_{tot}$) sono calcolate manualmente per derivare $R^2$.\n- La funzione `lm()` offre un modo alternativo e diretto per ottenere il modello di regressione e i relativi output statistici.\n\n### Traccia di una matrice\n\nSi definisce *traccia* di una matrice quadrata $\\boldsymbol{A}$\n$n \\times n$, e si denota con $tr(\\boldsymbol{A})$ la somma degli\nelementi sulla diagonale principale di $\\boldsymbol{A}$:\n\n$$\ntr(\\boldsymbol{A}) = \\sum_{i=1}^{n} a_{ii}.\n$$\n\nLa traccia gode delle seguenti proprietà: \n\n$$\n\\begin{aligned}\n&tr(\\rho \\boldsymbol{A}) = \\rho tr( \\boldsymbol{A}) \\notag \\\\\n&tr(\\boldsymbol{A} + \\boldsymbol{B}) =  tr( \\boldsymbol{A})+tr( \\boldsymbol{B}) \\notag \\\\\n&tr(\\boldsymbol{A}') =  tr( \\boldsymbol{A}) \\notag \\\\\n&tr(\\boldsymbol{AB}) =  tr( \\boldsymbol{BA}) \\notag\\end{aligned}\n$$ \n\nPer esempio, sia \n\n$$\n\\boldsymbol{A} =  \\left[ \\begin{array}{c c c}\n7 & 1 & 2\\\\\n1 & 8 & 3\\\\\n2 & 3 & 9 \\end{array} \\right]\n$$ \n\nallora\n\n$$\ntr(\\boldsymbol{A}) = 7 + 8 + 9 = 24.\n$$\n\n\n\n\n\n::: {.cell layout-align=\"center\" vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\nA <- matrix(\n  c(7,1, 2, 1, 8, 3, 2, 3, 9),\n  nrow = 3,\n  byrow = TRUE\n)\nA |> print()\n#>      [,1] [,2] [,3]\n#> [1,]    7    1    2\n#> [2,]    1    8    3\n#> [3,]    2    3    9\n```\n:::\n\n::: {.cell layout-align=\"center\" vscode='{\"languageId\":\"r\"}'}\n\n```{.r .cell-code}\nsum(diag(A)) |> print()\n#> [1] 24\n```\n:::\n\n\n\n\n\n### Dipendenza lineare\n\nSi consideri la matrice \n\n$$\n\\boldsymbol{A}=\n\\left(%\n\\begin{array}{ccc}\n  1 & 1 & 1 \\\\\n  3 & 1 & 5 \\\\\n  2 & 3 & 1 \\\\\n\\end{array}%\n\\right).\n$$ \n\nSiano $\\boldsymbol{c}_1$, $\\boldsymbol{c}_2$,\n$\\boldsymbol{c}_3$ le colonne di $\\boldsymbol{A}$. Si noti che\n\n$$\n2\\boldsymbol{c}_1 + -\\boldsymbol{c}_2 + - \\boldsymbol{c}_3 =\n\\boldsymbol{0}\n$$ \n\ndove $\\boldsymbol{0}$ è un vettore ($3 \\times 1$) di\nzeri.\n\nDato che le 3 colonne di $\\boldsymbol{A}$ possono essere combinate\nlinearmente in modo da produrre un vettore $\\boldsymbol{0}$ vi è\nchiaramente una qualche forma di relazione, o dipendenza, tra le\ninformazioni nelle colonne. Detto in un altro modo, sembra esserci una\nqualche duplicazione delle informazione nelle colonne. In generale, si\ndice che $k$ colonne $\\boldsymbol{c}_1, \\boldsymbol{c}_2,\n\\dots \\boldsymbol{c}_k$ di una matrice sono *linearmente dipendenti* se\nesiste un insieme di valori scalari $\\lambda_1,\n\\dots, \\lambda_k$ tale per cui\n\n$$\n\\lambda_1 \\boldsymbol{c}_1 + \\dots + \\lambda_k \\boldsymbol{c}_k=\\boldsymbol{0}\n$$\n\ne almeno uno dei valori $\\lambda_i$ non è uguale a 0.\n\nLa dipendenza lineare implica che ciascun vettore colonna è una\ncombinazione degli altri. Per esempio\n\n$$\n\\boldsymbol{c}_k= -(\\lambda_1 \\boldsymbol{c}_1 + \\dots + \\lambda_{k-1}\n   \\boldsymbol{c}_{k-1})/\\lambda_k.\n$$ \n   \nQuesto implica che tutta \"l'informazione\" della matrice è contenuta in un sottoinsieme delle colonne -- se $k-1$ colonne sono conosciute, l'ultima resta determinata. È in questo senso che abbiamo detto che l'informazione della matrice\nveniva \"duplicata\".\n\nSe l'unico insieme di valori scalari $\\lambda_i$ che soddisfa\nl'equazione\n\n$$\n\\lambda_1 \\boldsymbol{c}_1 + \\dots + \\lambda_k \\boldsymbol{c}_k=\\boldsymbol{0}\n$$\n\nè un vettore di zeri, allora questo significa che non vi è alcuna\nrelazione tra le colonne della matrice. Le colonne si dicono\n*linearmente indipendenti*, nel senso che non contengono alcuna\n\"duplicazione\" di informazione.\n\n### Rango di una matrice\n\nIl *rango della matrice* è il massimo numero di vettori colonna\nlinearmente indipendenti che possono essere selezionati dalla matrice.\nIn maniera equivalente, il rango di una matrice può essere definito come\nil massimo numero di vettori riga linermente indipendenti. Il rango\nminimo di una matrice è 1, il che significa che vi è una colonna tale\nper cui le altre colonne sono dei multipli di questa. Per l'esempio\nprecedente, il rango della matrice $\\boldsymbol{A}$ è 2.\n\nSe la matrice è quadrata, $\\boldsymbol{A}_{n \\times n}$, ed è costituita\nda vettori tutti indipendenti tra di loro, allora il suo rango è $n$.\nSe, invece, la matrice è rettangolare, $\\boldsymbol{A}_{m \\times n}$, allora il suo rango può essere al massimo il più piccolo tra\ni due valori *m* ed *n*, cioè:\n\n$$\nr(\\boldsymbol{A}_{m \\times n}) \\leq min(m,n).\n$$\n\n## Radici e vettori latenti\n\nDal determinante di una matrice si possono ricavare le *radici latenti*\no *autovalori* (denotati da $\\lambda_i$) e i *vettori latenti* o\n*autovettori* della matrice. Alle nozioni di autovalore e autovettore\nverrà qui fornita un'interpretazione geometrica.\n\nSimuliamo di dati di due variabili associate tra loro:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123456)\n\nnpoints <- 20\nx <- as.numeric(scale(rnorm(npoints, 0, 1)))\ny <- as.numeric(scale(3 * x + rnorm(npoints, 0, 2)))\nmean(x) |> print()\n#> [1] -2.776e-17\nmean(y) |> print()\n#> [1] -7.772e-17\ncor(x, y) |> print()\n#> [1] 0.8291\n```\n:::\n\n\n\n\n\nDisegnamo il diagramma di dispersione con un ellisse che contiene la\nnube di punti:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nY <- cbind(x, y)\ncar::dataEllipse(\n  Y[, 1], Y[, 2],\n  levels = 0.95,\n  lty = 2,\n  ylim = c(-3, 3),\n  xlim = c(-3, 3)\n)\n```\n\n::: {.cell-output-display}\n![](01_linear_algebra_files/figure-html/unnamed-chunk-27-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\nSe racchiudiamo le osservazioni ($v_1, v_2$) con un'ellisse, allora la\nlunghezza dei semiassi maggiori e minori dell'ellisse sarà proporzionale\na $\\sqrt{\\lambda_1}$ e $\\sqrt{\\lambda_2}$. L'asse maggiore è la linea\npassante per il punto ($\\bar{v_1}, \\bar{v_2}$) nella direzione\ndeterminata dal primo autovettore $\\boldsymbol{a}_1'$ con pendenza\nuguale a $a_{12}/a_{11}$. L'asse minore è la linea passante per il punto\n($\\bar{v_1}, \\bar{v_2}$) nella direzione determinata dal secondo\nautovettore $\\boldsymbol{a}_2$.\n\nCalcoliamo ora gli autovettori e gli autovalori:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ns <- cov(Y)\nee <- eigen(s)\nee |> print()\n#> eigen() decomposition\n#> $values\n#> [1] 1.8291 0.1709\n#> \n#> $vectors\n#>        [,1]    [,2]\n#> [1,] 0.7071 -0.7071\n#> [2,] 0.7071  0.7071\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# First eigenvector \nev_1 <- ee$vectors[, 1]\n\n# Slope of the first eigenvector\nev1_m <- ev_1[2] / ev_1[1]\n\n# Second eigenvector \nev_2 <- ee$vectors[, 2]\n\n# Slope of the second eigenvector\nev2_m <- ev_2[2] / ev_2[1]\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Scatter plot showing the span of both eigenvectors \ndata.frame(zx=scale(x), zy=scale(y))  |>\nggplot(aes(x = zx, y = zy)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  geom_abline(slope = ev1_m, color = \"blue\", linewidth = 0.7) +\n  geom_abline(slope = ev2_m, color = \"red\", linewidth = 0.7) \n```\n\n::: {.cell-output-display}\n![](01_linear_algebra_files/figure-html/unnamed-chunk-30-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\nGli autovettori sono ortogonali:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Multiply both eigenvectors \nprint(ev_1 %*% ev_2)\n#>           [,1]\n#> [1,] 2.237e-17\n```\n:::\n\n\n\n\n\nGeneriamo uno Scree Plot.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calculate the estimated variance for each eigenvalue\ne_var <- ee$values / (length(x) - 1)\n\n# Data frame with variance percentages\nvar_per <- tibble(\n  PC  = c(\"PC1\", \"PC2\"),\n  PER = c(e_var) * 100 / sum(e_var) # Calculate the percentage\n    )\n\n# Scree plot \nggplot(var_per, aes(x = PC, y = PER)) +\n  geom_col(width = 0.5, color = \"black\") +\n  xlab(\"Principal component\") +\n  ylab(\"Percentage of variation (%)\") \n```\n\n::: {.cell-output-display}\n![](01_linear_algebra_files/figure-html/unnamed-chunk-32-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\nVerifichiamo che la somma degli autovalori sia uguale alla varianza totale.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvar(x) + var(y)\n#> [1] 2\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nee$values |> sum()\n#> [1] 2\n```\n:::\n\n\n\n\n\nGli autovettori ottenuti utilizzando la funzione `eigen()` sono normalizzati. Ciò significa che la loro lunghezza è uguale a 1:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nt(as.matrix(ee$vectors[, 1])) %*% as.matrix(ee$vectors[, 1]) \n#>      [,1]\n#> [1,]    1\n```\n:::\n\n\n\n\n\nUtilizziamo le informazioni degli autovettori e degli autovalori per disegnare gli assi dell'ellisse: \n\n- gli autovettori determinano la direzione degli assi; \n- la radice quadrata degli autovalori è proporzionale alla lunghezza degli assi dell'ellisse.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncar::dataEllipse(\n  Y[, 1], Y[, 2],\n  levels = 0.95,\n  lty = 2,\n  ylim = c(-3, 3),\n  xlim = c(-3, 3)\n)\nk <- 2.65\narrows(\n  0, 0, \n  k * sqrt(ee$values[1]) * ee$vectors[1],\n  k * sqrt(ee$values[1]) * ee$vectors[2],\n  code = 2, \n  col = \"red\", \n  lwd = 2\n)\narrows(\n  0, 0, \n  k * sqrt(ee$values[2]) * ee$vectors[1],\n  k * sqrt(ee$values[2]) * -ee$vectors[2],\n  code = 2, \n  col = \"red\", \n  lwd = 2\n)\n```\n\n::: {.cell-output-display}\n![](01_linear_algebra_files/figure-html/unnamed-chunk-36-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n\nTale analisi si può estendere a qualunque numero di variabili. Per\nesempio, nel caso di tre variabili, possiamo pensare di disegnare un\nellisoide attorno ad una nube di punti nello spazio tridimensionale.\nAnche in questo caso, gli autovalori e gli associati autovettori\ncorrisponderanno agli assi dell'elissoide.\n\n## Scomposizione spettrale di una matrice\n\nData una matrice quadrata e simmetrica di dimensione $n$,\n$\\boldsymbol{A}$, esistono una matrice diagonale $\\boldsymbol{\\Lambda}$\ne una matrice ortogonale $\\boldsymbol{V}$ tali che\n\n$$\\boldsymbol{A} =\\boldsymbol{V} \\boldsymbol{\\Lambda} \\boldsymbol{V}',$$\ndove\n\n-   $\\boldsymbol{\\Lambda}$ è una matrice diagonale i cui elementi sono\n    gli autovalori di $\\boldsymbol{A}$:\n    $\\boldsymbol{\\Lambda} = diag(\\lambda_1, \\lambda_2,\n        \\dots, \\lambda_n)$;\n-   $\\boldsymbol{V}$ è una matrice ortogonale le cui colonne\n    $(v_1, v_2, \\dots, v_p)$ sono gli autovettori di $\\boldsymbol{A}$\n    associati ai rispettivi autovalori.\n\nIn maniera equivalente\n\n$$\\boldsymbol{A} \\boldsymbol{V} =  \\boldsymbol{\\Lambda} \\boldsymbol{V}'.$$\n\nPremoltiplicando entrambi i membri per $\\boldsymbol{V}'$ si ottiene\n\n$$\\boldsymbol{V}'\\boldsymbol{A} \\boldsymbol{V} =\n\\boldsymbol{\\Lambda},$$ \n\nda cui l'affermazione che la matrice degli autovettori diagonalizza $\\boldsymbol{A}$.\n\nPer esempio,\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsigma <- matrix(\n  data = c(1, 0.5, 0.5, 1.25), \n  nrow = 2, \n  ncol = 2\n)\nsigma |> print()\n#>      [,1] [,2]\n#> [1,]  1.0 0.50\n#> [2,]  0.5 1.25\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nout <- eigen(sigma)\nout |> print()\n#> eigen() decomposition\n#> $values\n#> [1] 1.6404 0.6096\n#> \n#> $vectors\n#>        [,1]    [,2]\n#> [1,] 0.6154 -0.7882\n#> [2,] 0.7882  0.6154\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nLambda <- diag(out$values)\nLambda |> print()\n#>      [,1]   [,2]\n#> [1,] 1.64 0.0000\n#> [2,] 0.00 0.6096\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nU <- out$vectors\nU |> print()\n#>        [,1]    [,2]\n#> [1,] 0.6154 -0.7882\n#> [2,] 0.7882  0.6154\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nU %*% Lambda %*% t(U) |> print()\n#>      [,1] [,2]\n#> [1,]  1.0 0.50\n#> [2,]  0.5 1.25\n```\n:::\n\n\n\n\n\n### Autovalori e determinante\n\nIl determinante di una matrice è il prodotto degli autovalori:\n\n$$\\begin{aligned}\n    |\\boldsymbol{A}| &= \\prod_{i=1}^{p} \\lambda_i. \\notag\n    \\end{aligned}$$\n\nLa traccia di una matrice è uguale alla somma degli autovalori:\n\n$$\\begin{aligned}\n    tr(\\boldsymbol{A}) &= \\sum_{i=1}^{p} \\lambda_i. \\notag\n    \\end{aligned}$$\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsigma <- matrix(data = c(1, 0.5, 0.5, 2), nrow = 2, ncol = 2)\nsigma |> print()\n#>      [,1] [,2]\n#> [1,]  1.0  0.5\n#> [2,]  0.5  2.0\n\nout <- eigen(sigma)\nout |> print()\n#> eigen() decomposition\n#> $values\n#> [1] 2.2071 0.7929\n#> \n#> $vectors\n#>        [,1]    [,2]\n#> [1,] 0.3827 -0.9239\n#> [2,] 0.9239  0.3827\n```\n:::\n\n\n\n\n\nLa traccia di una matrice è uguale alla somma degli autovalori:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsum(out$values) |> print()\n#> [1] 3\n```\n:::\n\n\n\n\n\nIl determinante di una matrice è il prodotto degli autovalori:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndet(sigma) |> print()\n#> [1] 1.75\n(out$values[1] * out$values[2]) |> print()\n#> [1] 1.75\n```\n:::\n\n\n\n\n\nGli autovalori di $\\boldsymbol{A}^{-1}$ sono i reciproci degli\nautovalori di $\\boldsymbol{A}$; gli autovettori sono coincidenti.\n\n## La distanza euclidea\n\nPer calcolare la **distanza euclidea** tra due punti utilizzando l'algebra matriciale, consideriamo i punti come vettori in uno spazio euclideo.\n\n\n### In due dimensioni:\n\nSiano dati i vettori:\n\n- **x** = $\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}$,\n- **y** = $\\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix}$.\n\nLa distanza euclidea tra **x** e **y** è la norma del vettore differenza ($\\mathbf{x} - \\mathbf{y}$):\n\n$$ d(\\mathbf{x}, \\mathbf{y}) = \\| \\mathbf{x} - \\mathbf{y} \\| $$.\n\nIn termini di algebra matriciale, questa norma è calcolata come:\n\n$$ d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ (\\mathbf{x} - \\mathbf{y})^\\top (\\mathbf{x} - \\mathbf{y}) } $$\n\n**Passaggi dettagliati:**\n\n1. **Calcolo del vettore differenza:**\n\n   $$\n   \\mathbf{x} - \\mathbf{y} = \\begin{bmatrix} x_1 - y_1 \\\\ x_2 - y_2 \\end{bmatrix}\n   $$\n\n2. **Calcolo del prodotto scalare:**\n\n   $$\n   (\\mathbf{x} - \\mathbf{y})^\\top (\\mathbf{x} - \\mathbf{y}) = (x_1 - y_1)^2 + (x_2 - y_2)^2\n   $$\n\n3. **Calcolo della radice quadrata:**\n\n   $$\n   d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ (x_1 - y_1)^2 + (x_2 - y_2)^2 }\n   $$\n\nQuesto risultato corrisponde alla formula classica per la distanza tra due punti nel piano cartesiano.\n\n\n### Estensione a più dimensioni\n\nPer vettori in uno spazio $ n $-dimensionale:\n\n- **x** = $\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$\n- **y** = $\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$\n\nLa distanza euclidea diventa:\n\n$$ d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ (\\mathbf{x} - \\mathbf{y})^\\top (\\mathbf{x} - \\mathbf{y}) } $$\n\nChe si espande in:\n\n$$ d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ \\sum_{i=1}^{n} (x_i - y_i)^2 } $$\n\nIn conclusione:\n\nl'utilizzo dell'algebra matriciale permette di esprimere in modo compatto e generalizzato il calcolo della distanza euclidea tra due punti in qualsiasi dimensione, sfruttando operazioni matriciali come la trasposizione e il prodotto scalare.\n\n## La distanza di Mahalanobis\n\nLa distanza euclidea presuppone che le variabili siano non correlate e su scale comparabili. Tuttavia, in molti casi, le variabili possono avere scale diverse e possono essere correlate tra loro. La **distanza di Mahalanobis** tiene conto di queste differenze utilizzando la matrice di covarianza, permettendo una misurazione della distanza che considera sia la scala che la correlazione tra le variabili.\n\n### Definizione\n\nLa distanza di Mahalanobis tra due vettori $\\mathbf{x}$ e $\\mathbf{y}$ è definita come:\n\n$$\nd_M(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ (\\mathbf{x} - \\mathbf{y})^\\top \\mathbf{S}^{-1} (\\mathbf{x} - \\mathbf{y}) }\n$$\n\ndove:\n\n- $\\mathbf{S}$ è la matrice di **covarianza** delle variabili.\n- $\\mathbf{S}^{-1}$ è l'**inversa** della matrice di covarianza.\n\n### Perché è necessaria la matrice di covarianza?\n\n- **Scala delle variabili**: Se le variabili hanno varianze diverse, la matrice di covarianza normalizza queste differenze, evitando che variabili con varianze maggiori dominino la misura della distanza.\n- **Correlazione tra variabili**: La matrice di covarianza tiene conto delle correlazioni tra le variabili, riducendo l'influenza delle variabili altamente correlate sulla distanza totale.\n\n### Esempio numerico\n\nSupponiamo di avere due punti in uno spazio bidimensionale:\n\n- **Punto A**: $\\mathbf{x} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}$\n- **Punto B**: $\\mathbf{y} = \\begin{bmatrix} 5 \\\\ 7 \\end{bmatrix}$\n\nE una matrice di covarianza stimata:\n\n$$\n\\mathbf{S} = \\begin{bmatrix}\n4 & 2 \\\\\n2 & 3\n\\end{bmatrix}\n$$\n\n#### Passaggi per il calcolo\n\n1. **Calcolo del vettore differenza $\\mathbf{d}$:**\n\n   $$\n   \\mathbf{d} = \\mathbf{x} - \\mathbf{y} = \\begin{bmatrix} 2 - 5 \\\\ 3 - 7 \\end{bmatrix} = \\begin{bmatrix} -3 \\\\ -4 \\end{bmatrix}\n   $$\n\n2. **Calcolo dell'inversa della matrice di covarianza $\\mathbf{S}^{-1}$:**\n\n   - **Determinante di $\\mathbf{S}$:**\n\n     $$\n     \\det(\\mathbf{S}) = (4)(3) - (2)(2) = 12 - 4 = 8\n     $$\n\n   - **Matrice aggiunta (comatrice trasposta) di $\\mathbf{S}$:**\n\n     $$\n     \\text{adj}(\\mathbf{S}) = \\begin{bmatrix}\n     3 & -2 \\\\\n     -2 & 4\n     \\end{bmatrix}\n     $$\n\n   - **Inversa di $\\mathbf{S}$:**\n\n     $$\n     \\mathbf{S}^{-1} = \\frac{1}{\\det(\\mathbf{S})} \\text{adj}(\\mathbf{S}) = \\frac{1}{8} \\begin{bmatrix}\n     3 & -2 \\\\\n     -2 & 4\n     \\end{bmatrix} = \\begin{bmatrix}\n     0.375 & -0.25 \\\\\n     -0.25 & 0.5\n     \\end{bmatrix}\n     $$\n\n3. **Calcolo della distanza di Mahalanobis:**\n\n   $$\n   d_M(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ \\mathbf{d}^\\top \\mathbf{S}^{-1} \\mathbf{d} }\n   $$\n\n   - **Calcolo del prodotto $\\mathbf{S}^{-1} \\mathbf{d}$:**\n\n     $$\n     \\mathbf{S}^{-1} \\mathbf{d} = \\begin{bmatrix}\n     0.375 & -0.25 \\\\\n     -0.25 & 0.5\n     \\end{bmatrix} \\begin{bmatrix} -3 \\\\ -4 \\end{bmatrix} = \\begin{bmatrix}\n     (-0.375 \\times 3) + (0.25 \\times 4) \\\\\n     (0.25 \\times 3) + (-0.5 \\times 4)\n     \\end{bmatrix} = \\begin{bmatrix}\n     -0.125 \\\\\n     -1.25\n     \\end{bmatrix}\n     $$\n\n   - **Calcolo del prodotto scalare:**\n\n     $$\n     \\mathbf{d}^\\top (\\mathbf{S}^{-1} \\mathbf{d}) = \\begin{bmatrix} -3 & -4 \\end{bmatrix} \\begin{bmatrix} -0.125 \\\\ -1.25 \\end{bmatrix} = (-3)(-0.125) + (-4)(-1.25) = 0.375 + 5 = 5.375\n     $$\n\n   - **Calcolo della distanza:**\n\n     $$\n     d_M(\\mathbf{x}, \\mathbf{y}) = \\sqrt{5.375} \\approx 2.318\n     $$\n\n\n### Confronto con la distanza euclidea\n\nLa distanza euclidea tra gli stessi punti è:\n\n$$\nd_E(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ (-3)^2 + (-4)^2 } = \\sqrt{9 + 16} = \\sqrt{25} = 5\n$$\n\nCome si può notare, la distanza di Mahalanobis ($\\approx 2.318$) è diversa dalla distanza euclidea (5) a causa della considerazione delle varianze e delle correlazioni tra le variabili.\n\n### Interpretazione\n\n- **Varianze diverse**: Se una variabile ha una varianza elevata, le differenze lungo quella direzione avranno meno peso nella distanza totale.\n- **Correlazioni**: Se due variabili sono altamente correlate, la distanza di Mahalanobis riduce l'importanza delle differenze lungo la direzione in cui le variabili sono correlate.\n\nIn conclusione, la distanza di Mahalanobis è particolarmente utile in contesti multivariati dove le variabili hanno scale diverse e possono essere correlate. Essa fornisce una misura di distanza che è invariante rispetto alle trasformazioni lineari dei dati, rendendola ideale per l'analisi di dati statistici e il rilevamento di outlier.\n\n**Nota:** È importante assicurarsi che la matrice di covarianza $\\mathbf{S}$ sia non singolare (invertibile). In pratica, quando si lavora con campioni di dati, $\\mathbf{S}$ viene stimata dai dati stessi.\n\n## Informazioni sull'Ambiente di Sviluppo {.unnumbered} \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.4.2 (2024-10-31)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.3.2\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] ggokabeito_0.1.0  see_0.11.0        MASS_7.3-65       viridis_0.6.5    \n#>  [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#>  [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#> [13] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#> [17] knitr_1.50        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#> [21] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#> [25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#> \n#> loaded via a namespace (and not attached):\n#>   [1] rstudioapi_0.17.1   jsonlite_1.9.1      magrittr_2.0.3     \n#>   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#>   [7] nloptr_2.2.1        rmarkdown_2.29      vctrs_0.6.5        \n#>  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#>  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#>  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#>  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#>  [22] mime_0.13           lifecycle_1.0.4     pkgconfig_2.0.3    \n#>  [25] Matrix_1.7-3        R6_2.6.1            fastmap_1.2.0      \n#>  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#>  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#>  [34] rprojroot_2.0.4     Hmisc_5.2-3         labeling_0.4.3     \n#>  [37] timechange_0.3.0    abind_1.4-8         compiler_4.4.2     \n#>  [40] withr_3.0.2         glasso_1.11         htmlTable_2.4.3    \n#>  [43] backports_1.5.0     carData_3.0-5       ggsignif_0.6.4     \n#>  [46] corpcor_1.6.10      gtools_3.9.5        tools_4.4.2        \n#>  [49] pbivnorm_0.6.0      foreign_0.8-88      zip_2.3.2          \n#>  [52] httpuv_1.6.15       nnet_7.3-20         glue_1.8.0         \n#>  [55] quadprog_1.5-8      nlme_3.1-167        promises_1.3.2     \n#>  [58] lisrelToR_0.3       grid_4.4.2          checkmate_2.3.2    \n#>  [61] cluster_2.1.8.1     reshape2_1.4.4      generics_0.1.3     \n#>  [64] gtable_0.3.6        tzdb_0.5.0          data.table_1.17.0  \n#>  [67] hms_1.1.3           car_3.1-3           sem_3.1-16         \n#>  [70] pillar_1.10.1       rockchalk_1.8.157   later_1.4.1        \n#>  [73] splines_4.4.2       lattice_0.22-6      survival_3.8-3     \n#>  [76] kutils_1.73         tidyselect_1.2.1    miniUI_0.1.1.1     \n#>  [79] pbapply_1.7-2       reformulas_0.4.0    stats4_4.4.2       \n#>  [82] xfun_0.51           qgraph_1.9.8        arm_1.14-4         \n#>  [85] stringi_1.8.4       yaml_2.3.10         pacman_0.5.1       \n#>  [88] boot_1.3-31         evaluate_1.0.3      codetools_0.2-20   \n#>  [91] mi_1.1              cli_3.6.4           RcppParallel_5.1.10\n#>  [94] rpart_4.1.24        xtable_1.8-4        Rdpack_2.6.3       \n#>  [97] munsell_0.5.1       Rcpp_1.0.14         coda_0.19-4.1      \n#> [100] png_0.1-8           XML_3.99-0.18       parallel_4.4.2     \n#> [103] jpeg_0.1-10         lme4_1.1-36         mvtnorm_1.3-3      \n#> [106] openxlsx_4.2.8      rlang_1.1.5         multcomp_1.4-28    \n#> [109] mnormt_2.1.1\n```\n:::\n",
    "supporting": [
      "01_linear_algebra_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}