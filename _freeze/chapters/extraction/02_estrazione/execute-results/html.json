{
  "hash": "0a09bbf525632ac07920a6aec0b638c9",
  "result": {
    "engine": "knitr",
    "markdown": "# L'estrazione dei fattori {#sec-extraction-factor-extraction}\n\n::: callout-important\n## In questo capitolo imparerai:\n\n- Come implementare il **metodo delle componenti principali** per l'estrazione delle saturazioni fattoriali utilizzando l'algebra matriciale.  \n- Come implementare il **metodo dei fattori principali** e comprenderne i fondamenti teorici.  \n- Il funzionamento del **metodo dei fattori principali iterato**, con un focus sul processo di convergenza.  \n- Le caratteristiche principali del **metodo di massima verosimiglianza** e il suo utilizzo nell'analisi fattoriale.  \n:::\n\n::: callout-tip\n## Prerequisiti\n\n- Leggere il capitolo *Factor Analysis and Principal Component Analysis* del testo di @petersen2024principles.\n:::\n\n::: callout-caution\n## Preparazione del Notebook\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(lavaan, psych)\n```\n:::\n\n\n\n\n:::\n\n## Introduzione\n\nL’**analisi fattoriale** è una tecnica statistica multivariata utilizzata per **identificare strutture latenti**, ovvero **fattori non osservabili**, che spiegano le **correlazioni tra variabili osservate**.\n\nIn altre parole, quando abbiamo molte variabili (es. item di un questionario), l’analisi fattoriale cerca di scoprire **quali gruppi di item misurano lo stesso costrutto psicologico sottostante**, riducendo così la complessità dei dati.\n\nQuesta tecnica è particolarmente utile nelle scienze sociali e in psicologia, dove **costrutti astratti** come *intelligenza*, *ansia* o *autostima* non possono essere misurati direttamente, ma solo attraverso più item. L’analisi fattoriale aiuta a verificare se questi item misurano effettivamente **un numero limitato di costrutti** sottostanti, rendendo i dati più interpretabili.\n\n\n### Il modello statistico dell’analisi fattoriale\n\nIl modello matematico alla base dell’analisi fattoriale si esprime così:\n\n$$\n\\boldsymbol{\\Sigma} = \\boldsymbol{\\Lambda} \\boldsymbol{\\Phi} \\boldsymbol{\\Lambda}^{\\mathsf{T}} + \\boldsymbol{\\Psi},\n$$\n\ndove:\n\n- $\\boldsymbol{\\Sigma}$ è la **matrice delle covarianze osservate** tra le variabili;\n- $\\boldsymbol{\\Lambda}$ è la **matrice dei carichi fattoriali**: ciascun elemento rappresenta l’intensità della relazione tra una variabile osservata e un fattore latente;\n- $\\boldsymbol{\\Phi}$ è la **matrice delle correlazioni tra i fattori** (è l’identità se i fattori sono ortogonali, cioè non correlati);\n- $\\boldsymbol{\\Psi}$ è una **matrice diagonale** contenente le **unicità** (la porzione di varianza di ciascuna variabile che non è spiegata dai fattori comuni).\n\nQuesto modello riflette l’idea che **ogni variabile osservata sia influenzata da uno o più fattori comuni**, più una componente specifica e casuale (l’unicità).\n\n\n## Estrazione dei Fattori: Panoramica dei Metodi\n\nL’**estrazione dei fattori** consiste nel **stimare i parametri** del modello (soprattutto $\\boldsymbol{\\Lambda}$), sulla base della matrice di correlazioni o covarianze. I diversi metodi si distinguono per:\n\n- le **assunzioni statistiche** (es. normalità dei dati);\n- il tipo di **informazione utilizzata** (es. varianza totale o varianza comune);\n- la possibilità di **testare l’adattamento del modello ai dati**.\n\n| Metodo                         | Tiene conto della specificità? | Richiede normalità? | Permette test di bontà del modello? |\n|-------------------------------|-------------------------------|----------------------|-------------------------------------|\n| Componenti principali (PCA)   | ❌ No                         | ❌ No               | ❌ No                              |\n| Fattori principali            | ✅ Sì                         | ❌ No               | ❌ No                              |\n| Fattori principali iterato    | ✅ Sì (con aggiornamenti)     | ❌ No               | ❌ No                              |\n| Massima verosimiglianza (ML)  | ✅ Sì                         | ✅ Sì               | ✅ Sì                              |\n\nVediamo ora in dettaglio ciascun metodo.\n\n\n## Metodo delle Componenti Principali (PCA)\n\n> ❗ **Importante:** Sebbene sia molto diffuso, il metodo delle componenti principali (*Principal Component Analysis*, PCA) **non è un vero metodo fattoriale**. Non fa distinzione tra **varianza comune** (quella condivisa tra variabili) e **varianza specifica** (quella unica di ogni variabile), e **non assume l’esistenza di fattori latenti**. Per questo motivo, in psicometria, viene usato **per riduzione della dimensionalità**, non per identificare costrutti teorici.\n\n### Obiettivo\n\nLa PCA costruisce un numero ridotto di **componenti principali**:\n\n- sono **combinazioni lineari** delle variabili originali;\n- sono **ortogonali** (cioè non correlate tra loro);\n- spiegano progressivamente la **massima varianza possibile** nei dati.\n\n### Fondamento teorico: il teorema spettrale\n\nLa PCA si basa sul **teorema spettrale**, che dice che ogni **matrice simmetrica** (come la matrice di correlazione $\\mathbf{R}$) può essere scomposta come:\n\n$$\n\\mathbf{R} = \\mathbf{C} \\mathbf{D} \\mathbf{C}^{\\mathsf{T}},\n$$\n\ndove:\n\n- $\\mathbf{C}$ è la matrice i cui **vettori colonna sono gli autovettori** (direzioni principali) di $\\mathbf{R}$;\n- $\\mathbf{D}$ è una **matrice diagonale** con gli **autovalori** (quantità di varianza spiegata da ciascuna direzione);\n- $\\mathbf{C}^{\\mathsf{T}}$ è la trasposta di $\\mathbf{C}$.\n\nQuesta è la **scomposizione spettrale** della matrice $\\mathbf{R}$.\n\n### Costruzione delle saturazioni (carichi)\n\nVogliamo una matrice $\\hat{\\boldsymbol{\\Lambda}}$ che approssimi la matrice $\\mathbf{R}$:\n\n$$\n\\mathbf{R} \\approx \\hat{\\boldsymbol{\\Lambda}} \\hat{\\boldsymbol{\\Lambda}}^\\mathsf{T}.\n$$\n\nPoiché $\\mathbf{D}$ è diagonale, possiamo scriverla come:\n\n$$\n\\mathbf{D} = \\mathbf{D}^{1/2} \\cdot \\mathbf{D}^{1/2},\n$$\n\ndove $\\mathbf{D}^{1/2}$ ha sulla diagonale le **radici quadrate degli autovalori**. Allora:\n\n$$\n\\mathbf{R} = \\mathbf{C} \\mathbf{D}^{1/2} \\cdot \\mathbf{D}^{1/2} \\mathbf{C}^{\\mathsf{T}}.\n$$\n\nDefiniamo:\n\n$$\n\\hat{\\boldsymbol{\\Lambda}} = \\mathbf{C} \\mathbf{D}^{1/2},\n$$\n\ne otteniamo:\n\n$$\n\\hat{\\boldsymbol{\\Lambda}} \\hat{\\boldsymbol{\\Lambda}}^\\mathsf{T} = \\mathbf{R}.\n$$\n\n> 💡 Le **saturazioni** si ottengono moltiplicando **ogni autovettore per la radice quadrata dell’autovalore corrispondente**. Questo consente di ricostruire esattamente la matrice $\\mathbf{R}$.\n\nOgni elemento $l_{ij}$ di $\\hat{\\boldsymbol{\\Lambda}}$ indica **quanto la variabile $i$ contribuisce alla componente $j$**.\n\nQuando si selezionano solo i primi $k$ autovalori e autovettori (cioè quelli che spiegano più varianza), si ottiene una **rappresentazione semplificata** dei dati, utile per la riduzione della dimensionalità.\n\n### Interpretazione\n\n- Gli **autovalori** indicano quanta varianza è spiegata da ciascuna componente.\n- Le **componenti principali** sono nuove variabili non osservate, che sintetizzano l’informazione contenuta nelle variabili originali.\n\n### Limiti della PCA come analisi fattoriale\n\n- Non separa **varianza comune** da **varianza specifica**.\n- Non assume **fattori latenti**.\n- Non consente di valutare l’adattamento del modello ai dati.\n\n### Quando usarla\n\nLa PCA è utile quando:\n\n- si vuole **ridurre il numero di variabili** mantenendo la massima varianza;\n- si desidera costruire **indici sintetici** (es. punteggi compositi);\n- si vuole **esplorare la struttura dei dati** in modo preliminare.\n\nNon è invece adatta quando l’obiettivo è **identificare costrutti latenti teorici**.\n\n\n## Metodo dei Fattori Principali\n\nIl **metodo dei fattori principali** (*principal factor method*) si differenzia dalla PCA perché considera solo la **varianza comune** tra le variabili, escludendo la varianza specifica e l’errore.\n\n### Procedura\n\n1. Si stima la **comunalità iniziale** di ogni variabile (cioè la quota di varianza spiegata da fattori comuni).\n2. Si **sostituiscono le varianze totali sulla diagonale** della matrice $\\mathbf{R}$ con le comunalità stimate.\n3. Si esegue la **decomposizione spettrale** sulla nuova matrice per ottenere autovettori e autovalori.\n4. Si costruisce la matrice dei **carichi fattoriali**.\n\n### Stima delle comunalità iniziali\n\nPuò essere fatta, ad esempio:\n\n- prendendo il **massimo quadrato della correlazione** della variabile con le altre;\n- oppure il **$R^2$** da una regressione multipla della variabile sulle altre.\n\n### Vantaggi\n\n- Tiene conto della **specificità** delle variabili.\n- È più **coerente con il modello fattoriale classico**.\n\n### Limiti\n\n- I risultati dipendono fortemente dalla **stima iniziale delle comunalità**.\n- Non permette test di bontà dell’adattamento del modello.\n\n\n## Metodo dei Fattori Principali Iterato\n\nQuesto metodo **affina** il precedente aggiornando iterativamente le comunalità:\n\n1. si calcolano carichi fattoriali e comunalità iniziali;\n2. si sostituiscono le nuove comunalità nella diagonale;\n3. si ripete la procedura finché i valori **convergono**.\n\n### Vantaggi\n\n- Fornisce **stime più stabili** delle comunalità.\n- Migliora la qualità della rappresentazione se la struttura è forte.\n\n### Limiti\n\n- Può generare **soluzioni improprie** (es. comunalità > 1: *problemi di Heywood*).\n- Non offre criteri interni per la scelta del numero di fattori.\n\n\n## Metodo della Massima Verosimiglianza (ML)\n\nIl metodo di **massima verosimiglianza** assume che i dati provengano da una **distribuzione normale multivariata**. Si basa sulla **stima dei parametri** che rendono massimamente probabile l’osservazione dei dati dati i parametri.\n\n### Caratteristiche\n\n- Permette di stimare **carichi**, **unicità** e **correlazioni tra fattori**.\n- Fornisce un **test statistico di bontà dell’adattamento** (test chi-quadro).\n- Permette **confronti tra modelli alternativi** (usando AIC, BIC, etc.).\n\n### Vantaggi\n\n- È il più coerente con un’interpretazione psicometrica.\n- Consente **analisi inferenziali** e confronti tra ipotesi.\n\n### Limiti\n\n- Sensibile alle **violazioni della normalità**.\n- Richiede **campioni sufficientemente numerosi**.\n- Può non convergere in presenza di dati problematici.\n\n\n> 🎓 *Suggerimento didattico:*  \n> Se l’obiettivo è **identificare costrutti psicologici latenti**, scegliete metodi coerenti con il modello fattoriale, come la **massima verosimiglianza**.  \n> Se invece volete solo **ridurre le dimensioni** dei dati per scopi descrittivi o pratici, allora la **PCA può essere sufficiente**.\n\n\n## Esempio pratico in R: Confronto tra metodi di estrazione\n\nPer illustrare i principali metodi di estrazione dei fattori, useremo un semplice esempio tratto da Rencher (2010). Una ragazza ha valutato 7 persone su 5 tratti personali:\n\n- **K** = Kind (Gentile)  \n- **I** = Intelligent (Intelligente)  \n- **H** = Happy (Felice)  \n- **L** = Likeable (Simpatica)  \n- **J** = Just (Giusta)\n\nLa matrice di correlazione tra i tratti è la seguente:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nR <- matrix(c(\n  1.000, .296, .881, .995, .545,\n  .296, 1.000, -.022, .326, .837,\n  .881, -.022, 1.000, .867, .130,\n  .995, .326, .867, 1.000, .544,\n  .545, .837, .130, .544, 1.000\n),\nncol = 5, byrow = TRUE,\ndimnames = list(c(\"K\", \"I\", \"H\", \"L\", \"J\"),\n                c(\"K\", \"I\", \"H\", \"L\", \"J\")))\nR\n#>       K      I      H     L     J\n#> K 1.000  0.296  0.881 0.995 0.545\n#> I 0.296  1.000 -0.022 0.326 0.837\n#> H 0.881 -0.022  1.000 0.867 0.130\n#> L 0.995  0.326  0.867 1.000 0.544\n#> J 0.545  0.837  0.130 0.544 1.000\n```\n:::\n\n\n\n\n\n\n## Metodo delle Componenti Principali (PCA)\n\n**1. Calcolo degli autovalori e autovettori.**\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ne <- eigen(R)\ne$values       # varianza spiegata da ciascuna componente\n#> [1] 3.2633766 1.5383821 0.1679693 0.0300298 0.0002422\ne$vectors      # coefficienti delle combinazioni lineari\n#>         [,1]    [,2]     [,3]    [,4]    [,5]\n#> [1,] -0.5367 -0.1860 -0.18992 -0.1248  0.7910\n#> [2,] -0.2875  0.6506  0.68489 -0.1198  0.1034\n#> [3,] -0.4344 -0.4737  0.40695  0.6137 -0.2116\n#> [4,] -0.5374 -0.1693 -0.09533 -0.6294 -0.5266\n#> [5,] -0.3897  0.5377 -0.56583  0.4442 -0.2037\n```\n:::\n\n\n\n\n\n- Gli **autovalori** indicano quanta varianza spiega ciascuna componente.\n- Gli **autovettori** sono le \"direzioni\" lungo cui le componenti combinano le variabili.\n\n**2. Verifica della decomposizione spettrale.**\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nround(e$vectors %*% diag(e$values) %*% t(e$vectors), 3)\n#>       [,1]   [,2]   [,3]  [,4]  [,5]\n#> [1,] 1.000  0.296  0.881 0.995 0.545\n#> [2,] 0.296  1.000 -0.022 0.326 0.837\n#> [3,] 0.881 -0.022  1.000 0.867 0.130\n#> [4,] 0.995  0.326  0.867 1.000 0.544\n#> [5,] 0.545  0.837  0.130 0.544 1.000\n```\n:::\n\n\n\n\n\nQuesta moltiplicazione ricostruisce la matrice di correlazione originale:  \n$\\mathbf{R} = \\mathbf{C} \\mathbf{D} \\mathbf{C}^{\\mathsf{T}}$\n\n**3. Varianza spiegata dai primi 2 fattori.**\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsum(e$values[1:2]) / sum(e$values)\n#> [1] 0.9604\n```\n:::\n\n\n\n\n\n**Interpretazione**: Se i primi due autovalori spiegano, ad esempio, il 96% della varianza totale, possiamo **ridurre da 5 a 2 dimensioni** con perdita minima di informazione.\n\n**4. Calcolo delle saturazioni fattoriali (matrice $\\hat{\\Lambda}$).**\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nL <- cbind(\n  e$vectors[, 1] * sqrt(e$values[1]),\n  e$vectors[, 2] * sqrt(e$values[2])\n)\nround(L, 3)\n#>        [,1]   [,2]\n#> [1,] -0.970 -0.231\n#> [2,] -0.519  0.807\n#> [3,] -0.785 -0.588\n#> [4,] -0.971 -0.210\n#> [5,] -0.704  0.667\n```\n:::\n\n\n\n\n\n- Ogni colonna rappresenta una componente.\n- Ogni riga rappresenta una variabile.\n- Gli elementi indicano **quanto una variabile satura su una componente**.\n\n**5. Matrice riprodotta e residui.**\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nR_hat <- round(L %*% t(L), 3)\nresidui <- round(R - R_hat, 3)\nresidui\n#>        K      I      H      L      J\n#> K  0.007 -0.021 -0.015  0.005  0.016\n#> I -0.021  0.079  0.045 -0.009 -0.067\n#> H -0.015  0.045  0.039 -0.018 -0.030\n#> L  0.005 -0.009 -0.018  0.013  0.001\n#> J  0.016 -0.067 -0.030  0.001  0.060\n```\n:::\n\n\n\n\n\nSe i **residui** (cioè la differenza tra $\\mathbf{R}$ e $\\hat{\\Lambda} \\hat{\\Lambda}^\\mathsf{T}$) sono piccoli, la soluzione a 2 fattori è soddisfacente.\n\n\n## Metodo dei Fattori Principali\n\nIl metodo dei fattori principali estrae **solo la varianza comune**, escludendo la varianza specifica.\n\n**1. Stima iniziale delle comunalità.**\n\nUna stima semplice: **il valore massimo della correlazione per ogni variabile** (approssimazione):\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nh.hat <- apply(abs(R), 1, max)\nround(h.hat, 3)\n#> K I H L J \n#> 1 1 1 1 1\n```\n:::\n\n\n\n\n\n**2. Matrice ridotta.**\n\nSostituiamo le varianze sulla diagonale con le comunalità stimate:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nR1 <- R\ndiag(R1) <- h.hat\nR1\n#>       K      I      H     L     J\n#> K 1.000  0.296  0.881 0.995 0.545\n#> I 0.296  1.000 -0.022 0.326 0.837\n#> H 0.881 -0.022  1.000 0.867 0.130\n#> L 0.995  0.326  0.867 1.000 0.544\n#> J 0.545  0.837  0.130 0.544 1.000\n```\n:::\n\n\n\n\n\n**3. Decomposizione della matrice ridotta.**\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nee <- eigen(R1)\nround(ee$values, 3)  # autovalori\n#> [1] 3.263 1.538 0.168 0.030 0.000\n```\n:::\n\n\n\n\n\n**4. Saturazioni fattoriali.**\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nL <- ee$vectors[, 1:2] %*% sqrt(diag(ee$values[1:2]))\nround(L, 3)\n#>        [,1]   [,2]\n#> [1,] -0.970 -0.231\n#> [2,] -0.519  0.807\n#> [3,] -0.785 -0.588\n#> [4,] -0.971 -0.210\n#> [5,] -0.704  0.667\n```\n:::\n\n\n\n\n\nLe saturazioni qui rappresentano la **relazione tra variabili e fattori latenti**, tenendo conto solo della **varianza comune**.\n\n\n## Metodo dei Fattori Principali Iterato\n\nQuesto metodo **aggiorna iterativamente** le stime delle comunalità finché le saturazioni non cambiano più (convergenza).\n\n**Esecuzione in R con il pacchetto `psych`.**\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npa <- psych::fa(R, nfactors = 2, rotate = \"none\", fm = \"pa\")\npa\n#> Factor Analysis using method =  pa\n#> Call: psych::fa(r = R, nfactors = 2, rotate = \"none\", fm = \"pa\")\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>    PA1   PA2   h2     u2 com\n#> K 0.98 -0.21 1.01 -0.008 1.1\n#> I 0.48  0.74 0.77  0.230 1.7\n#> H 0.78 -0.56 0.92  0.085 1.8\n#> L 0.98 -0.19 0.99  0.010 1.1\n#> J 0.69  0.69 0.95  0.049 2.0\n#> \n#>                        PA1  PA2\n#> SS loadings           3.22 1.41\n#> Proportion Var        0.64 0.28\n#> Cumulative Var        0.64 0.93\n#> Proportion Explained  0.70 0.30\n#> Cumulative Proportion 0.70 1.00\n#> \n#> Mean item complexity =  1.5\n#> Test of the hypothesis that 2 factors are sufficient.\n#> \n#> df null model =  10  with the objective function =  12\n#> df of  the model are 1  and the objective function was  5.6 \n#> \n#> The root mean square of the residuals (RMSR) is  0.01 \n#> The df corrected root mean square of the residuals is  0.04 \n#> \n#> Fit based upon off diagonal values = 1\n```\n:::\n\n\n\n\n\n- `fm = \"pa\"`: specifica il metodo dei fattori principali.\n- L'output include:\n  - saturazioni fattoriali;\n  - unicità ($1 - h^2$);\n  - varianza spiegata da ciascun fattore.\n\n⚠️ Se una **unicità > 1** o negativa → **soluzione impropria** (*caso di Heywood*).\n\n\n## Metodo di Massima Verosimiglianza (ML)\n\nQuesto è il metodo più coerente con l’analisi fattoriale teorica: **assume normalità multivariata** e consente **test formali di adattamento**.\n\n**Esecuzione con `factanal()`.**\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nml <- factanal(\n  covmat = R,\n  factors = 2,\n  rotation = \"none\",\n  n.obs = 225  # necessario per il test chi-quadro\n)\nml\n#> \n#> Call:\n#> factanal(factors = 2, covmat = R, n.obs = 225, rotation = \"none\")\n#> \n#> Uniquenesses:\n#>     K     I     H     L     J \n#> 0.005 0.268 0.055 0.008 0.005 \n#> \n#> Loadings:\n#>   Factor1 Factor2\n#> K  0.955  -0.289 \n#> I  0.528   0.673 \n#> H  0.720  -0.653 \n#> L  0.954  -0.287 \n#> J  0.764   0.642 \n#> \n#>                Factor1 Factor2\n#> SS loadings      3.203   1.457\n#> Proportion Var   0.641   0.291\n#> Cumulative Var   0.641   0.932\n#> \n#> Test of the hypothesis that 2 factors are sufficient.\n#> The chi square statistic is 648.1 on 1 degree of freedom.\n#> The p-value is 5.81e-143\n```\n:::\n\n\n\n\n\n- `loadings`: **saturazioni fattoriali**\n- `uniquenesses`: varianza specifica di ciascuna variabile\n- `test statistic`: **test chi-quadro** per valutare se i fattori estratti spiegano sufficientemente la correlazione tra le variabili\n\nIl **p-value** indica se il modello a 2 fattori è adeguato:  \n\n- p alto → il modello **spiega bene** i dati (non c’è differenza significativa con la matrice osservata).  \n- p basso → il modello è **insufficiente** (i residui sono troppo grandi).\n\nIn sintesi:\n\n| Metodo           | Obiettivo principale                  | Include specificità? | Test del modello? | Quando usarlo |\n|------------------|----------------------------------------|----------------------|-------------------|----------------|\n| **PCA**          | Ridurre dimensionalità                | ❌ No               | ❌ No             | Sintesi descrittiva |\n| **Fattori principali** | Isolare la varianza comune        | ✅ Sì               | ❌ No             | Analisi esplorativa |\n| **Fattori iterato**    | Raffinare le comunalità            | ✅ Sì               | ❌ No             | Soluzioni più stabili |\n| **ML**           | Testare modello fattoriale            | ✅ Sì               | ✅ Sì             | Verifica ipotesi psicologiche |\n\n🎓 *Suggerimento:*  \n\n- se lo scopo è **identificare costrutti teorici** (es. \"l’autostima ha due dimensioni?\"), si preferisce il metodo **ML**.  \n- se invece lo scopo è solo quello di **riassumendo dati** (es. da un questionario), anche la **PCA può andare bene**.\n\n## Riflessioni Conclusive\n\nL’analisi fattoriale non è una procedura automatica, ma un processo di modellizzazione che richiede **scelte motivate** e **valutazioni critiche** a ogni passaggio. I metodi di estrazione dei fattori, pur essendo matematicamente diversi, riflettono **concezioni differenti del ruolo della varianza** nelle variabili osservate e, quindi, **diverse filosofie di ricerca**.\n\nAd esempio, la **PCA** tende a trattare le variabili come manifestazioni dirette della varianza totale, rendendola utile per scopi pratici come la riduzione della dimensionalità, ma meno adatta per inferenze teoriche su costrutti latenti. Al contrario, i metodi che stimano la **varianza comune** (come i fattori principali o la massima verosimiglianza) assumono che ci siano **cause sottostanti e non osservabili** che generano le covarianze tra le variabili.\n\nMa oltre agli aspetti tecnici, è utile considerare alcune **domande chiave** che dovrebbero guidare la scelta del metodo:\n\n- Qual è il mio obiettivo? *Sintesi descrittiva*, *conferma di ipotesi teoriche*, *preparazione a un’analisi fattoriale confermativa*?\n- I dati rispettano le assunzioni richieste (normalità, dimensione del campione, struttura semplice)?\n- Quanto voglio spingermi nell’**interpretazione psicologica dei fattori**?  \n- Quanto è affidabile la mia **stima della varianza specifica o dell’errore di misura**?\n\nInoltre, è importante non trascurare che l’analisi fattoriale, per quanto potente, è **sensibile a molte scelte analitiche**: dal numero di fattori estratti, al metodo di rotazione, fino alle decisioni su quali variabili includere o escludere. Ogni decisione ha impatto sull’**interpretabilità**, sulla **stabilità** delle soluzioni e sulla **replicabilità** dei risultati.\n\n> *Un buon analista fattoriale non cerca solo di “ottenere carichi elevati”, ma si interroga su cosa quei carichi rappresentano, se sono coerenti con la teoria, e se possono essere replicati in un altro campione.*\n\nInfine, non dimentichiamo che l’analisi fattoriale **non esaurisce l’indagine sulla struttura latente** dei dati. È spesso il primo passo di un percorso più ampio che può includere:\n\n- **analisi fattoriale confermativa (CFA)**;\n- **modelli strutturali (SEM)**;\n- **analisi di validità di costrutto**.\n\nIn sintesi, padroneggiare i diversi metodi di estrazione non significa solo saperli applicare, ma anche **comprendere le implicazioni epistemologiche e psicometriche** delle scelte fatte. Il vero valore dell’analisi fattoriale non sta solo nella sintesi dei dati, ma nella sua capacità di **collegare numeri e teoria**, variabili osservate e costrutti latenti, **statistica e psicologia**.\n\n\n\n## Informazioni sull'Ambiente di Sviluppo {.unnumbered}\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.4.2 (2024-10-31)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.3.2\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] ggokabeito_0.1.0  see_0.11.0        MASS_7.3-65       viridis_0.6.5    \n#>  [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#>  [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#> [13] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#> [17] knitr_1.50        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#> [21] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#> [25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#> \n#> loaded via a namespace (and not attached):\n#>   [1] rstudioapi_0.17.1   jsonlite_1.9.1      magrittr_2.0.3     \n#>   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#>   [7] nloptr_2.2.1        rmarkdown_2.29      vctrs_0.6.5        \n#>  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#>  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#>  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#>  [19] emmeans_1.10.7      zoo_1.8-13          igraph_2.1.4       \n#>  [22] mime_0.13           lifecycle_1.0.4     pkgconfig_2.0.3    \n#>  [25] Matrix_1.7-3        R6_2.6.1            fastmap_1.2.0      \n#>  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#>  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#>  [34] rprojroot_2.0.4     Hmisc_5.2-3         timechange_0.3.0   \n#>  [37] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n#>  [40] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#>  [43] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n#>  [46] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#>  [49] foreign_0.8-88      zip_2.3.2           httpuv_1.6.15      \n#>  [52] nnet_7.3-20         glue_1.8.0          quadprog_1.5-8     \n#>  [55] nlme_3.1-167        promises_1.3.2      lisrelToR_0.3      \n#>  [58] grid_4.4.2          checkmate_2.3.2     cluster_2.1.8.1    \n#>  [61] reshape2_1.4.4      generics_0.1.3      gtable_0.3.6       \n#>  [64] tzdb_0.5.0          data.table_1.17.0   hms_1.1.3          \n#>  [67] car_3.1-3           sem_3.1-16          pillar_1.10.1      \n#>  [70] rockchalk_1.8.157   later_1.4.1         splines_4.4.2      \n#>  [73] lattice_0.22-6      survival_3.8-3      kutils_1.73        \n#>  [76] tidyselect_1.2.1    miniUI_0.1.1.1      pbapply_1.7-2      \n#>  [79] reformulas_0.4.0    stats4_4.4.2        xfun_0.51          \n#>  [82] qgraph_1.9.8        arm_1.14-4          stringi_1.8.4      \n#>  [85] pacman_0.5.1        boot_1.3-31         evaluate_1.0.3     \n#>  [88] codetools_0.2-20    mi_1.1              cli_3.6.4          \n#>  [91] RcppParallel_5.1.10 rpart_4.1.24        xtable_1.8-4       \n#>  [94] Rdpack_2.6.3        munsell_0.5.1       Rcpp_1.0.14        \n#>  [97] coda_0.19-4.1       png_0.1-8           XML_3.99-0.18      \n#> [100] parallel_4.4.2      jpeg_0.1-10         lme4_1.1-36        \n#> [103] mvtnorm_1.3-3       openxlsx_4.2.8      rlang_1.1.5        \n#> [106] multcomp_1.4-28     mnormt_2.1.1\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}