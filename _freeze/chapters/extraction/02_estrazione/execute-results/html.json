{
  "hash": "3414b3016564edfbaf7fe5abaedba0b0",
  "result": {
    "engine": "knitr",
    "markdown": "# L'estrazione dei fattori {#sec-extraction-factor-extraction}\n\n::: callout-important\n## In questo capitolo imparerai:\n\n- Come implementare il **metodo delle componenti principali** per l'estrazione delle saturazioni fattoriali utilizzando l'algebra matriciale.  \n- Come implementare il **metodo dei fattori principali** e comprenderne i fondamenti teorici.  \n- Il funzionamento del **metodo dei fattori principali iterato**, con un focus sul processo di convergenza.  \n- Le caratteristiche principali del **metodo di massima verosimiglianza** e il suo utilizzo nell'analisi fattoriale.  \n:::\n\n::: callout-tip\n## Prerequisiti\n\n- Leggere il capitolo *Factor Analysis and Principal Component Analysis* del testo di @petersen2024principles.\n:::\n\n::: callout-caution\n## Preparazione del Notebook\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(lavaan, psych)\n```\n:::\n\n\n\n:::\n\n## Introduzione\n\nL'analisi fattoriale è una tecnica statistica che semplifica un insieme complesso di variabili osservate identificando un numero ridotto di fattori latenti che spiegano le correlazioni tra queste variabili. L'obiettivo è individuare un insieme più contenuto di variabili non osservabili (fattori) che rappresentino le interrelazioni tra un ampio numero di variabili osservate.\n\nIl modello statistico dell'analisi fattoriale è rappresentato dalla seguente equazione:\n\n$$\n\\boldsymbol{\\Sigma} = \\boldsymbol{\\Lambda} \\boldsymbol{\\Phi} \\boldsymbol{\\Lambda}^{\\mathsf{T}} + \\boldsymbol{\\Psi},\n$$\n\ndove:\n\n- **$\\boldsymbol{\\Sigma}$** è la matrice delle covarianze tra le variabili osservate;\n- **$\\boldsymbol{\\Lambda}$** è la matrice dei carichi fattoriali, che rappresenta la relazione tra le variabili osservate e i fattori latenti;\n- **$\\boldsymbol{\\Phi}$** è la matrice delle correlazioni tra i fattori latenti;\n- **$\\boldsymbol{\\Psi}$** è una matrice diagonale contenente le unicità, ossia la varianza specifica di ciascuna variabile osservata non spiegata dai fattori comuni.\n\n### Estrazione dei Fattori\n\nL'estrazione dei fattori è il processo che consente di stimare la matrice dei carichi fattoriali $\\boldsymbol{\\Lambda}$. Esistono diversi metodi per eseguire questa stima, tra cui:\n\n1. **Metodo delle componenti principali**  \n   Massimizza la varianza spiegata dai fattori ma non considera esplicitamente l'errore di misura.\n\n2. **Metodo dei fattori principali**  \n   Variante del metodo delle componenti principali, tiene conto dell'errore di misura stimando inizialmente le comunalità.\n\n3. **Metodo dei fattori principali iterato**  \n   Versione iterativa del metodo dei fattori principali in cui le comunalità vengono stimate ripetutamente fino alla convergenza.\n\n4. **Metodo di massima verosimiglianza (ML)**  \n   Stima i parametri del modello assumendo che le variabili osservate seguano una distribuzione normale multivariata. Questo metodo è particolarmente utile per testare ipotesi sui fattori latenti e per confrontare modelli alternativi.\n\nCiascun metodo presenta vantaggi e limitazioni, e la scelta dipende dagli obiettivi dell'analisi e dalle caratteristiche del dataset.\n\n## Metodo delle Componenti Principali\n\nNonostante il nome, l'analisi fattoriale eseguita con il metodo delle componenti principali non coincide con un'analisi delle componenti principali. Quest'ultima è, invece, un'applicazione del teorema di scomposizione spettrale di una matrice.\n\nIl *teorema spettrale* stabilisce che, data una matrice simmetrica $\\textbf{S}_{p \\times p}$, è sempre possibile individuare una matrice ortogonale $\\textbf{C}_{p \\times p}$ tale che:\n\n$$\n\\textbf{S} = \\textbf{C}\\textbf{D}\\textbf{C}^{\\mathsf{T}} ,\n$$\n\ndove $\\textbf{D}$ è una matrice diagonale. Inoltre, il teorema specifica che gli elementi sulla diagonale di $\\textbf{D}$ sono gli autovalori della matrice $\\textbf{S}$, mentre le colonne di $\\textbf{C}$ rappresentano gli autovettori normalizzati associati a tali autovalori.\n\n### Esempio Numerico\n\nConsideriamo un esempio basato sui dati discussi da @rencher10methods. Una ragazza di 12 anni ha valutato sette persone a lei conosciute rispetto a cinque caratteristiche (*kind*, *intelligent*, *happy*, *likeable* e *just*), usando una scala a nove punti. La matrice di correlazione tra queste variabili è la seguente:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nR <- matrix(c(\n  1.000, .296, .881, .995, .545,\n  .296, 1.000, -.022, .326, .837,\n  .881, -.022, 1.000, .867, .130,\n  .995, .326, .867, 1.000, .544,\n  .545, .837, .130, .544, 1.000\n),\nncol = 5, byrow = TRUE, dimnames = list(\n  c(\"K\", \"I\", \"H\", \"L\", \"J\"),\n  c(\"K\", \"I\", \"H\", \"L\", \"J\")\n))\nR\n#>       K      I      H     L     J\n#> K 1.000  0.296  0.881 0.995 0.545\n#> I 0.296  1.000 -0.022 0.326 0.837\n#> H 0.881 -0.022  1.000 0.867 0.130\n#> L 0.995  0.326  0.867 1.000 0.544\n#> J 0.545  0.837  0.130 0.544 1.000\n```\n:::\n\n\n\n\nGli **autovalori** e gli **autovettori** di $\\textbf{R}$ si calcolano con la funzione `eigen()` in R:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ne <- eigen(R)\nprint(e)\n#> eigen() decomposition\n#> $values\n#> [1] 3.263377 1.538382 0.167969 0.030030 0.000242\n#> \n#> $vectors\n#>        [,1]   [,2]    [,3]   [,4]   [,5]\n#> [1,] -0.537 -0.186 -0.1899 -0.125  0.791\n#> [2,] -0.287  0.651  0.6849 -0.120  0.103\n#> [3,] -0.434 -0.474  0.4069  0.614 -0.212\n#> [4,] -0.537 -0.169 -0.0953 -0.629 -0.527\n#> [5,] -0.390  0.538 -0.5658  0.444 -0.204\n```\n:::\n\n\n\n\n#### Ricostruzione della Matrice\n\nCome indicato in precedenza, $\\textbf{R}$ può essere scomposta come:\n\n$$\n\\textbf{R} = \\textbf{C}\\textbf{D}\\textbf{C}^{\\mathsf{T}} .\n$$\n\nEseguendo i calcoli:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ne$vectors %*% diag(e$values) %*% t(e$vectors)\n#>       [,1]   [,2]   [,3]  [,4]  [,5]\n#> [1,] 1.000  0.296  0.881 0.995 0.545\n#> [2,] 0.296  1.000 -0.022 0.326 0.837\n#> [3,] 0.881 -0.022  1.000 0.867 0.130\n#> [4,] 0.995  0.326  0.867 1.000 0.544\n#> [5,] 0.545  0.837  0.130 0.544 1.000\n```\n:::\n\n\n\n\n#### Analisi degli Autovalori\n\nGli autovalori indicano la quantità di varianza spiegata da ciascuna componente. Nel nostro caso, i primi due autovalori spiegano il 96% della varianza totale:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(e$values[1] + e$values[2]) / 5\n#> [1] 0.96\n```\n:::\n\n\n\n\n#### Riduzione della Dimensionalità\n\nPer ridurre la dimensionalità dei dati, usiamo i primi due autovalori e i rispettivi autovettori. Questo ci permette di approssimare $\\textbf{R}$ mediante una matrice di rango ridotto:\n\n$$\n\\textbf{R} \\approx \\hat{\\boldsymbol{\\Lambda}} \\hat{\\boldsymbol{\\Lambda}}^{\\mathsf{T}} .\n$$\n\n#### Fattorizzazione\n\nScriviamo $\\textbf{D}$ come prodotto di due matrici $\\textbf{D}^{1/2}$:\n\n$$\n\\textbf{D} = \\textbf{D}^{1/2} \\textbf{D}^{1/2} ,\n$$\n\ncon $\\textbf{D}^{1/2}$ definita come:\n\n$$\n\\textbf{D}^{1/2} = \n\\begin{bmatrix}\n\\sqrt{\\theta_1} & 0 & \\dots & 0 \\\\\n0 & \\sqrt{\\theta_2} & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\sqrt{\\theta_p}\n\\end{bmatrix}\n$$\n\nDefiniamo ora $\\textbf{D}_1$ come la matrice diagonale contenente i primi $m$ autovalori di $\\textbf{R}$ e $\\textbf{C}_1$ come la matrice contenente i corrispondenti $m$ autovettori. Le **saturazioni fattoriali** stimate sono:\n\n$$\n\\hat{\\boldsymbol{\\Lambda}} = \\textbf{C}_1 \\textbf{D}_1^{1/2} .\n$$\n\nPer $m = 2$, calcoliamo $\\hat{\\boldsymbol{\\Lambda}}$:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nL <- cbind(\n  e$vectors[, 1] * sqrt(e$values[1]),\n  e$vectors[, 2] * sqrt(e$values[2])\n)\n\nround(L, 3)\n#>        [,1]   [,2]\n#> [1,] -0.970 -0.231\n#> [2,] -0.519  0.807\n#> [3,] -0.785 -0.588\n#> [4,] -0.971 -0.210\n#> [5,] -0.704  0.667\n```\n:::\n\n\n\n\n#### Matrice Riprodotta e Residuale\n\nLa matrice di correlazione approssimata è:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nR_hat <- round(L %*% t(L), 3)\nR_hat\n#>       [,1]   [,2]   [,3]  [,4]  [,5]\n#> [1,] 0.993  0.317  0.896 0.990 0.529\n#> [2,] 0.317  0.921 -0.067 0.335 0.904\n#> [3,] 0.896 -0.067  0.961 0.885 0.160\n#> [4,] 0.990  0.335  0.885 0.987 0.543\n#> [5,] 0.529  0.904  0.160 0.543 0.940\n```\n:::\n\n\n\n\nLa matrice residua, con le specificità sulla diagonale principale, è:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nR - R_hat\n#>        K      I      H      L      J\n#> K  0.007 -0.021 -0.015  0.005  0.016\n#> I -0.021  0.079  0.045 -0.009 -0.067\n#> H -0.015  0.045  0.039 -0.018 -0.030\n#> L  0.005 -0.009 -0.018  0.013  0.001\n#> J  0.016 -0.067 -0.030  0.001  0.060\n```\n:::\n\n\n\n\nIn conclusione, il nome *metodo delle componenti principali* deriva dal fatto che le saturazioni fattoriali sono proporzionali agli autovettori di $\\textbf{R}$. Tuttavia, l'interpretazione differisce da quella dell'analisi delle componenti principali.\n\nUn aspetto critico del metodo è la sua **non invarianza rispetto ai cambiamenti di scala**: le soluzioni ottenute con la matrice $\\textbf{S}$ di varianze-covarianze differiscono da quelle calcolate con la matrice $\\textbf{R}$ di correlazioni. Inoltre, il metodo non prevede un test di bontà di adattamento, disponibile invece con il metodo della massima verosimiglianza.\n\n## Metodo dei Fattori Principali\n\nIl *metodo dei fattori principali* (*Principal Factor Method* o *Principal Axis Method*) è uno dei metodi più comuni per stimare le **saturazioni fattoriali** e le **comunalità**. A differenza del metodo delle componenti principali, che trascura la specificità $\\boldsymbol{\\Psi}$ e si limita a fattorializzare direttamente la matrice delle covarianze $\\textbf{S}$ o delle correlazioni $\\textbf{R}$, il metodo dei fattori principali affronta questo limite introducendo una matrice **ridotta** di varianze-covarianze o correlazioni. \n\nQuesta matrice ridotta si ottiene sostituendo una **stima delle comunalità** alle varianze sulla diagonale principale di $\\textbf{S}$ o $\\textbf{R}$. Questo processo consente di isolare le comunalità dalle specificità e dall'errore.\n\n### Stima delle Comunalità\n\n#### Caso della Matrice di Correlazioni $\\textbf{R}$\n\nPer una matrice ridotta di correlazioni $\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}$, la comunalità $i$-esima $\\hat{h}^2_i$ può essere stimata come il quadrato del coefficiente di correlazione multipla tra la variabile $Y_i$ e le altre $p-1$ variabili. Questa stima si calcola come:\n\n$$\n\\hat{h}^2_i = R^2_i = 1 - \\frac{1}{r^{ii}} ,\n$$\n\ndove $r^{ii}$ è l'elemento diagonale $i$-esimo dell'inversa di $\\textbf{R}$.\n\n#### Caso della Matrice di Varianze-Covarianze $\\textbf{S}$\n\nSe si utilizza la matrice delle varianze-covarianze $\\textbf{S}$, la comunalità $i$-esima è stimata come:\n\n$$\n\\hat{h}^2_i = s_{ii} - \\frac{1}{r^{ii}} ,\n$$\n\ndove $s_{ii}$ è l'elemento diagonale $i$-esimo di $\\textbf{S}$.\n\n#### Gestione della Singolarità\n\nSe $\\textbf{R}$ è singolare, la comunalità $\\hat{h}^2_i$ può essere stimata come il valore assoluto del massimo coefficiente di correlazione lineare tra la variabile $Y_i$ e le altre variabili.\n\n### Matrice Ridotta\n\nSostituendo le stime delle comunalità nella diagonale principale di $\\textbf{S}$ o $\\textbf{R}$, otteniamo la matrice ridotta. Ad esempio, per la matrice delle varianze-covarianze:\n\n$$\n\\textbf{S} - \\hat{\\boldsymbol{\\Psi}} = \n\\begin{bmatrix}\n\\hat{h}^2_1 & s_{12} & \\dots & s_{1p} \\\\\ns_{21} & \\hat{h}^2_2 & \\dots & s_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ns_{p1} & s_{p2} & \\dots & \\hat{h}^2_p\n\\end{bmatrix} .\n$$\n\nAnalogamente, per la matrice delle correlazioni:\n\n$$\n\\textbf{R} - \\hat{\\boldsymbol{\\Psi}} = \n\\begin{bmatrix}\n\\hat{h}^2_1 & r_{12} & \\dots & r_{1p} \\\\\nr_{21} & \\hat{h}^2_2 & \\dots & r_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nr_{p1} & r_{p2} & \\dots & \\hat{h}^2_p\n\\end{bmatrix} .\n$$\n\n### Esempio Numerico\n\nUsiamo la matrice di correlazione dell'esempio precedente. Per stimare la comunalità $i$-esima, utilizziamo il valore massimo assoluto nella riga $i$-esima della matrice $\\textbf{R}$. Le stime delle comunalità sono:\n\n$$\n\\hat{h}^2 = \\{0.995, 0.837, 0.881, 0.995, 0.837\\} .\n$$\n\nSostituendo queste stime nella diagonale principale, otteniamo la matrice ridotta:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nR1 <- R\nh.hat <- c(.995, .837, .881, .995, .837)\nR1[cbind(1:5, 1:5)] <- h.hat\nR1\n#>       K      I      H     L     J\n#> K 0.995  0.296  0.881 0.995 0.545\n#> I 0.296  0.837 -0.022 0.326 0.837\n#> H 0.881 -0.022  0.881 0.867 0.130\n#> L 0.995  0.326  0.867 0.995 0.544\n#> J 0.545  0.837  0.130 0.544 0.837\n```\n:::\n\n\n\n\n#### Autovalori della Matrice Ridotta\n\nCalcoliamo gli autovalori della matrice ridotta:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nee <- eigen(R1)\nround(ee$values, 3)\n#> [1]  3.202  1.394  0.029  0.000 -0.080\n```\n:::\n\n\n\n\nLa somma degli autovalori è:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsum(ee$values)\n#> [1] 4.54\n```\n:::\n\n\n\n\n#### Stima delle Saturazioni Fattoriali\n\nI primi due autovalori e i rispettivi autovettori vengono usati per stimare le saturazioni fattoriali. Moltiplichiamo gli autovettori per la radice quadrata dei rispettivi autovalori:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nround(ee$vectors[, 1:2] %*% sqrt(diag(ee$values[1:2])), 3)\n#>       [,1]   [,2]\n#> [1,] 0.981 -0.209\n#> [2,] 0.487  0.774\n#> [3,] 0.772 -0.544\n#> [4,] 0.982 -0.187\n#> [5,] 0.667  0.648\n```\n:::\n\n\n\n\nIn conclusione, il metodo dei fattori principali consente di stimare le saturazioni fattoriali tenendo conto delle comunalità, a differenza del metodo delle componenti principali che trascura la specificità. Tuttavia, questo metodo richiede che $\\textbf{R}$ non sia singolare. Le soluzioni ottenute sono influenzate dalle scelte iniziali per le comunalità, ma risultano più interpretabili rispetto a quelle del metodo delle componenti principali, poiché cercano di separare la varianza comune dalla specificità e dall'errore.\n\n## Metodo dei Fattori Principali Iterato\n\nIl **metodo dei fattori principali iterato** migliora la stima delle comunalità attraverso un processo iterativo che aggiorna progressivamente la diagonale della matrice ridotta $\\textbf{S} - \\hat{\\boldsymbol{\\Psi}}$ o $\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}$. Questo approccio consente di ottenere stime più accurate delle comunalità e, di conseguenza, delle saturazioni fattoriali.\n\n### Procedura\n\n1. **Stima iniziale delle comunalità:**\n   Si parte con una stima iniziale delle comunalità $\\hat{h}^2_i$ per tutte le variabili. Queste stime iniziali possono derivare, ad esempio, dal massimo valore assoluto di correlazione per ciascuna variabile.\n\n2. **Costruzione della matrice ridotta:**\n   Sostituendo le comunalità iniziali nella diagonale principale, si ottiene la matrice ridotta $\\textbf{S} - \\hat{\\boldsymbol{\\Psi}}$ o $\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}$.\n\n3. **Stima delle saturazioni fattoriali:**\n   Dalla matrice ridotta, si calcolano gli **autovalori** e i corrispondenti **autovettori**, utilizzando i primi $m$ autovalori (e i relativi autovettori) per stimare le saturazioni fattoriali $\\hat{\\boldsymbol{\\Lambda}}$. Le saturazioni fattoriali per la variabile $i$ e il fattore $j$ si indicano con $\\hat{\\lambda}_{ij}$.\n\n4. **Aggiornamento delle comunalità:**\n   Le comunalità vengono ricalcolate come la somma dei quadrati delle saturazioni fattoriali per ciascuna variabile $i$:\n\n   $$\n   \\hat{h}^2_i = \\sum_{j=1}^m \\hat{\\lambda}_{ij}^2 .\n   $$\n\n5. **Sostituzione nella matrice ridotta:**\n   I nuovi valori di $\\hat{h}^2_i$ vengono sostituiti nella diagonale principale della matrice ridotta $\\textbf{S} - \\hat{\\boldsymbol{\\Psi}}$ o $\\textbf{R} - \\hat{\\boldsymbol{\\Psi}}$, e il processo viene ripetuto.\n\n6. **Iterazione fino alla convergenza:**\n   Questo ciclo iterativo continua finché i valori stimati delle comunalità $\\hat{h}^2_i$ non cambiano significativamente tra un'iterazione e l'altra, indicando la **convergenza**.\n\n7. **Stima finale dei pesi fattoriali:**\n   Una volta raggiunta la convergenza, gli **autovalori** e gli **autovettori** della matrice ridotta finale vengono utilizzati per stimare i pesi fattoriali definitivi.\n\n### Confronto con il Metodo delle Componenti Principali\n\n- **Similitudini:**  \n  Quando il numero di variabili $p$ è grande e le correlazioni tra le variabili sono alte, il metodo dei fattori principali iterato produce risultati simili a quelli del metodo delle componenti principali.\n\n- **Differenze:**  \n  Il metodo dei fattori principali iterato tiene conto delle **specificità** (diagonale $\\boldsymbol{\\Psi}$) e fornisce una stima più accurata delle comunalità. Al contrario, il metodo delle componenti principali assume implicitamente che tutta la varianza sia spiegata dai fattori comuni, trascurando la specificità.\n\n### Vantaggi del Metodo Iterato\n\n1. **Maggiore precisione:**  \n   L'iterazione migliora progressivamente le stime delle comunalità, riducendo l'impatto di errori iniziali.\n\n2. **Adattabilità:**  \n   Il metodo può essere applicato sia alla matrice delle covarianze $\\textbf{S}$ sia a quella delle correlazioni $\\textbf{R}$, rendendolo flessibile a diversi contesti analitici.\n\n3. **Convergenza:**  \n   Il processo iterativo garantisce che le stime finali siano ottimali rispetto al modello specificato.\n\nIn un'applicazione pratica, il metodo dei fattori principali iterato viene solitamente implementato utilizzando funzioni predefinite disponibili in software statistici come R o altre piattaforme dedicate all'analisi dei dati.\n\nIn conclusione, il metodo dei fattori principali iterato rappresenta un miglioramento rispetto ai metodi non iterativi, poiché tiene conto delle specificità e consente una stima più accurata delle comunalità e dei fattori. Pur essendo più complesso, è particolarmente utile in contesti in cui la precisione delle stime è essenziale.\n\n### Casi di Heywood\n\nUno degli inconvenienti del metodo dei fattori principali iterato è la possibilità di ottenere soluzioni **inammissibili**, note come **casi di Heywood**, che si verificano quando la matrice delle correlazioni $\\textbf{R}$ viene fattorizzata e alcune comunalità stimate risultano maggiori di 1. \n\nSe $\\hat{h}^2_i > 1$ per una comunalità stimata, significa che la specificità $\\hat{\\psi}_i$ è negativa, ossia:\n\n$$\n\\hat{\\psi}_i = s_{ii} - \\hat{h}^2_i < 0 .\n$$\n\nQuesto è logicamente inaccettabile, poiché una **varianza non può assumere valori negativi**. Un caso del genere indica un problema nella soluzione iterativa, spesso dovuto a:\n\n- **Inadeguatezza del modello fattoriale:** il numero di fattori scelto potrebbe non essere sufficiente a rappresentare correttamente i dati.\n- **Problemi di multicollinearità:** correlazioni molto alte tra le variabili possono complicare il processo di stima.\n- **Rumore nei dati:** errori di misura o dati imperfetti possono causare queste anomalie.\n\nQuando si verifica un caso di Heywood, il processo iterativo viene solitamente interrotto dal software, che segnala l'impossibilità di trovare una soluzione ammissibile.\n\nPer illustrare il problema, possiamo utilizzare la funzione `fa()` del pacchetto `psych` in R. Questa funzione implementa il metodo iterativo dei fattori principali.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Esecuzione del metodo dei fattori principali iterato\npa <- fa(R, nfactors = 2, rotate = \"none\", fm = \"pa\")\npa\n#> Factor Analysis using method =  pa\n#> Call: fa(r = R, nfactors = 2, rotate = \"none\", fm = \"pa\")\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>    PA1   PA2   h2     u2 com\n#> K 0.98 -0.21 1.01 -0.008 1.1\n#> I 0.48  0.74 0.77  0.230 1.7\n#> H 0.78 -0.56 0.92  0.085 1.8\n#> L 0.98 -0.19 0.99  0.010 1.1\n#> J 0.69  0.69 0.95  0.049 2.0\n#> \n#>                        PA1  PA2\n#> SS loadings           3.22 1.41\n#> Proportion Var        0.64 0.28\n#> Cumulative Var        0.64 0.93\n#> Proportion Explained  0.70 0.30\n#> Cumulative Proportion 0.70 1.00\n#> \n#> Mean item complexity =  1.5\n#> Test of the hypothesis that 2 factors are sufficient.\n#> \n#> df null model =  10  with the objective function =  12\n#> df of  the model are 1  and the objective function was  5.6 \n#> \n#> The root mean square of the residuals (RMSR) is  0.01 \n#> The df corrected root mean square of the residuals is  0.04 \n#> \n#> Fit based upon off diagonal values = 1\n```\n:::\n\n\n\n\nNel risultato ottenuto, è possibile osservare che alcune **unicità** (specificità, $\\hat{\\psi}_i$) sono negative. Questo è un chiaro segnale di una **soluzione impropria**, che si riconosce come un caso di Heywood.\n\n#### Come Gestire i Casi di Heywood\n\n1. **Aumentare il numero di fattori:**\n   Aggiungere un fattore al modello potrebbe aiutare a catturare una maggiore porzione della varianza totale, riducendo il rischio di comunalità superiori a 1.\n\n2. **Controllare la qualità dei dati:**\n   - Identificare ed eliminare eventuali errori di misura.\n   - Valutare la presenza di variabili con correlazioni eccessivamente alte.\n\n3. **Applicare regolarizzazioni:**\n   Alcuni metodi moderni di analisi fattoriale includono tecniche per evitare soluzioni improprie (ad esempio, vincolando le comunalità o utilizzando approcci bayesiani).\n\n4. **Interpretare con cautela:**\n   In presenza di un caso di Heywood, è importante non accettare automaticamente la soluzione proposta dal modello. Una revisione critica del numero di fattori e della metodologia è essenziale.\n\nIn conclusione, i casi di Heywood rappresentano una limitazione importante del metodo dei fattori principali iterato, specialmente quando applicato a dati complessi o modelli non adeguati. Identificarli e gestirli correttamente è fondamentale per garantire la validità delle conclusioni tratte dall'analisi.\n\n## Metodo di Massima Verosimiglianza\n\nIl metodo di **massima verosimiglianza** è particolarmente indicato quando si può ragionevolmente assumere che le variabili manifeste seguano una distribuzione normale multivariata. In tali condizioni, il metodo produce stime dei pesi fattoriali e delle specificità che sono quelle più verosimili date le correlazioni osservate. Questo metodo è spesso preferito rispetto ad altri, a patto che le sue ipotesi di base siano pienamente soddisfatte.\n\nIl metodo di massima verosimiglianza minimizza una funzione di discrepanza $F$, che misura la **distanza** tra la matrice di covarianze osservata $\\textbf{S}$ (o la matrice di correlazioni $\\textbf{R}$) e quella predetta dal modello $\\textbf{M}$. La funzione $F$ può essere espressa come:\n\n$$\nF(\\boldsymbol{\\Lambda}, \\boldsymbol{\\Psi}) = \\text{discrepanza tra } \\textbf{S} \\text{ e } \\textbf{M}.\n$$\n\nMinimizzando $F$, uguagliando a zero le derivate di $F$ rispetto ai parametri del modello $\\boldsymbol{\\Lambda}$ (pesi fattoriali) e $\\boldsymbol{\\Psi}$ (specificità), si ottengono le equazioni per le **stime di massima verosimiglianza**:\n\n$$\n\\hat{\\boldsymbol{\\Lambda}}, \\hat{\\boldsymbol{\\Psi}} .\n$$\n\nQueste equazioni non hanno una soluzione analitica diretta, quindi si ricorre a metodi numerici iterativi per trovare le stime dei parametri. Durante l'iterazione, si cerca di minimizzare la discrepanza tra la matrice osservata e quella predetta dal modello.\n\n### Caratteristiche del Metodo\n\n1. **Precisione delle stime:**  \n   Se le ipotesi sono rispettate, le stime di massima verosimiglianza sono asintoticamente efficienti, ossia hanno la minima varianza possibile tra gli stimatori.\n\n2. **Indipendenza dall'unità di misura:**  \n   La soluzione non dipende dall'unità di misura delle variabili manifeste. Questo significa che si ottiene la stessa soluzione analizzando la matrice di covarianze $\\textbf{S}$ o quella di correlazioni $\\textbf{R}$.\n\n3. **Test di bontà di adattamento:**  \n   Le stime di massima verosimiglianza consentono di eseguire un **test chi-quadrato** per valutare se la matrice predetta dal modello è coerente con quella osservata. Questo test offre una misura formale della bontà di adattamento del modello ai dati.\n\n4. **Limitazioni:**  \n   - **Problemi di convergenza:** In alcuni casi, il processo iterativo può non convergere, specialmente con dati problematici o ipotesi non rispettate.\n   - **Casi di Heywood:** Analogamente al metodo dei fattori principali iterato, possono verificarsi casi di Heywood in cui alcune comunalità stimate risultano maggiori di 1.\n\n### Applicazione Pratica\n\nPer calcolare le stime di massima verosimiglianza in R, è possibile utilizzare la funzione `factanal()`, che implementa questo metodo direttamente.\n\nConsideriamo i dati dell'esempio precedente e calcoliamo i parametri di massima verosimiglianza:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfactanal(\n    covmat = R,        # Matrice di correlazioni\n    factors = 2,       # Numero di fattori\n    rotation = \"none\", # Nessuna rotazione\n    n.obs = 225        # Numero di osservazioni\n)\n#> \n#> Call:\n#> factanal(factors = 2, covmat = R, n.obs = 225, rotation = \"none\")\n#> \n#> Uniquenesses:\n#>     K     I     H     L     J \n#> 0.005 0.268 0.055 0.008 0.005 \n#> \n#> Loadings:\n#>   Factor1 Factor2\n#> K  0.955  -0.289 \n#> I  0.528   0.673 \n#> H  0.720  -0.653 \n#> L  0.954  -0.287 \n#> J  0.764   0.642 \n#> \n#>                Factor1 Factor2\n#> SS loadings      3.203   1.457\n#> Proportion Var   0.641   0.291\n#> Cumulative Var   0.641   0.932\n#> \n#> Test of the hypothesis that 2 factors are sufficient.\n#> The chi square statistic is 648 on 1 degree of freedom.\n#> The p-value is 5.81e-143\n```\n:::\n\n\n\n\n1. **Pesi fattoriali:**  \n   I pesi fattoriali stimati $\\hat{\\boldsymbol{\\Lambda}}$ saranno molto simili a quelli ottenuti con il metodo dei fattori principali iterato.\n\n2. **Specificità:**  \n   Le specificità $\\hat{\\boldsymbol{\\Psi}}$ rifletteranno la porzione di varianza non spiegata dai fattori comuni.\n\n3. **Test di bontà di adattamento:**  \n   Il risultato include un test chi-quadrato che valuta la coerenza tra il modello e i dati osservati. Un valore di p elevato indica un buon adattamento del modello.\n\n### Confronto con Altri Metodi\n\nIl metodo di massima verosimiglianza si distingue per la sua capacità di fornire stime ottimali e una valutazione formale dell'adattamento del modello. Tuttavia, richiede che le variabili manifeste seguano una distribuzione normale multivariata, il che potrebbe non essere sempre soddisfatto nei dati reali. Quando le ipotesi di normalità sono violate, metodi alternativi come la stima mediante minimi quadrati ponderati (WLS) potrebbero essere più appropriati.\n\nIn conclusione, il metodo di massima verosimiglianza rappresenta uno standard per l'analisi fattoriale, offrendo stime efficienti e strumenti per valutare l'adattamento del modello ai dati. Tuttavia, l'efficacia del metodo dipende dalla validità delle ipotesi di normalità e dalla qualità dei dati utilizzati.\n\n## Riflessioni Conclusive\n\nL’analisi fattoriale rappresenta uno strumento fondamentale per la riduzione della dimensionalità e l’identificazione di strutture latenti nei dati. Ogni metodo analizzato, dal metodo delle componenti principali al metodo di massima verosimiglianza, offre vantaggi e limitazioni, risultando più o meno adatto a seconda delle caratteristiche del dataset e degli obiettivi dell’analisi. \n\nI metodi iterativi, come il metodo dei fattori principali iterato e quello di massima verosimiglianza, si distinguono per la loro capacità di fornire stime più precise, sebbene possano incontrare problemi di convergenza o casi di Heywood. La scelta del metodo più appropriato dipende quindi non solo dai vincoli teorici, come la normalità multivariata, ma anche dalla complessità dei dati e dalla necessità di strumenti formali per testare l’adattamento del modello.\n\n## Informazioni sull'Ambiente di Sviluppo {.unnumbered}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.4.2 (2024-10-31)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.3.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] ggokabeito_0.1.0  see_0.10.0        MASS_7.3-64       viridis_0.6.5    \n#>  [5] viridisLite_0.4.2 ggpubr_0.6.0      ggExtra_0.10.1    gridExtra_2.3    \n#>  [9] patchwork_1.3.0   bayesplot_1.11.1  semTools_0.5-6    semPlot_1.1.6    \n#> [13] lavaan_0.6-19     psych_2.4.12      scales_1.3.0      markdown_1.13    \n#> [17] knitr_1.49        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n#> [21] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n#> [25] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   here_1.0.1       \n#> \n#> loaded via a namespace (and not attached):\n#>   [1] rstudioapi_0.17.1   jsonlite_1.8.9      magrittr_2.0.3     \n#>   [4] TH.data_1.1-3       estimability_1.5.1  farver_2.1.2       \n#>   [7] nloptr_2.1.1        rmarkdown_2.29      vctrs_0.6.5        \n#>  [10] minqa_1.2.8         base64enc_0.1-3     rstatix_0.7.2      \n#>  [13] htmltools_0.5.8.1   broom_1.0.7         Formula_1.2-5      \n#>  [16] htmlwidgets_1.6.4   plyr_1.8.9          sandwich_3.1-1     \n#>  [19] emmeans_1.10.7      zoo_1.8-12          igraph_2.1.4       \n#>  [22] mime_0.12           lifecycle_1.0.4     pkgconfig_2.0.3    \n#>  [25] Matrix_1.7-2        R6_2.6.1            fastmap_1.2.0      \n#>  [28] rbibutils_2.3       shiny_1.10.0        digest_0.6.37      \n#>  [31] OpenMx_2.21.13      fdrtool_1.2.18      colorspace_2.1-1   \n#>  [34] rprojroot_2.0.4     Hmisc_5.2-2         timechange_0.3.0   \n#>  [37] abind_1.4-8         compiler_4.4.2      withr_3.0.2        \n#>  [40] glasso_1.11         htmlTable_2.4.3     backports_1.5.0    \n#>  [43] carData_3.0-5       ggsignif_0.6.4      corpcor_1.6.10     \n#>  [46] gtools_3.9.5        tools_4.4.2         pbivnorm_0.6.0     \n#>  [49] foreign_0.8-88      zip_2.3.2           httpuv_1.6.15      \n#>  [52] nnet_7.3-20         glue_1.8.0          quadprog_1.5-8     \n#>  [55] nlme_3.1-167        promises_1.3.2      lisrelToR_0.3      \n#>  [58] grid_4.4.2          checkmate_2.3.2     cluster_2.1.8      \n#>  [61] reshape2_1.4.4      generics_0.1.3      gtable_0.3.6       \n#>  [64] tzdb_0.4.0          data.table_1.16.4   hms_1.1.3          \n#>  [67] car_3.1-3           sem_3.1-16          pillar_1.10.1      \n#>  [70] rockchalk_1.8.157   later_1.4.1         splines_4.4.2      \n#>  [73] lattice_0.22-6      survival_3.8-3      kutils_1.73        \n#>  [76] tidyselect_1.2.1    miniUI_0.1.1.1      pbapply_1.7-2      \n#>  [79] reformulas_0.4.0    stats4_4.4.2        xfun_0.50          \n#>  [82] qgraph_1.9.8        arm_1.14-4          stringi_1.8.4      \n#>  [85] pacman_0.5.1        boot_1.3-31         evaluate_1.0.3     \n#>  [88] codetools_0.2-20    mi_1.1              cli_3.6.4          \n#>  [91] RcppParallel_5.1.10 rpart_4.1.24        xtable_1.8-4       \n#>  [94] Rdpack_2.6.2        munsell_0.5.1       Rcpp_1.0.14        \n#>  [97] coda_0.19-4.1       png_0.1-8           XML_3.99-0.18      \n#> [100] parallel_4.4.2      jpeg_0.1-10         lme4_1.1-36        \n#> [103] mvtnorm_1.3-3       openxlsx_4.2.8      rlang_1.1.5        \n#> [106] multcomp_1.4-28     mnormt_2.1.1\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}